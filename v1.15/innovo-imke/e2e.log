I0731 11:06:57.191052      15 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-280300738
I0731 11:06:57.191341      15 e2e.go:241] Starting e2e run "358ed29b-781c-4b5d-9a2f-1b3e8ee2904b" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1564571215 - Will randomize all specs
Will run 215 of 4411 specs

Jul 31 11:06:57.483: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 11:06:57.486: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 31 11:06:57.528: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 31 11:06:58.019: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 31 11:06:58.019: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Jul 31 11:06:58.019: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 31 11:06:58.034: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Jul 31 11:06:58.034: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'container-linux-update-agent' (0 seconds elapsed)
Jul 31 11:06:58.034: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul 31 11:06:58.034: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Jul 31 11:06:58.034: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Jul 31 11:06:58.034: INFO: e2e test version: v1.15.0
Jul 31 11:06:58.119: INFO: kube-apiserver version: v1.15.0
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:06:58.119: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename subpath
Jul 31 11:06:58.414: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-ccdw
STEP: Creating a pod to test atomic-volume-subpath
Jul 31 11:06:58.465: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-ccdw" in namespace "subpath-4874" to be "success or failure"
Jul 31 11:06:59.011: INFO: Pod "pod-subpath-test-secret-ccdw": Phase="Pending", Reason="", readiness=false. Elapsed: 545.867162ms
Jul 31 11:07:01.548: INFO: Pod "pod-subpath-test-secret-ccdw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.083311976s
Jul 31 11:07:03.711: INFO: Pod "pod-subpath-test-secret-ccdw": Phase="Pending", Reason="", readiness=false. Elapsed: 5.245655229s
Jul 31 11:07:05.718: INFO: Pod "pod-subpath-test-secret-ccdw": Phase="Pending", Reason="", readiness=false. Elapsed: 7.252833791s
Jul 31 11:07:08.011: INFO: Pod "pod-subpath-test-secret-ccdw": Phase="Running", Reason="", readiness=true. Elapsed: 9.545579691s
Jul 31 11:07:10.020: INFO: Pod "pod-subpath-test-secret-ccdw": Phase="Running", Reason="", readiness=true. Elapsed: 11.555408005s
Jul 31 11:07:12.410: INFO: Pod "pod-subpath-test-secret-ccdw": Phase="Running", Reason="", readiness=true. Elapsed: 13.945347636s
Jul 31 11:07:14.423: INFO: Pod "pod-subpath-test-secret-ccdw": Phase="Running", Reason="", readiness=true. Elapsed: 15.958197703s
Jul 31 11:07:16.616: INFO: Pod "pod-subpath-test-secret-ccdw": Phase="Running", Reason="", readiness=true. Elapsed: 18.150499986s
Jul 31 11:07:18.811: INFO: Pod "pod-subpath-test-secret-ccdw": Phase="Running", Reason="", readiness=true. Elapsed: 20.345790804s
Jul 31 11:07:21.010: INFO: Pod "pod-subpath-test-secret-ccdw": Phase="Running", Reason="", readiness=true. Elapsed: 22.54547503s
Jul 31 11:07:23.327: INFO: Pod "pod-subpath-test-secret-ccdw": Phase="Running", Reason="", readiness=true. Elapsed: 24.86246213s
Jul 31 11:07:26.511: INFO: Pod "pod-subpath-test-secret-ccdw": Phase="Running", Reason="", readiness=true. Elapsed: 28.045814916s
Jul 31 11:07:28.724: INFO: Pod "pod-subpath-test-secret-ccdw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.258598808s
STEP: Saw pod success
Jul 31 11:07:28.724: INFO: Pod "pod-subpath-test-secret-ccdw" satisfied condition "success or failure"
Jul 31 11:07:28.732: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-subpath-test-secret-ccdw container test-container-subpath-secret-ccdw: <nil>
STEP: delete the pod
Jul 31 11:07:30.313: INFO: Waiting for pod pod-subpath-test-secret-ccdw to disappear
Jul 31 11:07:30.420: INFO: Pod pod-subpath-test-secret-ccdw no longer exists
STEP: Deleting pod pod-subpath-test-secret-ccdw
Jul 31 11:07:30.420: INFO: Deleting pod "pod-subpath-test-secret-ccdw" in namespace "subpath-4874"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:07:30.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4874" for this suite.
Jul 31 11:07:38.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:07:42.619: INFO: namespace subpath-4874 deletion completed in 11.305866762s

• [SLOW TEST:44.499 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:07:42.619: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Jul 31 11:07:43.140: INFO: Waiting up to 5m0s for pod "var-expansion-d2e8b481-95c1-4521-b6d2-493685d4fc94" in namespace "var-expansion-496" to be "success or failure"
Jul 31 11:07:43.152: INFO: Pod "var-expansion-d2e8b481-95c1-4521-b6d2-493685d4fc94": Phase="Pending", Reason="", readiness=false. Elapsed: 11.870016ms
Jul 31 11:07:45.315: INFO: Pod "var-expansion-d2e8b481-95c1-4521-b6d2-493685d4fc94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.175022292s
Jul 31 11:07:47.334: INFO: Pod "var-expansion-d2e8b481-95c1-4521-b6d2-493685d4fc94": Phase="Pending", Reason="", readiness=false. Elapsed: 4.194338784s
Jul 31 11:07:49.416: INFO: Pod "var-expansion-d2e8b481-95c1-4521-b6d2-493685d4fc94": Phase="Pending", Reason="", readiness=false. Elapsed: 6.276158334s
Jul 31 11:07:51.711: INFO: Pod "var-expansion-d2e8b481-95c1-4521-b6d2-493685d4fc94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.571052149s
STEP: Saw pod success
Jul 31 11:07:51.711: INFO: Pod "var-expansion-d2e8b481-95c1-4521-b6d2-493685d4fc94" satisfied condition "success or failure"
Jul 31 11:07:52.111: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod var-expansion-d2e8b481-95c1-4521-b6d2-493685d4fc94 container dapi-container: <nil>
STEP: delete the pod
Jul 31 11:07:53.190: INFO: Waiting for pod var-expansion-d2e8b481-95c1-4521-b6d2-493685d4fc94 to disappear
Jul 31 11:07:53.196: INFO: Pod var-expansion-d2e8b481-95c1-4521-b6d2-493685d4fc94 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:07:53.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-496" for this suite.
Jul 31 11:08:00.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:08:06.300: INFO: namespace var-expansion-496 deletion completed in 13.091179458s

• [SLOW TEST:23.681 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:08:06.304: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6128
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6128
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6128
Jul 31 11:08:06.951: INFO: Found 0 stateful pods, waiting for 1
Jul 31 11:08:16.977: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 31 11:08:16.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 31 11:08:25.015: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 31 11:08:25.015: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 31 11:08:25.015: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 31 11:08:25.411: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 31 11:08:35.726: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 11:08:35.726: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 11:08:35.762: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999731s
Jul 31 11:08:36.816: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.989360149s
Jul 31 11:08:38.613: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.93506959s
Jul 31 11:08:39.816: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.137907275s
Jul 31 11:08:41.210: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.935290963s
Jul 31 11:08:42.219: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.540424374s
Jul 31 11:08:43.514: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.508547165s
Jul 31 11:08:44.616: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.237146405s
Jul 31 11:08:46.111: INFO: Verifying statefulset ss doesn't scale past 1 for another 134.949468ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6128
Jul 31 11:08:47.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:08:50.823: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 31 11:08:50.823: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 31 11:08:50.823: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 31 11:08:51.220: INFO: Found 1 stateful pods, waiting for 3
Jul 31 11:09:01.612: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:09:01.612: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:09:01.612: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 31 11:09:11.411: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:09:11.411: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:09:11.411: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 31 11:09:21.711: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:09:21.711: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:09:21.711: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 31 11:09:21.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 31 11:09:25.832: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 31 11:09:25.833: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 31 11:09:25.833: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 31 11:09:25.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 31 11:09:30.217: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 31 11:09:30.217: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 31 11:09:30.217: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 31 11:09:30.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 31 11:09:34.712: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 31 11:09:34.712: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 31 11:09:34.712: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 31 11:09:34.712: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 11:09:34.722: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul 31 11:09:45.034: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 11:09:45.034: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 11:09:45.034: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 11:09:45.511: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999951s
Jul 31 11:09:46.523: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.799650643s
Jul 31 11:09:48.011: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.787118611s
Jul 31 11:09:49.711: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.29889295s
Jul 31 11:09:51.012: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.592379225s
Jul 31 11:09:52.117: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.297265737s
Jul 31 11:09:53.311: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.19362858s
Jul 31 11:09:54.512: INFO: Verifying statefulset ss doesn't scale past 3 for another 998.090054ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6128
Jul 31 11:09:56.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:10:00.412: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 31 11:10:00.412: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 31 11:10:00.412: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 31 11:10:00.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:10:05.516: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 31 11:10:05.516: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 31 11:10:05.516: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 31 11:10:05.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:10:10.315: INFO: rc: 1
Jul 31 11:10:10.315: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc00289baa0 exit status 1 <nil> <nil> true [0xc0022d2a10 0xc0022d2a98 0xc0022d2ad8] [0xc0022d2a10 0xc0022d2a98 0xc0022d2ad8] [0xc0022d2a70 0xc0022d2ad0] [0x9d17b0 0x9d17b0] 0xc002995ec0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Jul 31 11:10:20.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:10:22.315: INFO: rc: 1
Jul 31 11:10:22.315: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc00289be30 exit status 1 <nil> <nil> true [0xc0022d2b50 0xc0022d2ba8 0xc0022d2c38] [0xc0022d2b50 0xc0022d2ba8 0xc0022d2c38] [0xc0022d2b88 0xc0022d2bf0] [0x9d17b0 0x9d17b0] 0xc002c029c0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Jul 31 11:10:32.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:10:32.744: INFO: rc: 1
Jul 31 11:10:32.744: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00221a240 exit status 1 <nil> <nil> true [0xc0022d2c40 0xc0022d2c58 0xc0022d2c90] [0xc0022d2c40 0xc0022d2c58 0xc0022d2c90] [0xc0022d2c50 0xc0022d2c70] [0x9d17b0 0x9d17b0] 0xc002c03560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:10:42.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:10:43.429: INFO: rc: 1
Jul 31 11:10:43.429: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00221a5d0 exit status 1 <nil> <nil> true [0xc0022d2ca8 0xc0022d2d48 0xc0022d2d88] [0xc0022d2ca8 0xc0022d2d48 0xc0022d2d88] [0xc0022d2cf0 0xc0022d2d80] [0x9d17b0 0x9d17b0] 0xc00199e000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:10:53.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:10:53.650: INFO: rc: 1
Jul 31 11:10:53.650: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00221a930 exit status 1 <nil> <nil> true [0xc0022d2dc0 0xc0022d2e20 0xc0022d2eb8] [0xc0022d2dc0 0xc0022d2e20 0xc0022d2eb8] [0xc0022d2e18 0xc0022d2e88] [0x9d17b0 0x9d17b0] 0xc00199e600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:11:03.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:11:03.840: INFO: rc: 1
Jul 31 11:11:03.840: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00289a360 exit status 1 <nil> <nil> true [0xc0022d2058 0xc0022d2100 0xc0022d21c0] [0xc0022d2058 0xc0022d2100 0xc0022d21c0] [0xc0022d20d0 0xc0022d2180] [0x9d17b0 0x9d17b0] 0xc002c02900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:11:13.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:11:14.016: INFO: rc: 1
Jul 31 11:11:14.016: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00289a6f0 exit status 1 <nil> <nil> true [0xc0022d21d8 0xc0022d2298 0xc0022d22f8] [0xc0022d21d8 0xc0022d2298 0xc0022d22f8] [0xc0022d2290 0xc0022d22b8] [0x9d17b0 0x9d17b0] 0xc002c03440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:11:24.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:11:24.122: INFO: rc: 1
Jul 31 11:11:24.122: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00289aae0 exit status 1 <nil> <nil> true [0xc0022d2348 0xc0022d2368 0xc0022d2428] [0xc0022d2348 0xc0022d2368 0xc0022d2428] [0xc0022d2358 0xc0022d2408] [0x9d17b0 0x9d17b0] 0xc002c03f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:11:34.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:11:35.313: INFO: rc: 1
Jul 31 11:11:35.313: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002350330 exit status 1 <nil> <nil> true [0xc0029b2010 0xc0029b2050 0xc0029b2068] [0xc0029b2010 0xc0029b2050 0xc0029b2068] [0xc0029b2048 0xc0029b2060] [0x9d17b0 0x9d17b0] 0xc002842f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:11:45.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:11:46.426: INFO: rc: 1
Jul 31 11:11:46.426: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00289b3b0 exit status 1 <nil> <nil> true [0xc0022d2430 0xc0022d24c0 0xc0022d2500] [0xc0022d2430 0xc0022d24c0 0xc0022d2500] [0xc0022d2478 0xc0022d24f8] [0x9d17b0 0x9d17b0] 0xc002994f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:11:56.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:11:57.344: INFO: rc: 1
Jul 31 11:11:57.360: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00289b770 exit status 1 <nil> <nil> true [0xc0022d2550 0xc0022d25d0 0xc0022d26d0] [0xc0022d2550 0xc0022d25d0 0xc0022d26d0] [0xc0022d25c0 0xc0022d2678] [0x9d17b0 0x9d17b0] 0xc002995aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:12:07.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:12:07.830: INFO: rc: 1
Jul 31 11:12:07.830: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023506f0 exit status 1 <nil> <nil> true [0xc0029b2070 0xc0029b20b0 0xc0029b20f8] [0xc0029b2070 0xc0029b20b0 0xc0029b20f8] [0xc0029b20a0 0xc0029b20d8] [0x9d17b0 0x9d17b0] 0xc00271c120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:12:17.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:12:18.838: INFO: rc: 1
Jul 31 11:12:18.838: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002350ab0 exit status 1 <nil> <nil> true [0xc0029b2118 0xc0029b2140 0xc0029b2168] [0xc0029b2118 0xc0029b2140 0xc0029b2168] [0xc0029b2138 0xc0029b2150] [0x9d17b0 0x9d17b0] 0xc00271d980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:12:28.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:12:28.963: INFO: rc: 1
Jul 31 11:12:28.963: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002350ea0 exit status 1 <nil> <nil> true [0xc0029b2180 0xc0029b2198 0xc0029b21b0] [0xc0029b2180 0xc0029b2198 0xc0029b21b0] [0xc0029b2190 0xc0029b21a8] [0x9d17b0 0x9d17b0] 0xc002004fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:12:38.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:12:39.514: INFO: rc: 1
Jul 31 11:12:39.514: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002351290 exit status 1 <nil> <nil> true [0xc0029b21b8 0xc0029b21e8 0xc0029b2228] [0xc0029b21b8 0xc0029b21e8 0xc0029b2228] [0xc0029b21c8 0xc0029b2220] [0x9d17b0 0x9d17b0] 0xc00203aa20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:12:49.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:12:50.917: INFO: rc: 1
Jul 31 11:12:50.918: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002351680 exit status 1 <nil> <nil> true [0xc0029b2230 0xc0029b2248 0xc0029b2290] [0xc0029b2230 0xc0029b2248 0xc0029b2290] [0xc0029b2240 0xc0029b2278] [0x9d17b0 0x9d17b0] 0xc0017a0360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:13:00.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:13:01.070: INFO: rc: 1
Jul 31 11:13:01.070: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023516e0 exit status 1 <nil> <nil> true [0xc0022d26e8 0xc0022d2730 0xc0022d2780] [0xc0022d26e8 0xc0022d2730 0xc0022d2780] [0xc0022d26f8 0xc0022d2760] [0x9d17b0 0x9d17b0] 0xc0017a0900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:13:11.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:13:11.621: INFO: rc: 1
Jul 31 11:13:11.621: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00289a390 exit status 1 <nil> <nil> true [0xc0022d2038 0xc0022d20d0 0xc0022d2180] [0xc0022d2038 0xc0022d20d0 0xc0022d2180] [0xc0022d2080 0xc0022d2120] [0x9d17b0 0x9d17b0] 0xc00203b5c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:13:21.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:13:21.913: INFO: rc: 1
Jul 31 11:13:21.913: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002350390 exit status 1 <nil> <nil> true [0xc0029b2030 0xc0029b2058 0xc0029b2070] [0xc0029b2030 0xc0029b2058 0xc0029b2070] [0xc0029b2050 0xc0029b2068] [0x9d17b0 0x9d17b0] 0xc0020052c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:13:31.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:13:34.715: INFO: rc: 1
Jul 31 11:13:34.715: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00289a750 exit status 1 <nil> <nil> true [0xc0022d21c0 0xc0022d2290 0xc0022d22b8] [0xc0022d21c0 0xc0022d2290 0xc0022d22b8] [0xc0022d2238 0xc0022d22a8] [0x9d17b0 0x9d17b0] 0xc00271cea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:13:44.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:13:45.812: INFO: rc: 1
Jul 31 11:13:45.812: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00289ab70 exit status 1 <nil> <nil> true [0xc0022d22f8 0xc0022d2358 0xc0022d2408] [0xc0022d22f8 0xc0022d2358 0xc0022d2408] [0xc0022d2350 0xc0022d23b8] [0x9d17b0 0x9d17b0] 0xc002842540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:13:55.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:13:57.112: INFO: rc: 1
Jul 31 11:13:57.112: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00289b470 exit status 1 <nil> <nil> true [0xc0022d2428 0xc0022d2478 0xc0022d24f8] [0xc0022d2428 0xc0022d2478 0xc0022d24f8] [0xc0022d2438 0xc0022d24e8] [0x9d17b0 0x9d17b0] 0xc0028439e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:14:07.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:14:07.412: INFO: rc: 1
Jul 31 11:14:07.412: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002350750 exit status 1 <nil> <nil> true [0xc0029b2088 0xc0029b20b8 0xc0029b2118] [0xc0029b2088 0xc0029b20b8 0xc0029b2118] [0xc0029b20b0 0xc0029b20f8] [0x9d17b0 0x9d17b0] 0xc0029949c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:14:17.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:14:18.715: INFO: rc: 1
Jul 31 11:14:18.715: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00289b890 exit status 1 <nil> <nil> true [0xc0022d2500 0xc0022d25c0 0xc0022d2678] [0xc0022d2500 0xc0022d25c0 0xc0022d2678] [0xc0022d25b0 0xc0022d25f0] [0x9d17b0 0x9d17b0] 0xc002c024e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:14:28.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:14:29.422: INFO: rc: 1
Jul 31 11:14:29.422: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00289bc50 exit status 1 <nil> <nil> true [0xc0022d26d0 0xc0022d2790 0xc0022d2828] [0xc0022d26d0 0xc0022d2790 0xc0022d2828] [0xc0022d2750 0xc0022d27e0] [0x9d17b0 0x9d17b0] 0xc002c03140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:14:39.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:14:40.531: INFO: rc: 1
Jul 31 11:14:40.531: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001eae180 exit status 1 <nil> <nil> true [0xc0022d2878 0xc0022d28d0 0xc0022d2928] [0xc0022d2878 0xc0022d28d0 0xc0022d2928] [0xc0022d28a0 0xc0022d2920] [0x9d17b0 0x9d17b0] 0xc002c03bc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:14:50.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:14:50.691: INFO: rc: 1
Jul 31 11:14:50.692: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002350b10 exit status 1 <nil> <nil> true [0xc0029b2130 0xc0029b2148 0xc0029b2180] [0xc0029b2130 0xc0029b2148 0xc0029b2180] [0xc0029b2140 0xc0029b2168] [0x9d17b0 0x9d17b0] 0xc002995560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:15:00.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:15:02.415: INFO: rc: 1
Jul 31 11:15:02.416: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002350f60 exit status 1 <nil> <nil> true [0xc0029b2188 0xc0029b21a0 0xc0029b21b8] [0xc0029b2188 0xc0029b21a0 0xc0029b21b8] [0xc0029b2198 0xc0029b21b0] [0x9d17b0 0x9d17b0] 0xc0017a0600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul 31 11:15:12.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-6128 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 11:15:13.816: INFO: rc: 1
Jul 31 11:15:13.816: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Jul 31 11:15:13.816: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 31 11:15:16.221: INFO: Deleting all statefulset in ns statefulset-6128
Jul 31 11:15:16.231: INFO: Scaling statefulset ss to 0
Jul 31 11:15:16.425: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 11:15:16.432: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:15:16.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6128" for this suite.
Jul 31 11:15:22.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:15:33.222: INFO: namespace statefulset-6128 deletion completed in 16.742887252s

• [SLOW TEST:446.919 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:15:33.239: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 31 11:15:48.529: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:15:48.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0731 11:15:48.529514      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-7629" for this suite.
Jul 31 11:15:57.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:16:03.045: INFO: namespace gc-7629 deletion completed in 14.502711109s

• [SLOW TEST:29.806 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:16:03.046: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4605.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4605.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4605.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4605.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 11:16:25.010: INFO: DNS probes using dns-test-e36efe99-2133-4a73-82ed-fe1ceb35c63c succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4605.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4605.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4605.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4605.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 11:16:33.910: INFO: DNS probes using dns-test-c6fffbd3-aa84-4fce-9a67-11b698a7b164 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4605.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4605.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4605.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4605.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 11:16:46.710: INFO: DNS probes using dns-test-e9c30655-6558-48da-9530-e3476cdf7fbd succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:16:47.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4605" for this suite.
Jul 31 11:16:55.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:17:00.935: INFO: namespace dns-4605 deletion completed in 12.623936741s

• [SLOW TEST:57.890 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:17:00.936: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 11:17:01.836: INFO: Waiting up to 5m0s for pod "downwardapi-volume-865b14da-e2b3-487a-81b4-961e33c9edc8" in namespace "downward-api-8817" to be "success or failure"
Jul 31 11:17:01.845: INFO: Pod "downwardapi-volume-865b14da-e2b3-487a-81b4-961e33c9edc8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.718665ms
Jul 31 11:17:03.853: INFO: Pod "downwardapi-volume-865b14da-e2b3-487a-81b4-961e33c9edc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017813754s
Jul 31 11:17:05.924: INFO: Pod "downwardapi-volume-865b14da-e2b3-487a-81b4-961e33c9edc8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.088704234s
Jul 31 11:17:09.110: INFO: Pod "downwardapi-volume-865b14da-e2b3-487a-81b4-961e33c9edc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.274757229s
STEP: Saw pod success
Jul 31 11:17:09.110: INFO: Pod "downwardapi-volume-865b14da-e2b3-487a-81b4-961e33c9edc8" satisfied condition "success or failure"
Jul 31 11:17:09.126: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downwardapi-volume-865b14da-e2b3-487a-81b4-961e33c9edc8 container client-container: <nil>
STEP: delete the pod
Jul 31 11:17:11.835: INFO: Waiting for pod downwardapi-volume-865b14da-e2b3-487a-81b4-961e33c9edc8 to disappear
Jul 31 11:17:11.841: INFO: Pod downwardapi-volume-865b14da-e2b3-487a-81b4-961e33c9edc8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:17:11.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8817" for this suite.
Jul 31 11:17:18.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:17:21.110: INFO: namespace downward-api-8817 deletion completed in 8.799395886s

• [SLOW TEST:20.174 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:17:21.112: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 31 11:17:22.644: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1399,SelfLink:/api/v1/namespaces/watch-1399/configmaps/e2e-watch-test-watch-closed,UID:b1ec3fc6-acd9-4078-b722-ac34044a3494,ResourceVersion:1082950,Generation:0,CreationTimestamp:2019-07-31 11:17:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 31 11:17:22.644: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1399,SelfLink:/api/v1/namespaces/watch-1399/configmaps/e2e-watch-test-watch-closed,UID:b1ec3fc6-acd9-4078-b722-ac34044a3494,ResourceVersion:1082951,Generation:0,CreationTimestamp:2019-07-31 11:17:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 31 11:17:24.426: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1399,SelfLink:/api/v1/namespaces/watch-1399/configmaps/e2e-watch-test-watch-closed,UID:b1ec3fc6-acd9-4078-b722-ac34044a3494,ResourceVersion:1082952,Generation:0,CreationTimestamp:2019-07-31 11:17:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 31 11:17:24.427: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1399,SelfLink:/api/v1/namespaces/watch-1399/configmaps/e2e-watch-test-watch-closed,UID:b1ec3fc6-acd9-4078-b722-ac34044a3494,ResourceVersion:1082955,Generation:0,CreationTimestamp:2019-07-31 11:17:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:17:24.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1399" for this suite.
Jul 31 11:17:31.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:17:35.854: INFO: namespace watch-1399 deletion completed in 11.416303s

• [SLOW TEST:14.742 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:17:35.855: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-dcb25a12-cc36-44a5-82de-fd82509c4752
STEP: Creating a pod to test consume configMaps
Jul 31 11:17:36.727: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fd24696f-7152-478c-a3ec-2df39a09b26f" in namespace "projected-8805" to be "success or failure"
Jul 31 11:17:36.739: INFO: Pod "pod-projected-configmaps-fd24696f-7152-478c-a3ec-2df39a09b26f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.038536ms
Jul 31 11:17:39.417: INFO: Pod "pod-projected-configmaps-fd24696f-7152-478c-a3ec-2df39a09b26f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.690684273s
Jul 31 11:17:41.716: INFO: Pod "pod-projected-configmaps-fd24696f-7152-478c-a3ec-2df39a09b26f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.988914477s
STEP: Saw pod success
Jul 31 11:17:41.716: INFO: Pod "pod-projected-configmaps-fd24696f-7152-478c-a3ec-2df39a09b26f" satisfied condition "success or failure"
Jul 31 11:17:41.728: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-projected-configmaps-fd24696f-7152-478c-a3ec-2df39a09b26f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 11:17:42.538: INFO: Waiting for pod pod-projected-configmaps-fd24696f-7152-478c-a3ec-2df39a09b26f to disappear
Jul 31 11:17:42.815: INFO: Pod pod-projected-configmaps-fd24696f-7152-478c-a3ec-2df39a09b26f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:17:42.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8805" for this suite.
Jul 31 11:17:49.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:17:53.140: INFO: namespace projected-8805 deletion completed in 10.312062716s

• [SLOW TEST:17.285 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:17:53.145: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Jul 31 11:17:53.267: INFO: Waiting up to 5m0s for pod "client-containers-741c7da8-8cad-48e4-a543-abeadd1bf2df" in namespace "containers-4852" to be "success or failure"
Jul 31 11:17:54.110: INFO: Pod "client-containers-741c7da8-8cad-48e4-a543-abeadd1bf2df": Phase="Pending", Reason="", readiness=false. Elapsed: 843.664298ms
Jul 31 11:17:57.111: INFO: Pod "client-containers-741c7da8-8cad-48e4-a543-abeadd1bf2df": Phase="Pending", Reason="", readiness=false. Elapsed: 3.843952701s
Jul 31 11:18:00.010: INFO: Pod "client-containers-741c7da8-8cad-48e4-a543-abeadd1bf2df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.743452217s
Jul 31 11:18:02.510: INFO: Pod "client-containers-741c7da8-8cad-48e4-a543-abeadd1bf2df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.24381596s
STEP: Saw pod success
Jul 31 11:18:02.510: INFO: Pod "client-containers-741c7da8-8cad-48e4-a543-abeadd1bf2df" satisfied condition "success or failure"
Jul 31 11:18:02.810: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod client-containers-741c7da8-8cad-48e4-a543-abeadd1bf2df container test-container: <nil>
STEP: delete the pod
Jul 31 11:18:04.312: INFO: Waiting for pod client-containers-741c7da8-8cad-48e4-a543-abeadd1bf2df to disappear
Jul 31 11:18:04.714: INFO: Pod client-containers-741c7da8-8cad-48e4-a543-abeadd1bf2df no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:18:04.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4852" for this suite.
Jul 31 11:18:12.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:18:18.110: INFO: namespace containers-4852 deletion completed in 13.198411054s

• [SLOW TEST:24.965 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:18:18.110: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 11:18:21.510: INFO: (0) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 897.391674ms)
Jul 31 11:18:22.310: INFO: (1) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 800.185871ms)
Jul 31 11:18:23.210: INFO: (2) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 899.60521ms)
Jul 31 11:18:24.010: INFO: (3) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 799.826648ms)
Jul 31 11:18:24.810: INFO: (4) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 800.012805ms)
Jul 31 11:18:25.512: INFO: (5) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 701.751071ms)
Jul 31 11:18:26.310: INFO: (6) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 797.86604ms)
Jul 31 11:18:26.710: INFO: (7) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 399.562845ms)
Jul 31 11:18:27.012: INFO: (8) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 301.796389ms)
Jul 31 11:18:28.410: INFO: (9) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 1.398377062s)
Jul 31 11:18:28.811: INFO: (10) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 400.890323ms)
Jul 31 11:18:30.311: INFO: (11) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 1.49982516s)
Jul 31 11:18:30.810: INFO: (12) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 499.299697ms)
Jul 31 11:18:31.110: INFO: (13) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 290.244706ms)
Jul 31 11:18:31.910: INFO: (14) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 800.198405ms)
Jul 31 11:18:33.010: INFO: (15) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 1.100068386s)
Jul 31 11:18:33.410: INFO: (16) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 399.356738ms)
Jul 31 11:18:33.810: INFO: (17) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 400.004052ms)
Jul 31 11:18:35.712: INFO: (18) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 1.901779098s)
Jul 31 11:18:35.912: INFO: (19) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 199.71061ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:18:35.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7223" for this suite.
Jul 31 11:18:42.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:18:52.710: INFO: namespace proxy-7223 deletion completed in 16.698800287s

• [SLOW TEST:34.600 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:18:52.712: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-bt5f
STEP: Creating a pod to test atomic-volume-subpath
Jul 31 11:18:54.510: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bt5f" in namespace "subpath-633" to be "success or failure"
Jul 31 11:18:54.810: INFO: Pod "pod-subpath-test-projected-bt5f": Phase="Pending", Reason="", readiness=false. Elapsed: 300.361645ms
Jul 31 11:18:57.523: INFO: Pod "pod-subpath-test-projected-bt5f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.013456166s
Jul 31 11:19:00.210: INFO: Pod "pod-subpath-test-projected-bt5f": Phase="Running", Reason="", readiness=true. Elapsed: 5.700485192s
Jul 31 11:19:02.612: INFO: Pod "pod-subpath-test-projected-bt5f": Phase="Running", Reason="", readiness=true. Elapsed: 8.102297303s
Jul 31 11:19:04.815: INFO: Pod "pod-subpath-test-projected-bt5f": Phase="Running", Reason="", readiness=true. Elapsed: 10.304939726s
Jul 31 11:19:06.825: INFO: Pod "pod-subpath-test-projected-bt5f": Phase="Running", Reason="", readiness=true. Elapsed: 12.314975151s
Jul 31 11:19:09.010: INFO: Pod "pod-subpath-test-projected-bt5f": Phase="Running", Reason="", readiness=true. Elapsed: 14.500616868s
Jul 31 11:19:11.310: INFO: Pod "pod-subpath-test-projected-bt5f": Phase="Running", Reason="", readiness=true. Elapsed: 16.800491489s
Jul 31 11:19:13.810: INFO: Pod "pod-subpath-test-projected-bt5f": Phase="Running", Reason="", readiness=true. Elapsed: 19.300746577s
Jul 31 11:19:16.127: INFO: Pod "pod-subpath-test-projected-bt5f": Phase="Running", Reason="", readiness=true. Elapsed: 21.6170966s
Jul 31 11:19:18.143: INFO: Pod "pod-subpath-test-projected-bt5f": Phase="Running", Reason="", readiness=true. Elapsed: 23.63282225s
Jul 31 11:19:20.510: INFO: Pod "pod-subpath-test-projected-bt5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.000313702s
STEP: Saw pod success
Jul 31 11:19:20.510: INFO: Pod "pod-subpath-test-projected-bt5f" satisfied condition "success or failure"
Jul 31 11:19:20.810: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-subpath-test-projected-bt5f container test-container-subpath-projected-bt5f: <nil>
STEP: delete the pod
Jul 31 11:19:21.310: INFO: Waiting for pod pod-subpath-test-projected-bt5f to disappear
Jul 31 11:19:21.317: INFO: Pod pod-subpath-test-projected-bt5f no longer exists
STEP: Deleting pod pod-subpath-test-projected-bt5f
Jul 31 11:19:21.317: INFO: Deleting pod "pod-subpath-test-projected-bt5f" in namespace "subpath-633"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:19:21.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-633" for this suite.
Jul 31 11:19:27.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:19:33.850: INFO: namespace subpath-633 deletion completed in 12.508230211s

• [SLOW TEST:41.138 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:19:33.852: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-ftb6p in namespace proxy-7199
I0731 11:19:35.113066      15 runners.go:180] Created replication controller with name: proxy-service-ftb6p, namespace: proxy-7199, replica count: 1
I0731 11:19:36.614471      15 runners.go:180] proxy-service-ftb6p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 11:19:37.614659      15 runners.go:180] proxy-service-ftb6p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 11:19:38.614939      15 runners.go:180] proxy-service-ftb6p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 11:19:39.619053      15 runners.go:180] proxy-service-ftb6p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 11:19:40.619237      15 runners.go:180] proxy-service-ftb6p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 11:19:41.619747      15 runners.go:180] proxy-service-ftb6p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0731 11:19:42.620237      15 runners.go:180] proxy-service-ftb6p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0731 11:19:43.623016      15 runners.go:180] proxy-service-ftb6p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0731 11:19:44.623368      15 runners.go:180] proxy-service-ftb6p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0731 11:19:45.623560      15 runners.go:180] proxy-service-ftb6p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0731 11:19:46.624924      15 runners.go:180] proxy-service-ftb6p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0731 11:19:47.625587      15 runners.go:180] proxy-service-ftb6p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0731 11:19:48.625760      15 runners.go:180] proxy-service-ftb6p Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 11:19:48.810: INFO: setup took 14.493704643s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 31 11:19:49.510: INFO: (0) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 699.521091ms)
Jul 31 11:19:49.510: INFO: (0) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 698.093844ms)
Jul 31 11:19:49.510: INFO: (0) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 698.241755ms)
Jul 31 11:19:49.510: INFO: (0) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 698.666465ms)
Jul 31 11:19:49.510: INFO: (0) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 697.997856ms)
Jul 31 11:19:49.510: INFO: (0) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 699.246654ms)
Jul 31 11:19:49.510: INFO: (0) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 697.972688ms)
Jul 31 11:19:49.510: INFO: (0) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 699.951474ms)
Jul 31 11:19:49.510: INFO: (0) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 697.808247ms)
Jul 31 11:19:49.510: INFO: (0) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 698.012118ms)
Jul 31 11:19:49.510: INFO: (0) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 699.37101ms)
Jul 31 11:19:50.010: INFO: (0) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 1.19980514s)
Jul 31 11:19:50.010: INFO: (0) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 1.1996605s)
Jul 31 11:19:50.010: INFO: (0) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 1.200531518s)
Jul 31 11:19:50.010: INFO: (0) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 1.199407372s)
Jul 31 11:19:50.010: INFO: (0) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 1.199210104s)
Jul 31 11:19:50.510: INFO: (1) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 499.419536ms)
Jul 31 11:19:50.510: INFO: (1) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 499.480865ms)
Jul 31 11:19:50.510: INFO: (1) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 499.010271ms)
Jul 31 11:19:50.510: INFO: (1) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 499.144288ms)
Jul 31 11:19:50.510: INFO: (1) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 499.452388ms)
Jul 31 11:19:50.510: INFO: (1) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 499.304409ms)
Jul 31 11:19:50.510: INFO: (1) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 499.361185ms)
Jul 31 11:19:50.510: INFO: (1) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 499.48133ms)
Jul 31 11:19:50.510: INFO: (1) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 499.53507ms)
Jul 31 11:19:50.510: INFO: (1) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 499.729146ms)
Jul 31 11:19:50.510: INFO: (1) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 499.671689ms)
Jul 31 11:19:50.510: INFO: (1) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 499.783049ms)
Jul 31 11:19:51.210: INFO: (1) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 1.199322997s)
Jul 31 11:19:51.210: INFO: (1) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 1.199343727s)
Jul 31 11:19:51.210: INFO: (1) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 1.199016458s)
Jul 31 11:19:51.210: INFO: (1) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 1.199219802s)
Jul 31 11:19:51.710: INFO: (2) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 498.982797ms)
Jul 31 11:19:51.710: INFO: (2) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 500.208497ms)
Jul 31 11:19:51.710: INFO: (2) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 498.890865ms)
Jul 31 11:19:51.710: INFO: (2) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 500.217586ms)
Jul 31 11:19:51.710: INFO: (2) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 499.478054ms)
Jul 31 11:19:51.710: INFO: (2) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 500.050098ms)
Jul 31 11:19:51.710: INFO: (2) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 499.029081ms)
Jul 31 11:19:51.710: INFO: (2) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 499.610624ms)
Jul 31 11:19:51.710: INFO: (2) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 499.301637ms)
Jul 31 11:19:51.710: INFO: (2) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 499.397303ms)
Jul 31 11:19:51.710: INFO: (2) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 499.183808ms)
Jul 31 11:19:51.710: INFO: (2) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 500.098203ms)
Jul 31 11:19:51.810: INFO: (2) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 598.914032ms)
Jul 31 11:19:51.810: INFO: (2) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 599.491008ms)
Jul 31 11:19:51.810: INFO: (2) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 599.02498ms)
Jul 31 11:19:51.810: INFO: (2) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 599.631452ms)
Jul 31 11:19:52.210: INFO: (3) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 399.230539ms)
Jul 31 11:19:52.210: INFO: (3) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 399.263347ms)
Jul 31 11:19:52.210: INFO: (3) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 399.787577ms)
Jul 31 11:19:52.210: INFO: (3) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 399.366282ms)
Jul 31 11:19:52.210: INFO: (3) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 399.658378ms)
Jul 31 11:19:52.210: INFO: (3) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 399.567371ms)
Jul 31 11:19:52.210: INFO: (3) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 399.473205ms)
Jul 31 11:19:52.210: INFO: (3) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 399.373356ms)
Jul 31 11:19:52.210: INFO: (3) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 399.279829ms)
Jul 31 11:19:52.210: INFO: (3) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 399.668215ms)
Jul 31 11:19:52.210: INFO: (3) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 399.54513ms)
Jul 31 11:19:52.210: INFO: (3) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 399.334822ms)
Jul 31 11:19:52.510: INFO: (3) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 699.117563ms)
Jul 31 11:19:52.510: INFO: (3) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 699.507755ms)
Jul 31 11:19:52.510: INFO: (3) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 699.259063ms)
Jul 31 11:19:52.510: INFO: (3) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 699.332707ms)
Jul 31 11:19:53.010: INFO: (4) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 497.3425ms)
Jul 31 11:19:53.010: INFO: (4) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 499.58421ms)
Jul 31 11:19:53.010: INFO: (4) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 498.727121ms)
Jul 31 11:19:53.010: INFO: (4) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 497.794738ms)
Jul 31 11:19:53.010: INFO: (4) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 499.120745ms)
Jul 31 11:19:53.010: INFO: (4) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 499.329496ms)
Jul 31 11:19:53.010: INFO: (4) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 499.564554ms)
Jul 31 11:19:53.010: INFO: (4) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 499.841333ms)
Jul 31 11:19:53.010: INFO: (4) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 499.720339ms)
Jul 31 11:19:53.010: INFO: (4) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 497.572805ms)
Jul 31 11:19:53.010: INFO: (4) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 499.543287ms)
Jul 31 11:19:53.010: INFO: (4) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 497.999089ms)
Jul 31 11:19:53.410: INFO: (4) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 897.236765ms)
Jul 31 11:19:53.410: INFO: (4) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 897.86747ms)
Jul 31 11:19:53.410: INFO: (4) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 899.648673ms)
Jul 31 11:19:53.421: INFO: (4) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 909.836085ms)
Jul 31 11:19:54.110: INFO: (5) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 686.659445ms)
Jul 31 11:19:54.110: INFO: (5) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 686.840232ms)
Jul 31 11:19:54.110: INFO: (5) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 688.55927ms)
Jul 31 11:19:54.110: INFO: (5) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 688.381579ms)
Jul 31 11:19:54.110: INFO: (5) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 687.074687ms)
Jul 31 11:19:54.110: INFO: (5) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 686.770711ms)
Jul 31 11:19:54.110: INFO: (5) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 687.939547ms)
Jul 31 11:19:54.110: INFO: (5) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 687.181648ms)
Jul 31 11:19:54.110: INFO: (5) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 688.192153ms)
Jul 31 11:19:54.110: INFO: (5) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 687.522703ms)
Jul 31 11:19:54.110: INFO: (5) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 687.982867ms)
Jul 31 11:19:54.110: INFO: (5) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 687.529382ms)
Jul 31 11:19:54.510: INFO: (5) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 1.087444467s)
Jul 31 11:19:54.510: INFO: (5) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 1.087020717s)
Jul 31 11:19:54.510: INFO: (5) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 1.08769681s)
Jul 31 11:19:54.510: INFO: (5) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 1.087250077s)
Jul 31 11:19:55.110: INFO: (6) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 599.014022ms)
Jul 31 11:19:55.110: INFO: (6) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 599.135143ms)
Jul 31 11:19:55.110: INFO: (6) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 598.66989ms)
Jul 31 11:19:55.110: INFO: (6) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 598.796521ms)
Jul 31 11:19:55.110: INFO: (6) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 599.757643ms)
Jul 31 11:19:55.110: INFO: (6) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 599.090381ms)
Jul 31 11:19:55.110: INFO: (6) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 599.833308ms)
Jul 31 11:19:55.110: INFO: (6) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 600.088503ms)
Jul 31 11:19:55.110: INFO: (6) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 599.572386ms)
Jul 31 11:19:55.111: INFO: (6) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 599.92239ms)
Jul 31 11:19:55.111: INFO: (6) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 599.605611ms)
Jul 31 11:19:55.111: INFO: (6) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 599.488033ms)
Jul 31 11:19:55.111: INFO: (6) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 599.764342ms)
Jul 31 11:19:55.111: INFO: (6) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 600.003445ms)
Jul 31 11:19:55.111: INFO: (6) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 600.20227ms)
Jul 31 11:19:55.111: INFO: (6) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 599.676787ms)
Jul 31 11:19:55.412: INFO: (7) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 300.724054ms)
Jul 31 11:19:55.414: INFO: (7) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 302.44581ms)
Jul 31 11:19:55.414: INFO: (7) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 302.654194ms)
Jul 31 11:19:55.414: INFO: (7) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 302.258905ms)
Jul 31 11:19:55.414: INFO: (7) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 302.267174ms)
Jul 31 11:19:55.414: INFO: (7) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 302.627783ms)
Jul 31 11:19:55.414: INFO: (7) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 302.623236ms)
Jul 31 11:19:55.414: INFO: (7) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 302.742718ms)
Jul 31 11:19:55.414: INFO: (7) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 302.79755ms)
Jul 31 11:19:55.414: INFO: (7) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 303.137092ms)
Jul 31 11:19:55.415: INFO: (7) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 303.170612ms)
Jul 31 11:19:55.415: INFO: (7) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 303.504387ms)
Jul 31 11:19:55.415: INFO: (7) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 303.224077ms)
Jul 31 11:19:55.415: INFO: (7) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 303.454737ms)
Jul 31 11:19:55.415: INFO: (7) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 303.428505ms)
Jul 31 11:19:55.415: INFO: (7) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 303.678817ms)
Jul 31 11:19:55.610: INFO: (8) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 194.054126ms)
Jul 31 11:19:55.610: INFO: (8) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 194.490874ms)
Jul 31 11:19:55.610: INFO: (8) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 193.546225ms)
Jul 31 11:19:55.611: INFO: (8) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 195.254604ms)
Jul 31 11:19:55.613: INFO: (8) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 196.711155ms)
Jul 31 11:19:55.613: INFO: (8) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 196.633218ms)
Jul 31 11:19:55.613: INFO: (8) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 197.415036ms)
Jul 31 11:19:55.613: INFO: (8) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 197.30569ms)
Jul 31 11:19:55.613: INFO: (8) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 196.859512ms)
Jul 31 11:19:55.810: INFO: (8) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 394.01179ms)
Jul 31 11:19:55.810: INFO: (8) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 394.168821ms)
Jul 31 11:19:55.810: INFO: (8) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 394.055928ms)
Jul 31 11:19:55.810: INFO: (8) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 394.006819ms)
Jul 31 11:19:55.810: INFO: (8) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 393.785081ms)
Jul 31 11:19:55.810: INFO: (8) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 393.997576ms)
Jul 31 11:19:55.810: INFO: (8) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 393.951942ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 398.492376ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 398.289477ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 398.216541ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 397.564772ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 397.712179ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 397.536702ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 398.228067ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 397.108004ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 397.253038ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 398.680354ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 397.357393ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 399.71268ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 397.832728ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 399.569554ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 397.487633ms)
Jul 31 11:19:56.210: INFO: (9) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 397.891291ms)
Jul 31 11:19:56.910: INFO: (10) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 698.012611ms)
Jul 31 11:19:56.910: INFO: (10) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 698.333919ms)
Jul 31 11:19:56.910: INFO: (10) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 698.523424ms)
Jul 31 11:19:56.910: INFO: (10) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 698.406657ms)
Jul 31 11:19:56.910: INFO: (10) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 698.169482ms)
Jul 31 11:19:56.910: INFO: (10) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 698.511503ms)
Jul 31 11:19:56.910: INFO: (10) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 698.269656ms)
Jul 31 11:19:56.910: INFO: (10) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 698.319115ms)
Jul 31 11:19:56.910: INFO: (10) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 698.372571ms)
Jul 31 11:19:56.910: INFO: (10) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 698.694782ms)
Jul 31 11:19:56.910: INFO: (10) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 698.444829ms)
Jul 31 11:19:56.910: INFO: (10) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 699.069401ms)
Jul 31 11:19:58.210: INFO: (10) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 1.99773781s)
Jul 31 11:19:58.210: INFO: (10) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 1.998052669s)
Jul 31 11:19:58.210: INFO: (10) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 1.997932817s)
Jul 31 11:19:58.210: INFO: (10) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 1.997898249s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 1.198986259s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 1.199509597s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 1.199077564s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 1.199590608s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 1.199509891s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 1.199360255s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 1.199794491s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 1.199069129s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 1.200084611s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 1.199925718s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 1.199516263s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 1.199374619s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 1.200296349s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 1.200078153s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 1.199223216s)
Jul 31 11:19:59.410: INFO: (11) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 1.199332146s)
Jul 31 11:20:00.510: INFO: (12) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 1.098917129s)
Jul 31 11:20:00.510: INFO: (12) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 1.098997683s)
Jul 31 11:20:00.510: INFO: (12) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 1.099622987s)
Jul 31 11:20:00.510: INFO: (12) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 1.099583828s)
Jul 31 11:20:00.510: INFO: (12) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 1.100090987s)
Jul 31 11:20:00.510: INFO: (12) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 1.099273014s)
Jul 31 11:20:00.510: INFO: (12) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 1.09987118s)
Jul 31 11:20:00.510: INFO: (12) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 1.099834542s)
Jul 31 11:20:00.510: INFO: (12) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 1.099404343s)
Jul 31 11:20:00.511: INFO: (12) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 1.099398435s)
Jul 31 11:20:00.511: INFO: (12) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 1.099809188s)
Jul 31 11:20:00.511: INFO: (12) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 1.099647111s)
Jul 31 11:20:00.511: INFO: (12) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 1.099524254s)
Jul 31 11:20:00.511: INFO: (12) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 1.09982652s)
Jul 31 11:20:00.511: INFO: (12) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 1.100303177s)
Jul 31 11:20:00.511: INFO: (12) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 1.100309194s)
Jul 31 11:20:01.011: INFO: (13) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 499.753911ms)
Jul 31 11:20:01.011: INFO: (13) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 499.318438ms)
Jul 31 11:20:01.011: INFO: (13) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 499.686856ms)
Jul 31 11:20:01.011: INFO: (13) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 499.63796ms)
Jul 31 11:20:01.411: INFO: (13) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 898.266815ms)
Jul 31 11:20:01.414: INFO: (13) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 901.705627ms)
Jul 31 11:20:01.422: INFO: (13) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 908.37413ms)
Jul 31 11:20:01.422: INFO: (13) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 909.818747ms)
Jul 31 11:20:01.422: INFO: (13) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 908.625795ms)
Jul 31 11:20:01.422: INFO: (13) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 909.742217ms)
Jul 31 11:20:01.422: INFO: (13) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 908.299154ms)
Jul 31 11:20:01.422: INFO: (13) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 908.393178ms)
Jul 31 11:20:01.422: INFO: (13) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 908.45277ms)
Jul 31 11:20:01.422: INFO: (13) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 908.395446ms)
Jul 31 11:20:02.710: INFO: (13) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 2.196882425s)
Jul 31 11:20:02.710: INFO: (13) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 2.196869998s)
Jul 31 11:20:03.509: INFO: (14) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 797.367672ms)
Jul 31 11:20:03.510: INFO: (14) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 794.8069ms)
Jul 31 11:20:03.510: INFO: (14) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 795.031582ms)
Jul 31 11:20:03.510: INFO: (14) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 794.987ms)
Jul 31 11:20:03.510: INFO: (14) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 798.178658ms)
Jul 31 11:20:03.510: INFO: (14) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 794.973136ms)
Jul 31 11:20:03.510: INFO: (14) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 794.862622ms)
Jul 31 11:20:03.510: INFO: (14) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 795.776461ms)
Jul 31 11:20:03.510: INFO: (14) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 799.221773ms)
Jul 31 11:20:03.510: INFO: (14) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 797.861872ms)
Jul 31 11:20:03.510: INFO: (14) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 797.989485ms)
Jul 31 11:20:03.510: INFO: (14) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 795.293096ms)
Jul 31 11:20:04.010: INFO: (14) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 1.294990521s)
Jul 31 11:20:04.010: INFO: (14) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 1.29736947s)
Jul 31 11:20:04.010: INFO: (14) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 1.295044163s)
Jul 31 11:20:04.010: INFO: (14) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 1.294782304s)
Jul 31 11:20:05.311: INFO: (15) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 1.300241306s)
Jul 31 11:20:05.311: INFO: (15) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 1.300720596s)
Jul 31 11:20:05.311: INFO: (15) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 1.300846402s)
Jul 31 11:20:05.311: INFO: (15) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 1.300426345s)
Jul 31 11:20:05.311: INFO: (15) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 1.301072237s)
Jul 31 11:20:05.311: INFO: (15) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 1.301268197s)
Jul 31 11:20:05.311: INFO: (15) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 1.300409846s)
Jul 31 11:20:05.311: INFO: (15) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 1.300491129s)
Jul 31 11:20:05.311: INFO: (15) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 1.301574264s)
Jul 31 11:20:05.311: INFO: (15) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 1.301205318s)
Jul 31 11:20:05.311: INFO: (15) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 1.30098001s)
Jul 31 11:20:05.311: INFO: (15) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 1.300975356s)
Jul 31 11:20:05.312: INFO: (15) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 1.301218288s)
Jul 31 11:20:05.312: INFO: (15) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 1.301223938s)
Jul 31 11:20:05.312: INFO: (15) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 1.30094951s)
Jul 31 11:20:05.312: INFO: (15) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 1.301235432s)
Jul 31 11:20:06.610: INFO: (16) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 1.29755199s)
Jul 31 11:20:06.610: INFO: (16) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 1.29768208s)
Jul 31 11:20:06.610: INFO: (16) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 1.298271686s)
Jul 31 11:20:06.610: INFO: (16) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 1.298066411s)
Jul 31 11:20:06.610: INFO: (16) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 1.297749287s)
Jul 31 11:20:06.610: INFO: (16) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 1.297972583s)
Jul 31 11:20:06.610: INFO: (16) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 1.297802519s)
Jul 31 11:20:06.610: INFO: (16) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 1.298159305s)
Jul 31 11:20:06.610: INFO: (16) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 1.297941398s)
Jul 31 11:20:06.610: INFO: (16) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 1.297991344s)
Jul 31 11:20:06.610: INFO: (16) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 1.298232534s)
Jul 31 11:20:06.610: INFO: (16) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 1.298333847s)
Jul 31 11:20:06.952: INFO: (16) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 1.639973356s)
Jul 31 11:20:06.952: INFO: (16) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 1.639920881s)
Jul 31 11:20:06.952: INFO: (16) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 1.639910518s)
Jul 31 11:20:06.952: INFO: (16) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 1.640089389s)
Jul 31 11:20:07.710: INFO: (17) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 757.198162ms)
Jul 31 11:20:07.710: INFO: (17) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 757.304951ms)
Jul 31 11:20:07.710: INFO: (17) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 757.47107ms)
Jul 31 11:20:07.710: INFO: (17) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 757.247881ms)
Jul 31 11:20:07.710: INFO: (17) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 757.568754ms)
Jul 31 11:20:07.710: INFO: (17) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 757.571482ms)
Jul 31 11:20:07.710: INFO: (17) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 757.371477ms)
Jul 31 11:20:07.710: INFO: (17) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 757.538206ms)
Jul 31 11:20:07.710: INFO: (17) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 757.456477ms)
Jul 31 11:20:07.710: INFO: (17) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 757.64251ms)
Jul 31 11:20:07.710: INFO: (17) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 757.925022ms)
Jul 31 11:20:07.710: INFO: (17) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 757.925533ms)
Jul 31 11:20:08.511: INFO: (17) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 1.558334192s)
Jul 31 11:20:08.511: INFO: (17) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 1.558329678s)
Jul 31 11:20:08.511: INFO: (17) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 1.558580736s)
Jul 31 11:20:08.511: INFO: (17) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 1.558406283s)
Jul 31 11:20:08.710: INFO: (18) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 198.53386ms)
Jul 31 11:20:08.710: INFO: (18) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 197.997775ms)
Jul 31 11:20:08.710: INFO: (18) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 197.943197ms)
Jul 31 11:20:08.710: INFO: (18) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 197.998502ms)
Jul 31 11:20:08.710: INFO: (18) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 198.261713ms)
Jul 31 11:20:08.710: INFO: (18) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 197.765586ms)
Jul 31 11:20:08.710: INFO: (18) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 197.836026ms)
Jul 31 11:20:08.710: INFO: (18) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 198.582939ms)
Jul 31 11:20:08.710: INFO: (18) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 198.02445ms)
Jul 31 11:20:08.710: INFO: (18) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 198.522981ms)
Jul 31 11:20:08.710: INFO: (18) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 197.728228ms)
Jul 31 11:20:09.710: INFO: (18) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 1.197934164s)
Jul 31 11:20:09.710: INFO: (18) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 1.197939458s)
Jul 31 11:20:09.710: INFO: (18) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 1.198697592s)
Jul 31 11:20:09.710: INFO: (18) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 1.198064953s)
Jul 31 11:20:09.710: INFO: (18) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 1.197926767s)
Jul 31 11:20:10.910: INFO: (19) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:460/proxy/: tls baz (200; 1.199106726s)
Jul 31 11:20:10.910: INFO: (19) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 1.196719429s)
Jul 31 11:20:10.910: INFO: (19) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname1/proxy/: tls baz (200; 1.197005252s)
Jul 31 11:20:10.910: INFO: (19) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">test<... (200; 1.196968146s)
Jul 31 11:20:10.910: INFO: (19) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:162/proxy/: bar (200; 1.1969005s)
Jul 31 11:20:10.910: INFO: (19) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname1/proxy/: foo (200; 1.197471231s)
Jul 31 11:20:10.910: INFO: (19) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr/proxy/rewriteme">test</a> (200; 1.197662139s)
Jul 31 11:20:10.910: INFO: (19) /api/v1/namespaces/proxy-7199/services/https:proxy-service-ftb6p:tlsportname2/proxy/: tls qux (200; 1.199649287s)
Jul 31 11:20:10.910: INFO: (19) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname1/proxy/: foo (200; 1.197795943s)
Jul 31 11:20:10.910: INFO: (19) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:462/proxy/: tls qux (200; 1.197681882s)
Jul 31 11:20:10.910: INFO: (19) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:1080/proxy/rewriteme">... (200; 1.197419341s)
Jul 31 11:20:10.910: INFO: (19) /api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/: <a href="/api/v1/namespaces/proxy-7199/pods/https:proxy-service-ftb6p-cp9cr:443/proxy/tlsrewritem... (200; 1.197584129s)
Jul 31 11:20:10.911: INFO: (19) /api/v1/namespaces/proxy-7199/services/http:proxy-service-ftb6p:portname2/proxy/: bar (200; 1.197825492s)
Jul 31 11:20:10.911: INFO: (19) /api/v1/namespaces/proxy-7199/pods/proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 1.197728893s)
Jul 31 11:20:10.911: INFO: (19) /api/v1/namespaces/proxy-7199/services/proxy-service-ftb6p:portname2/proxy/: bar (200; 1.197844502s)
Jul 31 11:20:10.911: INFO: (19) /api/v1/namespaces/proxy-7199/pods/http:proxy-service-ftb6p-cp9cr:160/proxy/: foo (200; 1.198064932s)
STEP: deleting ReplicationController proxy-service-ftb6p in namespace proxy-7199, will wait for the garbage collector to delete the pods
Jul 31 11:20:11.710: INFO: Deleting ReplicationController proxy-service-ftb6p took: 299.617134ms
Jul 31 11:20:12.210: INFO: Terminating ReplicationController proxy-service-ftb6p pods took: 500.380005ms
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:20:17.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7199" for this suite.
Jul 31 11:20:23.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:20:25.438: INFO: namespace proxy-7199 deletion completed in 7.619512277s

• [SLOW TEST:51.587 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:20:25.439: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 31 11:20:25.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-6471'
Jul 31 11:20:28.129: INFO: stderr: ""
Jul 31 11:20:28.129: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jul 31 11:20:33.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pod e2e-test-nginx-pod --namespace=kubectl-6471 -o json'
Jul 31 11:20:33.545: INFO: stderr: ""
Jul 31 11:20:33.546: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-07-31T11:20:28Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-6471\",\n        \"resourceVersion\": \"1083568\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6471/pods/e2e-test-nginx-pod\",\n        \"uid\": \"81c34631-3efb-4bf6-aa3d-ab1624f02a79\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-lzksl\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-2bh9r-78c4c5b4fb-czrvz\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-lzksl\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-lzksl\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-31T11:20:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-31T11:20:31Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-31T11:20:31Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-31T11:20:28Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://22375131829be6a1e0de36adb5b0f1033b210156bc275d8256d2ca941ad52b55\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-07-31T11:20:31Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.12\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.26.25\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-07-31T11:20:28Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 31 11:20:33.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 replace -f - --namespace=kubectl-6471'
Jul 31 11:20:35.216: INFO: stderr: ""
Jul 31 11:20:35.216: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Jul 31 11:20:35.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete pods e2e-test-nginx-pod --namespace=kubectl-6471'
Jul 31 11:20:42.212: INFO: stderr: ""
Jul 31 11:20:42.212: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:20:42.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6471" for this suite.
Jul 31 11:20:48.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:20:55.555: INFO: namespace kubectl-6471 deletion completed in 13.144446291s

• [SLOW TEST:30.116 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:20:55.556: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-4d5fc2a4-8e23-402f-b017-243db6485e8c
STEP: Creating a pod to test consume configMaps
Jul 31 11:20:57.210: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b5e63a59-9101-45ff-bb90-0d4c0a70289a" in namespace "projected-2669" to be "success or failure"
Jul 31 11:20:58.210: INFO: Pod "pod-projected-configmaps-b5e63a59-9101-45ff-bb90-0d4c0a70289a": Phase="Pending", Reason="", readiness=false. Elapsed: 999.783741ms
Jul 31 11:21:00.434: INFO: Pod "pod-projected-configmaps-b5e63a59-9101-45ff-bb90-0d4c0a70289a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.224090933s
Jul 31 11:21:03.010: INFO: Pod "pod-projected-configmaps-b5e63a59-9101-45ff-bb90-0d4c0a70289a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.800003251s
Jul 31 11:21:05.225: INFO: Pod "pod-projected-configmaps-b5e63a59-9101-45ff-bb90-0d4c0a70289a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014745834s
STEP: Saw pod success
Jul 31 11:21:05.225: INFO: Pod "pod-projected-configmaps-b5e63a59-9101-45ff-bb90-0d4c0a70289a" satisfied condition "success or failure"
Jul 31 11:21:05.410: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-projected-configmaps-b5e63a59-9101-45ff-bb90-0d4c0a70289a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 11:21:07.410: INFO: Waiting for pod pod-projected-configmaps-b5e63a59-9101-45ff-bb90-0d4c0a70289a to disappear
Jul 31 11:21:07.711: INFO: Pod pod-projected-configmaps-b5e63a59-9101-45ff-bb90-0d4c0a70289a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:21:07.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2669" for this suite.
Jul 31 11:21:15.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:21:22.843: INFO: namespace projected-2669 deletion completed in 14.831200741s

• [SLOW TEST:27.287 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:21:22.854: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-2470/configmap-test-05b0cb26-4200-462f-93c3-4355e0e77311
STEP: Creating a pod to test consume configMaps
Jul 31 11:21:24.310: INFO: Waiting up to 5m0s for pod "pod-configmaps-6376c2e2-0ce8-4e2c-a881-7f3d1e54b2ab" in namespace "configmap-2470" to be "success or failure"
Jul 31 11:21:24.810: INFO: Pod "pod-configmaps-6376c2e2-0ce8-4e2c-a881-7f3d1e54b2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 500.310514ms
Jul 31 11:21:27.010: INFO: Pod "pod-configmaps-6376c2e2-0ce8-4e2c-a881-7f3d1e54b2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.700254491s
Jul 31 11:21:29.225: INFO: Pod "pod-configmaps-6376c2e2-0ce8-4e2c-a881-7f3d1e54b2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.914869351s
Jul 31 11:21:31.710: INFO: Pod "pod-configmaps-6376c2e2-0ce8-4e2c-a881-7f3d1e54b2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.400142348s
Jul 31 11:21:34.410: INFO: Pod "pod-configmaps-6376c2e2-0ce8-4e2c-a881-7f3d1e54b2ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.099921251s
STEP: Saw pod success
Jul 31 11:21:34.410: INFO: Pod "pod-configmaps-6376c2e2-0ce8-4e2c-a881-7f3d1e54b2ab" satisfied condition "success or failure"
Jul 31 11:21:34.810: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-configmaps-6376c2e2-0ce8-4e2c-a881-7f3d1e54b2ab container env-test: <nil>
STEP: delete the pod
Jul 31 11:21:36.210: INFO: Waiting for pod pod-configmaps-6376c2e2-0ce8-4e2c-a881-7f3d1e54b2ab to disappear
Jul 31 11:21:36.310: INFO: Pod pod-configmaps-6376c2e2-0ce8-4e2c-a881-7f3d1e54b2ab no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:21:36.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2470" for this suite.
Jul 31 11:21:42.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:21:51.024: INFO: namespace configmap-2470 deletion completed in 14.59853385s

• [SLOW TEST:28.170 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:21:51.025: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Jul 31 11:21:51.238: INFO: Waiting up to 5m0s for pod "client-containers-e08a99b9-95af-41db-bc4d-eaf635d517fd" in namespace "containers-491" to be "success or failure"
Jul 31 11:21:51.252: INFO: Pod "client-containers-e08a99b9-95af-41db-bc4d-eaf635d517fd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.653584ms
Jul 31 11:21:53.611: INFO: Pod "client-containers-e08a99b9-95af-41db-bc4d-eaf635d517fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.373000353s
Jul 31 11:21:55.620: INFO: Pod "client-containers-e08a99b9-95af-41db-bc4d-eaf635d517fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.382107679s
STEP: Saw pod success
Jul 31 11:21:55.620: INFO: Pod "client-containers-e08a99b9-95af-41db-bc4d-eaf635d517fd" satisfied condition "success or failure"
Jul 31 11:21:55.629: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod client-containers-e08a99b9-95af-41db-bc4d-eaf635d517fd container test-container: <nil>
STEP: delete the pod
Jul 31 11:21:56.310: INFO: Waiting for pod client-containers-e08a99b9-95af-41db-bc4d-eaf635d517fd to disappear
Jul 31 11:21:56.510: INFO: Pod client-containers-e08a99b9-95af-41db-bc4d-eaf635d517fd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:21:56.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-491" for this suite.
Jul 31 11:22:03.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:22:12.110: INFO: namespace containers-491 deletion completed in 15.198598819s

• [SLOW TEST:21.085 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:22:12.110: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0731 11:22:24.911646      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 31 11:22:24.911: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:22:24.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5841" for this suite.
Jul 31 11:22:33.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:22:38.918: INFO: namespace gc-5841 deletion completed in 13.99202449s

• [SLOW TEST:26.808 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:22:38.919: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 11:22:40.710: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a52eaa57-7859-412f-bc6a-e12b901e2c8c" in namespace "projected-6644" to be "success or failure"
Jul 31 11:22:40.910: INFO: Pod "downwardapi-volume-a52eaa57-7859-412f-bc6a-e12b901e2c8c": Phase="Pending", Reason="", readiness=false. Elapsed: 199.911224ms
Jul 31 11:22:43.210: INFO: Pod "downwardapi-volume-a52eaa57-7859-412f-bc6a-e12b901e2c8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.499928086s
Jul 31 11:22:45.410: INFO: Pod "downwardapi-volume-a52eaa57-7859-412f-bc6a-e12b901e2c8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.700207729s
Jul 31 11:22:47.610: INFO: Pod "downwardapi-volume-a52eaa57-7859-412f-bc6a-e12b901e2c8c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.900402875s
Jul 31 11:22:49.810: INFO: Pod "downwardapi-volume-a52eaa57-7859-412f-bc6a-e12b901e2c8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.100683876s
STEP: Saw pod success
Jul 31 11:22:49.811: INFO: Pod "downwardapi-volume-a52eaa57-7859-412f-bc6a-e12b901e2c8c" satisfied condition "success or failure"
Jul 31 11:22:50.010: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-h22s9 pod downwardapi-volume-a52eaa57-7859-412f-bc6a-e12b901e2c8c container client-container: <nil>
STEP: delete the pod
Jul 31 11:22:50.610: INFO: Waiting for pod downwardapi-volume-a52eaa57-7859-412f-bc6a-e12b901e2c8c to disappear
Jul 31 11:22:50.918: INFO: Pod downwardapi-volume-a52eaa57-7859-412f-bc6a-e12b901e2c8c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:22:50.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6644" for this suite.
Jul 31 11:22:57.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:23:02.009: INFO: namespace projected-6644 deletion completed in 10.898371852s

• [SLOW TEST:23.090 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:23:02.015: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-e06678e9-2829-42d1-9f72-025f0ca642e0
STEP: Creating a pod to test consume secrets
Jul 31 11:23:04.110: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1539d608-8795-46d8-8b3a-6912508d8ee7" in namespace "projected-8909" to be "success or failure"
Jul 31 11:23:04.714: INFO: Pod "pod-projected-secrets-1539d608-8795-46d8-8b3a-6912508d8ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 604.192218ms
Jul 31 11:23:07.015: INFO: Pod "pod-projected-secrets-1539d608-8795-46d8-8b3a-6912508d8ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.905255432s
Jul 31 11:23:09.214: INFO: Pod "pod-projected-secrets-1539d608-8795-46d8-8b3a-6912508d8ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.103971719s
Jul 31 11:23:11.410: INFO: Pod "pod-projected-secrets-1539d608-8795-46d8-8b3a-6912508d8ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.299547256s
Jul 31 11:23:13.610: INFO: Pod "pod-projected-secrets-1539d608-8795-46d8-8b3a-6912508d8ee7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.49991382s
STEP: Saw pod success
Jul 31 11:23:13.610: INFO: Pod "pod-projected-secrets-1539d608-8795-46d8-8b3a-6912508d8ee7" satisfied condition "success or failure"
Jul 31 11:23:13.619: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-projected-secrets-1539d608-8795-46d8-8b3a-6912508d8ee7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 31 11:23:14.631: INFO: Waiting for pod pod-projected-secrets-1539d608-8795-46d8-8b3a-6912508d8ee7 to disappear
Jul 31 11:23:14.814: INFO: Pod pod-projected-secrets-1539d608-8795-46d8-8b3a-6912508d8ee7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:23:14.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8909" for this suite.
Jul 31 11:23:21.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:23:27.083: INFO: namespace projected-8909 deletion completed in 12.257627139s

• [SLOW TEST:25.069 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:23:27.084: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 11:23:28.378: INFO: Waiting up to 5m0s for pod "downwardapi-volume-157d5e43-8c3c-4b31-bdfa-e2e7cc8ca624" in namespace "downward-api-805" to be "success or failure"
Jul 31 11:23:28.396: INFO: Pod "downwardapi-volume-157d5e43-8c3c-4b31-bdfa-e2e7cc8ca624": Phase="Pending", Reason="", readiness=false. Elapsed: 18.0932ms
Jul 31 11:23:31.715: INFO: Pod "downwardapi-volume-157d5e43-8c3c-4b31-bdfa-e2e7cc8ca624": Phase="Pending", Reason="", readiness=false. Elapsed: 3.337038245s
Jul 31 11:23:34.722: INFO: Pod "downwardapi-volume-157d5e43-8c3c-4b31-bdfa-e2e7cc8ca624": Phase="Pending", Reason="", readiness=false. Elapsed: 6.344746088s
Jul 31 11:23:37.610: INFO: Pod "downwardapi-volume-157d5e43-8c3c-4b31-bdfa-e2e7cc8ca624": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.232203334s
STEP: Saw pod success
Jul 31 11:23:37.610: INFO: Pod "downwardapi-volume-157d5e43-8c3c-4b31-bdfa-e2e7cc8ca624" satisfied condition "success or failure"
Jul 31 11:23:37.810: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downwardapi-volume-157d5e43-8c3c-4b31-bdfa-e2e7cc8ca624 container client-container: <nil>
STEP: delete the pod
Jul 31 11:23:38.510: INFO: Waiting for pod downwardapi-volume-157d5e43-8c3c-4b31-bdfa-e2e7cc8ca624 to disappear
Jul 31 11:23:38.909: INFO: Pod downwardapi-volume-157d5e43-8c3c-4b31-bdfa-e2e7cc8ca624 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:23:38.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-805" for this suite.
Jul 31 11:23:45.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:23:50.020: INFO: namespace downward-api-805 deletion completed in 10.906433286s

• [SLOW TEST:22.937 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:23:50.023: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Jul 31 11:23:50.822: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jul 31 11:23:50.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 create -f - --namespace=kubectl-1681'
Jul 31 11:23:52.018: INFO: stderr: ""
Jul 31 11:23:52.018: INFO: stdout: "service/redis-slave created\n"
Jul 31 11:23:52.020: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jul 31 11:23:52.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 create -f - --namespace=kubectl-1681'
Jul 31 11:23:54.112: INFO: stderr: ""
Jul 31 11:23:54.112: INFO: stdout: "service/redis-master created\n"
Jul 31 11:23:54.112: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 31 11:23:54.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 create -f - --namespace=kubectl-1681'
Jul 31 11:23:55.140: INFO: stderr: ""
Jul 31 11:23:55.140: INFO: stdout: "service/frontend created\n"
Jul 31 11:23:55.140: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jul 31 11:23:55.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 create -f - --namespace=kubectl-1681'
Jul 31 11:23:56.216: INFO: stderr: ""
Jul 31 11:23:56.216: INFO: stdout: "deployment.apps/frontend created\n"
Jul 31 11:23:56.216: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 31 11:23:56.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 create -f - --namespace=kubectl-1681'
Jul 31 11:23:56.834: INFO: stderr: ""
Jul 31 11:23:56.834: INFO: stdout: "deployment.apps/redis-master created\n"
Jul 31 11:23:56.834: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jul 31 11:23:56.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 create -f - --namespace=kubectl-1681'
Jul 31 11:23:57.839: INFO: stderr: ""
Jul 31 11:23:57.839: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jul 31 11:23:57.839: INFO: Waiting for all frontend pods to be Running.
Jul 31 11:24:23.350: INFO: Waiting for frontend to serve content.
Jul 31 11:24:30.110: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jul 31 11:24:36.310: INFO: Trying to add a new entry to the guestbook.
Jul 31 11:24:36.760: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul 31 11:24:38.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete --grace-period=0 --force -f - --namespace=kubectl-1681'
Jul 31 11:24:38.711: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 11:24:38.711: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul 31 11:24:38.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete --grace-period=0 --force -f - --namespace=kubectl-1681'
Jul 31 11:24:39.716: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 11:24:39.716: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 31 11:24:39.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete --grace-period=0 --force -f - --namespace=kubectl-1681'
Jul 31 11:24:40.611: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 11:24:40.611: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 31 11:24:40.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete --grace-period=0 --force -f - --namespace=kubectl-1681'
Jul 31 11:24:42.011: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 11:24:42.011: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 31 11:24:42.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete --grace-period=0 --force -f - --namespace=kubectl-1681'
Jul 31 11:24:43.827: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 11:24:43.827: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 31 11:24:43.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete --grace-period=0 --force -f - --namespace=kubectl-1681'
Jul 31 11:24:44.666: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 11:24:44.666: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:24:44.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1681" for this suite.
Jul 31 11:25:26.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:25:35.333: INFO: namespace kubectl-1681 deletion completed in 50.521181763s

• [SLOW TEST:105.310 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:25:35.338: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 31 11:25:41.734: INFO: Successfully updated pod "labelsupdate27643bea-b067-493e-9a90-9e685c7238d7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:25:45.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1605" for this suite.
Jul 31 11:26:07.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:26:12.009: INFO: namespace downward-api-1605 deletion completed in 26.989822141s

• [SLOW TEST:36.672 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:26:12.011: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul 31 11:26:13.610: INFO: namespace kubectl-4421
Jul 31 11:26:13.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 create -f - --namespace=kubectl-4421'
Jul 31 11:26:14.116: INFO: stderr: ""
Jul 31 11:26:14.116: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 31 11:26:15.710: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 11:26:15.710: INFO: Found 0 / 1
Jul 31 11:26:16.310: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 11:26:16.310: INFO: Found 0 / 1
Jul 31 11:26:17.316: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 11:26:17.316: INFO: Found 0 / 1
Jul 31 11:26:20.109: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 11:26:20.109: INFO: Found 0 / 1
Jul 31 11:26:20.128: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 11:26:20.128: INFO: Found 0 / 1
Jul 31 11:26:21.310: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 11:26:21.310: INFO: Found 1 / 1
Jul 31 11:26:21.310: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 31 11:26:21.514: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 11:26:21.514: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 31 11:26:21.514: INFO: wait on redis-master startup in kubectl-4421 
Jul 31 11:26:21.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 logs redis-master-wjc8g redis-master --namespace=kubectl-4421'
Jul 31 11:26:22.123: INFO: stderr: ""
Jul 31 11:26:22.123: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 31 Jul 11:26:19.442 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 31 Jul 11:26:19.442 # Server started, Redis version 3.2.12\n1:M 31 Jul 11:26:19.442 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 31 Jul 11:26:19.442 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jul 31 11:26:22.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-4421'
Jul 31 11:26:23.221: INFO: stderr: ""
Jul 31 11:26:23.222: INFO: stdout: "service/rm2 exposed\n"
Jul 31 11:26:23.709: INFO: Service rm2 in namespace kubectl-4421 found.
STEP: exposing service
Jul 31 11:26:25.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-4421'
Jul 31 11:26:26.353: INFO: stderr: ""
Jul 31 11:26:26.353: INFO: stdout: "service/rm3 exposed\n"
Jul 31 11:26:26.370: INFO: Service rm3 in namespace kubectl-4421 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:26:29.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4421" for this suite.
Jul 31 11:26:52.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:26:58.763: INFO: namespace kubectl-4421 deletion completed in 28.852074889s

• [SLOW TEST:46.753 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:26:58.764: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 11:26:59.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 version'
Jul 31 11:27:01.956: INFO: stderr: ""
Jul 31 11:27:01.956: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:40:16Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:32:14Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:27:01.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4584" for this suite.
Jul 31 11:27:08.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:27:13.062: INFO: namespace kubectl-4584 deletion completed in 10.648389039s

• [SLOW TEST:14.299 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:27:13.064: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-aa5576e6-67e3-4ffe-896e-a2c0910bcf59
STEP: Creating a pod to test consume secrets
Jul 31 11:27:14.300: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eee1eed0-3efc-4870-8f07-b664dc691a2d" in namespace "projected-8462" to be "success or failure"
Jul 31 11:27:14.313: INFO: Pod "pod-projected-secrets-eee1eed0-3efc-4870-8f07-b664dc691a2d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.810144ms
Jul 31 11:27:16.914: INFO: Pod "pod-projected-secrets-eee1eed0-3efc-4870-8f07-b664dc691a2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.613685414s
Jul 31 11:27:19.826: INFO: Pod "pod-projected-secrets-eee1eed0-3efc-4870-8f07-b664dc691a2d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.525380152s
Jul 31 11:27:22.609: INFO: Pod "pod-projected-secrets-eee1eed0-3efc-4870-8f07-b664dc691a2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.308826962s
STEP: Saw pod success
Jul 31 11:27:22.609: INFO: Pod "pod-projected-secrets-eee1eed0-3efc-4870-8f07-b664dc691a2d" satisfied condition "success or failure"
Jul 31 11:27:22.618: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-projected-secrets-eee1eed0-3efc-4870-8f07-b664dc691a2d container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 31 11:27:23.858: INFO: Waiting for pod pod-projected-secrets-eee1eed0-3efc-4870-8f07-b664dc691a2d to disappear
Jul 31 11:27:23.864: INFO: Pod pod-projected-secrets-eee1eed0-3efc-4870-8f07-b664dc691a2d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:27:23.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8462" for this suite.
Jul 31 11:27:30.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:27:40.957: INFO: namespace projected-8462 deletion completed in 17.083150993s

• [SLOW TEST:27.894 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:27:40.964: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:27:50.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1544" for this suite.
Jul 31 11:27:57.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:28:02.809: INFO: namespace emptydir-wrapper-1544 deletion completed in 12.19844594s

• [SLOW TEST:21.846 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:28:02.810: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 11:28:03.510: INFO: Creating deployment "nginx-deployment"
Jul 31 11:28:03.711: INFO: Waiting for observed generation 1
Jul 31 11:28:04.013: INFO: Waiting for all required pods to come up
Jul 31 11:28:04.321: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 31 11:28:17.309: INFO: Waiting for deployment "nginx-deployment" to complete
Jul 31 11:28:17.709: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jul 31 11:28:18.413: INFO: Updating deployment nginx-deployment
Jul 31 11:28:18.413: INFO: Waiting for observed generation 2
Jul 31 11:28:18.610: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 31 11:28:18.811: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 31 11:28:19.009: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 31 11:28:23.613: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 31 11:28:23.613: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 31 11:28:24.110: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 31 11:28:24.909: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jul 31 11:28:24.909: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jul 31 11:28:25.224: INFO: Updating deployment nginx-deployment
Jul 31 11:28:25.224: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jul 31 11:28:25.264: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 31 11:28:27.512: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 31 11:28:28.211: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-7936,SelfLink:/apis/apps/v1/namespaces/deployment-7936/deployments/nginx-deployment,UID:a87fe468-8ce0-484c-89ff-a2dac483a9ab,ResourceVersion:1085753,Generation:3,CreationTimestamp:2019-07-31 11:28:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-07-31 11:28:25 +0000 UTC 2019-07-31 11:28:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-31 11:28:25 +0000 UTC 2019-07-31 11:28:03 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Jul 31 11:28:28.523: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-7936,SelfLink:/apis/apps/v1/namespaces/deployment-7936/replicasets/nginx-deployment-55fb7cb77f,UID:91b0934e-791e-4a2c-89ec-9be86f2d10b6,ResourceVersion:1085747,Generation:3,CreationTimestamp:2019-07-31 11:28:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a87fe468-8ce0-484c-89ff-a2dac483a9ab 0xc0008d31e7 0xc0008d31e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 31 11:28:28.523: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jul 31 11:28:28.523: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-7936,SelfLink:/apis/apps/v1/namespaces/deployment-7936/replicasets/nginx-deployment-7b8c6f4498,UID:1815724c-04db-4c7f-af2e-1155a1c891c4,ResourceVersion:1085749,Generation:3,CreationTimestamp:2019-07-31 11:28:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a87fe468-8ce0-484c-89ff-a2dac483a9ab 0xc0008d32b7 0xc0008d32b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jul 31 11:28:29.013: INFO: Pod "nginx-deployment-55fb7cb77f-c42zr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-c42zr,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-55fb7cb77f-c42zr,UID:736e57c9-5293-4a10-a3c4-01f4685afc6a,ResourceVersion:1085728,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 91b0934e-791e-4a2c-89ec-9be86f2d10b6 0xc0008d3d17 0xc0008d3d18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-96hfw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008d3d80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008d3da0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.013: INFO: Pod "nginx-deployment-55fb7cb77f-chb5p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-chb5p,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-55fb7cb77f-chb5p,UID:4cee1077-2c72-48cf-81ea-aaa4b399fc75,ResourceVersion:1085799,Generation:0,CreationTimestamp:2019-07-31 11:28:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 91b0934e-791e-4a2c-89ec-9be86f2d10b6 0xc0008d3e27 0xc0008d3e28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-96hfw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008d3e90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008d3eb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.8,PodIP:172.25.25.17,StartTime:2019-07-31 11:28:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.013: INFO: Pod "nginx-deployment-55fb7cb77f-ck2n7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-ck2n7,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-55fb7cb77f-ck2n7,UID:8f26244b-f7ad-4389-9cb1-ac46665ca002,ResourceVersion:1085664,Generation:0,CreationTimestamp:2019-07-31 11:28:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 91b0934e-791e-4a2c-89ec-9be86f2d10b6 0xc0008d3fe0 0xc0008d3fe1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-h22s9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0005ce170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0005ce350}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.13,PodIP:,StartTime:2019-07-31 11:28:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.013: INFO: Pod "nginx-deployment-55fb7cb77f-dr4tj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-dr4tj,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-55fb7cb77f-dr4tj,UID:d1641024-19a3-44b9-98f4-0bfd8b435fe0,ResourceVersion:1085801,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 91b0934e-791e-4a2c-89ec-9be86f2d10b6 0xc0005ce547 0xc0005ce548}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-h22s9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0005cf070} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0005cf100}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.13,PodIP:,StartTime:2019-07-31 11:28:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.013: INFO: Pod "nginx-deployment-55fb7cb77f-gxngl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-gxngl,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-55fb7cb77f-gxngl,UID:0144575b-4063-403d-a16e-02c23e1232f2,ResourceVersion:1085808,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 91b0934e-791e-4a2c-89ec-9be86f2d10b6 0xc0005cf687 0xc0005cf688}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0005cf760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0005cf7a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:,StartTime:2019-07-31 11:28:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.013: INFO: Pod "nginx-deployment-55fb7cb77f-hblbd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hblbd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-55fb7cb77f-hblbd,UID:fda1b1ab-fc3b-4cdd-b19b-4305359bd781,ResourceVersion:1085743,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 91b0934e-791e-4a2c-89ec-9be86f2d10b6 0xc0005cfae7 0xc0005cfae8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-96hfw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0005cfe00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0005cff40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.013: INFO: Pod "nginx-deployment-55fb7cb77f-hhjff" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hhjff,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-55fb7cb77f-hhjff,UID:5a5801ec-b67f-4e83-b4dd-658e0144c546,ResourceVersion:1085738,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 91b0934e-791e-4a2c-89ec-9be86f2d10b6 0xc000f3a137 0xc000f3a138}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-96hfw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f3a1d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f3a210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.014: INFO: Pod "nginx-deployment-55fb7cb77f-hzdpr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hzdpr,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-55fb7cb77f-hzdpr,UID:b2f0af3b-1d50-451d-aaf5-ef960873e15e,ResourceVersion:1085790,Generation:0,CreationTimestamp:2019-07-31 11:28:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 91b0934e-791e-4a2c-89ec-9be86f2d10b6 0xc000f3a387 0xc000f3a388}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-96hfw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f3a550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f3a5b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.8,PodIP:172.25.25.18,StartTime:2019-07-31 11:28:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.014: INFO: Pod "nginx-deployment-55fb7cb77f-mtgf6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mtgf6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-55fb7cb77f-mtgf6,UID:b37e2f0c-f79a-4615-9d6f-72af907fd1cb,ResourceVersion:1085685,Generation:0,CreationTimestamp:2019-07-31 11:28:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 91b0934e-791e-4a2c-89ec-9be86f2d10b6 0xc000f3a6f0 0xc000f3a6f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f3a760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f3a780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:,StartTime:2019-07-31 11:28:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.014: INFO: Pod "nginx-deployment-55fb7cb77f-p49d6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-p49d6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-55fb7cb77f-p49d6,UID:bd6cb8c7-7284-4649-bbf4-364a93aca987,ResourceVersion:1085781,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 91b0934e-791e-4a2c-89ec-9be86f2d10b6 0xc000f3a867 0xc000f3a868}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-h22s9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00299f2d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00299f2f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.13,PodIP:,StartTime:2019-07-31 11:28:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.014: INFO: Pod "nginx-deployment-55fb7cb77f-qht5k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-qht5k,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-55fb7cb77f-qht5k,UID:843d03d1-704d-42b9-a3d5-cbf3ec3df3ed,ResourceVersion:1085681,Generation:0,CreationTimestamp:2019-07-31 11:28:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 91b0934e-791e-4a2c-89ec-9be86f2d10b6 0xc00299f3c7 0xc00299f3c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00299f430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00299f450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.26.46,StartTime:2019-07-31 11:28:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.014: INFO: Pod "nginx-deployment-55fb7cb77f-v6s7z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-v6s7z,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-55fb7cb77f-v6s7z,UID:102723c6-3aa6-4246-aa6e-802c9b3c316f,ResourceVersion:1085804,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 91b0934e-791e-4a2c-89ec-9be86f2d10b6 0xc00299f557 0xc00299f558}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-h22s9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00299f5c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00299f5e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.13,PodIP:,StartTime:2019-07-31 11:28:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.014: INFO: Pod "nginx-deployment-55fb7cb77f-z787q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-z787q,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-55fb7cb77f-z787q,UID:f7c91b84-f711-4540-8b7f-2990279b8765,ResourceVersion:1085739,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 91b0934e-791e-4a2c-89ec-9be86f2d10b6 0xc00299f6b7 0xc00299f6b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00299f720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00299f740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.014: INFO: Pod "nginx-deployment-7b8c6f4498-25r9z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-25r9z,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-25r9z,UID:6e685fd3-36f9-41af-ae10-24b563009f9e,ResourceVersion:1085553,Generation:0,CreationTimestamp:2019-07-31 11:28:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc00299f7c7 0xc00299f7c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-h22s9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00299f830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00299f850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.13,PodIP:172.25.24.16,StartTime:2019-07-31 11:28:04 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-31 11:28:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://14012e21d718bc79f1e0e89d6023546d9d1195439bc0e381598434a8d49e48b4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.014: INFO: Pod "nginx-deployment-7b8c6f4498-2slgl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-2slgl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-2slgl,UID:df69844d-3e48-4f72-acaf-e59257958df0,ResourceVersion:1085811,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc00299f927 0xc00299f928}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-96hfw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00299f990} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00299f9b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.8,PodIP:,StartTime:2019-07-31 11:28:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.014: INFO: Pod "nginx-deployment-7b8c6f4498-58l7b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-58l7b,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-58l7b,UID:0d14f0db-822a-4d9d-8d29-0efdcf9a8b43,ResourceVersion:1085794,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc00299fa87 0xc00299fa88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00299faf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00299fb10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:,StartTime:2019-07-31 11:28:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.015: INFO: Pod "nginx-deployment-7b8c6f4498-8mb69" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8mb69,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-8mb69,UID:5d7514c8-13dd-4358-8a5e-882ab5b16022,ResourceVersion:1085523,Generation:0,CreationTimestamp:2019-07-31 11:28:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc00299fbd7 0xc00299fbd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00299fc40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00299fc60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.26.42,StartTime:2019-07-31 11:28:03 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-31 11:28:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c6e05db60c52fbf4761269a812e6c6ff12e33c5865218d85702a056a8ef5c045}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.015: INFO: Pod "nginx-deployment-7b8c6f4498-d6qj2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-d6qj2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-d6qj2,UID:ef0a3dd3-fb50-4d89-a032-aa88f03666ad,ResourceVersion:1085532,Generation:0,CreationTimestamp:2019-07-31 11:28:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc00299fd37 0xc00299fd38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-h22s9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00299fda0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00299fdc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.13,PodIP:172.25.24.15,StartTime:2019-07-31 11:28:04 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-31 11:28:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://42c8f79787810130d60362c9f5c4b69831029d863b755ad61112ca89850febee}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.015: INFO: Pod "nginx-deployment-7b8c6f4498-dbgjr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dbgjr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-dbgjr,UID:0f7d4180-4bc9-475b-b3d8-952f45aa20cb,ResourceVersion:1085797,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc00299fe97 0xc00299fe98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-h22s9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00299ff00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00299ff20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.13,PodIP:,StartTime:2019-07-31 11:28:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.015: INFO: Pod "nginx-deployment-7b8c6f4498-ffw2q" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ffw2q,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-ffw2q,UID:0019978b-7071-4a2f-ad35-a481a8848f15,ResourceVersion:1085545,Generation:0,CreationTimestamp:2019-07-31 11:28:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc00299ffe7 0xc00299ffe8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-96hfw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f6a060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f6a080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.8,PodIP:172.25.25.14,StartTime:2019-07-31 11:28:04 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-31 11:28:10 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f6fcb5be2eff7f94d7dc20f124f1a07f5a194186909aeea090c7490fdae985da}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.015: INFO: Pod "nginx-deployment-7b8c6f4498-fwfdz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fwfdz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-fwfdz,UID:d971ca9c-e33f-4611-abea-715198aa56ea,ResourceVersion:1085578,Generation:0,CreationTimestamp:2019-07-31 11:28:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc000f6a167 0xc000f6a168}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f6a1f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f6a210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.26.44,StartTime:2019-07-31 11:28:03 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-31 11:28:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://db96c9f09068e4779638e679a7d7627cc4ffac6269dd058f8b3529608beab2e3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.015: INFO: Pod "nginx-deployment-7b8c6f4498-hmzf8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hmzf8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-hmzf8,UID:97752d68-f1c7-43de-b383-e3ff729d8cb7,ResourceVersion:1085733,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc000f6a2f7 0xc000f6a2f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-96hfw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f6a360} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f6a380}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.015: INFO: Pod "nginx-deployment-7b8c6f4498-hn8tm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hn8tm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-hn8tm,UID:25aa619c-633a-4c18-8cc8-d94421db9be4,ResourceVersion:1085719,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc000f6a407 0xc000f6a408}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-96hfw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f6a470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f6a490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.015: INFO: Pod "nginx-deployment-7b8c6f4498-jm7vh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jm7vh,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-jm7vh,UID:279ee3c3-e878-4e09-afae-ce6a3ecb04a7,ResourceVersion:1085800,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc000f6a517 0xc000f6a518}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f6a580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f6a5a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:,StartTime:2019-07-31 11:28:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.015: INFO: Pod "nginx-deployment-7b8c6f4498-k2kk2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-k2kk2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-k2kk2,UID:dc604dfd-62ed-4935-b15c-e10e54762fab,ResourceVersion:1085805,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc000f6a677 0xc000f6a678}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f6a6e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f6a700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:,StartTime:2019-07-31 11:28:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.016: INFO: Pod "nginx-deployment-7b8c6f4498-lnpbq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-lnpbq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-lnpbq,UID:5d91adab-d4d7-4029-a2c2-8fd980afe24e,ResourceVersion:1085810,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc000f6a7c7 0xc000f6a7c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-96hfw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f6a830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f6a850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.8,PodIP:,StartTime:2019-07-31 11:28:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.016: INFO: Pod "nginx-deployment-7b8c6f4498-mjwf5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mjwf5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-mjwf5,UID:7c0f2b5b-5090-4191-a727-04f13ed5d40e,ResourceVersion:1085776,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc000f6a917 0xc000f6a918}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f6a980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f6a9a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:,StartTime:2019-07-31 11:28:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.016: INFO: Pod "nginx-deployment-7b8c6f4498-ptsrq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ptsrq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-ptsrq,UID:f7643932-32b7-4993-a9a8-ba8ade2eeaed,ResourceVersion:1085556,Generation:0,CreationTimestamp:2019-07-31 11:28:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc000f6aa67 0xc000f6aa68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-h22s9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f6aad0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f6aaf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.13,PodIP:172.25.24.17,StartTime:2019-07-31 11:28:04 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-31 11:28:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://fe4d82e5ea7d6c1469f4f354fadf4ae29e1becab7646e6c117f153567fb5d12b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.016: INFO: Pod "nginx-deployment-7b8c6f4498-r9l2v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-r9l2v,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-r9l2v,UID:8132be03-7849-4d64-a409-2ad17dc857fc,ResourceVersion:1085730,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc000f6abc7 0xc000f6abc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-96hfw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f6ac30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f6ac50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.016: INFO: Pod "nginx-deployment-7b8c6f4498-rl4h6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rl4h6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-rl4h6,UID:da0c8ecd-19c1-443f-98c8-a46eeed1a7db,ResourceVersion:1085514,Generation:0,CreationTimestamp:2019-07-31 11:28:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc000f6acf7 0xc000f6acf8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f6ad70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f6ad90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.26.43,StartTime:2019-07-31 11:28:03 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-31 11:28:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4585ff0226b7eb26a2ef3fd8925d9ae5738522a5e897cfe118c6408f986427c1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.016: INFO: Pod "nginx-deployment-7b8c6f4498-scqxw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-scqxw,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-scqxw,UID:040d21c9-4351-4100-95f5-8cb44612d0c4,ResourceVersion:1085798,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc000f6ae67 0xc000f6ae68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-h22s9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f6aed0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f6aef0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.13,PodIP:,StartTime:2019-07-31 11:28:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.016: INFO: Pod "nginx-deployment-7b8c6f4498-ssks5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ssks5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-ssks5,UID:c0a4f779-b6d3-4fc2-8c80-207b0c74a33f,ResourceVersion:1085803,Generation:0,CreationTimestamp:2019-07-31 11:28:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc000f6afb7 0xc000f6afb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-h22s9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f6b020} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f6b040}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:25 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.13,PodIP:,StartTime:2019-07-31 11:28:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 31 11:28:29.016: INFO: Pod "nginx-deployment-7b8c6f4498-xbnsl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xbnsl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7936,SelfLink:/api/v1/namespaces/deployment-7936/pods/nginx-deployment-7b8c6f4498-xbnsl,UID:410cd9b5-370c-4089-8145-49c1aeeed7cf,ResourceVersion:1085549,Generation:0,CreationTimestamp:2019-07-31 11:28:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1815724c-04db-4c7f-af2e-1155a1c891c4 0xc000f6b107 0xc000f6b108}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dbhvn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbhvn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbhvn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f6b170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f6b190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:28:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.26.45,StartTime:2019-07-31 11:28:03 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-31 11:28:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8dbc875d300af305dca3b10a665b68133f4b2b730076e086326abb104f176a87}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:28:29.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7936" for this suite.
Jul 31 11:28:38.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:28:50.415: INFO: namespace deployment-7936 deletion completed in 20.900605363s

• [SLOW TEST:47.605 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:28:50.415: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 31 11:28:50.595: INFO: Waiting up to 5m0s for pod "pod-1e442405-7e59-4e6e-8932-174c9b40129a" in namespace "emptydir-2345" to be "success or failure"
Jul 31 11:28:50.709: INFO: Pod "pod-1e442405-7e59-4e6e-8932-174c9b40129a": Phase="Pending", Reason="", readiness=false. Elapsed: 113.802082ms
Jul 31 11:28:54.410: INFO: Pod "pod-1e442405-7e59-4e6e-8932-174c9b40129a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.814215185s
Jul 31 11:28:56.518: INFO: Pod "pod-1e442405-7e59-4e6e-8932-174c9b40129a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.922390553s
Jul 31 11:28:59.909: INFO: Pod "pod-1e442405-7e59-4e6e-8932-174c9b40129a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.313826432s
STEP: Saw pod success
Jul 31 11:28:59.909: INFO: Pod "pod-1e442405-7e59-4e6e-8932-174c9b40129a" satisfied condition "success or failure"
Jul 31 11:28:59.916: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-1e442405-7e59-4e6e-8932-174c9b40129a container test-container: <nil>
STEP: delete the pod
Jul 31 11:29:03.411: INFO: Waiting for pod pod-1e442405-7e59-4e6e-8932-174c9b40129a to disappear
Jul 31 11:29:03.420: INFO: Pod pod-1e442405-7e59-4e6e-8932-174c9b40129a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:29:03.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2345" for this suite.
Jul 31 11:29:10.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:29:13.310: INFO: namespace emptydir-2345 deletion completed in 9.879805993s

• [SLOW TEST:22.895 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:29:13.310: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5040
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 31 11:29:13.622: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 31 11:29:48.835: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.26.57:8080/dial?request=hostName&protocol=udp&host=172.25.26.56&port=8081&tries=1'] Namespace:pod-network-test-5040 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 11:29:48.835: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 11:29:53.812: INFO: Waiting for endpoints: map[]
Jul 31 11:29:54.210: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.26.57:8080/dial?request=hostName&protocol=udp&host=172.25.24.27&port=8081&tries=1'] Namespace:pod-network-test-5040 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 11:29:54.210: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 11:29:59.610: INFO: Waiting for endpoints: map[]
Jul 31 11:29:59.716: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.26.57:8080/dial?request=hostName&protocol=udp&host=172.25.25.27&port=8081&tries=1'] Namespace:pod-network-test-5040 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 11:29:59.716: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 11:30:05.209: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:30:05.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5040" for this suite.
Jul 31 11:30:29.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:30:40.509: INFO: namespace pod-network-test-5040 deletion completed in 35.192619092s

• [SLOW TEST:87.199 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:30:40.510: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-fab03817-b375-4a1d-9c53-3542e305aecd
STEP: Creating a pod to test consume secrets
Jul 31 11:30:41.316: INFO: Waiting up to 5m0s for pod "pod-secrets-3092ea5b-c021-4dc5-8776-631ee1a9500f" in namespace "secrets-2828" to be "success or failure"
Jul 31 11:30:43.510: INFO: Pod "pod-secrets-3092ea5b-c021-4dc5-8776-631ee1a9500f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.193387648s
Jul 31 11:30:45.715: INFO: Pod "pod-secrets-3092ea5b-c021-4dc5-8776-631ee1a9500f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.398528711s
Jul 31 11:30:47.910: INFO: Pod "pod-secrets-3092ea5b-c021-4dc5-8776-631ee1a9500f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.593292825s
Jul 31 11:30:50.118: INFO: Pod "pod-secrets-3092ea5b-c021-4dc5-8776-631ee1a9500f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.801430952s
STEP: Saw pod success
Jul 31 11:30:50.118: INFO: Pod "pod-secrets-3092ea5b-c021-4dc5-8776-631ee1a9500f" satisfied condition "success or failure"
Jul 31 11:30:50.409: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-secrets-3092ea5b-c021-4dc5-8776-631ee1a9500f container secret-env-test: <nil>
STEP: delete the pod
Jul 31 11:30:51.412: INFO: Waiting for pod pod-secrets-3092ea5b-c021-4dc5-8776-631ee1a9500f to disappear
Jul 31 11:30:51.609: INFO: Pod pod-secrets-3092ea5b-c021-4dc5-8776-631ee1a9500f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:30:51.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2828" for this suite.
Jul 31 11:30:58.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:31:07.809: INFO: namespace secrets-2828 deletion completed in 15.798043672s

• [SLOW TEST:27.300 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:31:07.810: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-e8fed6a6-bd5b-4fb8-b593-685e8e8ca3cc
STEP: Creating secret with name s-test-opt-upd-4a66ff88-bd31-437c-ac8d-c8b042040fa0
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e8fed6a6-bd5b-4fb8-b593-685e8e8ca3cc
STEP: Updating secret s-test-opt-upd-4a66ff88-bd31-437c-ac8d-c8b042040fa0
STEP: Creating secret with name s-test-opt-create-d7478c96-1990-4413-9c9b-90f8e24aa5d4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:32:40.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1625" for this suite.
Jul 31 11:33:04.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:33:09.027: INFO: namespace projected-1625 deletion completed in 27.916147033s

• [SLOW TEST:121.217 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:33:09.027: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 11:33:09.720: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32ccf881-441a-48c8-8f9e-7fde2853d746" in namespace "projected-8219" to be "success or failure"
Jul 31 11:33:09.813: INFO: Pod "downwardapi-volume-32ccf881-441a-48c8-8f9e-7fde2853d746": Phase="Pending", Reason="", readiness=false. Elapsed: 92.804753ms
Jul 31 11:33:11.909: INFO: Pod "downwardapi-volume-32ccf881-441a-48c8-8f9e-7fde2853d746": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188703366s
Jul 31 11:33:14.109: INFO: Pod "downwardapi-volume-32ccf881-441a-48c8-8f9e-7fde2853d746": Phase="Running", Reason="", readiness=true. Elapsed: 4.388938818s
Jul 31 11:33:16.413: INFO: Pod "downwardapi-volume-32ccf881-441a-48c8-8f9e-7fde2853d746": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.692931943s
STEP: Saw pod success
Jul 31 11:33:16.414: INFO: Pod "downwardapi-volume-32ccf881-441a-48c8-8f9e-7fde2853d746" satisfied condition "success or failure"
Jul 31 11:33:16.610: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downwardapi-volume-32ccf881-441a-48c8-8f9e-7fde2853d746 container client-container: <nil>
STEP: delete the pod
Jul 31 11:33:17.910: INFO: Waiting for pod downwardapi-volume-32ccf881-441a-48c8-8f9e-7fde2853d746 to disappear
Jul 31 11:33:18.109: INFO: Pod downwardapi-volume-32ccf881-441a-48c8-8f9e-7fde2853d746 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:33:18.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8219" for this suite.
Jul 31 11:33:25.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:33:30.058: INFO: namespace projected-8219 deletion completed in 11.03532144s

• [SLOW TEST:21.031 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:33:30.059: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 31 11:33:30.431: INFO: Waiting up to 5m0s for pod "downward-api-2d36e751-3255-4913-89a9-f3abf005c37d" in namespace "downward-api-2630" to be "success or failure"
Jul 31 11:33:30.445: INFO: Pod "downward-api-2d36e751-3255-4913-89a9-f3abf005c37d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.744836ms
Jul 31 11:33:32.609: INFO: Pod "downward-api-2d36e751-3255-4913-89a9-f3abf005c37d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.177942893s
Jul 31 11:33:34.759: INFO: Pod "downward-api-2d36e751-3255-4913-89a9-f3abf005c37d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.327868609s
Jul 31 11:33:37.014: INFO: Pod "downward-api-2d36e751-3255-4913-89a9-f3abf005c37d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.58280056s
Jul 31 11:33:39.210: INFO: Pod "downward-api-2d36e751-3255-4913-89a9-f3abf005c37d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.779068088s
STEP: Saw pod success
Jul 31 11:33:39.211: INFO: Pod "downward-api-2d36e751-3255-4913-89a9-f3abf005c37d" satisfied condition "success or failure"
Jul 31 11:33:39.509: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downward-api-2d36e751-3255-4913-89a9-f3abf005c37d container dapi-container: <nil>
STEP: delete the pod
Jul 31 11:33:40.654: INFO: Waiting for pod downward-api-2d36e751-3255-4913-89a9-f3abf005c37d to disappear
Jul 31 11:33:41.013: INFO: Pod downward-api-2d36e751-3255-4913-89a9-f3abf005c37d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:33:41.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2630" for this suite.
Jul 31 11:33:47.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:33:50.470: INFO: namespace downward-api-2630 deletion completed in 9.443735463s

• [SLOW TEST:20.411 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:33:50.472: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 31 11:33:51.610: INFO: Waiting up to 5m0s for pod "pod-f6b6f36a-68db-487d-86dc-d97bba566eff" in namespace "emptydir-3448" to be "success or failure"
Jul 31 11:33:51.617: INFO: Pod "pod-f6b6f36a-68db-487d-86dc-d97bba566eff": Phase="Pending", Reason="", readiness=false. Elapsed: 7.717106ms
Jul 31 11:33:54.323: INFO: Pod "pod-f6b6f36a-68db-487d-86dc-d97bba566eff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.713113677s
Jul 31 11:33:56.617: INFO: Pod "pod-f6b6f36a-68db-487d-86dc-d97bba566eff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.007045264s
Jul 31 11:33:59.409: INFO: Pod "pod-f6b6f36a-68db-487d-86dc-d97bba566eff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.799558017s
STEP: Saw pod success
Jul 31 11:33:59.409: INFO: Pod "pod-f6b6f36a-68db-487d-86dc-d97bba566eff" satisfied condition "success or failure"
Jul 31 11:33:59.416: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-f6b6f36a-68db-487d-86dc-d97bba566eff container test-container: <nil>
STEP: delete the pod
Jul 31 11:34:00.409: INFO: Waiting for pod pod-f6b6f36a-68db-487d-86dc-d97bba566eff to disappear
Jul 31 11:34:00.809: INFO: Pod pod-f6b6f36a-68db-487d-86dc-d97bba566eff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:34:00.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3448" for this suite.
Jul 31 11:34:07.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:34:11.492: INFO: namespace emptydir-3448 deletion completed in 10.080477508s

• [SLOW TEST:21.020 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:34:11.492: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:34:21.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7818" for this suite.
Jul 31 11:34:30.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:34:36.537: INFO: namespace kubelet-test-7818 deletion completed in 14.325811013s

• [SLOW TEST:25.045 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:34:36.537: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 11:34:37.036: INFO: Waiting up to 5m0s for pod "downwardapi-volume-facbe9c6-62e4-47ee-9b9b-8efa888a3294" in namespace "projected-296" to be "success or failure"
Jul 31 11:34:37.209: INFO: Pod "downwardapi-volume-facbe9c6-62e4-47ee-9b9b-8efa888a3294": Phase="Pending", Reason="", readiness=false. Elapsed: 173.369052ms
Jul 31 11:34:39.509: INFO: Pod "downwardapi-volume-facbe9c6-62e4-47ee-9b9b-8efa888a3294": Phase="Pending", Reason="", readiness=false. Elapsed: 2.473619804s
Jul 31 11:34:41.809: INFO: Pod "downwardapi-volume-facbe9c6-62e4-47ee-9b9b-8efa888a3294": Phase="Pending", Reason="", readiness=false. Elapsed: 4.773391851s
Jul 31 11:34:43.924: INFO: Pod "downwardapi-volume-facbe9c6-62e4-47ee-9b9b-8efa888a3294": Phase="Pending", Reason="", readiness=false. Elapsed: 6.888049897s
Jul 31 11:34:46.213: INFO: Pod "downwardapi-volume-facbe9c6-62e4-47ee-9b9b-8efa888a3294": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.176758353s
STEP: Saw pod success
Jul 31 11:34:46.213: INFO: Pod "downwardapi-volume-facbe9c6-62e4-47ee-9b9b-8efa888a3294" satisfied condition "success or failure"
Jul 31 11:34:46.220: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downwardapi-volume-facbe9c6-62e4-47ee-9b9b-8efa888a3294 container client-container: <nil>
STEP: delete the pod
Jul 31 11:34:46.675: INFO: Waiting for pod downwardapi-volume-facbe9c6-62e4-47ee-9b9b-8efa888a3294 to disappear
Jul 31 11:34:46.682: INFO: Pod downwardapi-volume-facbe9c6-62e4-47ee-9b9b-8efa888a3294 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:34:46.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-296" for this suite.
Jul 31 11:34:53.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:34:57.350: INFO: namespace projected-296 deletion completed in 10.539517449s

• [SLOW TEST:20.813 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:34:57.351: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 11:34:57.709: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 31 11:34:57.915: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 31 11:35:02.110: INFO: Creating deployment "test-rolling-update-deployment"
Jul 31 11:35:02.309: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 31 11:35:02.810: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 31 11:35:03.209: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700169702, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700169702, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700169702, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700169702, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 11:35:05.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700169702, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700169702, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700169702, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700169702, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 11:35:07.809: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 31 11:35:07.840: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-9424,SelfLink:/apis/apps/v1/namespaces/deployment-9424/deployments/test-rolling-update-deployment,UID:a9759ce7-9dfd-45e3-bd22-9f605045bd29,ResourceVersion:1087380,Generation:1,CreationTimestamp:2019-07-31 11:35:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-31 11:35:02 +0000 UTC 2019-07-31 11:35:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-31 11:35:06 +0000 UTC 2019-07-31 11:35:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 31 11:35:07.847: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-9424,SelfLink:/apis/apps/v1/namespaces/deployment-9424/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:be8ceee9-db07-4176-880a-3cfdfdc6ba3e,ResourceVersion:1087370,Generation:1,CreationTimestamp:2019-07-31 11:35:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment a9759ce7-9dfd-45e3-bd22-9f605045bd29 0xc001e20787 0xc001e20788}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 31 11:35:07.847: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 31 11:35:07.847: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-9424,SelfLink:/apis/apps/v1/namespaces/deployment-9424/replicasets/test-rolling-update-controller,UID:9013ae1e-61c2-47ed-b834-e2ca571d0433,ResourceVersion:1087379,Generation:2,CreationTimestamp:2019-07-31 11:34:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment a9759ce7-9dfd-45e3-bd22-9f605045bd29 0xc001e206b7 0xc001e206b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 31 11:35:07.855: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-672dl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-672dl,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-9424,SelfLink:/api/v1/namespaces/deployment-9424/pods/test-rolling-update-deployment-79f6b9d75c-672dl,UID:559255fa-a8d0-479f-aa15-362d6013a27d,ResourceVersion:1087369,Generation:0,CreationTimestamp:2019-07-31 11:35:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c be8ceee9-db07-4176-880a-3cfdfdc6ba3e 0xc001c2a8e7 0xc001c2a8e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95mxz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95mxz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-95mxz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c2a950} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c2a970}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:35:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:35:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:35:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:35:02 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.26.66,StartTime:2019-07-31 11:35:02 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-31 11:35:05 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://10e1ea00ad6412f4991d0adc12cfd5e127a2af90d9016b44c02f54b5ff531e2a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:35:07.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9424" for this suite.
Jul 31 11:35:14.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:35:22.328: INFO: namespace deployment-9424 deletion completed in 14.462534221s

• [SLOW TEST:24.977 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:35:22.328: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-ffef3a7e-1725-4979-9c50-6ff14b3e98d7
STEP: Creating a pod to test consume secrets
Jul 31 11:35:23.311: INFO: Waiting up to 5m0s for pod "pod-secrets-29bf26b8-5c42-4d59-820b-101c529db5fd" in namespace "secrets-7271" to be "success or failure"
Jul 31 11:35:23.319: INFO: Pod "pod-secrets-29bf26b8-5c42-4d59-820b-101c529db5fd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.037077ms
Jul 31 11:35:25.326: INFO: Pod "pod-secrets-29bf26b8-5c42-4d59-820b-101c529db5fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015297864s
Jul 31 11:35:28.009: INFO: Pod "pod-secrets-29bf26b8-5c42-4d59-820b-101c529db5fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.698607545s
STEP: Saw pod success
Jul 31 11:35:28.009: INFO: Pod "pod-secrets-29bf26b8-5c42-4d59-820b-101c529db5fd" satisfied condition "success or failure"
Jul 31 11:35:28.021: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-secrets-29bf26b8-5c42-4d59-820b-101c529db5fd container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 11:35:28.583: INFO: Waiting for pod pod-secrets-29bf26b8-5c42-4d59-820b-101c529db5fd to disappear
Jul 31 11:35:29.509: INFO: Pod pod-secrets-29bf26b8-5c42-4d59-820b-101c529db5fd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:35:29.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7271" for this suite.
Jul 31 11:35:37.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:35:40.839: INFO: namespace secrets-7271 deletion completed in 9.71854586s

• [SLOW TEST:18.511 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:35:40.841: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 31 11:35:41.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-5776'
Jul 31 11:35:42.425: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 31 11:35:42.425: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Jul 31 11:35:42.925: INFO: scanned /root for discovery docs: <nil>
Jul 31 11:35:42.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-5776'
Jul 31 11:36:02.407: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 31 11:36:02.407: INFO: stdout: "Created e2e-test-nginx-rc-a333b9545ac00852f939c0dc93101399\nScaling up e2e-test-nginx-rc-a333b9545ac00852f939c0dc93101399 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-a333b9545ac00852f939c0dc93101399 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-a333b9545ac00852f939c0dc93101399 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jul 31 11:36:02.407: INFO: stdout: "Created e2e-test-nginx-rc-a333b9545ac00852f939c0dc93101399\nScaling up e2e-test-nginx-rc-a333b9545ac00852f939c0dc93101399 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-a333b9545ac00852f939c0dc93101399 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-a333b9545ac00852f939c0dc93101399 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jul 31 11:36:02.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-5776'
Jul 31 11:36:02.620: INFO: stderr: ""
Jul 31 11:36:02.620: INFO: stdout: "e2e-test-nginx-rc-a333b9545ac00852f939c0dc93101399-4q2vl "
Jul 31 11:36:02.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods e2e-test-nginx-rc-a333b9545ac00852f939c0dc93101399-4q2vl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5776'
Jul 31 11:36:03.726: INFO: stderr: ""
Jul 31 11:36:03.726: INFO: stdout: "true"
Jul 31 11:36:03.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods e2e-test-nginx-rc-a333b9545ac00852f939c0dc93101399-4q2vl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5776'
Jul 31 11:36:05.229: INFO: stderr: ""
Jul 31 11:36:05.229: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jul 31 11:36:05.229: INFO: e2e-test-nginx-rc-a333b9545ac00852f939c0dc93101399-4q2vl is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Jul 31 11:36:05.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete rc e2e-test-nginx-rc --namespace=kubectl-5776'
Jul 31 11:36:05.643: INFO: stderr: ""
Jul 31 11:36:05.643: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:36:05.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5776" for this suite.
Jul 31 11:36:29.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:36:31.352: INFO: namespace kubectl-5776 deletion completed in 24.933307646s

• [SLOW TEST:50.511 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:36:31.354: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 31 11:36:31.844: INFO: Waiting up to 5m0s for pod "pod-20f25526-9b0d-4473-ad90-7129cf4a1b9f" in namespace "emptydir-4015" to be "success or failure"
Jul 31 11:36:32.509: INFO: Pod "pod-20f25526-9b0d-4473-ad90-7129cf4a1b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 664.492513ms
Jul 31 11:36:34.816: INFO: Pod "pod-20f25526-9b0d-4473-ad90-7129cf4a1b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.971269978s
Jul 31 11:36:36.918: INFO: Pod "pod-20f25526-9b0d-4473-ad90-7129cf4a1b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.073580042s
Jul 31 11:36:39.110: INFO: Pod "pod-20f25526-9b0d-4473-ad90-7129cf4a1b9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.265070173s
STEP: Saw pod success
Jul 31 11:36:39.110: INFO: Pod "pod-20f25526-9b0d-4473-ad90-7129cf4a1b9f" satisfied condition "success or failure"
Jul 31 11:36:39.123: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-20f25526-9b0d-4473-ad90-7129cf4a1b9f container test-container: <nil>
STEP: delete the pod
Jul 31 11:36:39.911: INFO: Waiting for pod pod-20f25526-9b0d-4473-ad90-7129cf4a1b9f to disappear
Jul 31 11:36:39.920: INFO: Pod pod-20f25526-9b0d-4473-ad90-7129cf4a1b9f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:36:39.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4015" for this suite.
Jul 31 11:36:46.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:36:48.022: INFO: namespace emptydir-4015 deletion completed in 8.092132253s

• [SLOW TEST:16.669 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:36:48.023: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 11:36:48.134: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c6557b49-9a49-42c7-b1dd-abb2357a0488" in namespace "downward-api-5586" to be "success or failure"
Jul 31 11:36:48.143: INFO: Pod "downwardapi-volume-c6557b49-9a49-42c7-b1dd-abb2357a0488": Phase="Pending", Reason="", readiness=false. Elapsed: 9.153526ms
Jul 31 11:36:50.217: INFO: Pod "downwardapi-volume-c6557b49-9a49-42c7-b1dd-abb2357a0488": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082852082s
Jul 31 11:36:52.509: INFO: Pod "downwardapi-volume-c6557b49-9a49-42c7-b1dd-abb2357a0488": Phase="Pending", Reason="", readiness=false. Elapsed: 4.37550416s
Jul 31 11:36:54.709: INFO: Pod "downwardapi-volume-c6557b49-9a49-42c7-b1dd-abb2357a0488": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.575548948s
STEP: Saw pod success
Jul 31 11:36:54.709: INFO: Pod "downwardapi-volume-c6557b49-9a49-42c7-b1dd-abb2357a0488" satisfied condition "success or failure"
Jul 31 11:36:55.110: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downwardapi-volume-c6557b49-9a49-42c7-b1dd-abb2357a0488 container client-container: <nil>
STEP: delete the pod
Jul 31 11:36:55.722: INFO: Waiting for pod downwardapi-volume-c6557b49-9a49-42c7-b1dd-abb2357a0488 to disappear
Jul 31 11:36:56.009: INFO: Pod downwardapi-volume-c6557b49-9a49-42c7-b1dd-abb2357a0488 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:36:56.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5586" for this suite.
Jul 31 11:37:02.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:37:06.473: INFO: namespace downward-api-5586 deletion completed in 10.262787842s

• [SLOW TEST:18.450 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:37:06.474: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-f9f88e2e-323c-46bb-8203-a4f966b2638c
STEP: Creating a pod to test consume secrets
Jul 31 11:37:07.710: INFO: Waiting up to 5m0s for pod "pod-secrets-eb7068df-cfeb-4555-b2d1-aac56bad438d" in namespace "secrets-4149" to be "success or failure"
Jul 31 11:37:07.827: INFO: Pod "pod-secrets-eb7068df-cfeb-4555-b2d1-aac56bad438d": Phase="Pending", Reason="", readiness=false. Elapsed: 116.818704ms
Jul 31 11:37:09.923: INFO: Pod "pod-secrets-eb7068df-cfeb-4555-b2d1-aac56bad438d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.212593341s
Jul 31 11:37:11.946: INFO: Pod "pod-secrets-eb7068df-cfeb-4555-b2d1-aac56bad438d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.236078415s
Jul 31 11:37:15.810: INFO: Pod "pod-secrets-eb7068df-cfeb-4555-b2d1-aac56bad438d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.100242858s
STEP: Saw pod success
Jul 31 11:37:15.810: INFO: Pod "pod-secrets-eb7068df-cfeb-4555-b2d1-aac56bad438d" satisfied condition "success or failure"
Jul 31 11:37:17.217: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-secrets-eb7068df-cfeb-4555-b2d1-aac56bad438d container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 11:37:17.509: INFO: Waiting for pod pod-secrets-eb7068df-cfeb-4555-b2d1-aac56bad438d to disappear
Jul 31 11:37:17.523: INFO: Pod pod-secrets-eb7068df-cfeb-4555-b2d1-aac56bad438d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:37:17.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4149" for this suite.
Jul 31 11:37:24.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:37:25.737: INFO: namespace secrets-4149 deletion completed in 8.175345971s

• [SLOW TEST:19.263 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:37:25.737: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 11:37:26.811: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8fc0f87-5332-4eec-ab4f-e83bef929606" in namespace "downward-api-7713" to be "success or failure"
Jul 31 11:37:26.914: INFO: Pod "downwardapi-volume-f8fc0f87-5332-4eec-ab4f-e83bef929606": Phase="Pending", Reason="", readiness=false. Elapsed: 102.830352ms
Jul 31 11:37:29.114: INFO: Pod "downwardapi-volume-f8fc0f87-5332-4eec-ab4f-e83bef929606": Phase="Pending", Reason="", readiness=false. Elapsed: 2.303092263s
Jul 31 11:37:31.729: INFO: Pod "downwardapi-volume-f8fc0f87-5332-4eec-ab4f-e83bef929606": Phase="Pending", Reason="", readiness=false. Elapsed: 4.918721861s
Jul 31 11:37:33.814: INFO: Pod "downwardapi-volume-f8fc0f87-5332-4eec-ab4f-e83bef929606": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.003564488s
STEP: Saw pod success
Jul 31 11:37:33.814: INFO: Pod "downwardapi-volume-f8fc0f87-5332-4eec-ab4f-e83bef929606" satisfied condition "success or failure"
Jul 31 11:37:33.824: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downwardapi-volume-f8fc0f87-5332-4eec-ab4f-e83bef929606 container client-container: <nil>
STEP: delete the pod
Jul 31 11:37:37.911: INFO: Waiting for pod downwardapi-volume-f8fc0f87-5332-4eec-ab4f-e83bef929606 to disappear
Jul 31 11:37:38.509: INFO: Pod downwardapi-volume-f8fc0f87-5332-4eec-ab4f-e83bef929606 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:37:38.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7713" for this suite.
Jul 31 11:37:45.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:37:53.909: INFO: namespace downward-api-7713 deletion completed in 14.890374624s

• [SLOW TEST:28.172 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:37:53.909: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Jul 31 11:37:55.135: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-7003" to be "success or failure"
Jul 31 11:37:55.147: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 12.593219ms
Jul 31 11:37:57.157: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021850378s
Jul 31 11:38:00.625: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 5.489828149s
Jul 31 11:38:03.309: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.174247364s
Jul 31 11:38:05.420: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.285545662s
STEP: Saw pod success
Jul 31 11:38:05.420: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul 31 11:38:05.428: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul 31 11:38:06.309: INFO: Waiting for pod pod-host-path-test to disappear
Jul 31 11:38:06.509: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:38:06.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-7003" for this suite.
Jul 31 11:38:13.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:38:15.652: INFO: namespace hostpath-7003 deletion completed in 8.840644985s

• [SLOW TEST:21.743 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:38:15.653: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:38:22.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1396" for this suite.
Jul 31 11:39:15.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:39:17.240: INFO: namespace kubelet-test-1396 deletion completed in 54.826425983s

• [SLOW TEST:61.588 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:39:17.243: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 31 11:39:17.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8203'
Jul 31 11:39:18.188: INFO: stderr: ""
Jul 31 11:39:18.188: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Jul 31 11:39:18.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete pods e2e-test-nginx-pod --namespace=kubectl-8203'
Jul 31 11:39:23.012: INFO: stderr: ""
Jul 31 11:39:23.012: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:39:23.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8203" for this suite.
Jul 31 11:39:30.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:39:37.275: INFO: namespace kubectl-8203 deletion completed in 14.064240985s

• [SLOW TEST:20.033 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:39:37.276: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:39:44.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6102" for this suite.
Jul 31 11:39:51.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:39:59.336: INFO: namespace namespaces-6102 deletion completed in 14.615836513s
STEP: Destroying namespace "nsdeletetest-9630" for this suite.
Jul 31 11:39:59.344: INFO: Namespace nsdeletetest-9630 was already deleted
STEP: Destroying namespace "nsdeletetest-8066" for this suite.
Jul 31 11:40:06.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:40:15.833: INFO: namespace nsdeletetest-8066 deletion completed in 16.48953517s

• [SLOW TEST:38.558 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:40:15.834: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 31 11:40:24.109: INFO: Successfully updated pod "pod-update-fb44cf3a-fa03-48dd-80cf-2253d5545ef5"
STEP: verifying the updated pod is in kubernetes
Jul 31 11:40:24.522: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:40:24.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4979" for this suite.
Jul 31 11:40:47.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:40:53.223: INFO: namespace pods-4979 deletion completed in 28.512382269s

• [SLOW TEST:37.389 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:40:53.225: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Jul 31 11:40:54.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 create -f - --namespace=kubectl-4204'
Jul 31 11:40:55.418: INFO: stderr: ""
Jul 31 11:40:55.418: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Jul 31 11:40:56.809: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 11:40:56.810: INFO: Found 0 / 1
Jul 31 11:40:57.425: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 11:40:57.425: INFO: Found 0 / 1
Jul 31 11:40:58.425: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 11:40:58.425: INFO: Found 0 / 1
Jul 31 11:41:00.025: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 11:41:00.025: INFO: Found 0 / 1
Jul 31 11:41:01.118: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 11:41:01.118: INFO: Found 1 / 1
Jul 31 11:41:01.118: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 31 11:41:01.131: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 11:41:01.131: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jul 31 11:41:01.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 logs redis-master-xr9w2 redis-master --namespace=kubectl-4204'
Jul 31 11:41:02.518: INFO: stderr: ""
Jul 31 11:41:02.518: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 31 Jul 11:40:58.428 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 31 Jul 11:40:58.428 # Server started, Redis version 3.2.12\n1:M 31 Jul 11:40:58.428 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 31 Jul 11:40:58.428 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jul 31 11:41:02.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 log redis-master-xr9w2 redis-master --namespace=kubectl-4204 --tail=1'
Jul 31 11:41:04.053: INFO: stderr: ""
Jul 31 11:41:04.053: INFO: stdout: "1:M 31 Jul 11:40:58.428 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jul 31 11:41:04.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 log redis-master-xr9w2 redis-master --namespace=kubectl-4204 --limit-bytes=1'
Jul 31 11:41:04.517: INFO: stderr: ""
Jul 31 11:41:04.517: INFO: stdout: " "
STEP: exposing timestamps
Jul 31 11:41:04.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 log redis-master-xr9w2 redis-master --namespace=kubectl-4204 --tail=1 --timestamps'
Jul 31 11:41:05.615: INFO: stderr: ""
Jul 31 11:41:05.615: INFO: stdout: "2019-07-31T11:40:58.430189436Z 1:M 31 Jul 11:40:58.428 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jul 31 11:41:08.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 log redis-master-xr9w2 redis-master --namespace=kubectl-4204 --since=1s'
Jul 31 11:41:08.816: INFO: stderr: ""
Jul 31 11:41:08.816: INFO: stdout: ""
Jul 31 11:41:08.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 log redis-master-xr9w2 redis-master --namespace=kubectl-4204 --since=24h'
Jul 31 11:41:10.416: INFO: stderr: ""
Jul 31 11:41:10.416: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 31 Jul 11:40:58.428 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 31 Jul 11:40:58.428 # Server started, Redis version 3.2.12\n1:M 31 Jul 11:40:58.428 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 31 Jul 11:40:58.428 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Jul 31 11:41:10.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete --grace-period=0 --force -f - --namespace=kubectl-4204'
Jul 31 11:41:10.848: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 11:41:10.848: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jul 31 11:41:10.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get rc,svc -l name=nginx --no-headers --namespace=kubectl-4204'
Jul 31 11:41:10.953: INFO: stderr: "No resources found.\n"
Jul 31 11:41:10.953: INFO: stdout: ""
Jul 31 11:41:10.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -l name=nginx --namespace=kubectl-4204 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 31 11:41:11.811: INFO: stderr: ""
Jul 31 11:41:11.811: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:41:11.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4204" for this suite.
Jul 31 11:41:34.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:41:40.760: INFO: namespace kubectl-4204 deletion completed in 28.441524391s

• [SLOW TEST:47.535 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:41:40.761: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:41:48.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4628" for this suite.
Jul 31 11:42:10.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:42:14.415: INFO: namespace replication-controller-4628 deletion completed in 25.688727393s

• [SLOW TEST:33.654 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:42:14.419: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jul 31 11:42:21.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec pod-sharedvolume-aebdafd6-d749-4b2e-84cc-f2d366969cb9 -c busybox-main-container --namespace=emptydir-4757 -- cat /usr/share/volumeshare/shareddata.txt'
Jul 31 11:42:28.210: INFO: stderr: ""
Jul 31 11:42:28.210: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:42:28.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4757" for this suite.
Jul 31 11:42:35.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:42:42.040: INFO: namespace emptydir-4757 deletion completed in 13.530110152s

• [SLOW TEST:27.622 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:42:42.040: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-b53b77bf-9f37-4fa2-b131-146fab2a1e8d
STEP: Creating a pod to test consume secrets
Jul 31 11:42:43.009: INFO: Waiting up to 5m0s for pod "pod-secrets-e4d812fe-bf16-4f9d-b716-1a2e53ed1c14" in namespace "secrets-5335" to be "success or failure"
Jul 31 11:42:43.710: INFO: Pod "pod-secrets-e4d812fe-bf16-4f9d-b716-1a2e53ed1c14": Phase="Pending", Reason="", readiness=false. Elapsed: 701.234302ms
Jul 31 11:42:46.009: INFO: Pod "pod-secrets-e4d812fe-bf16-4f9d-b716-1a2e53ed1c14": Phase="Pending", Reason="", readiness=false. Elapsed: 3.000404931s
Jul 31 11:42:48.509: INFO: Pod "pod-secrets-e4d812fe-bf16-4f9d-b716-1a2e53ed1c14": Phase="Pending", Reason="", readiness=false. Elapsed: 5.500245487s
Jul 31 11:42:50.909: INFO: Pod "pod-secrets-e4d812fe-bf16-4f9d-b716-1a2e53ed1c14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.900408524s
STEP: Saw pod success
Jul 31 11:42:50.909: INFO: Pod "pod-secrets-e4d812fe-bf16-4f9d-b716-1a2e53ed1c14" satisfied condition "success or failure"
Jul 31 11:42:51.209: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-secrets-e4d812fe-bf16-4f9d-b716-1a2e53ed1c14 container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 11:42:52.481: INFO: Waiting for pod pod-secrets-e4d812fe-bf16-4f9d-b716-1a2e53ed1c14 to disappear
Jul 31 11:42:52.497: INFO: Pod pod-secrets-e4d812fe-bf16-4f9d-b716-1a2e53ed1c14 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:42:52.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5335" for this suite.
Jul 31 11:42:59.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:43:09.232: INFO: namespace secrets-5335 deletion completed in 16.720241532s

• [SLOW TEST:27.192 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:43:09.235: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-c6a7bae2-a79b-4174-9d4b-ac0a51062c1c in namespace container-probe-5324
Jul 31 11:43:21.909: INFO: Started pod busybox-c6a7bae2-a79b-4174-9d4b-ac0a51062c1c in namespace container-probe-5324
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 11:43:22.509: INFO: Initial restart count of pod busybox-c6a7bae2-a79b-4174-9d4b-ac0a51062c1c is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:47:24.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5324" for this suite.
Jul 31 11:47:31.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:47:36.807: INFO: namespace container-probe-5324 deletion completed in 12.180933354s

• [SLOW TEST:267.572 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:47:36.807: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 31 11:47:47.709: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3640 pod-service-account-57b71b86-2898-4a3c-a31c-9e6b90a30f42 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 31 11:47:55.510: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3640 pod-service-account-57b71b86-2898-4a3c-a31c-9e6b90a30f42 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 31 11:48:00.417: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3640 pod-service-account-57b71b86-2898-4a3c-a31c-9e6b90a30f42 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:48:07.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3640" for this suite.
Jul 31 11:48:14.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:48:21.709: INFO: namespace svcaccounts-3640 deletion completed in 13.977026193s

• [SLOW TEST:44.902 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:48:21.711: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-725
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul 31 11:48:23.409: INFO: Found 1 stateful pods, waiting for 3
Jul 31 11:48:33.724: INFO: Found 2 stateful pods, waiting for 3
Jul 31 11:48:43.518: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:48:43.519: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:48:43.519: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 31 11:48:43.578: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 31 11:48:54.349: INFO: Updating stateful set ss2
Jul 31 11:48:54.376: INFO: Waiting for Pod statefulset-725/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Jul 31 11:49:05.809: INFO: Found 2 stateful pods, waiting for 3
Jul 31 11:49:16.509: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:49:16.509: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:49:16.509: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 31 11:49:26.809: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:49:26.809: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:49:26.809: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 31 11:49:28.109: INFO: Updating stateful set ss2
Jul 31 11:49:28.138: INFO: Waiting for Pod statefulset-725/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 31 11:49:38.262: INFO: Updating stateful set ss2
Jul 31 11:49:38.283: INFO: Waiting for StatefulSet statefulset-725/ss2 to complete update
Jul 31 11:49:38.283: INFO: Waiting for Pod statefulset-725/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 31 11:49:49.110: INFO: Waiting for StatefulSet statefulset-725/ss2 to complete update
Jul 31 11:49:49.110: INFO: Waiting for Pod statefulset-725/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 31 11:49:58.924: INFO: Waiting for StatefulSet statefulset-725/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 31 11:50:09.719: INFO: Deleting all statefulset in ns statefulset-725
Jul 31 11:50:10.109: INFO: Scaling statefulset ss2 to 0
Jul 31 11:50:30.714: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 11:50:30.721: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:50:31.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-725" for this suite.
Jul 31 11:50:40.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:50:44.934: INFO: namespace statefulset-725 deletion completed in 13.609236229s

• [SLOW TEST:143.222 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:50:44.934: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul 31 11:50:45.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 create -f - --namespace=kubectl-3366'
Jul 31 11:50:45.930: INFO: stderr: ""
Jul 31 11:50:45.930: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 11:50:45.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3366'
Jul 31 11:50:46.044: INFO: stderr: ""
Jul 31 11:50:46.044: INFO: stdout: "update-demo-nautilus-9pb9l update-demo-nautilus-pddtw "
Jul 31 11:50:46.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-9pb9l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3366'
Jul 31 11:50:46.236: INFO: stderr: ""
Jul 31 11:50:46.236: INFO: stdout: ""
Jul 31 11:50:46.236: INFO: update-demo-nautilus-9pb9l is created but not running
Jul 31 11:50:51.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3366'
Jul 31 11:50:51.351: INFO: stderr: ""
Jul 31 11:50:51.351: INFO: stdout: "update-demo-nautilus-9pb9l update-demo-nautilus-pddtw "
Jul 31 11:50:51.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-9pb9l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3366'
Jul 31 11:50:51.458: INFO: stderr: ""
Jul 31 11:50:51.458: INFO: stdout: ""
Jul 31 11:50:51.458: INFO: update-demo-nautilus-9pb9l is created but not running
Jul 31 11:50:56.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3366'
Jul 31 11:50:56.602: INFO: stderr: ""
Jul 31 11:50:56.603: INFO: stdout: "update-demo-nautilus-9pb9l update-demo-nautilus-pddtw "
Jul 31 11:50:56.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-9pb9l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3366'
Jul 31 11:50:56.857: INFO: stderr: ""
Jul 31 11:50:56.857: INFO: stdout: "true"
Jul 31 11:50:56.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-9pb9l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3366'
Jul 31 11:50:56.975: INFO: stderr: ""
Jul 31 11:50:56.975: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 11:50:56.975: INFO: validating pod update-demo-nautilus-9pb9l
Jul 31 11:50:58.109: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 11:50:58.109: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 11:50:58.109: INFO: update-demo-nautilus-9pb9l is verified up and running
Jul 31 11:50:58.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-pddtw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3366'
Jul 31 11:50:58.411: INFO: stderr: ""
Jul 31 11:50:58.411: INFO: stdout: "true"
Jul 31 11:50:58.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-pddtw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3366'
Jul 31 11:50:58.920: INFO: stderr: ""
Jul 31 11:50:58.920: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 11:50:58.920: INFO: validating pod update-demo-nautilus-pddtw
Jul 31 11:50:59.611: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 11:50:59.611: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 11:50:59.611: INFO: update-demo-nautilus-pddtw is verified up and running
STEP: using delete to clean up resources
Jul 31 11:50:59.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete --grace-period=0 --force -f - --namespace=kubectl-3366'
Jul 31 11:51:00.315: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 11:51:00.315: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 31 11:51:00.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3366'
Jul 31 11:51:00.455: INFO: stderr: "No resources found.\n"
Jul 31 11:51:00.455: INFO: stdout: ""
Jul 31 11:51:00.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -l name=update-demo --namespace=kubectl-3366 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 31 11:51:00.565: INFO: stderr: ""
Jul 31 11:51:00.565: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:51:00.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3366" for this suite.
Jul 31 11:51:23.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:51:26.312: INFO: namespace kubectl-3366 deletion completed in 25.598126027s

• [SLOW TEST:41.378 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:51:26.312: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-c0025083-6520-4328-8566-71b85d4c44e2
STEP: Creating a pod to test consume configMaps
Jul 31 11:51:27.122: INFO: Waiting up to 5m0s for pod "pod-configmaps-66c52425-a966-4b15-9fae-54f751201210" in namespace "configmap-1615" to be "success or failure"
Jul 31 11:51:27.144: INFO: Pod "pod-configmaps-66c52425-a966-4b15-9fae-54f751201210": Phase="Pending", Reason="", readiness=false. Elapsed: 21.341459ms
Jul 31 11:51:30.009: INFO: Pod "pod-configmaps-66c52425-a966-4b15-9fae-54f751201210": Phase="Pending", Reason="", readiness=false. Elapsed: 2.886851069s
Jul 31 11:51:32.209: INFO: Pod "pod-configmaps-66c52425-a966-4b15-9fae-54f751201210": Phase="Pending", Reason="", readiness=false. Elapsed: 5.086629919s
Jul 31 11:51:34.609: INFO: Pod "pod-configmaps-66c52425-a966-4b15-9fae-54f751201210": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.486782755s
STEP: Saw pod success
Jul 31 11:51:34.609: INFO: Pod "pod-configmaps-66c52425-a966-4b15-9fae-54f751201210" satisfied condition "success or failure"
Jul 31 11:51:34.809: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-configmaps-66c52425-a966-4b15-9fae-54f751201210 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 11:51:36.609: INFO: Waiting for pod pod-configmaps-66c52425-a966-4b15-9fae-54f751201210 to disappear
Jul 31 11:51:37.018: INFO: Pod pod-configmaps-66c52425-a966-4b15-9fae-54f751201210 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:51:37.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1615" for this suite.
Jul 31 11:51:44.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:51:52.126: INFO: namespace configmap-1615 deletion completed in 14.815481758s

• [SLOW TEST:25.815 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:51:52.128: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jul 31 11:51:59.720: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-280300738 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul 31 11:52:16.612: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:52:17.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8684" for this suite.
Jul 31 11:52:24.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:52:28.422: INFO: namespace pods-8684 deletion completed in 10.293890935s

• [SLOW TEST:36.294 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:52:28.423: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 31 11:52:28.731: INFO: Waiting up to 5m0s for pod "pod-13b6f1f6-3ffa-4553-90a8-1533f9c8efd0" in namespace "emptydir-6513" to be "success or failure"
Jul 31 11:52:28.741: INFO: Pod "pod-13b6f1f6-3ffa-4553-90a8-1533f9c8efd0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.411418ms
Jul 31 11:52:31.815: INFO: Pod "pod-13b6f1f6-3ffa-4553-90a8-1533f9c8efd0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.083750991s
Jul 31 11:52:34.221: INFO: Pod "pod-13b6f1f6-3ffa-4553-90a8-1533f9c8efd0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.489476168s
Jul 31 11:52:38.109: INFO: Pod "pod-13b6f1f6-3ffa-4553-90a8-1533f9c8efd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.378045579s
STEP: Saw pod success
Jul 31 11:52:38.109: INFO: Pod "pod-13b6f1f6-3ffa-4553-90a8-1533f9c8efd0" satisfied condition "success or failure"
Jul 31 11:52:39.016: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-13b6f1f6-3ffa-4553-90a8-1533f9c8efd0 container test-container: <nil>
STEP: delete the pod
Jul 31 11:52:39.309: INFO: Waiting for pod pod-13b6f1f6-3ffa-4553-90a8-1533f9c8efd0 to disappear
Jul 31 11:52:39.319: INFO: Pod pod-13b6f1f6-3ffa-4553-90a8-1533f9c8efd0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:52:39.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6513" for this suite.
Jul 31 11:52:46.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:52:51.962: INFO: namespace emptydir-6513 deletion completed in 12.449300097s

• [SLOW TEST:23.539 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:52:51.962: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-6076585c-608b-4d8e-9308-dee7782de511
STEP: Creating a pod to test consume secrets
Jul 31 11:52:53.127: INFO: Waiting up to 5m0s for pod "pod-secrets-855868be-29a9-4932-8ca7-c0f9961bff85" in namespace "secrets-9365" to be "success or failure"
Jul 31 11:52:53.141: INFO: Pod "pod-secrets-855868be-29a9-4932-8ca7-c0f9961bff85": Phase="Pending", Reason="", readiness=false. Elapsed: 13.860916ms
Jul 31 11:52:55.148: INFO: Pod "pod-secrets-855868be-29a9-4932-8ca7-c0f9961bff85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021255429s
Jul 31 11:52:58.313: INFO: Pod "pod-secrets-855868be-29a9-4932-8ca7-c0f9961bff85": Phase="Pending", Reason="", readiness=false. Elapsed: 5.186741545s
Jul 31 11:53:01.110: INFO: Pod "pod-secrets-855868be-29a9-4932-8ca7-c0f9961bff85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.983396389s
STEP: Saw pod success
Jul 31 11:53:01.112: INFO: Pod "pod-secrets-855868be-29a9-4932-8ca7-c0f9961bff85" satisfied condition "success or failure"
Jul 31 11:53:01.145: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-secrets-855868be-29a9-4932-8ca7-c0f9961bff85 container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 11:53:04.216: INFO: Waiting for pod pod-secrets-855868be-29a9-4932-8ca7-c0f9961bff85 to disappear
Jul 31 11:53:04.224: INFO: Pod pod-secrets-855868be-29a9-4932-8ca7-c0f9961bff85 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:53:04.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9365" for this suite.
Jul 31 11:53:10.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:53:14.497: INFO: namespace secrets-9365 deletion completed in 10.262450142s

• [SLOW TEST:22.535 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:53:14.498: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 11:53:14.910: INFO: Creating deployment "test-recreate-deployment"
Jul 31 11:53:15.120: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 31 11:53:15.160: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 31 11:53:20.579: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 31 11:53:20.594: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700170795, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700170795, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700170795, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700170795, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 11:53:22.709: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 31 11:53:23.110: INFO: Updating deployment test-recreate-deployment
Jul 31 11:53:23.110: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 31 11:53:23.509: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-9800,SelfLink:/apis/apps/v1/namespaces/deployment-9800/deployments/test-recreate-deployment,UID:73f4d783-bdf2-44b2-927c-1c0790e8d5b7,ResourceVersion:1091106,Generation:2,CreationTimestamp:2019-07-31 11:53:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-07-31 11:53:23 +0000 UTC 2019-07-31 11:53:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-31 11:53:23 +0000 UTC 2019-07-31 11:53:15 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jul 31 11:53:23.810: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-9800,SelfLink:/apis/apps/v1/namespaces/deployment-9800/replicasets/test-recreate-deployment-5c8c9cc69d,UID:6e047969-44ce-4747-ad46-7e11d7defa41,ResourceVersion:1091105,Generation:1,CreationTimestamp:2019-07-31 11:53:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 73f4d783-bdf2-44b2-927c-1c0790e8d5b7 0xc000c0bdc7 0xc000c0bdc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 31 11:53:23.810: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 31 11:53:23.810: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-9800,SelfLink:/apis/apps/v1/namespaces/deployment-9800/replicasets/test-recreate-deployment-6df85df6b9,UID:f559d44b-f296-48a5-a48f-f77976fb7660,ResourceVersion:1091095,Generation:2,CreationTimestamp:2019-07-31 11:53:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 73f4d783-bdf2-44b2-927c-1c0790e8d5b7 0xc000c0be97 0xc000c0be98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 31 11:53:24.110: INFO: Pod "test-recreate-deployment-5c8c9cc69d-6cxpt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-6cxpt,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-9800,SelfLink:/api/v1/namespaces/deployment-9800/pods/test-recreate-deployment-5c8c9cc69d-6cxpt,UID:0e26cec4-e6db-46a6-b206-7f813e3a2a2c,ResourceVersion:1091110,Generation:0,CreationTimestamp:2019-07-31 11:53:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 6e047969-44ce-4747-ad46-7e11d7defa41 0xc0023017a7 0xc0023017a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnznr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnznr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnznr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002301810} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002301830}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:53:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:53:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:53:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 11:53:23 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:,StartTime:2019-07-31 11:53:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:53:24.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9800" for this suite.
Jul 31 11:53:30.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:53:35.309: INFO: namespace deployment-9800 deletion completed in 10.898853722s

• [SLOW TEST:20.812 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:53:35.310: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Jul 31 11:53:40.811: INFO: Pod pod-hostip-e531328b-b534-41bb-bbc7-8ecab0aa9ba1 has hostIP: 192.168.1.12
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:53:40.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2352" for this suite.
Jul 31 11:54:03.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:54:06.884: INFO: namespace pods-2352 deletion completed in 26.056795588s

• [SLOW TEST:31.574 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:54:06.884: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jul 31 11:54:37.348: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0731 11:54:37.348491      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:54:37.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3545" for this suite.
Jul 31 11:54:43.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:54:46.123: INFO: namespace gc-3545 deletion completed in 8.405284758s

• [SLOW TEST:39.239 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:54:46.128: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 31 11:54:47.149: INFO: Waiting up to 5m0s for pod "downward-api-ad668633-1acf-4906-9fca-7416af564e5f" in namespace "downward-api-3615" to be "success or failure"
Jul 31 11:54:47.163: INFO: Pod "downward-api-ad668633-1acf-4906-9fca-7416af564e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.895237ms
Jul 31 11:54:49.172: INFO: Pod "downward-api-ad668633-1acf-4906-9fca-7416af564e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022617322s
Jul 31 11:54:51.213: INFO: Pod "downward-api-ad668633-1acf-4906-9fca-7416af564e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063489787s
Jul 31 11:54:54.009: INFO: Pod "downward-api-ad668633-1acf-4906-9fca-7416af564e5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.859586686s
STEP: Saw pod success
Jul 31 11:54:54.010: INFO: Pod "downward-api-ad668633-1acf-4906-9fca-7416af564e5f" satisfied condition "success or failure"
Jul 31 11:54:54.210: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downward-api-ad668633-1acf-4906-9fca-7416af564e5f container dapi-container: <nil>
STEP: delete the pod
Jul 31 11:54:54.810: INFO: Waiting for pod downward-api-ad668633-1acf-4906-9fca-7416af564e5f to disappear
Jul 31 11:54:54.919: INFO: Pod downward-api-ad668633-1acf-4906-9fca-7416af564e5f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:54:54.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3615" for this suite.
Jul 31 11:55:00.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:55:02.717: INFO: namespace downward-api-3615 deletion completed in 7.785038177s

• [SLOW TEST:16.589 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:55:02.717: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 31 11:55:04.809: INFO: Number of nodes with available pods: 0
Jul 31 11:55:04.809: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 11:55:06.124: INFO: Number of nodes with available pods: 0
Jul 31 11:55:06.124: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 11:55:06.941: INFO: Number of nodes with available pods: 0
Jul 31 11:55:06.941: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 11:55:08.509: INFO: Number of nodes with available pods: 1
Jul 31 11:55:08.509: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 11:55:08.930: INFO: Number of nodes with available pods: 3
Jul 31 11:55:08.930: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 31 11:55:09.011: INFO: Number of nodes with available pods: 2
Jul 31 11:55:09.011: INFO: Node worker-2bh9r-78c4c5b4fb-czrvz is running more than one daemon pod
Jul 31 11:55:10.409: INFO: Number of nodes with available pods: 2
Jul 31 11:55:10.409: INFO: Node worker-2bh9r-78c4c5b4fb-czrvz is running more than one daemon pod
Jul 31 11:55:11.217: INFO: Number of nodes with available pods: 2
Jul 31 11:55:11.217: INFO: Node worker-2bh9r-78c4c5b4fb-czrvz is running more than one daemon pod
Jul 31 11:55:12.030: INFO: Number of nodes with available pods: 2
Jul 31 11:55:12.030: INFO: Node worker-2bh9r-78c4c5b4fb-czrvz is running more than one daemon pod
Jul 31 11:55:13.215: INFO: Number of nodes with available pods: 3
Jul 31 11:55:13.215: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6735, will wait for the garbage collector to delete the pods
Jul 31 11:55:13.509: INFO: Deleting DaemonSet.extensions daemon-set took: 216.714823ms
Jul 31 11:55:13.909: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.484236ms
Jul 31 11:55:24.709: INFO: Number of nodes with available pods: 0
Jul 31 11:55:24.709: INFO: Number of running nodes: 0, number of available pods: 0
Jul 31 11:55:24.910: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6735/daemonsets","resourceVersion":"1091612"},"items":null}

Jul 31 11:55:25.109: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6735/pods","resourceVersion":"1091613"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:55:26.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6735" for this suite.
Jul 31 11:55:33.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:55:36.415: INFO: namespace daemonsets-6735 deletion completed in 10.104404497s

• [SLOW TEST:33.698 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:55:36.416: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 31 11:55:37.711: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 31 11:55:39.325: INFO: Waiting for terminating namespaces to be deleted...
Jul 31 11:55:39.335: INFO: 
Logging pods the kubelet thinks is on node worker-2bh9r-78c4c5b4fb-96hfw before test
Jul 31 11:55:40.116: INFO: kube-proxy-khr7h from kube-system started at 2019-07-29 14:52:36 +0000 UTC (1 container statuses recorded)
Jul 31 11:55:40.116: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:55:40.116: INFO: openvpn-client-84b8ff9b6f-h6ccx from kube-system started at 2019-07-29 14:53:19 +0000 UTC (2 container statuses recorded)
Jul 31 11:55:40.116: INFO: 	Container dnat-controller ready: true, restart count 0
Jul 31 11:55:40.118: INFO: 	Container openvpn-client ready: true, restart count 0
Jul 31 11:55:40.118: INFO: sonobuoy-e2e-job-1dfc7533d0924fc3 from heptio-sonobuoy started at 2019-07-31 11:06:29 +0000 UTC (2 container statuses recorded)
Jul 31 11:55:40.118: INFO: 	Container e2e ready: true, restart count 0
Jul 31 11:55:40.118: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:55:40.118: INFO: canal-dtm6f from kube-system started at 2019-07-29 14:52:35 +0000 UTC (2 container statuses recorded)
Jul 31 11:55:40.118: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 11:55:40.118: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 31 11:55:40.118: INFO: coredns-9b6865ff9-kcflr from kube-system started at 2019-07-29 14:53:19 +0000 UTC (1 container statuses recorded)
Jul 31 11:55:40.118: INFO: 	Container coredns ready: true, restart count 0
Jul 31 11:55:40.118: INFO: node-exporter-h5vfd from kube-system started at 2019-07-29 14:52:36 +0000 UTC (2 container statuses recorded)
Jul 31 11:55:40.118: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 31 11:55:40.118: INFO: 	Container node-exporter ready: true, restart count 0
Jul 31 11:55:40.118: INFO: node-local-dns-rv254 from kube-system started at 2019-07-29 14:53:16 +0000 UTC (1 container statuses recorded)
Jul 31 11:55:40.118: INFO: 	Container node-cache ready: true, restart count 0
Jul 31 11:55:40.118: INFO: container-linux-update-agent-htx9q from kube-system started at 2019-07-29 14:53:16 +0000 UTC (1 container statuses recorded)
Jul 31 11:55:40.118: INFO: 	Container update-agent ready: true, restart count 1
Jul 31 11:55:40.119: INFO: sonobuoy-systemd-logs-daemon-set-adffb9aed1fd4488-8wclh from heptio-sonobuoy started at 2019-07-31 11:06:29 +0000 UTC (2 container statuses recorded)
Jul 31 11:55:40.119: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:55:40.119: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:55:40.119: INFO: 
Logging pods the kubelet thinks is on node worker-2bh9r-78c4c5b4fb-czrvz before test
Jul 31 11:55:42.311: INFO: node-exporter-c4vlv from kube-system started at 2019-07-29 14:55:03 +0000 UTC (2 container statuses recorded)
Jul 31 11:55:42.311: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 31 11:55:42.311: INFO: 	Container node-exporter ready: true, restart count 0
Jul 31 11:55:42.311: INFO: canal-fjc6d from kube-system started at 2019-07-29 14:55:03 +0000 UTC (2 container statuses recorded)
Jul 31 11:55:42.311: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 11:55:42.311: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 31 11:55:42.312: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-31 11:06:15 +0000 UTC (1 container statuses recorded)
Jul 31 11:55:42.312: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 31 11:55:42.312: INFO: sonobuoy-systemd-logs-daemon-set-adffb9aed1fd4488-4vrw8 from heptio-sonobuoy started at 2019-07-31 11:06:29 +0000 UTC (2 container statuses recorded)
Jul 31 11:55:42.312: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:55:42.312: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:55:42.312: INFO: container-linux-update-agent-5j26n from kube-system started at 2019-07-29 14:55:48 +0000 UTC (1 container statuses recorded)
Jul 31 11:55:42.312: INFO: 	Container update-agent ready: true, restart count 0
Jul 31 11:55:42.312: INFO: node-local-dns-8r55k from kube-system started at 2019-07-29 14:55:48 +0000 UTC (1 container statuses recorded)
Jul 31 11:55:42.312: INFO: 	Container node-cache ready: true, restart count 0
Jul 31 11:55:42.312: INFO: kubernetes-dashboard-584d5ffc75-xxvnm from kube-system started at 2019-07-29 14:55:51 +0000 UTC (1 container statuses recorded)
Jul 31 11:55:42.312: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 31 11:55:42.312: INFO: kube-proxy-q8bhf from kube-system started at 2019-07-29 14:55:04 +0000 UTC (1 container statuses recorded)
Jul 31 11:55:42.312: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:55:42.312: INFO: 
Logging pods the kubelet thinks is on node worker-2bh9r-78c4c5b4fb-h22s9 before test
Jul 31 11:55:44.212: INFO: node-local-dns-4slzn from kube-system started at 2019-07-29 14:50:30 +0000 UTC (1 container statuses recorded)
Jul 31 11:55:44.212: INFO: 	Container node-cache ready: true, restart count 0
Jul 31 11:55:44.212: INFO: container-linux-update-agent-zsb6g from kube-system started at 2019-07-29 14:50:30 +0000 UTC (1 container statuses recorded)
Jul 31 11:55:44.212: INFO: 	Container update-agent ready: true, restart count 1
Jul 31 11:55:44.212: INFO: container-linux-update-operator-55d5b65b4-tgkz8 from kube-system started at 2019-07-29 14:50:33 +0000 UTC (1 container statuses recorded)
Jul 31 11:55:44.212: INFO: 	Container update-operator ready: true, restart count 0
Jul 31 11:55:44.212: INFO: sonobuoy-systemd-logs-daemon-set-adffb9aed1fd4488-8m2w7 from heptio-sonobuoy started at 2019-07-31 11:06:30 +0000 UTC (2 container statuses recorded)
Jul 31 11:55:44.212: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:55:44.212: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:55:44.212: INFO: kube-proxy-jz7bw from kube-system started at 2019-07-29 14:49:48 +0000 UTC (1 container statuses recorded)
Jul 31 11:55:44.212: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:55:44.212: INFO: node-exporter-cg6lq from kube-system started at 2019-07-29 14:49:48 +0000 UTC (2 container statuses recorded)
Jul 31 11:55:44.212: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 31 11:55:44.212: INFO: 	Container node-exporter ready: true, restart count 0
Jul 31 11:55:44.212: INFO: coredns-9b6865ff9-qh2tz from kube-system started at 2019-07-29 14:55:52 +0000 UTC (1 container statuses recorded)
Jul 31 11:55:44.212: INFO: 	Container coredns ready: true, restart count 0
Jul 31 11:55:44.212: INFO: canal-jmmlf from kube-system started at 2019-07-29 14:49:48 +0000 UTC (2 container statuses recorded)
Jul 31 11:55:44.212: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 11:55:44.212: INFO: 	Container kube-flannel ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node worker-2bh9r-78c4c5b4fb-96hfw
STEP: verifying the node has the label node worker-2bh9r-78c4c5b4fb-czrvz
STEP: verifying the node has the label node worker-2bh9r-78c4c5b4fb-h22s9
Jul 31 11:55:44.826: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-2bh9r-78c4c5b4fb-czrvz
Jul 31 11:55:44.826: INFO: Pod sonobuoy-e2e-job-1dfc7533d0924fc3 requesting resource cpu=0m on Node worker-2bh9r-78c4c5b4fb-96hfw
Jul 31 11:55:44.826: INFO: Pod sonobuoy-systemd-logs-daemon-set-adffb9aed1fd4488-4vrw8 requesting resource cpu=0m on Node worker-2bh9r-78c4c5b4fb-czrvz
Jul 31 11:55:44.826: INFO: Pod sonobuoy-systemd-logs-daemon-set-adffb9aed1fd4488-8m2w7 requesting resource cpu=0m on Node worker-2bh9r-78c4c5b4fb-h22s9
Jul 31 11:55:44.826: INFO: Pod sonobuoy-systemd-logs-daemon-set-adffb9aed1fd4488-8wclh requesting resource cpu=0m on Node worker-2bh9r-78c4c5b4fb-96hfw
Jul 31 11:55:44.826: INFO: Pod canal-dtm6f requesting resource cpu=350m on Node worker-2bh9r-78c4c5b4fb-96hfw
Jul 31 11:55:44.826: INFO: Pod canal-fjc6d requesting resource cpu=350m on Node worker-2bh9r-78c4c5b4fb-czrvz
Jul 31 11:55:44.826: INFO: Pod canal-jmmlf requesting resource cpu=350m on Node worker-2bh9r-78c4c5b4fb-h22s9
Jul 31 11:55:44.826: INFO: Pod container-linux-update-agent-5j26n requesting resource cpu=0m on Node worker-2bh9r-78c4c5b4fb-czrvz
Jul 31 11:55:44.826: INFO: Pod container-linux-update-agent-htx9q requesting resource cpu=0m on Node worker-2bh9r-78c4c5b4fb-96hfw
Jul 31 11:55:44.827: INFO: Pod container-linux-update-agent-zsb6g requesting resource cpu=0m on Node worker-2bh9r-78c4c5b4fb-h22s9
Jul 31 11:55:44.827: INFO: Pod container-linux-update-operator-55d5b65b4-tgkz8 requesting resource cpu=0m on Node worker-2bh9r-78c4c5b4fb-h22s9
Jul 31 11:55:44.827: INFO: Pod coredns-9b6865ff9-kcflr requesting resource cpu=100m on Node worker-2bh9r-78c4c5b4fb-96hfw
Jul 31 11:55:44.827: INFO: Pod coredns-9b6865ff9-qh2tz requesting resource cpu=100m on Node worker-2bh9r-78c4c5b4fb-h22s9
Jul 31 11:55:44.827: INFO: Pod kube-proxy-jz7bw requesting resource cpu=75m on Node worker-2bh9r-78c4c5b4fb-h22s9
Jul 31 11:55:44.827: INFO: Pod kube-proxy-khr7h requesting resource cpu=75m on Node worker-2bh9r-78c4c5b4fb-96hfw
Jul 31 11:55:44.827: INFO: Pod kube-proxy-q8bhf requesting resource cpu=75m on Node worker-2bh9r-78c4c5b4fb-czrvz
Jul 31 11:55:44.827: INFO: Pod kubernetes-dashboard-584d5ffc75-xxvnm requesting resource cpu=75m on Node worker-2bh9r-78c4c5b4fb-czrvz
Jul 31 11:55:44.827: INFO: Pod node-exporter-c4vlv requesting resource cpu=20m on Node worker-2bh9r-78c4c5b4fb-czrvz
Jul 31 11:55:44.827: INFO: Pod node-exporter-cg6lq requesting resource cpu=20m on Node worker-2bh9r-78c4c5b4fb-h22s9
Jul 31 11:55:44.827: INFO: Pod node-exporter-h5vfd requesting resource cpu=20m on Node worker-2bh9r-78c4c5b4fb-96hfw
Jul 31 11:55:44.827: INFO: Pod node-local-dns-4slzn requesting resource cpu=25m on Node worker-2bh9r-78c4c5b4fb-h22s9
Jul 31 11:55:44.827: INFO: Pod node-local-dns-8r55k requesting resource cpu=25m on Node worker-2bh9r-78c4c5b4fb-czrvz
Jul 31 11:55:44.827: INFO: Pod node-local-dns-rv254 requesting resource cpu=25m on Node worker-2bh9r-78c4c5b4fb-96hfw
Jul 31 11:55:44.827: INFO: Pod openvpn-client-84b8ff9b6f-h6ccx requesting resource cpu=30m on Node worker-2bh9r-78c4c5b4fb-96hfw
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4df9d1a-935f-4e05-a629-33ddab8e5888.15b67be5424e3105], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3495/filler-pod-b4df9d1a-935f-4e05-a629-33ddab8e5888 to worker-2bh9r-78c4c5b4fb-czrvz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4df9d1a-935f-4e05-a629-33ddab8e5888.15b67be5b98b1097], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4df9d1a-935f-4e05-a629-33ddab8e5888.15b67be5ce3b1061], Reason = [Created], Message = [Created container filler-pod-b4df9d1a-935f-4e05-a629-33ddab8e5888]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b4df9d1a-935f-4e05-a629-33ddab8e5888.15b67be5e113edb0], Reason = [Started], Message = [Started container filler-pod-b4df9d1a-935f-4e05-a629-33ddab8e5888]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dca138a4-445d-4c36-903b-e201156d1467.15b67be5377db53e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3495/filler-pod-dca138a4-445d-4c36-903b-e201156d1467 to worker-2bh9r-78c4c5b4fb-96hfw]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dca138a4-445d-4c36-903b-e201156d1467.15b67be5d06f75b0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dca138a4-445d-4c36-903b-e201156d1467.15b67be5ec6bd3f8], Reason = [Created], Message = [Created container filler-pod-dca138a4-445d-4c36-903b-e201156d1467]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dca138a4-445d-4c36-903b-e201156d1467.15b67be603672c44], Reason = [Started], Message = [Started container filler-pod-dca138a4-445d-4c36-903b-e201156d1467]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eeaaf25f-a90e-464a-92a1-3813541ad7e8.15b67be545231d0b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3495/filler-pod-eeaaf25f-a90e-464a-92a1-3813541ad7e8 to worker-2bh9r-78c4c5b4fb-h22s9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eeaaf25f-a90e-464a-92a1-3813541ad7e8.15b67be6242670bc], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eeaaf25f-a90e-464a-92a1-3813541ad7e8.15b67be63aba196a], Reason = [Created], Message = [Created container filler-pod-eeaaf25f-a90e-464a-92a1-3813541ad7e8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eeaaf25f-a90e-464a-92a1-3813541ad7e8.15b67be64d7ac8ff], Reason = [Started], Message = [Started container filler-pod-eeaaf25f-a90e-464a-92a1-3813541ad7e8]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15b67be6b2943f21], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node worker-2bh9r-78c4c5b4fb-96hfw
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-2bh9r-78c4c5b4fb-czrvz
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-2bh9r-78c4c5b4fb-h22s9
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:55:55.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3495" for this suite.
Jul 31 11:56:01.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:56:07.345: INFO: namespace sched-pred-3495 deletion completed in 12.327370255s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:30.929 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:56:07.345: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 11:56:08.833: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e351e59-8f7b-4381-8f8c-6f9175b9302a" in namespace "projected-5572" to be "success or failure"
Jul 31 11:56:08.846: INFO: Pod "downwardapi-volume-1e351e59-8f7b-4381-8f8c-6f9175b9302a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.557858ms
Jul 31 11:56:11.009: INFO: Pod "downwardapi-volume-1e351e59-8f7b-4381-8f8c-6f9175b9302a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.175893094s
Jul 31 11:56:13.609: INFO: Pod "downwardapi-volume-1e351e59-8f7b-4381-8f8c-6f9175b9302a": Phase="Running", Reason="", readiness=true. Elapsed: 4.775804631s
Jul 31 11:56:15.914: INFO: Pod "downwardapi-volume-1e351e59-8f7b-4381-8f8c-6f9175b9302a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.081221273s
STEP: Saw pod success
Jul 31 11:56:15.914: INFO: Pod "downwardapi-volume-1e351e59-8f7b-4381-8f8c-6f9175b9302a" satisfied condition "success or failure"
Jul 31 11:56:15.921: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downwardapi-volume-1e351e59-8f7b-4381-8f8c-6f9175b9302a container client-container: <nil>
STEP: delete the pod
Jul 31 11:56:16.810: INFO: Waiting for pod downwardapi-volume-1e351e59-8f7b-4381-8f8c-6f9175b9302a to disappear
Jul 31 11:56:16.817: INFO: Pod downwardapi-volume-1e351e59-8f7b-4381-8f8c-6f9175b9302a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:56:16.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5572" for this suite.
Jul 31 11:56:24.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:56:26.154: INFO: namespace projected-5572 deletion completed in 9.325790053s

• [SLOW TEST:18.809 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:56:26.154: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-1c23a31c-e6e9-489b-9e5a-926173a012aa
STEP: Creating configMap with name cm-test-opt-upd-19f3a41d-ab7e-461a-9010-8818497e5244
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1c23a31c-e6e9-489b-9e5a-926173a012aa
STEP: Updating configmap cm-test-opt-upd-19f3a41d-ab7e-461a-9010-8818497e5244
STEP: Creating configMap with name cm-test-opt-create-14173884-316b-4215-b1a3-1d6d877ff556
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:57:53.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7167" for this suite.
Jul 31 11:58:16.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:58:17.859: INFO: namespace projected-7167 deletion completed in 24.14816487s

• [SLOW TEST:111.705 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:58:17.859: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-3bf78edc-c321-495e-8de5-5ce06a46f782
STEP: Creating a pod to test consume configMaps
Jul 31 11:58:20.431: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bbdcd074-ac4d-4a4c-8d21-dbbaf996516b" in namespace "projected-5697" to be "success or failure"
Jul 31 11:58:20.442: INFO: Pod "pod-projected-configmaps-bbdcd074-ac4d-4a4c-8d21-dbbaf996516b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.370136ms
Jul 31 11:58:22.620: INFO: Pod "pod-projected-configmaps-bbdcd074-ac4d-4a4c-8d21-dbbaf996516b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18814212s
Jul 31 11:58:25.309: INFO: Pod "pod-projected-configmaps-bbdcd074-ac4d-4a4c-8d21-dbbaf996516b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.877920395s
Jul 31 11:58:27.510: INFO: Pod "pod-projected-configmaps-bbdcd074-ac4d-4a4c-8d21-dbbaf996516b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.078392733s
STEP: Saw pod success
Jul 31 11:58:27.510: INFO: Pod "pod-projected-configmaps-bbdcd074-ac4d-4a4c-8d21-dbbaf996516b" satisfied condition "success or failure"
Jul 31 11:58:27.709: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-projected-configmaps-bbdcd074-ac4d-4a4c-8d21-dbbaf996516b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 11:58:28.209: INFO: Waiting for pod pod-projected-configmaps-bbdcd074-ac4d-4a4c-8d21-dbbaf996516b to disappear
Jul 31 11:58:28.509: INFO: Pod pod-projected-configmaps-bbdcd074-ac4d-4a4c-8d21-dbbaf996516b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:58:28.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5697" for this suite.
Jul 31 11:58:36.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:58:39.717: INFO: namespace projected-5697 deletion completed in 10.905511434s

• [SLOW TEST:21.857 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:58:39.717: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Jul 31 11:58:40.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 cluster-info'
Jul 31 11:58:42.355: INFO: stderr: ""
Jul 31 11:58:42.355: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.10.10.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.10.10.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:58:42.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3513" for this suite.
Jul 31 11:58:49.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:58:50.568: INFO: namespace kubectl-3513 deletion completed in 8.144953381s

• [SLOW TEST:10.851 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:58:50.568: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 31 11:58:51.825: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 11:59:05.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4943" for this suite.
Jul 31 11:59:11.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 11:59:20.067: INFO: namespace pods-4943 deletion completed in 14.549898517s

• [SLOW TEST:29.499 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 11:59:20.068: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 31 11:59:21.218: INFO: PodSpec: initContainers in spec.initContainers
Jul 31 12:00:17.916: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-eda7df00-76d9-4836-93dd-251658a9ffeb", GenerateName:"", Namespace:"init-container-3187", SelfLink:"/api/v1/namespaces/init-container-3187/pods/pod-init-eda7df00-76d9-4836-93dd-251658a9ffeb", UID:"4dab0f31-7751-4524-aa9b-e54cc953b2a1", ResourceVersion:"1092588", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63700171161, loc:(*time.Location)(0x80bb5c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"218211536"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-7474b", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000315680), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7474b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7474b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7474b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001947ea8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker-2bh9r-78c4c5b4fb-h22s9", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0028f8840), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001947f20)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001947f40)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001947f48), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001947f4c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700171161, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700171161, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700171161, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700171161, loc:(*time.Location)(0x80bb5c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.13", PodIP:"172.25.24.34", StartTime:(*v1.Time)(0xc0029f0d20), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000f585b0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000f58620)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://084acf8530236ff84e5ee484299e6e1655e0e699fdccc7b51bfa33c56e099bc4"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0029f0d60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0029f0d40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:00:17.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3187" for this suite.
Jul 31 12:00:41.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:00:49.250: INFO: namespace init-container-3187 deletion completed in 30.239680863s

• [SLOW TEST:89.182 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:00:49.251: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 31 12:00:56.209: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:00:56.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7261" for this suite.
Jul 31 12:01:04.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:01:11.493: INFO: namespace container-runtime-7261 deletion completed in 14.381289194s

• [SLOW TEST:22.243 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:01:11.493: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-2079
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2079 to expose endpoints map[]
Jul 31 12:01:12.910: INFO: successfully validated that service multi-endpoint-test in namespace services-2079 exposes endpoints map[] (200.458596ms elapsed)
STEP: Creating pod pod1 in namespace services-2079
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2079 to expose endpoints map[pod1:[100]]
Jul 31 12:01:20.128: INFO: successfully validated that service multi-endpoint-test in namespace services-2079 exposes endpoints map[pod1:[100]] (6.818209789s elapsed)
STEP: Creating pod pod2 in namespace services-2079
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2079 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 31 12:01:28.523: INFO: successfully validated that service multi-endpoint-test in namespace services-2079 exposes endpoints map[pod1:[100] pod2:[101]] (8.371907311s elapsed)
STEP: Deleting pod pod1 in namespace services-2079
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2079 to expose endpoints map[pod2:[101]]
Jul 31 12:01:29.815: INFO: successfully validated that service multi-endpoint-test in namespace services-2079 exposes endpoints map[pod2:[101]] (1.271844724s elapsed)
STEP: Deleting pod pod2 in namespace services-2079
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2079 to expose endpoints map[]
Jul 31 12:01:29.845: INFO: successfully validated that service multi-endpoint-test in namespace services-2079 exposes endpoints map[] (10.180283ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:01:29.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2079" for this suite.
Jul 31 12:01:36.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:01:44.709: INFO: namespace services-2079 deletion completed in 14.685171331s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:33.215 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:01:44.709: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 31 12:01:45.109: INFO: Waiting up to 5m0s for pod "pod-0d7fcf8e-5f9f-4999-9946-d0a06d84bfad" in namespace "emptydir-2788" to be "success or failure"
Jul 31 12:01:45.309: INFO: Pod "pod-0d7fcf8e-5f9f-4999-9946-d0a06d84bfad": Phase="Pending", Reason="", readiness=false. Elapsed: 200.166963ms
Jul 31 12:01:47.609: INFO: Pod "pod-0d7fcf8e-5f9f-4999-9946-d0a06d84bfad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.500035653s
Jul 31 12:01:49.910: INFO: Pod "pod-0d7fcf8e-5f9f-4999-9946-d0a06d84bfad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.800572057s
Jul 31 12:01:52.113: INFO: Pod "pod-0d7fcf8e-5f9f-4999-9946-d0a06d84bfad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.00403834s
STEP: Saw pod success
Jul 31 12:01:52.113: INFO: Pod "pod-0d7fcf8e-5f9f-4999-9946-d0a06d84bfad" satisfied condition "success or failure"
Jul 31 12:01:52.124: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-0d7fcf8e-5f9f-4999-9946-d0a06d84bfad container test-container: <nil>
STEP: delete the pod
Jul 31 12:01:52.949: INFO: Waiting for pod pod-0d7fcf8e-5f9f-4999-9946-d0a06d84bfad to disappear
Jul 31 12:01:52.957: INFO: Pod pod-0d7fcf8e-5f9f-4999-9946-d0a06d84bfad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:01:52.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2788" for this suite.
Jul 31 12:02:01.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:02:10.358: INFO: namespace emptydir-2788 deletion completed in 15.847730659s

• [SLOW TEST:25.649 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:02:10.359: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-9723, will wait for the garbage collector to delete the pods
Jul 31 12:02:17.177: INFO: Deleting Job.batch foo took: 17.995772ms
Jul 31 12:02:17.778: INFO: Terminating Job.batch foo pods took: 600.36894ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:03:03.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9723" for this suite.
Jul 31 12:03:10.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:03:15.663: INFO: namespace job-9723 deletion completed in 11.845382483s

• [SLOW TEST:65.304 seconds]
[sig-apps] Job
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:03:15.671: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4498
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4498
STEP: Creating statefulset with conflicting port in namespace statefulset-4498
STEP: Waiting until pod test-pod will start running in namespace statefulset-4498
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4498
Jul 31 12:03:21.033: INFO: Observed stateful pod in namespace: statefulset-4498, name: ss-0, uid: 12c46c30-ef1b-466f-97ff-21dccdf61f91, status phase: Pending. Waiting for statefulset controller to delete.
Jul 31 12:03:22.209: INFO: Observed stateful pod in namespace: statefulset-4498, name: ss-0, uid: 12c46c30-ef1b-466f-97ff-21dccdf61f91, status phase: Failed. Waiting for statefulset controller to delete.
Jul 31 12:03:22.210: INFO: Observed stateful pod in namespace: statefulset-4498, name: ss-0, uid: 12c46c30-ef1b-466f-97ff-21dccdf61f91, status phase: Failed. Waiting for statefulset controller to delete.
Jul 31 12:03:22.210: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4498
STEP: Removing pod with conflicting port in namespace statefulset-4498
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4498 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 31 12:03:29.809: INFO: Deleting all statefulset in ns statefulset-4498
Jul 31 12:03:30.309: INFO: Scaling statefulset ss to 0
Jul 31 12:03:41.714: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 12:03:41.731: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:03:41.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4498" for this suite.
Jul 31 12:03:49.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:03:54.927: INFO: namespace statefulset-4498 deletion completed in 12.917362343s

• [SLOW TEST:39.256 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:03:54.928: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Jul 31 12:03:55.137: INFO: Waiting up to 5m0s for pod "var-expansion-9c359225-85f4-44fa-87bf-c44bb701818e" in namespace "var-expansion-4892" to be "success or failure"
Jul 31 12:03:55.155: INFO: Pod "var-expansion-9c359225-85f4-44fa-87bf-c44bb701818e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.17217ms
Jul 31 12:03:57.162: INFO: Pod "var-expansion-9c359225-85f4-44fa-87bf-c44bb701818e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025307857s
Jul 31 12:03:59.409: INFO: Pod "var-expansion-9c359225-85f4-44fa-87bf-c44bb701818e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27165453s
Jul 31 12:04:01.709: INFO: Pod "var-expansion-9c359225-85f4-44fa-87bf-c44bb701818e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.571835772s
STEP: Saw pod success
Jul 31 12:04:01.709: INFO: Pod "var-expansion-9c359225-85f4-44fa-87bf-c44bb701818e" satisfied condition "success or failure"
Jul 31 12:04:02.013: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod var-expansion-9c359225-85f4-44fa-87bf-c44bb701818e container dapi-container: <nil>
STEP: delete the pod
Jul 31 12:04:04.009: INFO: Waiting for pod var-expansion-9c359225-85f4-44fa-87bf-c44bb701818e to disappear
Jul 31 12:04:04.309: INFO: Pod var-expansion-9c359225-85f4-44fa-87bf-c44bb701818e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:04:04.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4892" for this suite.
Jul 31 12:04:11.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:04:20.522: INFO: namespace var-expansion-4892 deletion completed in 15.809796381s

• [SLOW TEST:25.594 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:04:20.524: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Jul 31 12:04:20.913: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-280300738 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:04:23.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7686" for this suite.
Jul 31 12:04:31.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:04:33.122: INFO: namespace kubectl-7686 deletion completed in 9.597932839s

• [SLOW TEST:12.602 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:04:33.126: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-fe6d398d-40dd-43b4-adfa-b5df0187684c
STEP: Creating a pod to test consume configMaps
Jul 31 12:04:35.409: INFO: Waiting up to 5m0s for pod "pod-configmaps-67013d57-2a13-4c7a-9a66-cfb0f28f23ed" in namespace "configmap-5063" to be "success or failure"
Jul 31 12:04:35.415: INFO: Pod "pod-configmaps-67013d57-2a13-4c7a-9a66-cfb0f28f23ed": Phase="Pending", Reason="", readiness=false. Elapsed: 5.99789ms
Jul 31 12:04:37.814: INFO: Pod "pod-configmaps-67013d57-2a13-4c7a-9a66-cfb0f28f23ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.405390268s
Jul 31 12:04:40.109: INFO: Pod "pod-configmaps-67013d57-2a13-4c7a-9a66-cfb0f28f23ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699959127s
Jul 31 12:04:42.425: INFO: Pod "pod-configmaps-67013d57-2a13-4c7a-9a66-cfb0f28f23ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.015651899s
STEP: Saw pod success
Jul 31 12:04:42.425: INFO: Pod "pod-configmaps-67013d57-2a13-4c7a-9a66-cfb0f28f23ed" satisfied condition "success or failure"
Jul 31 12:04:42.436: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-configmaps-67013d57-2a13-4c7a-9a66-cfb0f28f23ed container configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 12:04:43.109: INFO: Waiting for pod pod-configmaps-67013d57-2a13-4c7a-9a66-cfb0f28f23ed to disappear
Jul 31 12:04:43.309: INFO: Pod pod-configmaps-67013d57-2a13-4c7a-9a66-cfb0f28f23ed no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:04:43.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5063" for this suite.
Jul 31 12:04:50.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:04:52.839: INFO: namespace configmap-5063 deletion completed in 9.018517582s

• [SLOW TEST:19.717 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:04:52.847: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9142
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 31 12:04:53.309: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 31 12:05:25.944: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.24.37:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9142 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 12:05:25.944: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 12:05:31.809: INFO: Found all expected endpoints: [netserver-0]
Jul 31 12:05:32.010: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.26.110:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9142 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 12:05:32.010: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 12:05:35.914: INFO: Found all expected endpoints: [netserver-1]
Jul 31 12:05:35.927: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.25.35:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9142 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 12:05:35.927: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 12:05:40.009: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:05:40.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9142" for this suite.
Jul 31 12:06:04.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:06:08.445: INFO: namespace pod-network-test-9142 deletion completed in 27.927155261s

• [SLOW TEST:75.599 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:06:08.448: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6008
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 31 12:06:08.631: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 31 12:06:41.873: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.25.36 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6008 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 12:06:41.873: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 12:06:47.914: INFO: Found all expected endpoints: [netserver-0]
Jul 31 12:06:49.116: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.24.38 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6008 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 12:06:49.116: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 12:06:53.509: INFO: Found all expected endpoints: [netserver-1]
Jul 31 12:06:53.717: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.26.112 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6008 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 12:06:53.717: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 12:06:57.109: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:06:57.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6008" for this suite.
Jul 31 12:07:23.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:07:30.118: INFO: namespace pod-network-test-6008 deletion completed in 30.907178002s

• [SLOW TEST:81.670 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:07:30.119: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-658b1d69-2df3-497d-b539-80613c6b9f1d
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:07:41.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2331" for this suite.
Jul 31 12:08:06.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:08:16.372: INFO: namespace configmap-2331 deletion completed in 33.751648125s

• [SLOW TEST:46.253 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:08:16.372: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-9695/configmap-test-98b18013-6d79-4a8d-8976-d7d5580a92c7
STEP: Creating a pod to test consume configMaps
Jul 31 12:08:17.944: INFO: Waiting up to 5m0s for pod "pod-configmaps-e0119749-708f-4d4e-8122-d3957242697c" in namespace "configmap-9695" to be "success or failure"
Jul 31 12:08:19.009: INFO: Pod "pod-configmaps-e0119749-708f-4d4e-8122-d3957242697c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.06560392s
Jul 31 12:08:21.118: INFO: Pod "pod-configmaps-e0119749-708f-4d4e-8122-d3957242697c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.17432926s
Jul 31 12:08:23.221: INFO: Pod "pod-configmaps-e0119749-708f-4d4e-8122-d3957242697c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.277358682s
Jul 31 12:08:25.837: INFO: Pod "pod-configmaps-e0119749-708f-4d4e-8122-d3957242697c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.892815864s
Jul 31 12:08:28.009: INFO: Pod "pod-configmaps-e0119749-708f-4d4e-8122-d3957242697c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.065244871s
STEP: Saw pod success
Jul 31 12:08:28.009: INFO: Pod "pod-configmaps-e0119749-708f-4d4e-8122-d3957242697c" satisfied condition "success or failure"
Jul 31 12:08:28.509: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-configmaps-e0119749-708f-4d4e-8122-d3957242697c container env-test: <nil>
STEP: delete the pod
Jul 31 12:08:29.342: INFO: Waiting for pod pod-configmaps-e0119749-708f-4d4e-8122-d3957242697c to disappear
Jul 31 12:08:30.309: INFO: Pod pod-configmaps-e0119749-708f-4d4e-8122-d3957242697c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:08:30.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9695" for this suite.
Jul 31 12:08:36.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:08:42.545: INFO: namespace configmap-9695 deletion completed in 12.226596849s

• [SLOW TEST:26.173 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:08:42.552: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Jul 31 12:08:43.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 --namespace=kubectl-8528 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul 31 12:09:00.211: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul 31 12:09:00.211: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:09:03.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8528" for this suite.
Jul 31 12:09:09.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:09:18.009: INFO: namespace kubectl-8528 deletion completed in 14.868199462s

• [SLOW TEST:35.457 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:09:18.009: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-0e6d12a3-3046-4bcc-990b-83344bda7f55
STEP: Creating a pod to test consume configMaps
Jul 31 12:09:20.610: INFO: Waiting up to 5m0s for pod "pod-configmaps-75dda627-d9c3-4f91-b5bc-07d62fb5a71c" in namespace "configmap-8069" to be "success or failure"
Jul 31 12:09:20.626: INFO: Pod "pod-configmaps-75dda627-d9c3-4f91-b5bc-07d62fb5a71c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.215217ms
Jul 31 12:09:22.809: INFO: Pod "pod-configmaps-75dda627-d9c3-4f91-b5bc-07d62fb5a71c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.199619879s
Jul 31 12:09:24.913: INFO: Pod "pod-configmaps-75dda627-d9c3-4f91-b5bc-07d62fb5a71c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.303495441s
Jul 31 12:09:27.209: INFO: Pod "pod-configmaps-75dda627-d9c3-4f91-b5bc-07d62fb5a71c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.599332564s
STEP: Saw pod success
Jul 31 12:09:27.209: INFO: Pod "pod-configmaps-75dda627-d9c3-4f91-b5bc-07d62fb5a71c" satisfied condition "success or failure"
Jul 31 12:09:27.410: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-configmaps-75dda627-d9c3-4f91-b5bc-07d62fb5a71c container configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 12:09:28.541: INFO: Waiting for pod pod-configmaps-75dda627-d9c3-4f91-b5bc-07d62fb5a71c to disappear
Jul 31 12:09:28.547: INFO: Pod pod-configmaps-75dda627-d9c3-4f91-b5bc-07d62fb5a71c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:09:28.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8069" for this suite.
Jul 31 12:09:35.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:09:42.011: INFO: namespace configmap-8069 deletion completed in 13.400050196s

• [SLOW TEST:24.002 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:09:42.012: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 31 12:10:01.911: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 12:10:02.410: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 31 12:10:04.410: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 12:10:04.609: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 31 12:10:06.410: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 12:10:06.614: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 31 12:10:08.412: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 12:10:08.609: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 31 12:10:10.410: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 12:10:10.609: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 31 12:10:12.410: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 12:10:12.709: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 31 12:10:14.410: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 12:10:14.812: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 31 12:10:16.410: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 12:10:16.709: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 31 12:10:18.410: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 12:10:18.909: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 31 12:10:20.410: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 12:10:20.419: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 31 12:10:22.412: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 12:10:22.709: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:10:22.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8902" for this suite.
Jul 31 12:10:46.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:10:53.444: INFO: namespace container-lifecycle-hook-8902 deletion completed in 29.733585836s

• [SLOW TEST:71.432 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:10:53.445: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-7188
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7188
STEP: Deleting pre-stop pod
Jul 31 12:11:15.209: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:11:15.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7188" for this suite.
Jul 31 12:11:56.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:11:58.039: INFO: namespace prestop-7188 deletion completed in 42.228671479s

• [SLOW TEST:64.594 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:11:58.040: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 31 12:12:06.011: INFO: Pod name wrapped-volume-race-6106b310-127d-449b-be65-a8aefc81b7b5: Found 1 pods out of 5
Jul 31 12:12:11.119: INFO: Pod name wrapped-volume-race-6106b310-127d-449b-be65-a8aefc81b7b5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6106b310-127d-449b-be65-a8aefc81b7b5 in namespace emptydir-wrapper-3949, will wait for the garbage collector to delete the pods
Jul 31 12:12:29.409: INFO: Deleting ReplicationController wrapped-volume-race-6106b310-127d-449b-be65-a8aefc81b7b5 took: 390.995312ms
Jul 31 12:12:29.910: INFO: Terminating ReplicationController wrapped-volume-race-6106b310-127d-449b-be65-a8aefc81b7b5 pods took: 500.224863ms
STEP: Creating RC which spawns configmap-volume pods
Jul 31 12:13:12.274: INFO: Pod name wrapped-volume-race-057fc3f2-e0c6-4353-8fe9-63c2799648c9: Found 0 pods out of 5
Jul 31 12:13:17.535: INFO: Pod name wrapped-volume-race-057fc3f2-e0c6-4353-8fe9-63c2799648c9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-057fc3f2-e0c6-4353-8fe9-63c2799648c9 in namespace emptydir-wrapper-3949, will wait for the garbage collector to delete the pods
Jul 31 12:13:36.743: INFO: Deleting ReplicationController wrapped-volume-race-057fc3f2-e0c6-4353-8fe9-63c2799648c9 took: 83.319947ms
Jul 31 12:13:37.451: INFO: Terminating ReplicationController wrapped-volume-race-057fc3f2-e0c6-4353-8fe9-63c2799648c9 pods took: 708.259899ms
STEP: Creating RC which spawns configmap-volume pods
Jul 31 12:14:20.272: INFO: Pod name wrapped-volume-race-511dd393-fce6-4c76-b01e-5c2bacf26edc: Found 0 pods out of 5
Jul 31 12:14:25.811: INFO: Pod name wrapped-volume-race-511dd393-fce6-4c76-b01e-5c2bacf26edc: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-511dd393-fce6-4c76-b01e-5c2bacf26edc in namespace emptydir-wrapper-3949, will wait for the garbage collector to delete the pods
Jul 31 12:14:44.409: INFO: Deleting ReplicationController wrapped-volume-race-511dd393-fce6-4c76-b01e-5c2bacf26edc took: 149.055393ms
Jul 31 12:14:44.909: INFO: Terminating ReplicationController wrapped-volume-race-511dd393-fce6-4c76-b01e-5c2bacf26edc pods took: 500.208682ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:15:33.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3949" for this suite.
Jul 31 12:15:44.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:15:47.660: INFO: namespace emptydir-wrapper-3949 deletion completed in 13.84095873s

• [SLOW TEST:229.620 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:15:47.667: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 31 12:15:55.428: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ead7fa07-68f1-4a1a-a6f9-25a84d6a210f"
Jul 31 12:15:55.428: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ead7fa07-68f1-4a1a-a6f9-25a84d6a210f" in namespace "pods-4853" to be "terminated due to deadline exceeded"
Jul 31 12:15:55.434: INFO: Pod "pod-update-activedeadlineseconds-ead7fa07-68f1-4a1a-a6f9-25a84d6a210f": Phase="Running", Reason="", readiness=true. Elapsed: 5.927429ms
Jul 31 12:15:58.614: INFO: Pod "pod-update-activedeadlineseconds-ead7fa07-68f1-4a1a-a6f9-25a84d6a210f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 3.18629823s
Jul 31 12:15:58.614: INFO: Pod "pod-update-activedeadlineseconds-ead7fa07-68f1-4a1a-a6f9-25a84d6a210f" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:15:58.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4853" for this suite.
Jul 31 12:16:05.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:16:14.210: INFO: namespace pods-4853 deletion completed in 15.578388224s

• [SLOW TEST:26.543 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:16:14.210: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 12:16:16.240: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88fb1c1c-c111-4f68-a687-da323918337b" in namespace "downward-api-6596" to be "success or failure"
Jul 31 12:16:16.251: INFO: Pod "downwardapi-volume-88fb1c1c-c111-4f68-a687-da323918337b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.523229ms
Jul 31 12:16:18.509: INFO: Pod "downwardapi-volume-88fb1c1c-c111-4f68-a687-da323918337b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.268589334s
Jul 31 12:16:20.722: INFO: Pod "downwardapi-volume-88fb1c1c-c111-4f68-a687-da323918337b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.481892028s
Jul 31 12:16:23.009: INFO: Pod "downwardapi-volume-88fb1c1c-c111-4f68-a687-da323918337b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.768839636s
STEP: Saw pod success
Jul 31 12:16:23.009: INFO: Pod "downwardapi-volume-88fb1c1c-c111-4f68-a687-da323918337b" satisfied condition "success or failure"
Jul 31 12:16:23.209: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-h22s9 pod downwardapi-volume-88fb1c1c-c111-4f68-a687-da323918337b container client-container: <nil>
STEP: delete the pod
Jul 31 12:16:24.009: INFO: Waiting for pod downwardapi-volume-88fb1c1c-c111-4f68-a687-da323918337b to disappear
Jul 31 12:16:24.114: INFO: Pod downwardapi-volume-88fb1c1c-c111-4f68-a687-da323918337b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:16:24.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6596" for this suite.
Jul 31 12:16:31.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:16:35.921: INFO: namespace downward-api-6596 deletion completed in 11.790642797s

• [SLOW TEST:21.712 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:16:35.922: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-3641/secret-test-f97d1a36-86f7-4c23-b54b-22a7784155dd
STEP: Creating a pod to test consume secrets
Jul 31 12:16:37.810: INFO: Waiting up to 5m0s for pod "pod-configmaps-6dd29cce-5ecf-4037-959f-56ac8b33726c" in namespace "secrets-3641" to be "success or failure"
Jul 31 12:16:37.817: INFO: Pod "pod-configmaps-6dd29cce-5ecf-4037-959f-56ac8b33726c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.920117ms
Jul 31 12:16:40.009: INFO: Pod "pod-configmaps-6dd29cce-5ecf-4037-959f-56ac8b33726c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.199165684s
Jul 31 12:16:42.209: INFO: Pod "pod-configmaps-6dd29cce-5ecf-4037-959f-56ac8b33726c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.399379056s
Jul 31 12:16:44.409: INFO: Pod "pod-configmaps-6dd29cce-5ecf-4037-959f-56ac8b33726c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.599510487s
STEP: Saw pod success
Jul 31 12:16:44.409: INFO: Pod "pod-configmaps-6dd29cce-5ecf-4037-959f-56ac8b33726c" satisfied condition "success or failure"
Jul 31 12:16:44.711: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-configmaps-6dd29cce-5ecf-4037-959f-56ac8b33726c container env-test: <nil>
STEP: delete the pod
Jul 31 12:16:45.810: INFO: Waiting for pod pod-configmaps-6dd29cce-5ecf-4037-959f-56ac8b33726c to disappear
Jul 31 12:16:46.009: INFO: Pod pod-configmaps-6dd29cce-5ecf-4037-959f-56ac8b33726c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:16:46.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3641" for this suite.
Jul 31 12:16:52.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:16:57.909: INFO: namespace secrets-3641 deletion completed in 11.698294744s

• [SLOW TEST:21.987 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:16:57.913: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 31 12:16:58.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-1052'
Jul 31 12:17:01.492: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 31 12:17:01.492: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Jul 31 12:17:05.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete deployment e2e-test-nginx-deployment --namespace=kubectl-1052'
Jul 31 12:17:07.116: INFO: stderr: ""
Jul 31 12:17:07.116: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:17:07.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1052" for this suite.
Jul 31 12:17:16.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:17:20.257: INFO: namespace kubectl-1052 deletion completed in 11.946700095s

• [SLOW TEST:22.344 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:17:20.258: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 31 12:17:21.034: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-481,SelfLink:/api/v1/namespaces/watch-481/configmaps/e2e-watch-test-configmap-a,UID:76a22f6e-7cc1-44a6-85b2-dc4edc5584d4,ResourceVersion:1096868,Generation:0,CreationTimestamp:2019-07-31 12:17:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 31 12:17:21.035: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-481,SelfLink:/api/v1/namespaces/watch-481/configmaps/e2e-watch-test-configmap-a,UID:76a22f6e-7cc1-44a6-85b2-dc4edc5584d4,ResourceVersion:1096868,Generation:0,CreationTimestamp:2019-07-31 12:17:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 31 12:17:34.109: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-481,SelfLink:/api/v1/namespaces/watch-481/configmaps/e2e-watch-test-configmap-a,UID:76a22f6e-7cc1-44a6-85b2-dc4edc5584d4,ResourceVersion:1096896,Generation:0,CreationTimestamp:2019-07-31 12:17:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 31 12:17:34.109: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-481,SelfLink:/api/v1/namespaces/watch-481/configmaps/e2e-watch-test-configmap-a,UID:76a22f6e-7cc1-44a6-85b2-dc4edc5584d4,ResourceVersion:1096896,Generation:0,CreationTimestamp:2019-07-31 12:17:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 31 12:17:44.530: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-481,SelfLink:/api/v1/namespaces/watch-481/configmaps/e2e-watch-test-configmap-a,UID:76a22f6e-7cc1-44a6-85b2-dc4edc5584d4,ResourceVersion:1096926,Generation:0,CreationTimestamp:2019-07-31 12:17:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 31 12:17:44.530: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-481,SelfLink:/api/v1/namespaces/watch-481/configmaps/e2e-watch-test-configmap-a,UID:76a22f6e-7cc1-44a6-85b2-dc4edc5584d4,ResourceVersion:1096926,Generation:0,CreationTimestamp:2019-07-31 12:17:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 31 12:17:55.283: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-481,SelfLink:/api/v1/namespaces/watch-481/configmaps/e2e-watch-test-configmap-a,UID:76a22f6e-7cc1-44a6-85b2-dc4edc5584d4,ResourceVersion:1096953,Generation:0,CreationTimestamp:2019-07-31 12:17:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 31 12:17:55.283: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-481,SelfLink:/api/v1/namespaces/watch-481/configmaps/e2e-watch-test-configmap-a,UID:76a22f6e-7cc1-44a6-85b2-dc4edc5584d4,ResourceVersion:1096953,Generation:0,CreationTimestamp:2019-07-31 12:17:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 31 12:18:05.307: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-481,SelfLink:/api/v1/namespaces/watch-481/configmaps/e2e-watch-test-configmap-b,UID:a72c41e2-8219-474c-ade0-b645b5064dd3,ResourceVersion:1096978,Generation:0,CreationTimestamp:2019-07-31 12:18:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 31 12:18:05.307: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-481,SelfLink:/api/v1/namespaces/watch-481/configmaps/e2e-watch-test-configmap-b,UID:a72c41e2-8219-474c-ade0-b645b5064dd3,ResourceVersion:1096978,Generation:0,CreationTimestamp:2019-07-31 12:18:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 31 12:18:15.609: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-481,SelfLink:/api/v1/namespaces/watch-481/configmaps/e2e-watch-test-configmap-b,UID:a72c41e2-8219-474c-ade0-b645b5064dd3,ResourceVersion:1097003,Generation:0,CreationTimestamp:2019-07-31 12:18:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 31 12:18:15.609: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-481,SelfLink:/api/v1/namespaces/watch-481/configmaps/e2e-watch-test-configmap-b,UID:a72c41e2-8219-474c-ade0-b645b5064dd3,ResourceVersion:1097003,Generation:0,CreationTimestamp:2019-07-31 12:18:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:18:25.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-481" for this suite.
Jul 31 12:18:33.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:18:45.228: INFO: namespace watch-481 deletion completed in 19.417580813s

• [SLOW TEST:84.970 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:18:45.230: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 31 12:18:47.958: INFO: Number of nodes with available pods: 0
Jul 31 12:18:47.958: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:18:50.239: INFO: Number of nodes with available pods: 0
Jul 31 12:18:50.239: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:18:53.409: INFO: Number of nodes with available pods: 1
Jul 31 12:18:53.410: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:18:55.109: INFO: Number of nodes with available pods: 2
Jul 31 12:18:55.109: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:18:57.409: INFO: Number of nodes with available pods: 3
Jul 31 12:18:57.409: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 31 12:18:58.220: INFO: Number of nodes with available pods: 2
Jul 31 12:18:58.220: INFO: Node worker-2bh9r-78c4c5b4fb-h22s9 is running more than one daemon pod
Jul 31 12:18:59.327: INFO: Number of nodes with available pods: 2
Jul 31 12:18:59.327: INFO: Node worker-2bh9r-78c4c5b4fb-h22s9 is running more than one daemon pod
Jul 31 12:19:00.245: INFO: Number of nodes with available pods: 2
Jul 31 12:19:00.245: INFO: Node worker-2bh9r-78c4c5b4fb-h22s9 is running more than one daemon pod
Jul 31 12:19:02.009: INFO: Number of nodes with available pods: 2
Jul 31 12:19:02.009: INFO: Node worker-2bh9r-78c4c5b4fb-h22s9 is running more than one daemon pod
Jul 31 12:19:02.709: INFO: Number of nodes with available pods: 2
Jul 31 12:19:02.709: INFO: Node worker-2bh9r-78c4c5b4fb-h22s9 is running more than one daemon pod
Jul 31 12:19:03.809: INFO: Number of nodes with available pods: 2
Jul 31 12:19:03.809: INFO: Node worker-2bh9r-78c4c5b4fb-h22s9 is running more than one daemon pod
Jul 31 12:19:04.709: INFO: Number of nodes with available pods: 2
Jul 31 12:19:04.709: INFO: Node worker-2bh9r-78c4c5b4fb-h22s9 is running more than one daemon pod
Jul 31 12:19:05.614: INFO: Number of nodes with available pods: 2
Jul 31 12:19:05.614: INFO: Node worker-2bh9r-78c4c5b4fb-h22s9 is running more than one daemon pod
Jul 31 12:19:06.248: INFO: Number of nodes with available pods: 2
Jul 31 12:19:06.248: INFO: Node worker-2bh9r-78c4c5b4fb-h22s9 is running more than one daemon pod
Jul 31 12:19:07.709: INFO: Number of nodes with available pods: 2
Jul 31 12:19:07.709: INFO: Node worker-2bh9r-78c4c5b4fb-h22s9 is running more than one daemon pod
Jul 31 12:19:08.619: INFO: Number of nodes with available pods: 2
Jul 31 12:19:08.619: INFO: Node worker-2bh9r-78c4c5b4fb-h22s9 is running more than one daemon pod
Jul 31 12:19:09.909: INFO: Number of nodes with available pods: 3
Jul 31 12:19:09.909: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8333, will wait for the garbage collector to delete the pods
Jul 31 12:19:10.809: INFO: Deleting DaemonSet.extensions daemon-set took: 249.751783ms
Jul 31 12:19:11.309: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.192665ms
Jul 31 12:19:23.209: INFO: Number of nodes with available pods: 0
Jul 31 12:19:23.209: INFO: Number of running nodes: 0, number of available pods: 0
Jul 31 12:19:23.514: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8333/daemonsets","resourceVersion":"1097245"},"items":null}

Jul 31 12:19:23.809: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8333/pods","resourceVersion":"1097245"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:19:24.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8333" for this suite.
Jul 31 12:19:31.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:19:39.436: INFO: namespace daemonsets-8333 deletion completed in 14.418922351s

• [SLOW TEST:54.207 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:19:39.438: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5478.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5478.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 12:19:52.009: INFO: DNS probes using dns-5478/dns-test-5c72b470-ff22-45d8-8777-bd51932ee5dc succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:19:52.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5478" for this suite.
Jul 31 12:20:00.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:20:06.758: INFO: namespace dns-5478 deletion completed in 14.046477348s

• [SLOW TEST:27.320 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:20:06.758: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 31 12:20:18.009: INFO: Successfully updated pod "annotationupdatefe3bfd84-0b15-4df7-b48c-b58bada03628"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:20:20.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4094" for this suite.
Jul 31 12:20:43.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:20:51.338: INFO: namespace projected-4094 deletion completed in 30.427740464s

• [SLOW TEST:44.581 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:20:51.343: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 12:20:53.689: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 31 12:20:54.811: INFO: Number of nodes with available pods: 0
Jul 31 12:20:54.811: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 31 12:20:54.861: INFO: Number of nodes with available pods: 0
Jul 31 12:20:54.861: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:20:56.015: INFO: Number of nodes with available pods: 0
Jul 31 12:20:56.015: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:20:57.612: INFO: Number of nodes with available pods: 0
Jul 31 12:20:57.612: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:20:57.914: INFO: Number of nodes with available pods: 0
Jul 31 12:20:57.914: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:20:58.868: INFO: Number of nodes with available pods: 0
Jul 31 12:20:58.868: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:00.409: INFO: Number of nodes with available pods: 0
Jul 31 12:21:00.409: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:01.323: INFO: Number of nodes with available pods: 0
Jul 31 12:21:01.324: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:02.016: INFO: Number of nodes with available pods: 1
Jul 31 12:21:02.016: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 31 12:21:02.920: INFO: Number of nodes with available pods: 0
Jul 31 12:21:02.921: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 31 12:21:02.945: INFO: Number of nodes with available pods: 0
Jul 31 12:21:02.945: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:04.818: INFO: Number of nodes with available pods: 0
Jul 31 12:21:04.818: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:06.009: INFO: Number of nodes with available pods: 0
Jul 31 12:21:06.009: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:07.809: INFO: Number of nodes with available pods: 0
Jul 31 12:21:07.810: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:08.209: INFO: Number of nodes with available pods: 0
Jul 31 12:21:08.211: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:09.511: INFO: Number of nodes with available pods: 0
Jul 31 12:21:09.511: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:11.009: INFO: Number of nodes with available pods: 0
Jul 31 12:21:11.009: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:12.609: INFO: Number of nodes with available pods: 0
Jul 31 12:21:12.609: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:13.209: INFO: Number of nodes with available pods: 0
Jul 31 12:21:13.209: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:14.128: INFO: Number of nodes with available pods: 0
Jul 31 12:21:14.129: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:15.209: INFO: Number of nodes with available pods: 0
Jul 31 12:21:15.209: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:16.309: INFO: Number of nodes with available pods: 0
Jul 31 12:21:16.309: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:17.509: INFO: Number of nodes with available pods: 0
Jul 31 12:21:17.510: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:18.310: INFO: Number of nodes with available pods: 0
Jul 31 12:21:18.310: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:21:19.909: INFO: Number of nodes with available pods: 1
Jul 31 12:21:19.909: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2237, will wait for the garbage collector to delete the pods
Jul 31 12:21:24.009: INFO: Deleting DaemonSet.extensions daemon-set took: 449.659286ms
Jul 31 12:21:24.609: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.290763ms
Jul 31 12:21:28.420: INFO: Number of nodes with available pods: 0
Jul 31 12:21:28.420: INFO: Number of running nodes: 0, number of available pods: 0
Jul 31 12:21:28.429: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2237/daemonsets","resourceVersion":"1097704"},"items":null}

Jul 31 12:21:28.442: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2237/pods","resourceVersion":"1097705"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:21:29.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2237" for this suite.
Jul 31 12:21:37.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:21:43.182: INFO: namespace daemonsets-2237 deletion completed in 13.25172625s

• [SLOW TEST:51.843 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:21:43.186: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:22:15.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3877" for this suite.
Jul 31 12:22:22.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:22:31.709: INFO: namespace namespaces-3877 deletion completed in 15.798255897s
STEP: Destroying namespace "nsdeletetest-8270" for this suite.
Jul 31 12:22:31.721: INFO: Namespace nsdeletetest-8270 was already deleted
STEP: Destroying namespace "nsdeletetest-3210" for this suite.
Jul 31 12:22:38.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:22:44.520: INFO: namespace nsdeletetest-3210 deletion completed in 12.798549148s

• [SLOW TEST:61.338 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:22:44.535: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Jul 31 12:22:44.840: INFO: Waiting up to 5m0s for pod "client-containers-f8040a9f-de02-4a3c-bccb-dcd8c48d573f" in namespace "containers-8582" to be "success or failure"
Jul 31 12:22:44.855: INFO: Pod "client-containers-f8040a9f-de02-4a3c-bccb-dcd8c48d573f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.660729ms
Jul 31 12:22:47.422: INFO: Pod "client-containers-f8040a9f-de02-4a3c-bccb-dcd8c48d573f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.577819937s
Jul 31 12:22:49.431: INFO: Pod "client-containers-f8040a9f-de02-4a3c-bccb-dcd8c48d573f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.586892337s
STEP: Saw pod success
Jul 31 12:22:49.431: INFO: Pod "client-containers-f8040a9f-de02-4a3c-bccb-dcd8c48d573f" satisfied condition "success or failure"
Jul 31 12:22:49.437: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod client-containers-f8040a9f-de02-4a3c-bccb-dcd8c48d573f container test-container: <nil>
STEP: delete the pod
Jul 31 12:22:51.609: INFO: Waiting for pod client-containers-f8040a9f-de02-4a3c-bccb-dcd8c48d573f to disappear
Jul 31 12:22:51.617: INFO: Pod client-containers-f8040a9f-de02-4a3c-bccb-dcd8c48d573f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:22:51.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8582" for this suite.
Jul 31 12:22:58.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:22:59.303: INFO: namespace containers-8582 deletion completed in 7.6766441s

• [SLOW TEST:14.769 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:22:59.309: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3162
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-3162
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3162
Jul 31 12:22:59.754: INFO: Found 0 stateful pods, waiting for 1
Jul 31 12:23:09.815: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 31 12:23:09.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 31 12:23:14.810: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 31 12:23:14.810: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 31 12:23:14.810: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 31 12:23:15.220: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 31 12:23:25.415: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 12:23:25.415: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 12:23:26.409: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999609s
Jul 31 12:23:27.514: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.081300011s
Jul 31 12:23:28.809: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975714437s
Jul 31 12:23:30.110: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.680710876s
Jul 31 12:23:31.509: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.379731208s
Jul 31 12:23:32.615: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.981428604s
Jul 31 12:23:33.809: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.871751192s
Jul 31 12:23:35.109: INFO: Verifying statefulset ss doesn't scale past 3 for another 681.086104ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3162
Jul 31 12:23:36.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:23:39.316: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 31 12:23:39.316: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 31 12:23:39.316: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 31 12:23:39.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:23:43.611: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 31 12:23:43.611: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 31 12:23:43.611: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 31 12:23:43.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:23:50.327: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 31 12:23:50.327: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 31 12:23:50.327: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 31 12:23:50.909: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 12:23:50.909: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 12:23:50.909: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 31 12:23:51.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 31 12:23:56.115: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 31 12:23:56.115: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 31 12:23:56.115: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 31 12:23:56.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 31 12:23:59.613: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 31 12:23:59.613: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 31 12:23:59.613: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 31 12:23:59.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 31 12:24:04.913: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 31 12:24:04.913: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 31 12:24:04.913: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 31 12:24:04.913: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 12:24:05.208: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul 31 12:24:16.309: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 12:24:16.309: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 12:24:16.309: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 12:24:17.309: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jul 31 12:24:17.309: INFO: ss-0  worker-2bh9r-78c4c5b4fb-czrvz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  }]
Jul 31 12:24:17.309: INFO: ss-1  worker-2bh9r-78c4c5b4fb-h22s9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  }]
Jul 31 12:24:17.309: INFO: ss-2  worker-2bh9r-78c4c5b4fb-96hfw  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  }]
Jul 31 12:24:17.309: INFO: 
Jul 31 12:24:17.309: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 31 12:24:18.508: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jul 31 12:24:18.508: INFO: ss-0  worker-2bh9r-78c4c5b4fb-czrvz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  }]
Jul 31 12:24:18.508: INFO: ss-1  worker-2bh9r-78c4c5b4fb-h22s9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  }]
Jul 31 12:24:18.509: INFO: ss-2  worker-2bh9r-78c4c5b4fb-96hfw  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  }]
Jul 31 12:24:18.509: INFO: 
Jul 31 12:24:18.509: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 31 12:24:19.719: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jul 31 12:24:19.719: INFO: ss-0  worker-2bh9r-78c4c5b4fb-czrvz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  }]
Jul 31 12:24:19.719: INFO: ss-1  worker-2bh9r-78c4c5b4fb-h22s9  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  }]
Jul 31 12:24:19.719: INFO: ss-2  worker-2bh9r-78c4c5b4fb-96hfw  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  }]
Jul 31 12:24:19.719: INFO: 
Jul 31 12:24:19.719: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 31 12:24:21.329: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jul 31 12:24:21.329: INFO: ss-0  worker-2bh9r-78c4c5b4fb-czrvz  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  }]
Jul 31 12:24:21.330: INFO: ss-1  worker-2bh9r-78c4c5b4fb-h22s9  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  }]
Jul 31 12:24:21.330: INFO: ss-2  worker-2bh9r-78c4c5b4fb-96hfw  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  }]
Jul 31 12:24:21.335: INFO: 
Jul 31 12:24:21.335: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 31 12:24:22.415: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jul 31 12:24:22.415: INFO: ss-0  worker-2bh9r-78c4c5b4fb-czrvz  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  }]
Jul 31 12:24:22.415: INFO: ss-2  worker-2bh9r-78c4c5b4fb-96hfw  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  }]
Jul 31 12:24:22.415: INFO: 
Jul 31 12:24:22.415: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 31 12:24:23.919: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jul 31 12:24:23.919: INFO: ss-0  worker-2bh9r-78c4c5b4fb-czrvz  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  }]
Jul 31 12:24:23.919: INFO: ss-2  worker-2bh9r-78c4c5b4fb-96hfw  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  }]
Jul 31 12:24:23.919: INFO: 
Jul 31 12:24:23.919: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 31 12:24:25.213: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jul 31 12:24:25.213: INFO: ss-0  worker-2bh9r-78c4c5b4fb-czrvz  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  }]
Jul 31 12:24:25.213: INFO: ss-2  worker-2bh9r-78c4c5b4fb-96hfw  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:25 +0000 UTC  }]
Jul 31 12:24:25.213: INFO: 
Jul 31 12:24:25.213: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 31 12:24:27.308: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jul 31 12:24:27.308: INFO: ss-0  worker-2bh9r-78c4c5b4fb-czrvz  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:23:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:22:59 +0000 UTC  }]
Jul 31 12:24:27.308: INFO: 
Jul 31 12:24:27.308: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3162
Jul 31 12:24:28.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:24:31.914: INFO: rc: 1
Jul 31 12:24:31.914: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc001ebd950 exit status 1 <nil> <nil> true [0xc0022d2500 0xc0022d25c0 0xc0022d26d0] [0xc0022d2500 0xc0022d25c0 0xc0022d26d0] [0xc0022d25b0 0xc0022d25f0] [0x9d17b0 0x9d17b0] 0xc000ac7320 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Jul 31 12:24:41.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:24:43.319: INFO: rc: 1
Jul 31 12:24:43.319: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00298a330 exit status 1 <nil> <nil> true [0xc0015024d8 0xc001502518 0xc001502530] [0xc0015024d8 0xc001502518 0xc001502530] [0xc001502510 0xc001502528] [0x9d17b0 0x9d17b0] 0xc0028f9c20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:24:53.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:24:54.324: INFO: rc: 1
Jul 31 12:24:54.324: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ebdd10 exit status 1 <nil> <nil> true [0xc0022d26f0 0xc0022d2790 0xc0022d2828] [0xc0022d26f0 0xc0022d2790 0xc0022d2828] [0xc0022d2780 0xc0022d27e0] [0x9d17b0 0x9d17b0] 0xc00199e2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:25:04.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:25:05.109: INFO: rc: 1
Jul 31 12:25:05.109: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019200c0 exit status 1 <nil> <nil> true [0xc0022d2878 0xc0022d28d0 0xc0022d2928] [0xc0022d2878 0xc0022d28d0 0xc0022d2928] [0xc0022d28a0 0xc0022d2920] [0x9d17b0 0x9d17b0] 0xc00199e8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:25:15.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:25:15.725: INFO: rc: 1
Jul 31 12:25:15.725: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001920450 exit status 1 <nil> <nil> true [0xc0022d2958 0xc0022d29b0 0xc0022d29e8] [0xc0022d2958 0xc0022d29b0 0xc0022d29e8] [0xc0022d29a0 0xc0022d29c8] [0x9d17b0 0x9d17b0] 0xc00199f080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:25:25.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:25:26.338: INFO: rc: 1
Jul 31 12:25:26.338: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ebc570 exit status 1 <nil> <nil> true [0xc000c981e8 0xc000c98470 0xc000c98650] [0xc000c981e8 0xc000c98470 0xc000c98650] [0xc000c98368 0xc000c98618] [0x9d17b0 0x9d17b0] 0xc000ac6420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:25:36.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:25:36.491: INFO: rc: 1
Jul 31 12:25:36.491: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ebc8d0 exit status 1 <nil> <nil> true [0xc000c986e0 0xc000c98848 0xc000c989a0] [0xc000c986e0 0xc000c98848 0xc000c989a0] [0xc000c987f0 0xc000c98920] [0x9d17b0 0x9d17b0] 0xc000ac6780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:25:46.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:25:47.533: INFO: rc: 1
Jul 31 12:25:47.533: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ebcc30 exit status 1 <nil> <nil> true [0xc000c98a00 0xc000c98a90 0xc000c98cc8] [0xc000c98a00 0xc000c98a90 0xc000c98cc8] [0xc000c98a68 0xc000c98c18] [0x9d17b0 0x9d17b0] 0xc000ac6c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:25:57.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:25:59.209: INFO: rc: 1
Jul 31 12:25:59.209: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ebcfc0 exit status 1 <nil> <nil> true [0xc000c98cd8 0xc000c98fb8 0xc000c992a8] [0xc000c98cd8 0xc000c98fb8 0xc000c992a8] [0xc000c98ea8 0xc000c99268] [0x9d17b0 0x9d17b0] 0xc000ac7320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:26:09.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:26:09.421: INFO: rc: 1
Jul 31 12:26:09.421: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ebd350 exit status 1 <nil> <nil> true [0xc000c99488 0xc000c99870 0xc000c99c00] [0xc000c99488 0xc000c99870 0xc000c99c00] [0xc000c99778 0xc000c99b08] [0x9d17b0 0x9d17b0] 0xc002994de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:26:19.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:26:21.209: INFO: rc: 1
Jul 31 12:26:21.209: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ebd860 exit status 1 <nil> <nil> true [0xc000c99c48 0xc0001f7bf0 0xc0001f7d50] [0xc000c99c48 0xc0001f7bf0 0xc0001f7d50] [0xc0001f7a18 0xc0001f7c70] [0x9d17b0 0x9d17b0] 0xc002520180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:26:31.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:26:32.514: INFO: rc: 1
Jul 31 12:26:32.514: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eae690 exit status 1 <nil> <nil> true [0xc000010ac0 0xc000f8a948 0xc000f8ac40] [0xc000010ac0 0xc000f8a948 0xc000f8ac40] [0xc000f8a088 0xc000f8abb0] [0x9d17b0 0x9d17b0] 0xc002aaa2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:26:42.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:26:43.544: INFO: rc: 1
Jul 31 12:26:43.544: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eaea50 exit status 1 <nil> <nil> true [0xc000f8ac48 0xc000f8b660 0xc000f8bae0] [0xc000f8ac48 0xc000f8b660 0xc000f8bae0] [0xc000f8b020 0xc000f8b8e0] [0x9d17b0 0x9d17b0] 0xc002aaa660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:26:53.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:26:53.678: INFO: rc: 1
Jul 31 12:26:53.678: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ebdc20 exit status 1 <nil> <nil> true [0xc0001f7dd8 0xc0001f7ed8 0xc0007a6050] [0xc0001f7dd8 0xc0001f7ed8 0xc0007a6050] [0xc0001f7e28 0xc0001f7f70] [0x9d17b0 0x9d17b0] 0xc0025204e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:27:03.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:27:04.141: INFO: rc: 1
Jul 31 12:27:04.141: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eaedb0 exit status 1 <nil> <nil> true [0xc000f8baf8 0xc000f8bc58 0xc000f8bdf8] [0xc000f8baf8 0xc000f8bc58 0xc000f8bdf8] [0xc000f8bba8 0xc000f8bd60] [0x9d17b0 0x9d17b0] 0xc002aaac60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:27:14.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:27:14.255: INFO: rc: 1
Jul 31 12:27:14.255: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00250c030 exit status 1 <nil> <nil> true [0xc0007a6248 0xc0007a7240 0xc0007a77e8] [0xc0007a6248 0xc0007a7240 0xc0007a77e8] [0xc0007a66b8 0xc0007a7710] [0x9d17b0 0x9d17b0] 0xc002520900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:27:24.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:27:24.384: INFO: rc: 1
Jul 31 12:27:24.384: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eaf140 exit status 1 <nil> <nil> true [0xc000f8be38 0xc000f8bee0 0xc000f8bf30] [0xc000f8be38 0xc000f8bee0 0xc000f8bf30] [0xc000f8bed0 0xc000f8bf28] [0x9d17b0 0x9d17b0] 0xc002aab260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:27:34.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:27:34.507: INFO: rc: 1
Jul 31 12:27:34.507: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00250c360 exit status 1 <nil> <nil> true [0xc0001f7900 0xc0001f7c60 0xc0001f7dd8] [0xc0001f7900 0xc0001f7c60 0xc0001f7dd8] [0xc0001f7bf0 0xc0001f7d50] [0x9d17b0 0x9d17b0] 0xc002995aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:27:44.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:27:45.108: INFO: rc: 1
Jul 31 12:27:45.108: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00250c6f0 exit status 1 <nil> <nil> true [0xc0001f7df8 0xc0001f7f48 0xc000c981e8] [0xc0001f7df8 0xc0001f7f48 0xc000c981e8] [0xc0001f7ed8 0xc000c98168] [0x9d17b0 0x9d17b0] 0xc000ac6420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:27:55.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:27:57.214: INFO: rc: 1
Jul 31 12:27:57.214: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00250ca80 exit status 1 <nil> <nil> true [0xc000c982f0 0xc000c98580 0xc000c986e0] [0xc000c982f0 0xc000c98580 0xc000c986e0] [0xc000c98470 0xc000c98650] [0x9d17b0 0x9d17b0] 0xc000ac6780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:28:07.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:28:07.715: INFO: rc: 1
Jul 31 12:28:07.715: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00250cea0 exit status 1 <nil> <nil> true [0xc000c98780 0xc000c988e8 0xc000c98a00] [0xc000c98780 0xc000c988e8 0xc000c98a00] [0xc000c98848 0xc000c989a0] [0x9d17b0 0x9d17b0] 0xc000ac6c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:28:17.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:28:17.937: INFO: rc: 1
Jul 31 12:28:17.937: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ebc630 exit status 1 <nil> <nil> true [0xc0007a6050 0xc0007a66b8 0xc0007a7710] [0xc0007a6050 0xc0007a66b8 0xc0007a7710] [0xc0007a6320 0xc0007a75b8] [0x9d17b0 0x9d17b0] 0xc0025202a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:28:27.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:28:28.045: INFO: rc: 1
Jul 31 12:28:28.045: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00250d260 exit status 1 <nil> <nil> true [0xc000c98a38 0xc000c98b18 0xc000c98cd8] [0xc000c98a38 0xc000c98b18 0xc000c98cd8] [0xc000c98a90 0xc000c98cc8] [0x9d17b0 0x9d17b0] 0xc000ac7320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:28:38.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:28:38.172: INFO: rc: 1
Jul 31 12:28:38.172: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00250d620 exit status 1 <nil> <nil> true [0xc000c98de8 0xc000c98fc8 0xc000c99488] [0xc000c98de8 0xc000c98fc8 0xc000c99488] [0xc000c98fb8 0xc000c992a8] [0x9d17b0 0x9d17b0] 0xc002aaa180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:28:48.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:28:49.117: INFO: rc: 1
Jul 31 12:28:49.117: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00250d9e0 exit status 1 <nil> <nil> true [0xc000c99510 0xc000c99ac0 0xc000c99c48] [0xc000c99510 0xc000c99ac0 0xc000c99c48] [0xc000c99870 0xc000c99c00] [0x9d17b0 0x9d17b0] 0xc002aaa4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:28:59.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:28:59.830: INFO: rc: 1
Jul 31 12:28:59.830: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00250dd70 exit status 1 <nil> <nil> true [0xc000f8a018 0xc000f8aa78 0xc000f8ac48] [0xc000f8a018 0xc000f8aa78 0xc000f8ac48] [0xc000f8a948 0xc000f8ac40] [0x9d17b0 0x9d17b0] 0xc002aaaa20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:29:09.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:29:10.241: INFO: rc: 1
Jul 31 12:29:10.241: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eae2a0 exit status 1 <nil> <nil> true [0xc000f8ad70 0xc000f8b7f8 0xc000f8baf8] [0xc000f8ad70 0xc000f8b7f8 0xc000f8baf8] [0xc000f8b660 0xc000f8bae0] [0x9d17b0 0x9d17b0] 0xc002aab4a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:29:20.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:29:20.339: INFO: rc: 1
Jul 31 12:29:20.339: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001eae7b0 exit status 1 <nil> <nil> true [0xc000f8bb30 0xc000f8bd40 0xc000f8be60] [0xc000f8bb30 0xc000f8bd40 0xc000f8be60] [0xc000f8bc58 0xc000f8bdf8] [0x9d17b0 0x9d17b0] 0xc002aabbc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 31 12:29:30.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-3162 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:29:31.708: INFO: rc: 1
Jul 31 12:29:31.708: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Jul 31 12:29:31.708: INFO: Scaling statefulset ss to 0
Jul 31 12:29:33.011: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 31 12:29:33.019: INFO: Deleting all statefulset in ns statefulset-3162
Jul 31 12:29:33.032: INFO: Scaling statefulset ss to 0
Jul 31 12:29:33.061: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 12:29:33.067: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:29:33.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3162" for this suite.
Jul 31 12:29:40.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:29:47.916: INFO: namespace statefulset-3162 deletion completed in 14.195061652s

• [SLOW TEST:408.608 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:29:47.918: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-72b5bc44-67b7-4852-a496-a6bd81bf9dda
STEP: Creating secret with name s-test-opt-upd-d0c79de6-a2b9-4671-b4ad-2c6ac2b0224a
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-72b5bc44-67b7-4852-a496-a6bd81bf9dda
STEP: Updating secret s-test-opt-upd-d0c79de6-a2b9-4671-b4ad-2c6ac2b0224a
STEP: Creating secret with name s-test-opt-create-8680507b-5ea3-4435-8e1d-9b4f5691acad
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:31:24.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5567" for this suite.
Jul 31 12:31:48.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:31:56.011: INFO: namespace secrets-5567 deletion completed in 31.098441027s

• [SLOW TEST:128.094 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:31:56.012: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-c2e0381a-4264-44f8-9232-30f308568f27
STEP: Creating a pod to test consume secrets
Jul 31 12:31:56.538: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0343869f-b7fa-4de9-9379-37bbbb639d8d" in namespace "projected-857" to be "success or failure"
Jul 31 12:31:56.551: INFO: Pod "pod-projected-secrets-0343869f-b7fa-4de9-9379-37bbbb639d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.47419ms
Jul 31 12:31:58.617: INFO: Pod "pod-projected-secrets-0343869f-b7fa-4de9-9379-37bbbb639d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078917399s
Jul 31 12:32:01.116: INFO: Pod "pod-projected-secrets-0343869f-b7fa-4de9-9379-37bbbb639d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.578319111s
Jul 31 12:32:03.728: INFO: Pod "pod-projected-secrets-0343869f-b7fa-4de9-9379-37bbbb639d8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.190017551s
STEP: Saw pod success
Jul 31 12:32:03.728: INFO: Pod "pod-projected-secrets-0343869f-b7fa-4de9-9379-37bbbb639d8d" satisfied condition "success or failure"
Jul 31 12:32:03.735: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-projected-secrets-0343869f-b7fa-4de9-9379-37bbbb639d8d container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 31 12:32:06.111: INFO: Waiting for pod pod-projected-secrets-0343869f-b7fa-4de9-9379-37bbbb639d8d to disappear
Jul 31 12:32:06.715: INFO: Pod pod-projected-secrets-0343869f-b7fa-4de9-9379-37bbbb639d8d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:32:06.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-857" for this suite.
Jul 31 12:32:13.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:32:17.944: INFO: namespace projected-857 deletion completed in 11.217121007s

• [SLOW TEST:21.932 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:32:17.945: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 31 12:32:18.716: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3742,SelfLink:/api/v1/namespaces/watch-3742/configmaps/e2e-watch-test-resource-version,UID:14af46ce-6cdc-4b02-976b-9d9f12bfc3b1,ResourceVersion:1099640,Generation:0,CreationTimestamp:2019-07-31 12:32:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 31 12:32:18.716: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3742,SelfLink:/api/v1/namespaces/watch-3742/configmaps/e2e-watch-test-resource-version,UID:14af46ce-6cdc-4b02-976b-9d9f12bfc3b1,ResourceVersion:1099641,Generation:0,CreationTimestamp:2019-07-31 12:32:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:32:18.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3742" for this suite.
Jul 31 12:32:24.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:32:33.041: INFO: namespace watch-3742 deletion completed in 14.301558704s

• [SLOW TEST:15.097 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:32:33.044: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 31 12:32:39.127: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:32:39.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7180" for this suite.
Jul 31 12:32:45.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:32:53.511: INFO: namespace container-runtime-7180 deletion completed in 14.269000081s

• [SLOW TEST:20.467 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:32:53.511: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ef2a10fb-7f26-4b87-9001-8a6aa383342c
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-ef2a10fb-7f26-4b87-9001-8a6aa383342c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:34:27.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9638" for this suite.
Jul 31 12:34:51.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:34:56.080: INFO: namespace projected-9638 deletion completed in 27.967531332s

• [SLOW TEST:122.569 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:34:56.080: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:34:56.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7106" for this suite.
Jul 31 12:35:18.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:35:25.015: INFO: namespace kubelet-test-7106 deletion completed in 28.72132613s

• [SLOW TEST:28.938 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:35:25.019: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 12:35:26.014: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 31 12:35:29.876: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:35:29.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-13" for this suite.
Jul 31 12:35:36.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:35:39.612: INFO: namespace replication-controller-13 deletion completed in 9.401653705s

• [SLOW TEST:14.593 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:35:39.615: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-63ed9d29-67ec-49ec-9f3f-ec007299f552
STEP: Creating a pod to test consume secrets
Jul 31 12:35:40.810: INFO: Waiting up to 5m0s for pod "pod-secrets-a9a5cbcc-bf2c-4ffa-bd04-d83d21060862" in namespace "secrets-5707" to be "success or failure"
Jul 31 12:35:41.110: INFO: Pod "pod-secrets-a9a5cbcc-bf2c-4ffa-bd04-d83d21060862": Phase="Pending", Reason="", readiness=false. Elapsed: 299.588127ms
Jul 31 12:35:43.312: INFO: Pod "pod-secrets-a9a5cbcc-bf2c-4ffa-bd04-d83d21060862": Phase="Pending", Reason="", readiness=false. Elapsed: 2.501632184s
Jul 31 12:35:45.611: INFO: Pod "pod-secrets-a9a5cbcc-bf2c-4ffa-bd04-d83d21060862": Phase="Pending", Reason="", readiness=false. Elapsed: 4.80052588s
Jul 31 12:35:47.811: INFO: Pod "pod-secrets-a9a5cbcc-bf2c-4ffa-bd04-d83d21060862": Phase="Pending", Reason="", readiness=false. Elapsed: 7.000375444s
Jul 31 12:35:50.110: INFO: Pod "pod-secrets-a9a5cbcc-bf2c-4ffa-bd04-d83d21060862": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.299701883s
STEP: Saw pod success
Jul 31 12:35:50.110: INFO: Pod "pod-secrets-a9a5cbcc-bf2c-4ffa-bd04-d83d21060862" satisfied condition "success or failure"
Jul 31 12:35:51.011: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-secrets-a9a5cbcc-bf2c-4ffa-bd04-d83d21060862 container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 12:35:51.550: INFO: Waiting for pod pod-secrets-a9a5cbcc-bf2c-4ffa-bd04-d83d21060862 to disappear
Jul 31 12:35:51.556: INFO: Pod pod-secrets-a9a5cbcc-bf2c-4ffa-bd04-d83d21060862 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:35:51.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5707" for this suite.
Jul 31 12:35:57.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:36:07.510: INFO: namespace secrets-5707 deletion completed in 15.945478767s

• [SLOW TEST:27.895 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:36:07.511: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 31 12:36:17.211: INFO: Successfully updated pod "labelsupdate6af39401-758d-4313-a324-86f417104dad"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:36:18.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3666" for this suite.
Jul 31 12:36:41.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:36:48.411: INFO: namespace projected-3666 deletion completed in 29.672025798s

• [SLOW TEST:40.901 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:36:48.412: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1525
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul 31 12:36:51.142: INFO: Found 0 stateful pods, waiting for 3
Jul 31 12:37:01.312: INFO: Found 2 stateful pods, waiting for 3
Jul 31 12:37:11.612: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 12:37:11.612: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 12:37:11.612: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 12:37:13.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-1525 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 31 12:37:21.107: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 31 12:37:21.107: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 31 12:37:21.107: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 31 12:37:31.681: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 31 12:37:33.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-1525 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:37:36.326: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 31 12:37:36.326: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 31 12:37:36.326: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 31 12:38:08.743: INFO: Waiting for StatefulSet statefulset-1525/ss2 to complete update
STEP: Rolling back to a previous revision
Jul 31 12:38:19.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-1525 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 31 12:38:22.129: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 31 12:38:22.129: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 31 12:38:22.129: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 31 12:38:34.814: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 31 12:38:35.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 exec --namespace=statefulset-1525 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 31 12:38:39.014: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 31 12:38:39.014: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 31 12:38:39.014: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 31 12:38:40.064: INFO: Waiting for StatefulSet statefulset-1525/ss2 to complete update
Jul 31 12:38:40.064: INFO: Waiting for Pod statefulset-1525/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jul 31 12:38:40.064: INFO: Waiting for Pod statefulset-1525/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jul 31 12:38:40.064: INFO: Waiting for Pod statefulset-1525/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jul 31 12:38:51.110: INFO: Waiting for StatefulSet statefulset-1525/ss2 to complete update
Jul 31 12:38:51.110: INFO: Waiting for Pod statefulset-1525/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jul 31 12:38:51.110: INFO: Waiting for Pod statefulset-1525/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jul 31 12:39:00.510: INFO: Waiting for StatefulSet statefulset-1525/ss2 to complete update
Jul 31 12:39:00.510: INFO: Waiting for Pod statefulset-1525/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 31 12:39:10.810: INFO: Deleting all statefulset in ns statefulset-1525
Jul 31 12:39:11.009: INFO: Scaling statefulset ss2 to 0
Jul 31 12:39:31.710: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 12:39:31.910: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:39:32.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1525" for this suite.
Jul 31 12:39:39.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:39:46.910: INFO: namespace statefulset-1525 deletion completed in 13.995047886s

• [SLOW TEST:178.498 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:39:46.911: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-0c8a5f7c-e66d-4d0b-bda1-c3fd57cd585e in namespace container-probe-3830
Jul 31 12:39:57.111: INFO: Started pod liveness-0c8a5f7c-e66d-4d0b-bda1-c3fd57cd585e in namespace container-probe-3830
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 12:39:57.311: INFO: Initial restart count of pod liveness-0c8a5f7c-e66d-4d0b-bda1-c3fd57cd585e is 0
Jul 31 12:40:16.611: INFO: Restart count of pod container-probe-3830/liveness-0c8a5f7c-e66d-4d0b-bda1-c3fd57cd585e is now 1 (19.299978727s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:40:16.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3830" for this suite.
Jul 31 12:40:23.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:40:28.565: INFO: namespace container-probe-3830 deletion completed in 11.45386057s

• [SLOW TEST:41.654 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:40:28.566: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 31 12:40:29.929: INFO: Waiting up to 5m0s for pod "pod-3874f28c-a5b4-4d52-b5ed-2c1cddad7ed1" in namespace "emptydir-9622" to be "success or failure"
Jul 31 12:40:29.938: INFO: Pod "pod-3874f28c-a5b4-4d52-b5ed-2c1cddad7ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.304505ms
Jul 31 12:40:32.110: INFO: Pod "pod-3874f28c-a5b4-4d52-b5ed-2c1cddad7ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180795687s
Jul 31 12:40:34.219: INFO: Pod "pod-3874f28c-a5b4-4d52-b5ed-2c1cddad7ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.290433138s
Jul 31 12:40:36.710: INFO: Pod "pod-3874f28c-a5b4-4d52-b5ed-2c1cddad7ed1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.781301815s
STEP: Saw pod success
Jul 31 12:40:36.710: INFO: Pod "pod-3874f28c-a5b4-4d52-b5ed-2c1cddad7ed1" satisfied condition "success or failure"
Jul 31 12:40:37.212: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-3874f28c-a5b4-4d52-b5ed-2c1cddad7ed1 container test-container: <nil>
STEP: delete the pod
Jul 31 12:40:38.459: INFO: Waiting for pod pod-3874f28c-a5b4-4d52-b5ed-2c1cddad7ed1 to disappear
Jul 31 12:40:38.915: INFO: Pod pod-3874f28c-a5b4-4d52-b5ed-2c1cddad7ed1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:40:38.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9622" for this suite.
Jul 31 12:40:45.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:40:46.770: INFO: namespace emptydir-9622 deletion completed in 7.844699409s

• [SLOW TEST:18.205 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:40:46.780: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8757.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8757.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8757.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8757.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8757.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8757.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8757.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8757.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8757.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8757.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8757.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8757.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8757.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 7.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.7_udp@PTR;check="$$(dig +tcp +noall +answer +search 7.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.7_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8757.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8757.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8757.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8757.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8757.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8757.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8757.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8757.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8757.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8757.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8757.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8757.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8757.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 7.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.7_udp@PTR;check="$$(dig +tcp +noall +answer +search 7.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.7_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 12:40:57.510: INFO: Unable to read wheezy_udp@dns-test-service.dns-8757.svc.cluster.local from pod dns-8757/dns-test-8a399dab-04f8-4228-917f-11d5f639b984: the server could not find the requested resource (get pods dns-test-8a399dab-04f8-4228-917f-11d5f639b984)
Jul 31 12:40:59.310: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8757.svc.cluster.local from pod dns-8757/dns-test-8a399dab-04f8-4228-917f-11d5f639b984: the server could not find the requested resource (get pods dns-test-8a399dab-04f8-4228-917f-11d5f639b984)
Jul 31 12:41:10.711: INFO: Lookups using dns-8757/dns-test-8a399dab-04f8-4228-917f-11d5f639b984 failed for: [wheezy_udp@dns-test-service.dns-8757.svc.cluster.local wheezy_tcp@dns-test-service.dns-8757.svc.cluster.local]

Jul 31 12:41:29.507: INFO: DNS probes using dns-8757/dns-test-8a399dab-04f8-4228-917f-11d5f639b984 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:41:30.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8757" for this suite.
Jul 31 12:41:37.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:41:44.314: INFO: namespace dns-8757 deletion completed in 13.102294876s

• [SLOW TEST:57.534 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:41:44.315: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-3667
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3667 to expose endpoints map[]
Jul 31 12:41:45.610: INFO: successfully validated that service endpoint-test2 in namespace services-3667 exposes endpoints map[] (195.075858ms elapsed)
STEP: Creating pod pod1 in namespace services-3667
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3667 to expose endpoints map[pod1:[80]]
Jul 31 12:41:51.511: INFO: successfully validated that service endpoint-test2 in namespace services-3667 exposes endpoints map[pod1:[80]] (5.700743282s elapsed)
STEP: Creating pod pod2 in namespace services-3667
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3667 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 31 12:41:58.238: INFO: successfully validated that service endpoint-test2 in namespace services-3667 exposes endpoints map[pod1:[80] pod2:[80]] (6.528217152s elapsed)
STEP: Deleting pod pod1 in namespace services-3667
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3667 to expose endpoints map[pod2:[80]]
Jul 31 12:41:58.280: INFO: successfully validated that service endpoint-test2 in namespace services-3667 exposes endpoints map[pod2:[80]] (19.60935ms elapsed)
STEP: Deleting pod pod2 in namespace services-3667
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3667 to expose endpoints map[]
Jul 31 12:41:58.615: INFO: successfully validated that service endpoint-test2 in namespace services-3667 exposes endpoints map[] (312.095108ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:41:58.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3667" for this suite.
Jul 31 12:42:05.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:42:07.923: INFO: namespace services-3667 deletion completed in 9.21167378s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:23.613 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:42:07.928: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 12:42:08.754: INFO: Create a RollingUpdate DaemonSet
Jul 31 12:42:08.779: INFO: Check that daemon pods launch on every node of the cluster
Jul 31 12:42:09.410: INFO: Number of nodes with available pods: 0
Jul 31 12:42:09.410: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:42:12.717: INFO: Number of nodes with available pods: 0
Jul 31 12:42:12.717: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 12:42:14.933: INFO: Number of nodes with available pods: 2
Jul 31 12:42:14.934: INFO: Node worker-2bh9r-78c4c5b4fb-h22s9 is running more than one daemon pod
Jul 31 12:42:15.911: INFO: Number of nodes with available pods: 3
Jul 31 12:42:15.911: INFO: Number of running nodes: 3, number of available pods: 3
Jul 31 12:42:15.911: INFO: Update the DaemonSet to trigger a rollout
Jul 31 12:42:16.509: INFO: Updating DaemonSet daemon-set
Jul 31 12:42:20.845: INFO: Roll back the DaemonSet before rollout is complete
Jul 31 12:42:21.425: INFO: Updating DaemonSet daemon-set
Jul 31 12:42:21.425: INFO: Make sure DaemonSet rollback is complete
Jul 31 12:42:21.433: INFO: Wrong image for pod: daemon-set-xg82l. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 31 12:42:21.435: INFO: Pod daemon-set-xg82l is not available
Jul 31 12:42:22.710: INFO: Wrong image for pod: daemon-set-xg82l. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 31 12:42:22.710: INFO: Pod daemon-set-xg82l is not available
Jul 31 12:42:23.910: INFO: Pod daemon-set-wltkl is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-761, will wait for the garbage collector to delete the pods
Jul 31 12:42:24.309: INFO: Deleting DaemonSet.extensions daemon-set took: 317.491588ms
Jul 31 12:42:24.715: INFO: Terminating DaemonSet.extensions daemon-set pods took: 405.273407ms
Jul 31 12:42:31.509: INFO: Number of nodes with available pods: 0
Jul 31 12:42:31.509: INFO: Number of running nodes: 0, number of available pods: 0
Jul 31 12:42:33.810: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-761/daemonsets","resourceVersion":"1101952"},"items":null}

Jul 31 12:42:34.015: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-761/pods","resourceVersion":"1101955"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:42:34.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-761" for this suite.
Jul 31 12:42:41.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:42:46.510: INFO: namespace daemonsets-761 deletion completed in 12.44917087s

• [SLOW TEST:38.581 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:42:46.510: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 12:42:47.809: INFO: (0) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 583.585908ms)
Jul 31 12:42:49.011: INFO: (1) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 1.201840818s)
Jul 31 12:42:50.109: INFO: (2) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 1.098140815s)
Jul 31 12:42:50.609: INFO: (3) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 499.805685ms)
Jul 31 12:42:51.409: INFO: (4) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 799.405018ms)
Jul 31 12:42:51.710: INFO: (5) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 299.262835ms)
Jul 31 12:42:52.509: INFO: (6) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 799.374421ms)
Jul 31 12:42:53.615: INFO: (7) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 1.106067993s)
Jul 31 12:42:54.309: INFO: (8) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 693.793194ms)
Jul 31 12:42:54.616: INFO: (9) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 306.4103ms)
Jul 31 12:42:54.809: INFO: (10) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 193.496804ms)
Jul 31 12:42:55.510: INFO: (11) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 700.341933ms)
Jul 31 12:42:56.809: INFO: (12) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 1.299857064s)
Jul 31 12:42:57.009: INFO: (13) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 199.59719ms)
Jul 31 12:42:57.410: INFO: (14) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 400.445134ms)
Jul 31 12:42:57.709: INFO: (15) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 298.609184ms)
Jul 31 12:42:59.210: INFO: (16) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 1.500241084s)
Jul 31 12:42:59.710: INFO: (17) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 500.329963ms)
Jul 31 12:43:00.309: INFO: (18) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 599.236568ms)
Jul 31 12:43:01.009: INFO: (19) /api/v1/nodes/worker-2bh9r-78c4c5b4fb-96hfw:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 699.743128ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:43:01.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4985" for this suite.
Jul 31 12:43:09.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:43:16.710: INFO: namespace proxy-4985 deletion completed in 15.498901437s

• [SLOW TEST:30.200 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:43:16.710: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 31 12:43:26.709: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:43:27.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9233" for this suite.
Jul 31 12:43:34.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:43:38.851: INFO: namespace container-runtime-9233 deletion completed in 11.116167084s

• [SLOW TEST:22.140 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:43:38.851: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Jul 31 12:43:40.709: INFO: created pod pod-service-account-defaultsa
Jul 31 12:43:40.710: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 31 12:43:41.009: INFO: created pod pod-service-account-mountsa
Jul 31 12:43:41.009: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 31 12:43:41.310: INFO: created pod pod-service-account-nomountsa
Jul 31 12:43:41.310: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 31 12:43:41.509: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 31 12:43:41.509: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 31 12:43:41.719: INFO: created pod pod-service-account-mountsa-mountspec
Jul 31 12:43:41.719: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 31 12:43:41.910: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 31 12:43:41.910: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 31 12:43:42.322: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 31 12:43:42.322: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 31 12:43:42.409: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 31 12:43:42.409: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 31 12:43:42.450: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 31 12:43:42.450: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:43:42.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9823" for this suite.
Jul 31 12:44:05.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:44:12.531: INFO: namespace svcaccounts-9823 deletion completed in 30.061387839s

• [SLOW TEST:33.680 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:44:12.532: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-2ea150dc-6233-491c-a0c4-3990e294076f
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-2ea150dc-6233-491c-a0c4-3990e294076f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:45:53.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4123" for this suite.
Jul 31 12:46:18.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:46:24.745: INFO: namespace configmap-4123 deletion completed in 31.127469239s

• [SLOW TEST:132.214 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:46:24.747: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 12:46:25.731: INFO: Waiting up to 5m0s for pod "downwardapi-volume-225e3ae4-970c-4698-89ce-65cc2bc326b5" in namespace "projected-6823" to be "success or failure"
Jul 31 12:46:25.751: INFO: Pod "downwardapi-volume-225e3ae4-970c-4698-89ce-65cc2bc326b5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.160646ms
Jul 31 12:46:27.758: INFO: Pod "downwardapi-volume-225e3ae4-970c-4698-89ce-65cc2bc326b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026621474s
Jul 31 12:46:29.766: INFO: Pod "downwardapi-volume-225e3ae4-970c-4698-89ce-65cc2bc326b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034280679s
Jul 31 12:46:32.213: INFO: Pod "downwardapi-volume-225e3ae4-970c-4698-89ce-65cc2bc326b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.481777922s
STEP: Saw pod success
Jul 31 12:46:32.213: INFO: Pod "downwardapi-volume-225e3ae4-970c-4698-89ce-65cc2bc326b5" satisfied condition "success or failure"
Jul 31 12:46:32.220: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downwardapi-volume-225e3ae4-970c-4698-89ce-65cc2bc326b5 container client-container: <nil>
STEP: delete the pod
Jul 31 12:46:34.909: INFO: Waiting for pod downwardapi-volume-225e3ae4-970c-4698-89ce-65cc2bc326b5 to disappear
Jul 31 12:46:35.109: INFO: Pod downwardapi-volume-225e3ae4-970c-4698-89ce-65cc2bc326b5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:46:35.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6823" for this suite.
Jul 31 12:46:45.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:46:51.911: INFO: namespace projected-6823 deletion completed in 16.294837345s

• [SLOW TEST:27.164 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:46:51.912: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 12:46:53.311: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 31 12:46:59.710: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 31 12:47:10.223: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-4758,SelfLink:/apis/apps/v1/namespaces/deployment-4758/deployments/test-cleanup-deployment,UID:b9053b5c-c62e-4675-9c61-015e67fea992,ResourceVersion:1102881,Generation:1,CreationTimestamp:2019-07-31 12:47:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-31 12:47:00 +0000 UTC 2019-07-31 12:47:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-31 12:47:07 +0000 UTC 2019-07-31 12:47:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 31 12:47:10.230: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-4758,SelfLink:/apis/apps/v1/namespaces/deployment-4758/replicasets/test-cleanup-deployment-55bbcbc84c,UID:92b68247-bdd3-4525-b2e1-195f90a5ec55,ResourceVersion:1102872,Generation:1,CreationTimestamp:2019-07-31 12:47:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment b9053b5c-c62e-4675-9c61-015e67fea992 0xc0004f9bc7 0xc0004f9bc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 31 12:47:10.237: INFO: Pod "test-cleanup-deployment-55bbcbc84c-47dwb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-47dwb,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-4758,SelfLink:/api/v1/namespaces/deployment-4758/pods/test-cleanup-deployment-55bbcbc84c-47dwb,UID:f46dac63-b922-4bdc-aa5e-3aef6b319fda,ResourceVersion:1102871,Generation:0,CreationTimestamp:2019-07-31 12:47:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 92b68247-bdd3-4525-b2e1-195f90a5ec55 0xc00299f4d7 0xc00299f4d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wgw7q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wgw7q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-wgw7q true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-h22s9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00299f640} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00299f700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:47:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:47:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:47:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 12:47:00 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.13,PodIP:172.25.24.52,StartTime:2019-07-31 12:47:00 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-31 12:47:06 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://7c9098b29b42c809b55dc8b739c24ab67a7fec7edaa6811c73eff00c711363c7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:47:10.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4758" for this suite.
Jul 31 12:47:17.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:47:21.530: INFO: namespace deployment-4758 deletion completed in 11.282959763s

• [SLOW TEST:29.618 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:47:21.532: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:47:23.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3288" for this suite.
Jul 31 12:47:30.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:47:37.840: INFO: namespace services-3288 deletion completed in 13.907355275s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:16.308 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:47:37.840: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Jul 31 12:47:39.309: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-280300738 proxy --unix-socket=/tmp/kubectl-proxy-unix381525817/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:47:39.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7132" for this suite.
Jul 31 12:47:47.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:47:58.009: INFO: namespace kubectl-7132 deletion completed in 18.198307408s

• [SLOW TEST:20.169 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:47:58.010: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-efc593e1-0bfb-4e52-9f21-4f39cb7d25fa in namespace container-probe-6847
Jul 31 12:48:06.509: INFO: Started pod liveness-efc593e1-0bfb-4e52-9f21-4f39cb7d25fa in namespace container-probe-6847
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 12:48:06.714: INFO: Initial restart count of pod liveness-efc593e1-0bfb-4e52-9f21-4f39cb7d25fa is 0
Jul 31 12:48:20.121: INFO: Restart count of pod container-probe-6847/liveness-efc593e1-0bfb-4e52-9f21-4f39cb7d25fa is now 1 (13.407027419s elapsed)
Jul 31 12:48:37.509: INFO: Restart count of pod container-probe-6847/liveness-efc593e1-0bfb-4e52-9f21-4f39cb7d25fa is now 2 (30.794694955s elapsed)
Jul 31 12:48:59.411: INFO: Restart count of pod container-probe-6847/liveness-efc593e1-0bfb-4e52-9f21-4f39cb7d25fa is now 3 (52.696169154s elapsed)
Jul 31 12:49:18.312: INFO: Restart count of pod container-probe-6847/liveness-efc593e1-0bfb-4e52-9f21-4f39cb7d25fa is now 4 (1m11.597774359s elapsed)
Jul 31 12:50:23.809: INFO: Restart count of pod container-probe-6847/liveness-efc593e1-0bfb-4e52-9f21-4f39cb7d25fa is now 5 (2m17.094539653s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:50:24.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6847" for this suite.
Jul 31 12:50:30.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:50:37.309: INFO: namespace container-probe-6847 deletion completed in 13.258463802s

• [SLOW TEST:159.301 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:50:37.311: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-46d26033-8e3d-4ffa-a054-8964b2b7c49e in namespace container-probe-5344
Jul 31 12:50:44.913: INFO: Started pod busybox-46d26033-8e3d-4ffa-a054-8964b2b7c49e in namespace container-probe-5344
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 12:50:45.209: INFO: Initial restart count of pod busybox-46d26033-8e3d-4ffa-a054-8964b2b7c49e is 0
Jul 31 12:51:38.809: INFO: Restart count of pod container-probe-5344/busybox-46d26033-8e3d-4ffa-a054-8964b2b7c49e is now 1 (53.599995814s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:51:39.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5344" for this suite.
Jul 31 12:51:45.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:51:50.848: INFO: namespace container-probe-5344 deletion completed in 11.726767131s

• [SLOW TEST:73.538 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:51:50.848: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 12:51:52.009: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5589c16-2903-4798-a97f-f2fba71e24ae" in namespace "downward-api-2109" to be "success or failure"
Jul 31 12:51:52.312: INFO: Pod "downwardapi-volume-d5589c16-2903-4798-a97f-f2fba71e24ae": Phase="Pending", Reason="", readiness=false. Elapsed: 303.249394ms
Jul 31 12:51:54.510: INFO: Pod "downwardapi-volume-d5589c16-2903-4798-a97f-f2fba71e24ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.500777783s
Jul 31 12:51:56.909: INFO: Pod "downwardapi-volume-d5589c16-2903-4798-a97f-f2fba71e24ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.900252741s
STEP: Saw pod success
Jul 31 12:51:56.909: INFO: Pod "downwardapi-volume-d5589c16-2903-4798-a97f-f2fba71e24ae" satisfied condition "success or failure"
Jul 31 12:51:57.409: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downwardapi-volume-d5589c16-2903-4798-a97f-f2fba71e24ae container client-container: <nil>
STEP: delete the pod
Jul 31 12:51:59.212: INFO: Waiting for pod downwardapi-volume-d5589c16-2903-4798-a97f-f2fba71e24ae to disappear
Jul 31 12:51:59.609: INFO: Pod downwardapi-volume-d5589c16-2903-4798-a97f-f2fba71e24ae no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:51:59.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2109" for this suite.
Jul 31 12:52:08.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:52:13.638: INFO: namespace downward-api-2109 deletion completed in 13.427983234s

• [SLOW TEST:22.790 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:52:13.654: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul 31 12:52:14.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 create -f - --namespace=kubectl-6863'
Jul 31 12:52:18.717: INFO: stderr: ""
Jul 31 12:52:18.717: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 12:52:18.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6863'
Jul 31 12:52:18.840: INFO: stderr: ""
Jul 31 12:52:18.840: INFO: stdout: "update-demo-nautilus-pbdxp update-demo-nautilus-sgkfb "
Jul 31 12:52:18.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-pbdxp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6863'
Jul 31 12:52:18.989: INFO: stderr: ""
Jul 31 12:52:18.989: INFO: stdout: ""
Jul 31 12:52:18.989: INFO: update-demo-nautilus-pbdxp is created but not running
Jul 31 12:52:23.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6863'
Jul 31 12:52:24.517: INFO: stderr: ""
Jul 31 12:52:24.517: INFO: stdout: "update-demo-nautilus-pbdxp update-demo-nautilus-sgkfb "
Jul 31 12:52:24.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-pbdxp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6863'
Jul 31 12:52:25.907: INFO: stderr: ""
Jul 31 12:52:25.907: INFO: stdout: ""
Jul 31 12:52:25.907: INFO: update-demo-nautilus-pbdxp is created but not running
Jul 31 12:52:30.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6863'
Jul 31 12:52:31.912: INFO: stderr: ""
Jul 31 12:52:31.912: INFO: stdout: "update-demo-nautilus-pbdxp update-demo-nautilus-sgkfb "
Jul 31 12:52:31.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-pbdxp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6863'
Jul 31 12:52:32.111: INFO: stderr: ""
Jul 31 12:52:32.111: INFO: stdout: "true"
Jul 31 12:52:32.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-pbdxp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6863'
Jul 31 12:52:33.108: INFO: stderr: ""
Jul 31 12:52:33.108: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 12:52:33.108: INFO: validating pod update-demo-nautilus-pbdxp
Jul 31 12:52:34.909: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 12:52:34.909: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 12:52:34.909: INFO: update-demo-nautilus-pbdxp is verified up and running
Jul 31 12:52:34.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-sgkfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6863'
Jul 31 12:52:35.004: INFO: stderr: ""
Jul 31 12:52:35.004: INFO: stdout: "true"
Jul 31 12:52:35.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-sgkfb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6863'
Jul 31 12:52:35.511: INFO: stderr: ""
Jul 31 12:52:35.511: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 12:52:35.511: INFO: validating pod update-demo-nautilus-sgkfb
Jul 31 12:52:37.108: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 12:52:37.109: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 12:52:37.109: INFO: update-demo-nautilus-sgkfb is verified up and running
STEP: scaling down the replication controller
Jul 31 12:52:37.113: INFO: scanned /root for discovery docs: <nil>
Jul 31 12:52:37.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-6863'
Jul 31 12:52:39.806: INFO: stderr: ""
Jul 31 12:52:39.813: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 12:52:39.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6863'
Jul 31 12:52:40.616: INFO: stderr: ""
Jul 31 12:52:40.616: INFO: stdout: "update-demo-nautilus-pbdxp update-demo-nautilus-sgkfb "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 31 12:52:45.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6863'
Jul 31 12:52:46.607: INFO: stderr: ""
Jul 31 12:52:46.607: INFO: stdout: "update-demo-nautilus-pbdxp "
Jul 31 12:52:46.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-pbdxp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6863'
Jul 31 12:52:47.117: INFO: stderr: ""
Jul 31 12:52:47.117: INFO: stdout: "true"
Jul 31 12:52:47.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-pbdxp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6863'
Jul 31 12:52:47.332: INFO: stderr: ""
Jul 31 12:52:47.332: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 12:52:47.332: INFO: validating pod update-demo-nautilus-pbdxp
Jul 31 12:52:47.909: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 12:52:47.909: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 12:52:47.909: INFO: update-demo-nautilus-pbdxp is verified up and running
STEP: scaling up the replication controller
Jul 31 12:52:47.910: INFO: scanned /root for discovery docs: <nil>
Jul 31 12:52:47.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-6863'
Jul 31 12:52:49.816: INFO: stderr: ""
Jul 31 12:52:49.816: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 12:52:49.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6863'
Jul 31 12:52:49.963: INFO: stderr: ""
Jul 31 12:52:49.963: INFO: stdout: "update-demo-nautilus-pbdxp update-demo-nautilus-wcxrr "
Jul 31 12:52:49.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-pbdxp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6863'
Jul 31 12:52:50.077: INFO: stderr: ""
Jul 31 12:52:50.078: INFO: stdout: "true"
Jul 31 12:52:50.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-pbdxp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6863'
Jul 31 12:52:51.207: INFO: stderr: ""
Jul 31 12:52:51.207: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 12:52:51.207: INFO: validating pod update-demo-nautilus-pbdxp
Jul 31 12:52:52.809: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 12:52:52.810: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 12:52:52.810: INFO: update-demo-nautilus-pbdxp is verified up and running
Jul 31 12:52:52.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-wcxrr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6863'
Jul 31 12:52:53.718: INFO: stderr: ""
Jul 31 12:52:53.718: INFO: stdout: ""
Jul 31 12:52:53.718: INFO: update-demo-nautilus-wcxrr is created but not running
Jul 31 12:52:58.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6863'
Jul 31 12:53:00.210: INFO: stderr: ""
Jul 31 12:53:00.210: INFO: stdout: "update-demo-nautilus-pbdxp update-demo-nautilus-wcxrr "
Jul 31 12:53:00.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-pbdxp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6863'
Jul 31 12:53:00.385: INFO: stderr: ""
Jul 31 12:53:00.385: INFO: stdout: "true"
Jul 31 12:53:00.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-pbdxp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6863'
Jul 31 12:53:00.752: INFO: stderr: ""
Jul 31 12:53:00.752: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 12:53:00.752: INFO: validating pod update-demo-nautilus-pbdxp
Jul 31 12:53:01.219: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 12:53:01.219: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 12:53:01.219: INFO: update-demo-nautilus-pbdxp is verified up and running
Jul 31 12:53:01.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-wcxrr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6863'
Jul 31 12:53:02.116: INFO: stderr: ""
Jul 31 12:53:02.116: INFO: stdout: "true"
Jul 31 12:53:02.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-wcxrr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6863'
Jul 31 12:53:02.417: INFO: stderr: ""
Jul 31 12:53:02.417: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 12:53:02.417: INFO: validating pod update-demo-nautilus-wcxrr
Jul 31 12:53:03.514: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 12:53:03.514: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 12:53:03.514: INFO: update-demo-nautilus-wcxrr is verified up and running
STEP: using delete to clean up resources
Jul 31 12:53:03.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete --grace-period=0 --force -f - --namespace=kubectl-6863'
Jul 31 12:53:04.209: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 12:53:04.209: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 31 12:53:04.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6863'
Jul 31 12:53:04.403: INFO: stderr: "No resources found.\n"
Jul 31 12:53:04.403: INFO: stdout: ""
Jul 31 12:53:04.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -l name=update-demo --namespace=kubectl-6863 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 31 12:53:04.713: INFO: stderr: ""
Jul 31 12:53:04.713: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:53:04.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6863" for this suite.
Jul 31 12:53:14.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:53:22.438: INFO: namespace kubectl-6863 deletion completed in 17.428835183s

• [SLOW TEST:68.784 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:53:22.438: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0731 12:53:35.330596      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 31 12:53:35.330: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:53:35.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5919" for this suite.
Jul 31 12:53:42.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:53:47.892: INFO: namespace gc-5919 deletion completed in 12.54864977s

• [SLOW TEST:25.455 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:53:47.893: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 31 12:53:49.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-576'
Jul 31 12:53:50.101: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 31 12:53:50.101: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jul 31 12:53:50.511: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-rvkjf]
Jul 31 12:53:50.512: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-rvkjf" in namespace "kubectl-576" to be "running and ready"
Jul 31 12:53:50.528: INFO: Pod "e2e-test-nginx-rc-rvkjf": Phase="Pending", Reason="", readiness=false. Elapsed: 15.988239ms
Jul 31 12:53:52.713: INFO: Pod "e2e-test-nginx-rc-rvkjf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.201309865s
Jul 31 12:53:56.409: INFO: Pod "e2e-test-nginx-rc-rvkjf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.89733412s
Jul 31 12:53:58.419: INFO: Pod "e2e-test-nginx-rc-rvkjf": Phase="Running", Reason="", readiness=true. Elapsed: 7.907362696s
Jul 31 12:53:58.419: INFO: Pod "e2e-test-nginx-rc-rvkjf" satisfied condition "running and ready"
Jul 31 12:53:58.419: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-rvkjf]
Jul 31 12:53:58.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 logs rc/e2e-test-nginx-rc --namespace=kubectl-576'
Jul 31 12:53:59.205: INFO: stderr: ""
Jul 31 12:53:59.205: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Jul 31 12:53:59.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete rc e2e-test-nginx-rc --namespace=kubectl-576'
Jul 31 12:53:59.910: INFO: stderr: ""
Jul 31 12:53:59.910: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:53:59.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-576" for this suite.
Jul 31 12:54:22.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:54:26.912: INFO: namespace kubectl-576 deletion completed in 26.983608056s

• [SLOW TEST:39.019 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:54:26.912: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 12:54:27.475: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:54:36.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3857" for this suite.
Jul 31 12:55:18.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:55:21.750: INFO: namespace pods-3857 deletion completed in 45.433228983s

• [SLOW TEST:54.838 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:55:21.750: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-01cd5782-4f28-4c9d-ae62-f74bd63c0f54
STEP: Creating a pod to test consume secrets
Jul 31 12:55:23.796: INFO: Waiting up to 5m0s for pod "pod-secrets-5a474f7f-50aa-43e6-b200-5cf5b6ad9bb6" in namespace "secrets-3259" to be "success or failure"
Jul 31 12:55:24.109: INFO: Pod "pod-secrets-5a474f7f-50aa-43e6-b200-5cf5b6ad9bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 313.037171ms
Jul 31 12:55:26.209: INFO: Pod "pod-secrets-5a474f7f-50aa-43e6-b200-5cf5b6ad9bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.412419673s
Jul 31 12:55:28.709: INFO: Pod "pod-secrets-5a474f7f-50aa-43e6-b200-5cf5b6ad9bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.912714171s
Jul 31 12:55:30.813: INFO: Pod "pod-secrets-5a474f7f-50aa-43e6-b200-5cf5b6ad9bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.016841739s
Jul 31 12:55:33.914: INFO: Pod "pod-secrets-5a474f7f-50aa-43e6-b200-5cf5b6ad9bb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.117956865s
STEP: Saw pod success
Jul 31 12:55:33.914: INFO: Pod "pod-secrets-5a474f7f-50aa-43e6-b200-5cf5b6ad9bb6" satisfied condition "success or failure"
Jul 31 12:55:33.930: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-secrets-5a474f7f-50aa-43e6-b200-5cf5b6ad9bb6 container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 12:55:34.809: INFO: Waiting for pod pod-secrets-5a474f7f-50aa-43e6-b200-5cf5b6ad9bb6 to disappear
Jul 31 12:55:35.108: INFO: Pod pod-secrets-5a474f7f-50aa-43e6-b200-5cf5b6ad9bb6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:55:35.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3259" for this suite.
Jul 31 12:55:43.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:55:48.697: INFO: namespace secrets-3259 deletion completed in 13.286530686s
STEP: Destroying namespace "secret-namespace-4544" for this suite.
Jul 31 12:55:56.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:56:03.931: INFO: namespace secret-namespace-4544 deletion completed in 15.233391723s

• [SLOW TEST:42.181 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:56:03.932: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 12:56:04.608: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2343c671-d070-4976-8384-4bd27e6d8743" in namespace "downward-api-3570" to be "success or failure"
Jul 31 12:56:04.910: INFO: Pod "downwardapi-volume-2343c671-d070-4976-8384-4bd27e6d8743": Phase="Pending", Reason="", readiness=false. Elapsed: 301.283178ms
Jul 31 12:56:08.009: INFO: Pod "downwardapi-volume-2343c671-d070-4976-8384-4bd27e6d8743": Phase="Pending", Reason="", readiness=false. Elapsed: 3.400319052s
Jul 31 12:56:10.209: INFO: Pod "downwardapi-volume-2343c671-d070-4976-8384-4bd27e6d8743": Phase="Pending", Reason="", readiness=false. Elapsed: 5.600198468s
Jul 31 12:56:12.809: INFO: Pod "downwardapi-volume-2343c671-d070-4976-8384-4bd27e6d8743": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.200086929s
STEP: Saw pod success
Jul 31 12:56:12.809: INFO: Pod "downwardapi-volume-2343c671-d070-4976-8384-4bd27e6d8743" satisfied condition "success or failure"
Jul 31 12:56:13.210: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-h22s9 pod downwardapi-volume-2343c671-d070-4976-8384-4bd27e6d8743 container client-container: <nil>
STEP: delete the pod
Jul 31 12:56:14.009: INFO: Waiting for pod downwardapi-volume-2343c671-d070-4976-8384-4bd27e6d8743 to disappear
Jul 31 12:56:14.209: INFO: Pod downwardapi-volume-2343c671-d070-4976-8384-4bd27e6d8743 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:56:14.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3570" for this suite.
Jul 31 12:56:21.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:56:29.647: INFO: namespace downward-api-3570 deletion completed in 15.123239619s

• [SLOW TEST:25.715 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:56:29.648: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Jul 31 12:56:30.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 create -f - --namespace=kubectl-2140'
Jul 31 12:56:32.707: INFO: stderr: ""
Jul 31 12:56:32.707: INFO: stdout: "pod/pause created\n"
Jul 31 12:56:32.707: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 31 12:56:32.707: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2140" to be "running and ready"
Jul 31 12:56:33.713: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.006209428s
Jul 31 12:56:35.736: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.029183599s
Jul 31 12:56:38.014: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 5.30731481s
Jul 31 12:56:38.014: INFO: Pod "pause" satisfied condition "running and ready"
Jul 31 12:56:38.014: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 31 12:56:38.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 label pods pause testing-label=testing-label-value --namespace=kubectl-2140'
Jul 31 12:56:38.639: INFO: stderr: ""
Jul 31 12:56:38.639: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 31 12:56:38.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pod pause -L testing-label --namespace=kubectl-2140'
Jul 31 12:56:38.938: INFO: stderr: ""
Jul 31 12:56:38.938: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 31 12:56:38.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 label pods pause testing-label- --namespace=kubectl-2140'
Jul 31 12:56:39.510: INFO: stderr: ""
Jul 31 12:56:39.510: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 31 12:56:39.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pod pause -L testing-label --namespace=kubectl-2140'
Jul 31 12:56:40.507: INFO: stderr: ""
Jul 31 12:56:40.507: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          8s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Jul 31 12:56:40.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete --grace-period=0 --force -f - --namespace=kubectl-2140'
Jul 31 12:56:41.806: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 12:56:41.806: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 31 12:56:41.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get rc,svc -l name=pause --no-headers --namespace=kubectl-2140'
Jul 31 12:56:43.010: INFO: stderr: "No resources found.\n"
Jul 31 12:56:43.010: INFO: stdout: ""
Jul 31 12:56:43.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -l name=pause --namespace=kubectl-2140 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 31 12:56:43.415: INFO: stderr: ""
Jul 31 12:56:43.415: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:56:43.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2140" for this suite.
Jul 31 12:56:50.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:56:53.809: INFO: namespace kubectl-2140 deletion completed in 9.891378371s

• [SLOW TEST:24.161 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:56:53.809: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:56:54.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6066" for this suite.
Jul 31 12:57:18.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:57:22.043: INFO: namespace pods-6066 deletion completed in 27.322521942s

• [SLOW TEST:28.235 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:57:22.044: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 12:57:22.642: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ce56c037-b3d5-4ac1-8570-8dc2f0030ffb" in namespace "downward-api-1985" to be "success or failure"
Jul 31 12:57:22.657: INFO: Pod "downwardapi-volume-ce56c037-b3d5-4ac1-8570-8dc2f0030ffb": Phase="Pending", Reason="", readiness=false. Elapsed: 15.134147ms
Jul 31 12:57:24.715: INFO: Pod "downwardapi-volume-ce56c037-b3d5-4ac1-8570-8dc2f0030ffb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073404117s
Jul 31 12:57:27.020: INFO: Pod "downwardapi-volume-ce56c037-b3d5-4ac1-8570-8dc2f0030ffb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.378477516s
Jul 31 12:57:30.109: INFO: Pod "downwardapi-volume-ce56c037-b3d5-4ac1-8570-8dc2f0030ffb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.466720926s
STEP: Saw pod success
Jul 31 12:57:30.109: INFO: Pod "downwardapi-volume-ce56c037-b3d5-4ac1-8570-8dc2f0030ffb" satisfied condition "success or failure"
Jul 31 12:57:30.123: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downwardapi-volume-ce56c037-b3d5-4ac1-8570-8dc2f0030ffb container client-container: <nil>
STEP: delete the pod
Jul 31 12:57:32.010: INFO: Waiting for pod downwardapi-volume-ce56c037-b3d5-4ac1-8570-8dc2f0030ffb to disappear
Jul 31 12:57:33.243: INFO: Pod downwardapi-volume-ce56c037-b3d5-4ac1-8570-8dc2f0030ffb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:57:33.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1985" for this suite.
Jul 31 12:57:41.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:57:47.752: INFO: namespace downward-api-1985 deletion completed in 13.73851863s

• [SLOW TEST:25.708 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:57:47.771: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-f6f4995c-f3ab-4f62-b536-f66b66629d5f
STEP: Creating a pod to test consume configMaps
Jul 31 12:57:48.156: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bcdfe4a5-be4c-4112-8852-81a35d370c4c" in namespace "projected-7939" to be "success or failure"
Jul 31 12:57:48.166: INFO: Pod "pod-projected-configmaps-bcdfe4a5-be4c-4112-8852-81a35d370c4c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.401397ms
Jul 31 12:57:51.309: INFO: Pod "pod-projected-configmaps-bcdfe4a5-be4c-4112-8852-81a35d370c4c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.152574769s
Jul 31 12:57:54.209: INFO: Pod "pod-projected-configmaps-bcdfe4a5-be4c-4112-8852-81a35d370c4c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052655881s
Jul 31 12:57:56.222: INFO: Pod "pod-projected-configmaps-bcdfe4a5-be4c-4112-8852-81a35d370c4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.066228535s
STEP: Saw pod success
Jul 31 12:57:56.223: INFO: Pod "pod-projected-configmaps-bcdfe4a5-be4c-4112-8852-81a35d370c4c" satisfied condition "success or failure"
Jul 31 12:57:56.229: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-projected-configmaps-bcdfe4a5-be4c-4112-8852-81a35d370c4c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 12:57:56.578: INFO: Waiting for pod pod-projected-configmaps-bcdfe4a5-be4c-4112-8852-81a35d370c4c to disappear
Jul 31 12:57:56.584: INFO: Pod pod-projected-configmaps-bcdfe4a5-be4c-4112-8852-81a35d370c4c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:57:56.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7939" for this suite.
Jul 31 12:58:03.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:58:12.132: INFO: namespace projected-7939 deletion completed in 15.534712954s

• [SLOW TEST:24.368 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:58:12.176: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-edb68bc0-6ee8-4c23-a189-3f6c1edd58b6
STEP: Creating a pod to test consume configMaps
Jul 31 12:58:12.299: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5f7f8bb5-8872-457e-bc6a-87ca99562802" in namespace "projected-4111" to be "success or failure"
Jul 31 12:58:12.515: INFO: Pod "pod-projected-configmaps-5f7f8bb5-8872-457e-bc6a-87ca99562802": Phase="Pending", Reason="", readiness=false. Elapsed: 215.658279ms
Jul 31 12:58:14.525: INFO: Pod "pod-projected-configmaps-5f7f8bb5-8872-457e-bc6a-87ca99562802": Phase="Pending", Reason="", readiness=false. Elapsed: 2.225914024s
Jul 31 12:58:17.508: INFO: Pod "pod-projected-configmaps-5f7f8bb5-8872-457e-bc6a-87ca99562802": Phase="Pending", Reason="", readiness=false. Elapsed: 5.208756575s
Jul 31 12:58:19.908: INFO: Pod "pod-projected-configmaps-5f7f8bb5-8872-457e-bc6a-87ca99562802": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.608820163s
STEP: Saw pod success
Jul 31 12:58:19.909: INFO: Pod "pod-projected-configmaps-5f7f8bb5-8872-457e-bc6a-87ca99562802" satisfied condition "success or failure"
Jul 31 12:58:20.309: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-projected-configmaps-5f7f8bb5-8872-457e-bc6a-87ca99562802 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 12:58:22.409: INFO: Waiting for pod pod-projected-configmaps-5f7f8bb5-8872-457e-bc6a-87ca99562802 to disappear
Jul 31 12:58:23.109: INFO: Pod pod-projected-configmaps-5f7f8bb5-8872-457e-bc6a-87ca99562802 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:58:23.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4111" for this suite.
Jul 31 12:58:30.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:58:33.967: INFO: namespace projected-4111 deletion completed in 10.545558759s

• [SLOW TEST:21.792 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:58:33.968: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-35dd1b00-2e0e-4e0c-8169-fd6c88b3784b
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:58:36.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4131" for this suite.
Jul 31 12:58:43.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:58:51.709: INFO: namespace secrets-4131 deletion completed in 15.098457864s

• [SLOW TEST:17.741 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:58:51.710: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-8650cde3-8dd0-4269-8442-2108c97ec459
STEP: Creating a pod to test consume secrets
Jul 31 12:58:53.309: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-951004a7-80c9-4cc9-972a-f3f5722ca1a8" in namespace "projected-4985" to be "success or failure"
Jul 31 12:58:53.714: INFO: Pod "pod-projected-secrets-951004a7-80c9-4cc9-972a-f3f5722ca1a8": Phase="Pending", Reason="", readiness=false. Elapsed: 404.503306ms
Jul 31 12:58:55.910: INFO: Pod "pod-projected-secrets-951004a7-80c9-4cc9-972a-f3f5722ca1a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.600344573s
Jul 31 12:58:57.917: INFO: Pod "pod-projected-secrets-951004a7-80c9-4cc9-972a-f3f5722ca1a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.607523046s
Jul 31 12:59:00.708: INFO: Pod "pod-projected-secrets-951004a7-80c9-4cc9-972a-f3f5722ca1a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.398983861s
STEP: Saw pod success
Jul 31 12:59:00.709: INFO: Pod "pod-projected-secrets-951004a7-80c9-4cc9-972a-f3f5722ca1a8" satisfied condition "success or failure"
Jul 31 12:59:01.022: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-projected-secrets-951004a7-80c9-4cc9-972a-f3f5722ca1a8 container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 12:59:02.109: INFO: Waiting for pod pod-projected-secrets-951004a7-80c9-4cc9-972a-f3f5722ca1a8 to disappear
Jul 31 12:59:02.308: INFO: Pod pod-projected-secrets-951004a7-80c9-4cc9-972a-f3f5722ca1a8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:59:02.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4985" for this suite.
Jul 31 12:59:09.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 12:59:21.244: INFO: namespace projected-4985 deletion completed in 18.730412663s

• [SLOW TEST:29.534 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 12:59:21.245: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 12:59:27.866: INFO: Waiting up to 5m0s for pod "client-envvars-d4fda646-e263-4be8-a7e7-3604af87a030" in namespace "pods-7563" to be "success or failure"
Jul 31 12:59:28.009: INFO: Pod "client-envvars-d4fda646-e263-4be8-a7e7-3604af87a030": Phase="Pending", Reason="", readiness=false. Elapsed: 142.297686ms
Jul 31 12:59:30.619: INFO: Pod "client-envvars-d4fda646-e263-4be8-a7e7-3604af87a030": Phase="Pending", Reason="", readiness=false. Elapsed: 2.752845546s
Jul 31 12:59:32.913: INFO: Pod "client-envvars-d4fda646-e263-4be8-a7e7-3604af87a030": Phase="Pending", Reason="", readiness=false. Elapsed: 5.046343858s
Jul 31 12:59:36.508: INFO: Pod "client-envvars-d4fda646-e263-4be8-a7e7-3604af87a030": Phase="Pending", Reason="", readiness=false. Elapsed: 8.642093033s
Jul 31 12:59:38.708: INFO: Pod "client-envvars-d4fda646-e263-4be8-a7e7-3604af87a030": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.842167275s
STEP: Saw pod success
Jul 31 12:59:38.709: INFO: Pod "client-envvars-d4fda646-e263-4be8-a7e7-3604af87a030" satisfied condition "success or failure"
Jul 31 12:59:39.108: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-h22s9 pod client-envvars-d4fda646-e263-4be8-a7e7-3604af87a030 container env3cont: <nil>
STEP: delete the pod
Jul 31 12:59:40.608: INFO: Waiting for pod client-envvars-d4fda646-e263-4be8-a7e7-3604af87a030 to disappear
Jul 31 12:59:40.812: INFO: Pod client-envvars-d4fda646-e263-4be8-a7e7-3604af87a030 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 12:59:40.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7563" for this suite.
Jul 31 13:00:25.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:00:26.964: INFO: namespace pods-7563 deletion completed in 45.954481739s

• [SLOW TEST:65.719 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:00:26.964: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Jul 31 13:00:28.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 create -f - --namespace=kubectl-9806'
Jul 31 13:00:29.466: INFO: stderr: ""
Jul 31 13:00:29.466: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 13:00:29.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9806'
Jul 31 13:00:30.609: INFO: stderr: ""
Jul 31 13:00:30.609: INFO: stdout: "update-demo-nautilus-2rj7f update-demo-nautilus-q7jrn "
Jul 31 13:00:30.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-2rj7f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9806'
Jul 31 13:00:30.738: INFO: stderr: ""
Jul 31 13:00:30.738: INFO: stdout: ""
Jul 31 13:00:30.738: INFO: update-demo-nautilus-2rj7f is created but not running
Jul 31 13:00:35.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9806'
Jul 31 13:00:35.875: INFO: stderr: ""
Jul 31 13:00:35.875: INFO: stdout: "update-demo-nautilus-2rj7f update-demo-nautilus-q7jrn "
Jul 31 13:00:35.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-2rj7f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9806'
Jul 31 13:00:38.221: INFO: stderr: ""
Jul 31 13:00:38.221: INFO: stdout: "true"
Jul 31 13:00:38.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-2rj7f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9806'
Jul 31 13:00:39.106: INFO: stderr: ""
Jul 31 13:00:39.106: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 13:00:39.106: INFO: validating pod update-demo-nautilus-2rj7f
Jul 31 13:00:39.808: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 13:00:39.808: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 13:00:39.808: INFO: update-demo-nautilus-2rj7f is verified up and running
Jul 31 13:00:39.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-q7jrn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9806'
Jul 31 13:00:40.605: INFO: stderr: ""
Jul 31 13:00:40.605: INFO: stdout: "true"
Jul 31 13:00:40.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-nautilus-q7jrn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9806'
Jul 31 13:00:41.122: INFO: stderr: ""
Jul 31 13:00:41.122: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 31 13:00:41.122: INFO: validating pod update-demo-nautilus-q7jrn
Jul 31 13:00:46.508: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 13:00:46.508: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 13:00:46.508: INFO: update-demo-nautilus-q7jrn is verified up and running
STEP: rolling-update to new replication controller
Jul 31 13:00:46.510: INFO: scanned /root for discovery docs: <nil>
Jul 31 13:00:46.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-9806'
Jul 31 13:01:21.533: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 31 13:01:21.533: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 13:01:21.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9806'
Jul 31 13:01:22.413: INFO: stderr: ""
Jul 31 13:01:22.413: INFO: stdout: "update-demo-kitten-869l9 update-demo-kitten-t9cj6 "
Jul 31 13:01:22.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-kitten-869l9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9806'
Jul 31 13:01:22.561: INFO: stderr: ""
Jul 31 13:01:22.561: INFO: stdout: "true"
Jul 31 13:01:22.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-kitten-869l9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9806'
Jul 31 13:01:23.230: INFO: stderr: ""
Jul 31 13:01:23.230: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 31 13:01:23.230: INFO: validating pod update-demo-kitten-869l9
Jul 31 13:01:24.408: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 31 13:01:24.408: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 31 13:01:24.408: INFO: update-demo-kitten-869l9 is verified up and running
Jul 31 13:01:24.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-kitten-t9cj6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9806'
Jul 31 13:01:24.813: INFO: stderr: ""
Jul 31 13:01:24.813: INFO: stdout: "true"
Jul 31 13:01:24.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 get pods update-demo-kitten-t9cj6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9806'
Jul 31 13:01:27.113: INFO: stderr: ""
Jul 31 13:01:27.113: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 31 13:01:27.113: INFO: validating pod update-demo-kitten-t9cj6
Jul 31 13:01:28.208: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 31 13:01:28.208: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 31 13:01:28.208: INFO: update-demo-kitten-t9cj6 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:01:28.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9806" for this suite.
Jul 31 13:01:51.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:01:58.342: INFO: namespace kubectl-9806 deletion completed in 29.531580161s

• [SLOW TEST:91.377 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:01:58.342: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-14feb808-f566-48e7-a7ac-ff1bbd9d98b9
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:01:59.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3380" for this suite.
Jul 31 13:02:06.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:02:12.042: INFO: namespace configmap-3380 deletion completed in 12.328827956s

• [SLOW TEST:13.699 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:02:12.043: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 31 13:02:12.233: INFO: Waiting up to 5m0s for pod "pod-f119d826-5b61-462d-8767-a1722fde2ffd" in namespace "emptydir-1036" to be "success or failure"
Jul 31 13:02:12.241: INFO: Pod "pod-f119d826-5b61-462d-8767-a1722fde2ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.207196ms
Jul 31 13:02:14.418: INFO: Pod "pod-f119d826-5b61-462d-8767-a1722fde2ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184763917s
Jul 31 13:02:18.013: INFO: Pod "pod-f119d826-5b61-462d-8767-a1722fde2ffd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.780029933s
STEP: Saw pod success
Jul 31 13:02:18.013: INFO: Pod "pod-f119d826-5b61-462d-8767-a1722fde2ffd" satisfied condition "success or failure"
Jul 31 13:02:18.019: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-f119d826-5b61-462d-8767-a1722fde2ffd container test-container: <nil>
STEP: delete the pod
Jul 31 13:02:18.908: INFO: Waiting for pod pod-f119d826-5b61-462d-8767-a1722fde2ffd to disappear
Jul 31 13:02:19.308: INFO: Pod pod-f119d826-5b61-462d-8767-a1722fde2ffd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:02:19.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1036" for this suite.
Jul 31 13:02:26.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:02:30.810: INFO: namespace emptydir-1036 deletion completed in 11.100140016s

• [SLOW TEST:18.767 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:02:30.811: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul 31 13:02:31.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 create -f - --namespace=kubectl-7296'
Jul 31 13:02:34.713: INFO: stderr: ""
Jul 31 13:02:34.713: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 31 13:02:35.913: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 13:02:35.913: INFO: Found 0 / 1
Jul 31 13:02:36.720: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 13:02:36.720: INFO: Found 0 / 1
Jul 31 13:02:38.013: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 13:02:38.013: INFO: Found 0 / 1
Jul 31 13:02:39.313: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 13:02:39.313: INFO: Found 0 / 1
Jul 31 13:02:39.912: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 13:02:39.912: INFO: Found 0 / 1
Jul 31 13:02:40.915: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 13:02:40.915: INFO: Found 0 / 1
Jul 31 13:02:42.421: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 13:02:42.421: INFO: Found 1 / 1
Jul 31 13:02:42.423: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 31 13:02:42.473: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 13:02:42.473: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 31 13:02:42.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 patch pod redis-master-tkvsm --namespace=kubectl-7296 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 31 13:02:42.842: INFO: stderr: ""
Jul 31 13:02:42.842: INFO: stdout: "pod/redis-master-tkvsm patched\n"
STEP: checking annotations
Jul 31 13:02:43.113: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 13:02:43.113: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:02:43.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7296" for this suite.
Jul 31 13:03:06.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:03:14.336: INFO: namespace kubectl-7296 deletion completed in 31.209823134s

• [SLOW TEST:43.525 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:03:14.337: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 31 13:03:14.908: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-546,SelfLink:/api/v1/namespaces/watch-546/configmaps/e2e-watch-test-label-changed,UID:6b3cf9bf-6725-4d25-93f2-746dc10eb872,ResourceVersion:1106051,Generation:0,CreationTimestamp:2019-07-31 13:03:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 31 13:03:14.908: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-546,SelfLink:/api/v1/namespaces/watch-546/configmaps/e2e-watch-test-label-changed,UID:6b3cf9bf-6725-4d25-93f2-746dc10eb872,ResourceVersion:1106052,Generation:0,CreationTimestamp:2019-07-31 13:03:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 31 13:03:14.908: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-546,SelfLink:/api/v1/namespaces/watch-546/configmaps/e2e-watch-test-label-changed,UID:6b3cf9bf-6725-4d25-93f2-746dc10eb872,ResourceVersion:1106053,Generation:0,CreationTimestamp:2019-07-31 13:03:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 31 13:03:26.928: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-546,SelfLink:/api/v1/namespaces/watch-546/configmaps/e2e-watch-test-label-changed,UID:6b3cf9bf-6725-4d25-93f2-746dc10eb872,ResourceVersion:1106085,Generation:0,CreationTimestamp:2019-07-31 13:03:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 31 13:03:26.928: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-546,SelfLink:/api/v1/namespaces/watch-546/configmaps/e2e-watch-test-label-changed,UID:6b3cf9bf-6725-4d25-93f2-746dc10eb872,ResourceVersion:1106086,Generation:0,CreationTimestamp:2019-07-31 13:03:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul 31 13:03:26.929: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-546,SelfLink:/api/v1/namespaces/watch-546/configmaps/e2e-watch-test-label-changed,UID:6b3cf9bf-6725-4d25-93f2-746dc10eb872,ResourceVersion:1106087,Generation:0,CreationTimestamp:2019-07-31 13:03:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:03:26.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-546" for this suite.
Jul 31 13:03:33.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:03:35.648: INFO: namespace watch-546 deletion completed in 8.706251793s

• [SLOW TEST:21.312 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:03:35.648: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 31 13:03:36.311: INFO: Waiting up to 5m0s for pod "pod-7c8feb10-e618-48d4-9627-b95ecb047db3" in namespace "emptydir-7783" to be "success or failure"
Jul 31 13:03:36.317: INFO: Pod "pod-7c8feb10-e618-48d4-9627-b95ecb047db3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.257948ms
Jul 31 13:03:38.608: INFO: Pod "pod-7c8feb10-e618-48d4-9627-b95ecb047db3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.297420394s
Jul 31 13:03:41.614: INFO: Pod "pod-7c8feb10-e618-48d4-9627-b95ecb047db3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.303329682s
STEP: Saw pod success
Jul 31 13:03:41.614: INFO: Pod "pod-7c8feb10-e618-48d4-9627-b95ecb047db3" satisfied condition "success or failure"
Jul 31 13:03:41.622: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-7c8feb10-e618-48d4-9627-b95ecb047db3 container test-container: <nil>
STEP: delete the pod
Jul 31 13:03:41.857: INFO: Waiting for pod pod-7c8feb10-e618-48d4-9627-b95ecb047db3 to disappear
Jul 31 13:03:41.870: INFO: Pod pod-7c8feb10-e618-48d4-9627-b95ecb047db3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:03:41.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7783" for this suite.
Jul 31 13:03:48.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:03:55.619: INFO: namespace emptydir-7783 deletion completed in 13.72842433s

• [SLOW TEST:19.971 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:03:55.620: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 13:04:21.910: INFO: Container started at 2019-07-31 13:03:58 +0000 UTC, pod became ready at 2019-07-31 13:04:20 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:04:21.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5096" for this suite.
Jul 31 13:04:44.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:04:50.808: INFO: namespace container-probe-5096 deletion completed in 28.888670225s

• [SLOW TEST:55.189 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:04:50.809: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Jul 31 13:04:51.608: INFO: Waiting up to 5m0s for pod "var-expansion-6b49f09c-804a-4659-8e32-b7302b54b792" in namespace "var-expansion-1897" to be "success or failure"
Jul 31 13:04:51.916: INFO: Pod "var-expansion-6b49f09c-804a-4659-8e32-b7302b54b792": Phase="Pending", Reason="", readiness=false. Elapsed: 308.312539ms
Jul 31 13:04:54.408: INFO: Pod "var-expansion-6b49f09c-804a-4659-8e32-b7302b54b792": Phase="Pending", Reason="", readiness=false. Elapsed: 2.799928586s
Jul 31 13:04:56.608: INFO: Pod "var-expansion-6b49f09c-804a-4659-8e32-b7302b54b792": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.000008648s
STEP: Saw pod success
Jul 31 13:04:56.608: INFO: Pod "var-expansion-6b49f09c-804a-4659-8e32-b7302b54b792" satisfied condition "success or failure"
Jul 31 13:04:56.808: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod var-expansion-6b49f09c-804a-4659-8e32-b7302b54b792 container dapi-container: <nil>
STEP: delete the pod
Jul 31 13:04:57.708: INFO: Waiting for pod var-expansion-6b49f09c-804a-4659-8e32-b7302b54b792 to disappear
Jul 31 13:04:57.908: INFO: Pod var-expansion-6b49f09c-804a-4659-8e32-b7302b54b792 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:04:57.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1897" for this suite.
Jul 31 13:05:05.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:05:12.708: INFO: namespace var-expansion-1897 deletion completed in 14.395330044s

• [SLOW TEST:21.900 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:05:12.716: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-efd00a42-b82c-4542-ba28-051cf48d8bc2
STEP: Creating a pod to test consume configMaps
Jul 31 13:05:14.008: INFO: Waiting up to 5m0s for pod "pod-configmaps-59acb3e7-f267-4aa3-9794-0403a8f7e949" in namespace "configmap-8454" to be "success or failure"
Jul 31 13:05:14.208: INFO: Pod "pod-configmaps-59acb3e7-f267-4aa3-9794-0403a8f7e949": Phase="Pending", Reason="", readiness=false. Elapsed: 199.49429ms
Jul 31 13:05:16.214: INFO: Pod "pod-configmaps-59acb3e7-f267-4aa3-9794-0403a8f7e949": Phase="Pending", Reason="", readiness=false. Elapsed: 2.205643919s
Jul 31 13:05:18.817: INFO: Pod "pod-configmaps-59acb3e7-f267-4aa3-9794-0403a8f7e949": Phase="Pending", Reason="", readiness=false. Elapsed: 4.80860597s
Jul 31 13:05:21.313: INFO: Pod "pod-configmaps-59acb3e7-f267-4aa3-9794-0403a8f7e949": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.304731736s
STEP: Saw pod success
Jul 31 13:05:21.313: INFO: Pod "pod-configmaps-59acb3e7-f267-4aa3-9794-0403a8f7e949" satisfied condition "success or failure"
Jul 31 13:05:21.320: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-configmaps-59acb3e7-f267-4aa3-9794-0403a8f7e949 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 13:05:22.281: INFO: Waiting for pod pod-configmaps-59acb3e7-f267-4aa3-9794-0403a8f7e949 to disappear
Jul 31 13:05:22.292: INFO: Pod pod-configmaps-59acb3e7-f267-4aa3-9794-0403a8f7e949 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:05:22.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8454" for this suite.
Jul 31 13:05:29.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:05:35.623: INFO: namespace configmap-8454 deletion completed in 13.313616458s

• [SLOW TEST:22.907 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:05:35.624: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 31 13:05:43.230: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-9c707945-473e-49a0-93b4-20c6fd9346a7,GenerateName:,Namespace:events-380,SelfLink:/api/v1/namespaces/events-380/pods/send-events-9c707945-473e-49a0-93b4-20c6fd9346a7,UID:1b554cc1-c217-4678-95fa-590d78a20388,ResourceVersion:1106540,Generation:0,CreationTimestamp:2019-07-31 13:05:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 508563818,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-pnktw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pnktw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-pnktw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002436350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002436370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 13:05:36 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 13:05:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 13:05:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 13:05:36 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.26.178,StartTime:2019-07-31 13:05:36 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-07-31 13:05:40 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://62d05d114675742877049885cee955d5ddbcfd65bfa268df623c10b991698107}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jul 31 13:05:45.313: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 31 13:05:47.513: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:05:47.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-380" for this suite.
Jul 31 13:06:25.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:06:28.069: INFO: namespace events-380 deletion completed in 40.529485961s

• [SLOW TEST:52.446 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:06:28.070: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 31 13:06:28.512: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:06:35.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8965" for this suite.
Jul 31 13:06:41.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:06:49.808: INFO: namespace init-container-8965 deletion completed in 14.390187994s

• [SLOW TEST:21.738 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:06:49.809: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-34d8969d-8bcd-4802-abc9-def64c41ed9c
STEP: Creating a pod to test consume configMaps
Jul 31 13:06:51.848: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3833b04-efbe-4d37-a1a2-8a2fa80803bf" in namespace "configmap-8422" to be "success or failure"
Jul 31 13:06:52.008: INFO: Pod "pod-configmaps-f3833b04-efbe-4d37-a1a2-8a2fa80803bf": Phase="Pending", Reason="", readiness=false. Elapsed: 160.299524ms
Jul 31 13:06:55.116: INFO: Pod "pod-configmaps-f3833b04-efbe-4d37-a1a2-8a2fa80803bf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.26828615s
Jul 31 13:06:58.213: INFO: Pod "pod-configmaps-f3833b04-efbe-4d37-a1a2-8a2fa80803bf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.365054831s
Jul 31 13:07:00.323: INFO: Pod "pod-configmaps-f3833b04-efbe-4d37-a1a2-8a2fa80803bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.475127785s
STEP: Saw pod success
Jul 31 13:07:00.323: INFO: Pod "pod-configmaps-f3833b04-efbe-4d37-a1a2-8a2fa80803bf" satisfied condition "success or failure"
Jul 31 13:07:00.341: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-configmaps-f3833b04-efbe-4d37-a1a2-8a2fa80803bf container configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 13:07:03.027: INFO: Waiting for pod pod-configmaps-f3833b04-efbe-4d37-a1a2-8a2fa80803bf to disappear
Jul 31 13:07:03.041: INFO: Pod pod-configmaps-f3833b04-efbe-4d37-a1a2-8a2fa80803bf no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:07:03.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8422" for this suite.
Jul 31 13:07:10.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:07:13.740: INFO: namespace configmap-8422 deletion completed in 10.687124826s

• [SLOW TEST:23.932 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:07:13.740: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 31 13:07:13.815: INFO: Waiting up to 5m0s for pod "pod-5f84f6e9-061f-48c0-94eb-c59a1b033c9b" in namespace "emptydir-4933" to be "success or failure"
Jul 31 13:07:13.822: INFO: Pod "pod-5f84f6e9-061f-48c0-94eb-c59a1b033c9b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.060048ms
Jul 31 13:07:16.908: INFO: Pod "pod-5f84f6e9-061f-48c0-94eb-c59a1b033c9b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.093035424s
Jul 31 13:07:19.712: INFO: Pod "pod-5f84f6e9-061f-48c0-94eb-c59a1b033c9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.897562863s
STEP: Saw pod success
Jul 31 13:07:19.712: INFO: Pod "pod-5f84f6e9-061f-48c0-94eb-c59a1b033c9b" satisfied condition "success or failure"
Jul 31 13:07:19.721: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-5f84f6e9-061f-48c0-94eb-c59a1b033c9b container test-container: <nil>
STEP: delete the pod
Jul 31 13:07:20.936: INFO: Waiting for pod pod-5f84f6e9-061f-48c0-94eb-c59a1b033c9b to disappear
Jul 31 13:07:20.943: INFO: Pod pod-5f84f6e9-061f-48c0-94eb-c59a1b033c9b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:07:20.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4933" for this suite.
Jul 31 13:07:27.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:07:34.936: INFO: namespace emptydir-4933 deletion completed in 13.979290617s

• [SLOW TEST:21.196 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:07:34.937: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 31 13:07:42.420: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:07:42.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3799" for this suite.
Jul 31 13:07:49.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:07:55.108: INFO: namespace container-runtime-3799 deletion completed in 12.361624166s

• [SLOW TEST:20.171 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:07:55.109: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 13:07:55.192: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f01de4f8-d9c5-4104-8693-e5f71fffa760" in namespace "projected-982" to be "success or failure"
Jul 31 13:07:55.217: INFO: Pod "downwardapi-volume-f01de4f8-d9c5-4104-8693-e5f71fffa760": Phase="Pending", Reason="", readiness=false. Elapsed: 24.191181ms
Jul 31 13:07:57.413: INFO: Pod "downwardapi-volume-f01de4f8-d9c5-4104-8693-e5f71fffa760": Phase="Pending", Reason="", readiness=false. Elapsed: 2.220488162s
Jul 31 13:07:59.808: INFO: Pod "downwardapi-volume-f01de4f8-d9c5-4104-8693-e5f71fffa760": Phase="Pending", Reason="", readiness=false. Elapsed: 4.615745711s
Jul 31 13:08:01.816: INFO: Pod "downwardapi-volume-f01de4f8-d9c5-4104-8693-e5f71fffa760": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.624024851s
STEP: Saw pod success
Jul 31 13:08:01.817: INFO: Pod "downwardapi-volume-f01de4f8-d9c5-4104-8693-e5f71fffa760" satisfied condition "success or failure"
Jul 31 13:08:01.830: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downwardapi-volume-f01de4f8-d9c5-4104-8693-e5f71fffa760 container client-container: <nil>
STEP: delete the pod
Jul 31 13:08:03.213: INFO: Waiting for pod downwardapi-volume-f01de4f8-d9c5-4104-8693-e5f71fffa760 to disappear
Jul 31 13:08:03.221: INFO: Pod downwardapi-volume-f01de4f8-d9c5-4104-8693-e5f71fffa760 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:08:03.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-982" for this suite.
Jul 31 13:08:10.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:08:15.355: INFO: namespace projected-982 deletion completed in 12.08112277s

• [SLOW TEST:20.246 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:08:15.356: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 31 13:08:16.406: INFO: Waiting up to 5m0s for pod "pod-ff59fa1c-c1ee-4913-bf5f-1b856ea47300" in namespace "emptydir-5310" to be "success or failure"
Jul 31 13:08:16.612: INFO: Pod "pod-ff59fa1c-c1ee-4913-bf5f-1b856ea47300": Phase="Pending", Reason="", readiness=false. Elapsed: 205.362356ms
Jul 31 13:08:18.622: INFO: Pod "pod-ff59fa1c-c1ee-4913-bf5f-1b856ea47300": Phase="Pending", Reason="", readiness=false. Elapsed: 2.215531949s
Jul 31 13:08:21.722: INFO: Pod "pod-ff59fa1c-c1ee-4913-bf5f-1b856ea47300": Phase="Pending", Reason="", readiness=false. Elapsed: 5.315991392s
Jul 31 13:08:24.512: INFO: Pod "pod-ff59fa1c-c1ee-4913-bf5f-1b856ea47300": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.105698775s
STEP: Saw pod success
Jul 31 13:08:24.512: INFO: Pod "pod-ff59fa1c-c1ee-4913-bf5f-1b856ea47300" satisfied condition "success or failure"
Jul 31 13:08:26.408: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-ff59fa1c-c1ee-4913-bf5f-1b856ea47300 container test-container: <nil>
STEP: delete the pod
Jul 31 13:08:26.844: INFO: Waiting for pod pod-ff59fa1c-c1ee-4913-bf5f-1b856ea47300 to disappear
Jul 31 13:08:26.857: INFO: Pod pod-ff59fa1c-c1ee-4913-bf5f-1b856ea47300 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:08:26.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5310" for this suite.
Jul 31 13:08:33.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:08:37.708: INFO: namespace emptydir-5310 deletion completed in 10.84190553s

• [SLOW TEST:22.351 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:08:37.708: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-dbzw
STEP: Creating a pod to test atomic-volume-subpath
Jul 31 13:08:39.206: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-dbzw" in namespace "subpath-188" to be "success or failure"
Jul 31 13:08:39.413: INFO: Pod "pod-subpath-test-configmap-dbzw": Phase="Pending", Reason="", readiness=false. Elapsed: 206.967808ms
Jul 31 13:08:41.508: INFO: Pod "pod-subpath-test-configmap-dbzw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.30178216s
Jul 31 13:08:43.612: INFO: Pod "pod-subpath-test-configmap-dbzw": Phase="Running", Reason="", readiness=true. Elapsed: 4.406148032s
Jul 31 13:08:45.808: INFO: Pod "pod-subpath-test-configmap-dbzw": Phase="Running", Reason="", readiness=true. Elapsed: 6.601791093s
Jul 31 13:08:48.008: INFO: Pod "pod-subpath-test-configmap-dbzw": Phase="Running", Reason="", readiness=true. Elapsed: 8.801652205s
Jul 31 13:08:50.413: INFO: Pod "pod-subpath-test-configmap-dbzw": Phase="Running", Reason="", readiness=true. Elapsed: 11.2069581s
Jul 31 13:08:53.326: INFO: Pod "pod-subpath-test-configmap-dbzw": Phase="Running", Reason="", readiness=true. Elapsed: 14.119603948s
Jul 31 13:08:56.410: INFO: Pod "pod-subpath-test-configmap-dbzw": Phase="Running", Reason="", readiness=true. Elapsed: 17.203848621s
Jul 31 13:08:58.613: INFO: Pod "pod-subpath-test-configmap-dbzw": Phase="Running", Reason="", readiness=true. Elapsed: 19.4073385s
Jul 31 13:09:00.922: INFO: Pod "pod-subpath-test-configmap-dbzw": Phase="Running", Reason="", readiness=true. Elapsed: 21.715519255s
Jul 31 13:09:03.209: INFO: Pod "pod-subpath-test-configmap-dbzw": Phase="Running", Reason="", readiness=true. Elapsed: 24.002762025s
Jul 31 13:09:05.408: INFO: Pod "pod-subpath-test-configmap-dbzw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.201830525s
STEP: Saw pod success
Jul 31 13:09:05.408: INFO: Pod "pod-subpath-test-configmap-dbzw" satisfied condition "success or failure"
Jul 31 13:09:05.808: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-subpath-test-configmap-dbzw container test-container-subpath-configmap-dbzw: <nil>
STEP: delete the pod
Jul 31 13:09:07.108: INFO: Waiting for pod pod-subpath-test-configmap-dbzw to disappear
Jul 31 13:09:07.407: INFO: Pod pod-subpath-test-configmap-dbzw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-dbzw
Jul 31 13:09:07.407: INFO: Deleting pod "pod-subpath-test-configmap-dbzw" in namespace "subpath-188"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:09:07.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-188" for this suite.
Jul 31 13:09:16.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:09:21.608: INFO: namespace subpath-188 deletion completed in 12.998035641s

• [SLOW TEST:43.899 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:09:21.608: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-22e83e59-e05c-4856-87d7-466bc6bf326b
Jul 31 13:09:23.314: INFO: Pod name my-hostname-basic-22e83e59-e05c-4856-87d7-466bc6bf326b: Found 1 pods out of 1
Jul 31 13:09:23.314: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-22e83e59-e05c-4856-87d7-466bc6bf326b" are running
Jul 31 13:09:30.109: INFO: Pod "my-hostname-basic-22e83e59-e05c-4856-87d7-466bc6bf326b-cmnbc" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-31 13:09:22 +0000 UTC Reason: Message:}])
Jul 31 13:09:30.109: INFO: Trying to dial the pod
Jul 31 13:09:39.108: INFO: Controller my-hostname-basic-22e83e59-e05c-4856-87d7-466bc6bf326b: Got expected result from replica 1 [my-hostname-basic-22e83e59-e05c-4856-87d7-466bc6bf326b-cmnbc]: "my-hostname-basic-22e83e59-e05c-4856-87d7-466bc6bf326b-cmnbc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:09:39.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1617" for this suite.
Jul 31 13:09:45.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:09:47.449: INFO: namespace replication-controller-1617 deletion completed in 8.328984752s

• [SLOW TEST:25.841 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:09:47.450: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 13:09:48.059: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 31 13:09:53.112: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 31 13:09:53.112: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 31 13:09:56.208: INFO: Creating deployment "test-rollover-deployment"
Jul 31 13:09:58.408: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 31 13:09:58.418: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 31 13:10:00.309: INFO: Ensure that both replica sets have 1 created replica
Jul 31 13:10:00.347: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 31 13:10:00.827: INFO: Updating deployment test-rollover-deployment
Jul 31 13:10:00.827: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 31 13:10:02.913: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 31 13:10:02.930: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 31 13:10:02.943: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 13:10:02.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175401, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 13:10:05.036: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 13:10:05.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175401, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 13:10:07.509: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 13:10:07.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175405, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 13:10:10.809: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 13:10:10.809: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175405, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 13:10:11.124: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 13:10:11.124: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175405, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 13:10:13.915: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 13:10:13.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175405, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 13:10:15.408: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 13:10:15.408: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175405, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700175396, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 13:10:18.008: INFO: 
Jul 31 13:10:18.008: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 31 13:10:18.826: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-7313,SelfLink:/apis/apps/v1/namespaces/deployment-7313/deployments/test-rollover-deployment,UID:c5ffbb5c-a1a7-4e43-9358-faf6f8a10cd3,ResourceVersion:1107557,Generation:2,CreationTimestamp:2019-07-31 13:09:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-31 13:09:56 +0000 UTC 2019-07-31 13:09:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-31 13:10:15 +0000 UTC 2019-07-31 13:09:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 31 13:10:18.837: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-7313,SelfLink:/apis/apps/v1/namespaces/deployment-7313/replicasets/test-rollover-deployment-854595fc44,UID:e501a5ca-70d5-44ac-b158-c76e36f3a5f4,ResourceVersion:1107547,Generation:2,CreationTimestamp:2019-07-31 13:10:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment c5ffbb5c-a1a7-4e43-9358-faf6f8a10cd3 0xc0008e3807 0xc0008e3808}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 31 13:10:18.837: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 31 13:10:18.837: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-7313,SelfLink:/apis/apps/v1/namespaces/deployment-7313/replicasets/test-rollover-controller,UID:e5e14d39-b198-4164-8238-6c9b91383035,ResourceVersion:1107556,Generation:2,CreationTimestamp:2019-07-31 13:09:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment c5ffbb5c-a1a7-4e43-9358-faf6f8a10cd3 0xc0008e3737 0xc0008e3738}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 31 13:10:18.837: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-7313,SelfLink:/apis/apps/v1/namespaces/deployment-7313/replicasets/test-rollover-deployment-9b8b997cf,UID:7fd77fdc-da16-4b1e-ab50-709d1af307ea,ResourceVersion:1107498,Generation:2,CreationTimestamp:2019-07-31 13:09:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment c5ffbb5c-a1a7-4e43-9358-faf6f8a10cd3 0xc0008e38d0 0xc0008e38d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 31 13:10:18.844: INFO: Pod "test-rollover-deployment-854595fc44-jdjbv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-jdjbv,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-7313,SelfLink:/api/v1/namespaces/deployment-7313/pods/test-rollover-deployment-854595fc44-jdjbv,UID:c3733932-c33d-4367-bc8e-3654c49feefe,ResourceVersion:1107521,Generation:0,CreationTimestamp:2019-07-31 13:10:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 e501a5ca-70d5-44ac-b158-c76e36f3a5f4 0xc000d7c6d7 0xc000d7c6d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vlpvv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vlpvv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-vlpvv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2bh9r-78c4c5b4fb-czrvz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d7c740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d7c760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 13:10:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 13:10:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 13:10:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-31 13:10:00 +0000 UTC  }],Message:,Reason:,HostIP:192.168.1.12,PodIP:172.25.26.188,StartTime:2019-07-31 13:10:01 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-31 13:10:04 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://7f8d0741532f5edfd74bc596c04726259b5df4e93ad9399a976aac77d81fa569}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:10:18.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7313" for this suite.
Jul 31 13:10:26.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:10:29.946: INFO: namespace deployment-7313 deletion completed in 11.081169043s

• [SLOW TEST:42.495 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:10:29.946: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-25753927-8f40-4503-81d1-51a992df9a7c in namespace container-probe-3938
Jul 31 13:10:36.908: INFO: Started pod test-webserver-25753927-8f40-4503-81d1-51a992df9a7c in namespace container-probe-3938
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 13:10:36.916: INFO: Initial restart count of pod test-webserver-25753927-8f40-4503-81d1-51a992df9a7c is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:14:37.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3938" for this suite.
Jul 31 13:14:43.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:14:50.807: INFO: namespace container-probe-3938 deletion completed in 13.190700779s

• [SLOW TEST:260.862 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:14:50.808: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 13:14:52.307: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a0d425f-4162-47e3-b71a-1001c7961930" in namespace "projected-2422" to be "success or failure"
Jul 31 13:14:52.508: INFO: Pod "downwardapi-volume-4a0d425f-4162-47e3-b71a-1001c7961930": Phase="Pending", Reason="", readiness=false. Elapsed: 200.711233ms
Jul 31 13:14:54.708: INFO: Pod "downwardapi-volume-4a0d425f-4162-47e3-b71a-1001c7961930": Phase="Pending", Reason="", readiness=false. Elapsed: 2.400784375s
Jul 31 13:14:57.008: INFO: Pod "downwardapi-volume-4a0d425f-4162-47e3-b71a-1001c7961930": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.700735103s
STEP: Saw pod success
Jul 31 13:14:57.008: INFO: Pod "downwardapi-volume-4a0d425f-4162-47e3-b71a-1001c7961930" satisfied condition "success or failure"
Jul 31 13:14:57.207: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-h22s9 pod downwardapi-volume-4a0d425f-4162-47e3-b71a-1001c7961930 container client-container: <nil>
STEP: delete the pod
Jul 31 13:15:00.412: INFO: Waiting for pod downwardapi-volume-4a0d425f-4162-47e3-b71a-1001c7961930 to disappear
Jul 31 13:15:00.707: INFO: Pod downwardapi-volume-4a0d425f-4162-47e3-b71a-1001c7961930 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:15:00.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2422" for this suite.
Jul 31 13:15:07.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:15:09.271: INFO: namespace projected-2422 deletion completed in 8.359865347s

• [SLOW TEST:18.464 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:15:09.274: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 13:15:09.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 create -f - --namespace=kubectl-1124'
Jul 31 13:15:12.709: INFO: stderr: ""
Jul 31 13:15:12.709: INFO: stdout: "replicationcontroller/redis-master created\n"
Jul 31 13:15:12.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 create -f - --namespace=kubectl-1124'
Jul 31 13:15:13.611: INFO: stderr: ""
Jul 31 13:15:13.611: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 31 13:15:14.713: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 13:15:14.713: INFO: Found 0 / 1
Jul 31 13:15:17.207: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 13:15:17.207: INFO: Found 0 / 1
Jul 31 13:15:17.712: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 13:15:17.712: INFO: Found 1 / 1
Jul 31 13:15:17.712: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 31 13:15:17.719: INFO: Selector matched 1 pods for map[app:redis]
Jul 31 13:15:17.719: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 31 13:15:17.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 describe pod redis-master-bx2gv --namespace=kubectl-1124'
Jul 31 13:15:17.869: INFO: stderr: ""
Jul 31 13:15:17.869: INFO: stdout: "Name:           redis-master-bx2gv\nNamespace:      kubectl-1124\nPriority:       0\nNode:           worker-2bh9r-78c4c5b4fb-czrvz/192.168.1.12\nStart Time:     Wed, 31 Jul 2019 13:15:12 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             172.25.26.190\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://20abfded6d58b6f6d733ab39bc027d79be3f3028d7fbbe8d6a13999ab743c971\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 31 Jul 2019 13:15:16 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-dnbd7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-dnbd7:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-dnbd7\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                    Message\n  ----    ------     ----  ----                                    -------\n  Normal  Scheduled  5s    default-scheduler                       Successfully assigned kubectl-1124/redis-master-bx2gv to worker-2bh9r-78c4c5b4fb-czrvz\n  Normal  Pulled     3s    kubelet, worker-2bh9r-78c4c5b4fb-czrvz  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, worker-2bh9r-78c4c5b4fb-czrvz  Created container redis-master\n  Normal  Started    1s    kubelet, worker-2bh9r-78c4c5b4fb-czrvz  Started container redis-master\n"
Jul 31 13:15:17.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 describe rc redis-master --namespace=kubectl-1124'
Jul 31 13:15:19.289: INFO: stderr: ""
Jul 31 13:15:19.289: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1124\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  7s    replication-controller  Created pod: redis-master-bx2gv\n"
Jul 31 13:15:19.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 describe service redis-master --namespace=kubectl-1124'
Jul 31 13:15:20.626: INFO: stderr: ""
Jul 31 13:15:20.626: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1124\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.10.10.190\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.25.26.190:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 31 13:15:20.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 describe node worker-2bh9r-78c4c5b4fb-96hfw'
Jul 31 13:15:21.606: INFO: stderr: ""
Jul 31 13:15:21.606: INFO: stdout: "Name:               worker-2bh9r-78c4c5b4fb-96hfw\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b7c4fa0b-7960-4311-a86b-507dbf58e8ac\n                    beta.kubernetes.io/os=linux\n                    container-linux-update.v1.coreos.com/group=stable\n                    container-linux-update.v1.coreos.com/id=coreos\n                    container-linux-update.v1.coreos.com/reboot-needed=false\n                    container-linux-update.v1.coreos.com/version=2135.5.0\n                    failure-domain.beta.kubernetes.io/zone=ix1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=worker-2bh9r-78c4c5b4fb-96hfw\n                    kubernetes.io/os=linux\n                    kubernetes.io/uses-container-linux=true\n                    machine-controller/owned-by=f4b790ed-36de-4908-a4f4-113b0cf1fe6b\nAnnotations:        cluster.k8s.io/machine: kube-system/worker-2bh9r-78c4c5b4fb-96hfw\n                    container-linux-update.v1.coreos.com/last-checked-time: 1564577759\n                    container-linux-update.v1.coreos.com/new-version: 0.0.0\n                    container-linux-update.v1.coreos.com/reboot-in-progress: false\n                    container-linux-update.v1.coreos.com/reboot-needed: false\n                    container-linux-update.v1.coreos.com/status: UPDATE_STATUS_IDLE\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"66:c0:bb:8a:93:76\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.1.8\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 29 Jul 2019 14:52:33 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 31 Jul 2019 13:15:13 +0000   Mon, 29 Jul 2019 14:52:33 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 31 Jul 2019 13:15:13 +0000   Mon, 29 Jul 2019 14:52:33 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 31 Jul 2019 13:15:13 +0000   Mon, 29 Jul 2019 14:52:33 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 31 Jul 2019 13:15:13 +0000   Mon, 29 Jul 2019 14:53:15 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.1.8\n  ExternalIP:  185.116.247.201\n  Hostname:    worker-2bh9r-78c4c5b4fb-96hfw\nCapacity:\n attachable-volumes-cinder:  256\n cpu:                        2\n ephemeral-storage:          17897500Ki\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     4039640Ki\n pods:                       110\nAllocatable:\n attachable-volumes-cinder:  256\n cpu:                        1800m\n ephemeral-storage:          14346852325\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     3732440Ki\n pods:                       110\nSystem Info:\n Machine ID:                 135a193978474d2b8ef09522071f16e9\n System UUID:                135a1939-7847-4d2b-8ef0-9522071f16e9\n Boot ID:                    e495729b-e927-44f6-a04f-3389d6d3d438\n Kernel Version:             4.19.50-coreos-r1\n OS Image:                   Container Linux by CoreOS 2135.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.15.0\n Kube-Proxy Version:         v1.15.0\nPodCIDR:                     172.25.25.0/24\nProviderID:                  openstack:///135a1939-7847-4d2b-8ef0-9522071f16e9\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-e2e-job-1dfc7533d0924fc3                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         128m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-adffb9aed1fd4488-8wclh    0 (0%)        0 (0%)      0 (0%)           0 (0%)         128m\n  kube-system                canal-dtm6f                                                350m (19%)    100m (5%)   50Mi (1%)        50Mi (1%)      46h\n  kube-system                container-linux-update-agent-htx9q                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         46h\n  kube-system                coredns-9b6865ff9-kcflr                                    100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     46h\n  kube-system                kube-proxy-khr7h                                           75m (4%)      250m (13%)  50Mi (1%)        250Mi (6%)     46h\n  kube-system                node-exporter-h5vfd                                        20m (1%)      45m (2%)    48Mi (1%)        96Mi (2%)      46h\n  kube-system                node-local-dns-rv254                                       25m (1%)      0 (0%)      5Mi (0%)         30Mi (0%)      46h\n  kube-system                openvpn-client-84b8ff9b6f-h6ccx                            30m (1%)      200m (11%)  30Mi (0%)        82Mi (2%)      46h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests    Limits\n  --------                   --------    ------\n  cpu                        600m (33%)  595m (33%)\n  memory                     253Mi (6%)  678Mi (18%)\n  ephemeral-storage          0 (0%)      0 (0%)\n  attachable-volumes-cinder  0           0\nEvents:                      <none>\n"
Jul 31 13:15:21.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 describe namespace kubectl-1124'
Jul 31 13:15:22.932: INFO: stderr: ""
Jul 31 13:15:22.933: INFO: stdout: "Name:         kubectl-1124\nLabels:       e2e-framework=kubectl\n              e2e-run=358ed29b-781c-4b5d-9a2f-1b3e8ee2904b\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:15:22.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1124" for this suite.
Jul 31 13:15:48.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:15:49.644: INFO: namespace kubectl-1124 deletion completed in 25.134881241s

• [SLOW TEST:40.370 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:15:49.645: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-2qw4
STEP: Creating a pod to test atomic-volume-subpath
Jul 31 13:15:50.140: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2qw4" in namespace "subpath-5999" to be "success or failure"
Jul 31 13:15:51.007: INFO: Pod "pod-subpath-test-configmap-2qw4": Phase="Pending", Reason="", readiness=false. Elapsed: 867.234118ms
Jul 31 13:15:55.507: INFO: Pod "pod-subpath-test-configmap-2qw4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.367384484s
Jul 31 13:15:57.611: INFO: Pod "pod-subpath-test-configmap-2qw4": Phase="Running", Reason="", readiness=true. Elapsed: 7.470961626s
Jul 31 13:15:59.719: INFO: Pod "pod-subpath-test-configmap-2qw4": Phase="Running", Reason="", readiness=true. Elapsed: 9.578788265s
Jul 31 13:16:02.608: INFO: Pod "pod-subpath-test-configmap-2qw4": Phase="Running", Reason="", readiness=true. Elapsed: 12.468110486s
Jul 31 13:16:04.969: INFO: Pod "pod-subpath-test-configmap-2qw4": Phase="Running", Reason="", readiness=true. Elapsed: 14.829372097s
Jul 31 13:16:07.108: INFO: Pod "pod-subpath-test-configmap-2qw4": Phase="Running", Reason="", readiness=true. Elapsed: 16.967846908s
Jul 31 13:16:09.708: INFO: Pod "pod-subpath-test-configmap-2qw4": Phase="Running", Reason="", readiness=true. Elapsed: 19.567626021s
Jul 31 13:16:11.909: INFO: Pod "pod-subpath-test-configmap-2qw4": Phase="Running", Reason="", readiness=true. Elapsed: 21.768651704s
Jul 31 13:16:14.311: INFO: Pod "pod-subpath-test-configmap-2qw4": Phase="Running", Reason="", readiness=true. Elapsed: 24.171425456s
Jul 31 13:16:16.708: INFO: Pod "pod-subpath-test-configmap-2qw4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.567706681s
STEP: Saw pod success
Jul 31 13:16:16.708: INFO: Pod "pod-subpath-test-configmap-2qw4" satisfied condition "success or failure"
Jul 31 13:16:16.714: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-subpath-test-configmap-2qw4 container test-container-subpath-configmap-2qw4: <nil>
STEP: delete the pod
Jul 31 13:16:17.407: INFO: Waiting for pod pod-subpath-test-configmap-2qw4 to disappear
Jul 31 13:16:18.007: INFO: Pod pod-subpath-test-configmap-2qw4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-2qw4
Jul 31 13:16:18.007: INFO: Deleting pod "pod-subpath-test-configmap-2qw4" in namespace "subpath-5999"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:16:19.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5999" for this suite.
Jul 31 13:16:25.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:16:33.923: INFO: namespace subpath-5999 deletion completed in 14.902318945s

• [SLOW TEST:44.278 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:16:33.923: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 13:16:37.310: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 31 13:16:37.382: INFO: Number of nodes with available pods: 0
Jul 31 13:16:37.383: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 13:16:38.424: INFO: Number of nodes with available pods: 0
Jul 31 13:16:38.424: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 13:16:40.009: INFO: Number of nodes with available pods: 0
Jul 31 13:16:40.009: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 13:16:40.424: INFO: Number of nodes with available pods: 0
Jul 31 13:16:40.424: INFO: Node worker-2bh9r-78c4c5b4fb-96hfw is running more than one daemon pod
Jul 31 13:16:44.116: INFO: Number of nodes with available pods: 2
Jul 31 13:16:44.116: INFO: Node worker-2bh9r-78c4c5b4fb-h22s9 is running more than one daemon pod
Jul 31 13:16:45.114: INFO: Number of nodes with available pods: 3
Jul 31 13:16:45.114: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 31 13:16:48.812: INFO: Wrong image for pod: daemon-set-h7kjc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:48.812: INFO: Wrong image for pod: daemon-set-hdjsq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:48.812: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:50.311: INFO: Wrong image for pod: daemon-set-h7kjc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:50.311: INFO: Pod daemon-set-h7kjc is not available
Jul 31 13:16:50.312: INFO: Wrong image for pod: daemon-set-hdjsq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:50.312: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:51.219: INFO: Pod daemon-set-bn7kj is not available
Jul 31 13:16:51.219: INFO: Wrong image for pod: daemon-set-hdjsq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:51.219: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:53.111: INFO: Pod daemon-set-bn7kj is not available
Jul 31 13:16:53.111: INFO: Wrong image for pod: daemon-set-hdjsq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:53.111: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:54.408: INFO: Pod daemon-set-bn7kj is not available
Jul 31 13:16:54.408: INFO: Wrong image for pod: daemon-set-hdjsq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:54.408: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:55.508: INFO: Pod daemon-set-bn7kj is not available
Jul 31 13:16:55.508: INFO: Wrong image for pod: daemon-set-hdjsq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:55.508: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:56.508: INFO: Wrong image for pod: daemon-set-hdjsq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:56.508: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:57.408: INFO: Wrong image for pod: daemon-set-hdjsq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:57.408: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:58.416: INFO: Wrong image for pod: daemon-set-hdjsq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:58.416: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:59.408: INFO: Wrong image for pod: daemon-set-hdjsq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:16:59.408: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:17:00.409: INFO: Pod daemon-set-5qr7h is not available
Jul 31 13:17:00.409: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:17:02.020: INFO: Pod daemon-set-5qr7h is not available
Jul 31 13:17:02.020: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:17:02.409: INFO: Pod daemon-set-5qr7h is not available
Jul 31 13:17:02.409: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:17:03.913: INFO: Pod daemon-set-5qr7h is not available
Jul 31 13:17:03.913: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:17:04.612: INFO: Pod daemon-set-5qr7h is not available
Jul 31 13:17:04.612: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:17:06.209: INFO: Pod daemon-set-5qr7h is not available
Jul 31 13:17:06.209: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:17:08.008: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:17:08.907: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:17:10.513: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:17:12.308: INFO: Wrong image for pod: daemon-set-wjbcd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 31 13:17:12.308: INFO: Pod daemon-set-wjbcd is not available
Jul 31 13:17:13.408: INFO: Pod daemon-set-fg7gd is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 31 13:17:14.507: INFO: Number of nodes with available pods: 2
Jul 31 13:17:14.507: INFO: Node worker-2bh9r-78c4c5b4fb-h22s9 is running more than one daemon pod
Jul 31 13:17:16.008: INFO: Number of nodes with available pods: 2
Jul 31 13:17:16.008: INFO: Node worker-2bh9r-78c4c5b4fb-h22s9 is running more than one daemon pod
Jul 31 13:17:16.909: INFO: Number of nodes with available pods: 3
Jul 31 13:17:16.910: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-991, will wait for the garbage collector to delete the pods
Jul 31 13:17:18.107: INFO: Deleting DaemonSet.extensions daemon-set took: 113.976179ms
Jul 31 13:17:18.608: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.36741ms
Jul 31 13:17:34.907: INFO: Number of nodes with available pods: 0
Jul 31 13:17:34.907: INFO: Number of running nodes: 0, number of available pods: 0
Jul 31 13:17:35.707: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-991/daemonsets","resourceVersion":"1108898"},"items":null}

Jul 31 13:17:35.911: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-991/pods","resourceVersion":"1108902"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:17:36.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-991" for this suite.
Jul 31 13:17:43.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:17:51.108: INFO: namespace daemonsets-991 deletion completed in 14.696009501s

• [SLOW TEST:77.185 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:17:51.108: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-92be80a7-b7f5-4bf8-aaac-fde8b0fd4e91
STEP: Creating a pod to test consume secrets
Jul 31 13:17:52.343: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ea4b5f3e-f05f-4d0b-894c-e4f9f38fec0d" in namespace "projected-7164" to be "success or failure"
Jul 31 13:17:52.709: INFO: Pod "pod-projected-secrets-ea4b5f3e-f05f-4d0b-894c-e4f9f38fec0d": Phase="Pending", Reason="", readiness=false. Elapsed: 365.260376ms
Jul 31 13:17:54.811: INFO: Pod "pod-projected-secrets-ea4b5f3e-f05f-4d0b-894c-e4f9f38fec0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.467468946s
Jul 31 13:17:57.618: INFO: Pod "pod-projected-secrets-ea4b5f3e-f05f-4d0b-894c-e4f9f38fec0d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.274491968s
Jul 31 13:18:00.207: INFO: Pod "pod-projected-secrets-ea4b5f3e-f05f-4d0b-894c-e4f9f38fec0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.864047219s
STEP: Saw pod success
Jul 31 13:18:00.207: INFO: Pod "pod-projected-secrets-ea4b5f3e-f05f-4d0b-894c-e4f9f38fec0d" satisfied condition "success or failure"
Jul 31 13:18:00.215: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-projected-secrets-ea4b5f3e-f05f-4d0b-894c-e4f9f38fec0d container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 31 13:18:02.141: INFO: Waiting for pod pod-projected-secrets-ea4b5f3e-f05f-4d0b-894c-e4f9f38fec0d to disappear
Jul 31 13:18:02.147: INFO: Pod pod-projected-secrets-ea4b5f3e-f05f-4d0b-894c-e4f9f38fec0d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:18:02.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7164" for this suite.
Jul 31 13:18:09.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:18:14.820: INFO: namespace projected-7164 deletion completed in 12.661681731s

• [SLOW TEST:23.712 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:18:14.821: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-8e3c7fd9-2b44-4be7-a706-c6a87ec6add9
STEP: Creating a pod to test consume configMaps
Jul 31 13:18:16.008: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b6a7798d-4aa6-4bb6-a953-eced3015f387" in namespace "projected-5516" to be "success or failure"
Jul 31 13:18:16.407: INFO: Pod "pod-projected-configmaps-b6a7798d-4aa6-4bb6-a953-eced3015f387": Phase="Pending", Reason="", readiness=false. Elapsed: 399.148403ms
Jul 31 13:18:19.407: INFO: Pod "pod-projected-configmaps-b6a7798d-4aa6-4bb6-a953-eced3015f387": Phase="Pending", Reason="", readiness=false. Elapsed: 3.398992722s
Jul 31 13:18:21.516: INFO: Pod "pod-projected-configmaps-b6a7798d-4aa6-4bb6-a953-eced3015f387": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.507382877s
STEP: Saw pod success
Jul 31 13:18:21.516: INFO: Pod "pod-projected-configmaps-b6a7798d-4aa6-4bb6-a953-eced3015f387" satisfied condition "success or failure"
Jul 31 13:18:21.530: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-projected-configmaps-b6a7798d-4aa6-4bb6-a953-eced3015f387 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 13:18:21.947: INFO: Waiting for pod pod-projected-configmaps-b6a7798d-4aa6-4bb6-a953-eced3015f387 to disappear
Jul 31 13:18:21.954: INFO: Pod pod-projected-configmaps-b6a7798d-4aa6-4bb6-a953-eced3015f387 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:18:21.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5516" for this suite.
Jul 31 13:18:30.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:18:38.307: INFO: namespace projected-5516 deletion completed in 16.344679744s

• [SLOW TEST:23.486 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:18:38.308: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 31 13:18:38.468: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 31 13:18:38.487: INFO: Waiting for terminating namespaces to be deleted...
Jul 31 13:18:38.494: INFO: 
Logging pods the kubelet thinks is on node worker-2bh9r-78c4c5b4fb-96hfw before test
Jul 31 13:18:39.610: INFO: coredns-9b6865ff9-kcflr from kube-system started at 2019-07-29 14:53:19 +0000 UTC (1 container statuses recorded)
Jul 31 13:18:39.610: INFO: 	Container coredns ready: true, restart count 0
Jul 31 13:18:39.610: INFO: canal-dtm6f from kube-system started at 2019-07-29 14:52:35 +0000 UTC (2 container statuses recorded)
Jul 31 13:18:39.610: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 13:18:39.610: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 31 13:18:39.610: INFO: node-local-dns-rv254 from kube-system started at 2019-07-29 14:53:16 +0000 UTC (1 container statuses recorded)
Jul 31 13:18:39.610: INFO: 	Container node-cache ready: true, restart count 0
Jul 31 13:18:39.611: INFO: container-linux-update-agent-htx9q from kube-system started at 2019-07-29 14:53:16 +0000 UTC (1 container statuses recorded)
Jul 31 13:18:39.611: INFO: 	Container update-agent ready: true, restart count 1
Jul 31 13:18:39.611: INFO: sonobuoy-systemd-logs-daemon-set-adffb9aed1fd4488-8wclh from heptio-sonobuoy started at 2019-07-31 11:06:29 +0000 UTC (2 container statuses recorded)
Jul 31 13:18:39.611: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jul 31 13:18:39.611: INFO: 	Container systemd-logs ready: true, restart count 2
Jul 31 13:18:39.611: INFO: node-exporter-h5vfd from kube-system started at 2019-07-29 14:52:36 +0000 UTC (2 container statuses recorded)
Jul 31 13:18:39.611: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 31 13:18:39.611: INFO: 	Container node-exporter ready: true, restart count 0
Jul 31 13:18:39.611: INFO: openvpn-client-84b8ff9b6f-h6ccx from kube-system started at 2019-07-29 14:53:19 +0000 UTC (2 container statuses recorded)
Jul 31 13:18:39.611: INFO: 	Container dnat-controller ready: true, restart count 0
Jul 31 13:18:39.611: INFO: 	Container openvpn-client ready: true, restart count 0
Jul 31 13:18:39.611: INFO: sonobuoy-e2e-job-1dfc7533d0924fc3 from heptio-sonobuoy started at 2019-07-31 11:06:29 +0000 UTC (2 container statuses recorded)
Jul 31 13:18:39.611: INFO: 	Container e2e ready: true, restart count 0
Jul 31 13:18:39.611: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 13:18:39.611: INFO: kube-proxy-khr7h from kube-system started at 2019-07-29 14:52:36 +0000 UTC (1 container statuses recorded)
Jul 31 13:18:39.611: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 13:18:39.611: INFO: 
Logging pods the kubelet thinks is on node worker-2bh9r-78c4c5b4fb-czrvz before test
Jul 31 13:18:40.511: INFO: node-exporter-c4vlv from kube-system started at 2019-07-29 14:55:03 +0000 UTC (2 container statuses recorded)
Jul 31 13:18:40.511: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 31 13:18:40.511: INFO: 	Container node-exporter ready: true, restart count 0
Jul 31 13:18:40.511: INFO: canal-fjc6d from kube-system started at 2019-07-29 14:55:03 +0000 UTC (2 container statuses recorded)
Jul 31 13:18:40.511: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 13:18:40.511: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 31 13:18:40.511: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-31 11:06:15 +0000 UTC (1 container statuses recorded)
Jul 31 13:18:40.511: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 31 13:18:40.511: INFO: sonobuoy-systemd-logs-daemon-set-adffb9aed1fd4488-4vrw8 from heptio-sonobuoy started at 2019-07-31 11:06:29 +0000 UTC (2 container statuses recorded)
Jul 31 13:18:40.511: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jul 31 13:18:40.511: INFO: 	Container systemd-logs ready: true, restart count 2
Jul 31 13:18:40.511: INFO: container-linux-update-agent-5j26n from kube-system started at 2019-07-29 14:55:48 +0000 UTC (1 container statuses recorded)
Jul 31 13:18:40.511: INFO: 	Container update-agent ready: true, restart count 0
Jul 31 13:18:40.511: INFO: node-local-dns-8r55k from kube-system started at 2019-07-29 14:55:48 +0000 UTC (1 container statuses recorded)
Jul 31 13:18:40.511: INFO: 	Container node-cache ready: true, restart count 0
Jul 31 13:18:40.511: INFO: kubernetes-dashboard-584d5ffc75-xxvnm from kube-system started at 2019-07-29 14:55:51 +0000 UTC (1 container statuses recorded)
Jul 31 13:18:40.511: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 31 13:18:40.511: INFO: kube-proxy-q8bhf from kube-system started at 2019-07-29 14:55:04 +0000 UTC (1 container statuses recorded)
Jul 31 13:18:40.511: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 13:18:40.511: INFO: 
Logging pods the kubelet thinks is on node worker-2bh9r-78c4c5b4fb-h22s9 before test
Jul 31 13:18:42.219: INFO: container-linux-update-agent-zsb6g from kube-system started at 2019-07-29 14:50:30 +0000 UTC (1 container statuses recorded)
Jul 31 13:18:42.219: INFO: 	Container update-agent ready: true, restart count 1
Jul 31 13:18:42.220: INFO: container-linux-update-operator-55d5b65b4-tgkz8 from kube-system started at 2019-07-29 14:50:33 +0000 UTC (1 container statuses recorded)
Jul 31 13:18:42.220: INFO: 	Container update-operator ready: true, restart count 0
Jul 31 13:18:42.221: INFO: sonobuoy-systemd-logs-daemon-set-adffb9aed1fd4488-8m2w7 from heptio-sonobuoy started at 2019-07-31 11:06:30 +0000 UTC (2 container statuses recorded)
Jul 31 13:18:42.221: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jul 31 13:18:42.221: INFO: 	Container systemd-logs ready: true, restart count 2
Jul 31 13:18:42.222: INFO: kube-proxy-jz7bw from kube-system started at 2019-07-29 14:49:48 +0000 UTC (1 container statuses recorded)
Jul 31 13:18:42.222: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 13:18:42.222: INFO: node-exporter-cg6lq from kube-system started at 2019-07-29 14:49:48 +0000 UTC (2 container statuses recorded)
Jul 31 13:18:42.222: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 31 13:18:42.222: INFO: 	Container node-exporter ready: true, restart count 0
Jul 31 13:18:42.222: INFO: coredns-9b6865ff9-qh2tz from kube-system started at 2019-07-29 14:55:52 +0000 UTC (1 container statuses recorded)
Jul 31 13:18:42.222: INFO: 	Container coredns ready: true, restart count 0
Jul 31 13:18:42.223: INFO: canal-jmmlf from kube-system started at 2019-07-29 14:49:48 +0000 UTC (2 container statuses recorded)
Jul 31 13:18:42.223: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 13:18:42.224: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 31 13:18:42.224: INFO: node-local-dns-4slzn from kube-system started at 2019-07-29 14:50:30 +0000 UTC (1 container statuses recorded)
Jul 31 13:18:42.224: INFO: 	Container node-cache ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-7d9ba4e5-e02e-4ab3-9d2e-e9a6dfd288de 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-7d9ba4e5-e02e-4ab3-9d2e-e9a6dfd288de off the node worker-2bh9r-78c4c5b4fb-czrvz
STEP: verifying the node doesn't have the label kubernetes.io/e2e-7d9ba4e5-e02e-4ab3-9d2e-e9a6dfd288de
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:18:57.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6077" for this suite.
Jul 31 13:19:17.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:19:19.833: INFO: namespace sched-pred-6077 deletion completed in 22.124160845s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:41.525 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:19:19.834: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 13:19:20.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca5577f2-d1d8-4369-a6f7-fb459137e18e" in namespace "projected-2270" to be "success or failure"
Jul 31 13:19:20.427: INFO: Pod "downwardapi-volume-ca5577f2-d1d8-4369-a6f7-fb459137e18e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.956591ms
Jul 31 13:19:22.526: INFO: Pod "downwardapi-volume-ca5577f2-d1d8-4369-a6f7-fb459137e18e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111789955s
Jul 31 13:19:25.510: INFO: Pod "downwardapi-volume-ca5577f2-d1d8-4369-a6f7-fb459137e18e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.095490236s
STEP: Saw pod success
Jul 31 13:19:25.510: INFO: Pod "downwardapi-volume-ca5577f2-d1d8-4369-a6f7-fb459137e18e" satisfied condition "success or failure"
Jul 31 13:19:25.516: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-h22s9 pod downwardapi-volume-ca5577f2-d1d8-4369-a6f7-fb459137e18e container client-container: <nil>
STEP: delete the pod
Jul 31 13:19:25.842: INFO: Waiting for pod downwardapi-volume-ca5577f2-d1d8-4369-a6f7-fb459137e18e to disappear
Jul 31 13:19:25.852: INFO: Pod downwardapi-volume-ca5577f2-d1d8-4369-a6f7-fb459137e18e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:19:25.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2270" for this suite.
Jul 31 13:19:34.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:19:37.442: INFO: namespace projected-2270 deletion completed in 10.333158897s

• [SLOW TEST:17.611 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:19:37.449: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 31 13:19:51.308: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 13:19:51.308: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 13:19:54.907: INFO: Exec stderr: ""
Jul 31 13:19:54.907: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 13:19:54.907: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 13:19:58.103: INFO: Exec stderr: ""
Jul 31 13:19:58.103: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 13:19:58.103: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 13:20:04.006: INFO: Exec stderr: ""
Jul 31 13:20:04.006: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 13:20:04.006: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 13:20:08.007: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 31 13:20:08.007: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 13:20:08.007: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 13:20:11.006: INFO: Exec stderr: ""
Jul 31 13:20:11.006: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 13:20:11.006: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 13:20:14.908: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 31 13:20:14.908: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 13:20:14.908: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 13:20:18.603: INFO: Exec stderr: ""
Jul 31 13:20:18.603: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 13:20:18.603: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 13:20:21.610: INFO: Exec stderr: ""
Jul 31 13:20:21.610: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 13:20:21.610: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 13:20:25.407: INFO: Exec stderr: ""
Jul 31 13:20:25.407: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 13:20:25.407: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 13:20:28.106: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:20:28.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1757" for this suite.
Jul 31 13:21:09.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:21:14.939: INFO: namespace e2e-kubelet-etc-hosts-1757 deletion completed in 46.329810517s

• [SLOW TEST:97.491 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:21:14.943: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 31 13:21:15.333: INFO: Waiting up to 5m0s for pod "downward-api-e1b32bb7-458b-43ed-b17a-1be0c34093fd" in namespace "downward-api-3752" to be "success or failure"
Jul 31 13:21:15.345: INFO: Pod "downward-api-e1b32bb7-458b-43ed-b17a-1be0c34093fd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.765473ms
Jul 31 13:21:17.507: INFO: Pod "downward-api-e1b32bb7-458b-43ed-b17a-1be0c34093fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.173783953s
Jul 31 13:21:19.807: INFO: Pod "downward-api-e1b32bb7-458b-43ed-b17a-1be0c34093fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.47396916s
Jul 31 13:21:22.010: INFO: Pod "downward-api-e1b32bb7-458b-43ed-b17a-1be0c34093fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.676626974s
STEP: Saw pod success
Jul 31 13:21:22.013: INFO: Pod "downward-api-e1b32bb7-458b-43ed-b17a-1be0c34093fd" satisfied condition "success or failure"
Jul 31 13:21:22.025: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downward-api-e1b32bb7-458b-43ed-b17a-1be0c34093fd container dapi-container: <nil>
STEP: delete the pod
Jul 31 13:21:23.409: INFO: Waiting for pod downward-api-e1b32bb7-458b-43ed-b17a-1be0c34093fd to disappear
Jul 31 13:21:23.807: INFO: Pod downward-api-e1b32bb7-458b-43ed-b17a-1be0c34093fd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:21:23.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3752" for this suite.
Jul 31 13:21:30.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:21:35.556: INFO: namespace downward-api-3752 deletion completed in 11.441560331s

• [SLOW TEST:20.614 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:21:35.558: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 31 13:21:36.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2339'
Jul 31 13:21:37.035: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 31 13:21:37.035: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Jul 31 13:21:37.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete jobs e2e-test-nginx-job --namespace=kubectl-2339'
Jul 31 13:21:37.708: INFO: stderr: ""
Jul 31 13:21:37.708: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:21:37.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2339" for this suite.
Jul 31 13:21:45.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:21:54.414: INFO: namespace kubectl-2339 deletion completed in 16.398452848s

• [SLOW TEST:18.856 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:21:54.414: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-24fef208-f980-45b8-9369-fcd9264d4005
STEP: Creating secret with name secret-projected-all-test-volume-bafaef09-662b-4c0a-9320-0ca327c0cc21
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 31 13:21:56.214: INFO: Waiting up to 5m0s for pod "projected-volume-e7c0939a-4115-4c64-a6b8-955c2696fb83" in namespace "projected-4093" to be "success or failure"
Jul 31 13:21:56.412: INFO: Pod "projected-volume-e7c0939a-4115-4c64-a6b8-955c2696fb83": Phase="Pending", Reason="", readiness=false. Elapsed: 196.479923ms
Jul 31 13:21:58.807: INFO: Pod "projected-volume-e7c0939a-4115-4c64-a6b8-955c2696fb83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.592047223s
Jul 31 13:22:00.912: INFO: Pod "projected-volume-e7c0939a-4115-4c64-a6b8-955c2696fb83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.696870862s
Jul 31 13:22:03.117: INFO: Pod "projected-volume-e7c0939a-4115-4c64-a6b8-955c2696fb83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.901644815s
STEP: Saw pod success
Jul 31 13:22:03.117: INFO: Pod "projected-volume-e7c0939a-4115-4c64-a6b8-955c2696fb83" satisfied condition "success or failure"
Jul 31 13:22:03.128: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod projected-volume-e7c0939a-4115-4c64-a6b8-955c2696fb83 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 31 13:22:03.848: INFO: Waiting for pod projected-volume-e7c0939a-4115-4c64-a6b8-955c2696fb83 to disappear
Jul 31 13:22:03.853: INFO: Pod projected-volume-e7c0939a-4115-4c64-a6b8-955c2696fb83 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:22:03.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4093" for this suite.
Jul 31 13:22:10.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:22:14.611: INFO: namespace projected-4093 deletion completed in 10.74537948s

• [SLOW TEST:20.197 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:22:14.611: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:23:07.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5126" for this suite.
Jul 31 13:23:14.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:23:24.028: INFO: namespace watch-5126 deletion completed in 16.3895339s

• [SLOW TEST:69.417 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:23:24.029: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6667.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6667.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6667.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6667.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6667.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6667.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 13:23:39.011: INFO: DNS probes using dns-6667/dns-test-2f23f5f4-13e6-443d-bf48-72bc5bd64ec8 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:23:39.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6667" for this suite.
Jul 31 13:23:45.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:23:49.026: INFO: namespace dns-6667 deletion completed in 9.116365672s

• [SLOW TEST:24.998 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:23:49.028: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-4hln
STEP: Creating a pod to test atomic-volume-subpath
Jul 31 13:23:49.642: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-4hln" in namespace "subpath-5130" to be "success or failure"
Jul 31 13:23:49.807: INFO: Pod "pod-subpath-test-downwardapi-4hln": Phase="Pending", Reason="", readiness=false. Elapsed: 165.193014ms
Jul 31 13:23:52.007: INFO: Pod "pod-subpath-test-downwardapi-4hln": Phase="Pending", Reason="", readiness=false. Elapsed: 2.365307423s
Jul 31 13:23:54.118: INFO: Pod "pod-subpath-test-downwardapi-4hln": Phase="Running", Reason="", readiness=true. Elapsed: 4.476086443s
Jul 31 13:23:56.219: INFO: Pod "pod-subpath-test-downwardapi-4hln": Phase="Running", Reason="", readiness=true. Elapsed: 6.577233356s
Jul 31 13:23:58.507: INFO: Pod "pod-subpath-test-downwardapi-4hln": Phase="Running", Reason="", readiness=true. Elapsed: 8.864970952s
Jul 31 13:24:00.707: INFO: Pod "pod-subpath-test-downwardapi-4hln": Phase="Running", Reason="", readiness=true. Elapsed: 11.065184202s
Jul 31 13:24:02.907: INFO: Pod "pod-subpath-test-downwardapi-4hln": Phase="Running", Reason="", readiness=true. Elapsed: 13.264778697s
Jul 31 13:24:05.110: INFO: Pod "pod-subpath-test-downwardapi-4hln": Phase="Running", Reason="", readiness=true. Elapsed: 15.467671221s
Jul 31 13:24:08.307: INFO: Pod "pod-subpath-test-downwardapi-4hln": Phase="Running", Reason="", readiness=true. Elapsed: 18.665218575s
Jul 31 13:24:10.511: INFO: Pod "pod-subpath-test-downwardapi-4hln": Phase="Running", Reason="", readiness=true. Elapsed: 20.869157572s
Jul 31 13:24:13.707: INFO: Pod "pod-subpath-test-downwardapi-4hln": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.065228644s
STEP: Saw pod success
Jul 31 13:24:13.707: INFO: Pod "pod-subpath-test-downwardapi-4hln" satisfied condition "success or failure"
Jul 31 13:24:13.714: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-subpath-test-downwardapi-4hln container test-container-subpath-downwardapi-4hln: <nil>
STEP: delete the pod
Jul 31 13:24:14.435: INFO: Waiting for pod pod-subpath-test-downwardapi-4hln to disappear
Jul 31 13:24:14.442: INFO: Pod pod-subpath-test-downwardapi-4hln no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-4hln
Jul 31 13:24:14.442: INFO: Deleting pod "pod-subpath-test-downwardapi-4hln" in namespace "subpath-5130"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:24:14.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5130" for this suite.
Jul 31 13:24:21.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:24:27.564: INFO: namespace subpath-5130 deletion completed in 13.106686138s

• [SLOW TEST:38.536 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:24:27.564: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 31 13:24:28.211: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:24:37.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9257" for this suite.
Jul 31 13:25:00.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:25:04.039: INFO: namespace init-container-9257 deletion completed in 26.416895053s

• [SLOW TEST:36.475 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:25:04.040: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 31 13:25:10.407: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:25:11.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4841" for this suite.
Jul 31 13:25:33.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:25:38.172: INFO: namespace replicaset-4841 deletion completed in 26.862021076s

• [SLOW TEST:34.133 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:25:38.173: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 31 13:25:41.107: INFO: Waiting up to 5m0s for pod "pod-041c9618-1929-4735-9257-baf7eb34c061" in namespace "emptydir-2109" to be "success or failure"
Jul 31 13:25:43.207: INFO: Pod "pod-041c9618-1929-4735-9257-baf7eb34c061": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100505316s
Jul 31 13:25:46.712: INFO: Pod "pod-041c9618-1929-4735-9257-baf7eb34c061": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.604908939s
STEP: Saw pod success
Jul 31 13:25:46.712: INFO: Pod "pod-041c9618-1929-4735-9257-baf7eb34c061" satisfied condition "success or failure"
Jul 31 13:25:46.723: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-041c9618-1929-4735-9257-baf7eb34c061 container test-container: <nil>
STEP: delete the pod
Jul 31 13:25:47.039: INFO: Waiting for pod pod-041c9618-1929-4735-9257-baf7eb34c061 to disappear
Jul 31 13:25:47.045: INFO: Pod pod-041c9618-1929-4735-9257-baf7eb34c061 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:25:47.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2109" for this suite.
Jul 31 13:25:54.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:26:02.543: INFO: namespace emptydir-2109 deletion completed in 15.48867641s

• [SLOW TEST:24.370 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:26:02.545: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 31 13:26:10.507: INFO: Successfully updated pod "annotationupdateb5ebc892-0065-4e82-b119-140a3fc19261"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:26:15.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1333" for this suite.
Jul 31 13:26:37.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:26:44.358: INFO: namespace downward-api-1333 deletion completed in 29.041609503s

• [SLOW TEST:41.813 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:26:44.359: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jul 31 13:26:45.107: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Jul 31 13:26:46.254: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 31 13:26:52.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 13:26:57.507: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 13:26:59.407: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 13:27:01.008: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 13:27:02.548: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63700176406, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 13:27:07.908: INFO: Waited 2.70001949s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:27:21.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1371" for this suite.
Jul 31 13:27:31.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:27:41.821: INFO: namespace aggregator-1371 deletion completed in 19.905493477s

• [SLOW TEST:57.463 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:27:41.823: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-772203c1-6eb9-4288-ae36-74fdc3008c55
STEP: Creating a pod to test consume configMaps
Jul 31 13:27:43.134: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8f6eae97-90b4-446d-b23a-77d53ff33a7b" in namespace "projected-4325" to be "success or failure"
Jul 31 13:27:43.142: INFO: Pod "pod-projected-configmaps-8f6eae97-90b4-446d-b23a-77d53ff33a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.18442ms
Jul 31 13:27:45.211: INFO: Pod "pod-projected-configmaps-8f6eae97-90b4-446d-b23a-77d53ff33a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077339365s
Jul 31 13:27:47.707: INFO: Pod "pod-projected-configmaps-8f6eae97-90b4-446d-b23a-77d53ff33a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.572963437s
Jul 31 13:27:50.107: INFO: Pod "pod-projected-configmaps-8f6eae97-90b4-446d-b23a-77d53ff33a7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.972827608s
STEP: Saw pod success
Jul 31 13:27:50.107: INFO: Pod "pod-projected-configmaps-8f6eae97-90b4-446d-b23a-77d53ff33a7b" satisfied condition "success or failure"
Jul 31 13:27:50.507: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-projected-configmaps-8f6eae97-90b4-446d-b23a-77d53ff33a7b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 13:27:52.007: INFO: Waiting for pod pod-projected-configmaps-8f6eae97-90b4-446d-b23a-77d53ff33a7b to disappear
Jul 31 13:27:52.307: INFO: Pod pod-projected-configmaps-8f6eae97-90b4-446d-b23a-77d53ff33a7b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:27:52.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4325" for this suite.
Jul 31 13:27:59.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:28:08.107: INFO: namespace projected-4325 deletion completed in 15.198428435s

• [SLOW TEST:26.285 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:28:08.110: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8383
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 31 13:28:08.907: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 31 13:28:47.024: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.26.210:8080/dial?request=hostName&protocol=http&host=172.25.25.64&port=8080&tries=1'] Namespace:pod-network-test-8383 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 13:28:47.024: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 13:28:48.907: INFO: Waiting for endpoints: map[]
Jul 31 13:28:49.107: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.26.210:8080/dial?request=hostName&protocol=http&host=172.25.26.209&port=8080&tries=1'] Namespace:pod-network-test-8383 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 13:28:49.107: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 13:28:51.404: INFO: Waiting for endpoints: map[]
Jul 31 13:28:51.807: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.26.210:8080/dial?request=hostName&protocol=http&host=172.25.24.68&port=8080&tries=1'] Namespace:pod-network-test-8383 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 31 13:28:51.807: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
Jul 31 13:28:54.407: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:28:54.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8383" for this suite.
Jul 31 13:29:16.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:29:20.729: INFO: namespace pod-network-test-8383 deletion completed in 26.114912262s

• [SLOW TEST:72.619 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:29:20.729: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 31 13:29:21.414: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:29:22.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2699" for this suite.
Jul 31 13:29:28.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:29:31.911: INFO: namespace replication-controller-2699 deletion completed in 9.788601263s

• [SLOW TEST:11.182 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:29:31.911: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-cfbb2a70-47a8-4967-986b-9e5c7f6a12cc
STEP: Creating configMap with name cm-test-opt-upd-84c32c5b-1919-4148-988f-da3d194b6e82
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-cfbb2a70-47a8-4967-986b-9e5c7f6a12cc
STEP: Updating configmap cm-test-opt-upd-84c32c5b-1919-4148-988f-da3d194b6e82
STEP: Creating configMap with name cm-test-opt-create-3fdec9b4-716e-4d07-8e83-6eedfe4a1dbb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:30:55.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5110" for this suite.
Jul 31 13:31:17.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:31:28.328: INFO: namespace configmap-5110 deletion completed in 32.875929683s

• [SLOW TEST:116.417 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:31:28.329: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:31:38.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3637" for this suite.
Jul 31 13:32:19.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:32:22.915: INFO: namespace kubelet-test-3637 deletion completed in 44.597522603s

• [SLOW TEST:54.587 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:32:22.916: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 31 13:32:24.630: INFO: Waiting up to 5m0s for pod "downward-api-7a71f702-e015-4bbc-b1c9-0d8372d46995" in namespace "downward-api-7484" to be "success or failure"
Jul 31 13:32:24.639: INFO: Pod "downward-api-7a71f702-e015-4bbc-b1c9-0d8372d46995": Phase="Pending", Reason="", readiness=false. Elapsed: 8.404215ms
Jul 31 13:32:27.412: INFO: Pod "downward-api-7a71f702-e015-4bbc-b1c9-0d8372d46995": Phase="Pending", Reason="", readiness=false. Elapsed: 2.782021664s
Jul 31 13:32:30.413: INFO: Pod "downward-api-7a71f702-e015-4bbc-b1c9-0d8372d46995": Phase="Running", Reason="", readiness=true. Elapsed: 5.782488493s
Jul 31 13:32:32.526: INFO: Pod "downward-api-7a71f702-e015-4bbc-b1c9-0d8372d46995": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.896155688s
STEP: Saw pod success
Jul 31 13:32:32.527: INFO: Pod "downward-api-7a71f702-e015-4bbc-b1c9-0d8372d46995" satisfied condition "success or failure"
Jul 31 13:32:32.709: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downward-api-7a71f702-e015-4bbc-b1c9-0d8372d46995 container dapi-container: <nil>
STEP: delete the pod
Jul 31 13:32:34.708: INFO: Waiting for pod downward-api-7a71f702-e015-4bbc-b1c9-0d8372d46995 to disappear
Jul 31 13:32:35.708: INFO: Pod downward-api-7a71f702-e015-4bbc-b1c9-0d8372d46995 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:32:35.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7484" for this suite.
Jul 31 13:32:42.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:32:50.509: INFO: namespace downward-api-7484 deletion completed in 14.598606688s

• [SLOW TEST:27.594 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:32:50.511: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 31 13:32:51.019: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 31 13:32:51.109: INFO: Waiting for terminating namespaces to be deleted...
Jul 31 13:32:51.308: INFO: 
Logging pods the kubelet thinks is on node worker-2bh9r-78c4c5b4fb-96hfw before test
Jul 31 13:32:52.313: INFO: kube-proxy-khr7h from kube-system started at 2019-07-29 14:52:36 +0000 UTC (1 container statuses recorded)
Jul 31 13:32:52.314: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 13:32:52.314: INFO: openvpn-client-84b8ff9b6f-h6ccx from kube-system started at 2019-07-29 14:53:19 +0000 UTC (2 container statuses recorded)
Jul 31 13:32:52.314: INFO: 	Container dnat-controller ready: true, restart count 0
Jul 31 13:32:52.314: INFO: 	Container openvpn-client ready: true, restart count 0
Jul 31 13:32:52.314: INFO: sonobuoy-e2e-job-1dfc7533d0924fc3 from heptio-sonobuoy started at 2019-07-31 11:06:29 +0000 UTC (2 container statuses recorded)
Jul 31 13:32:52.314: INFO: 	Container e2e ready: true, restart count 0
Jul 31 13:32:52.314: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 13:32:52.314: INFO: canal-dtm6f from kube-system started at 2019-07-29 14:52:35 +0000 UTC (2 container statuses recorded)
Jul 31 13:32:52.314: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 13:32:52.314: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 31 13:32:52.314: INFO: coredns-9b6865ff9-kcflr from kube-system started at 2019-07-29 14:53:19 +0000 UTC (1 container statuses recorded)
Jul 31 13:32:52.314: INFO: 	Container coredns ready: true, restart count 0
Jul 31 13:32:52.314: INFO: node-exporter-h5vfd from kube-system started at 2019-07-29 14:52:36 +0000 UTC (2 container statuses recorded)
Jul 31 13:32:52.314: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 31 13:32:52.314: INFO: 	Container node-exporter ready: true, restart count 0
Jul 31 13:32:52.314: INFO: node-local-dns-rv254 from kube-system started at 2019-07-29 14:53:16 +0000 UTC (1 container statuses recorded)
Jul 31 13:32:52.314: INFO: 	Container node-cache ready: true, restart count 0
Jul 31 13:32:52.314: INFO: container-linux-update-agent-htx9q from kube-system started at 2019-07-29 14:53:16 +0000 UTC (1 container statuses recorded)
Jul 31 13:32:52.315: INFO: 	Container update-agent ready: true, restart count 1
Jul 31 13:32:52.315: INFO: sonobuoy-systemd-logs-daemon-set-adffb9aed1fd4488-8wclh from heptio-sonobuoy started at 2019-07-31 11:06:29 +0000 UTC (2 container statuses recorded)
Jul 31 13:32:52.315: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jul 31 13:32:52.315: INFO: 	Container systemd-logs ready: true, restart count 2
Jul 31 13:32:52.315: INFO: 
Logging pods the kubelet thinks is on node worker-2bh9r-78c4c5b4fb-czrvz before test
Jul 31 13:32:53.417: INFO: node-exporter-c4vlv from kube-system started at 2019-07-29 14:55:03 +0000 UTC (2 container statuses recorded)
Jul 31 13:32:53.417: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 31 13:32:53.417: INFO: 	Container node-exporter ready: true, restart count 0
Jul 31 13:32:53.417: INFO: canal-fjc6d from kube-system started at 2019-07-29 14:55:03 +0000 UTC (2 container statuses recorded)
Jul 31 13:32:53.418: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 13:32:53.418: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 31 13:32:53.418: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-31 11:06:15 +0000 UTC (1 container statuses recorded)
Jul 31 13:32:53.418: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 31 13:32:53.418: INFO: sonobuoy-systemd-logs-daemon-set-adffb9aed1fd4488-4vrw8 from heptio-sonobuoy started at 2019-07-31 11:06:29 +0000 UTC (2 container statuses recorded)
Jul 31 13:32:53.418: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jul 31 13:32:53.418: INFO: 	Container systemd-logs ready: true, restart count 2
Jul 31 13:32:53.418: INFO: container-linux-update-agent-5j26n from kube-system started at 2019-07-29 14:55:48 +0000 UTC (1 container statuses recorded)
Jul 31 13:32:53.418: INFO: 	Container update-agent ready: true, restart count 0
Jul 31 13:32:53.418: INFO: node-local-dns-8r55k from kube-system started at 2019-07-29 14:55:48 +0000 UTC (1 container statuses recorded)
Jul 31 13:32:53.418: INFO: 	Container node-cache ready: true, restart count 0
Jul 31 13:32:53.418: INFO: kubernetes-dashboard-584d5ffc75-xxvnm from kube-system started at 2019-07-29 14:55:51 +0000 UTC (1 container statuses recorded)
Jul 31 13:32:53.418: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 31 13:32:53.418: INFO: kube-proxy-q8bhf from kube-system started at 2019-07-29 14:55:04 +0000 UTC (1 container statuses recorded)
Jul 31 13:32:53.418: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 13:32:53.418: INFO: 
Logging pods the kubelet thinks is on node worker-2bh9r-78c4c5b4fb-h22s9 before test
Jul 31 13:32:53.912: INFO: canal-jmmlf from kube-system started at 2019-07-29 14:49:48 +0000 UTC (2 container statuses recorded)
Jul 31 13:32:53.912: INFO: 	Container calico-node ready: true, restart count 0
Jul 31 13:32:53.912: INFO: 	Container kube-flannel ready: true, restart count 0
Jul 31 13:32:53.912: INFO: node-exporter-cg6lq from kube-system started at 2019-07-29 14:49:48 +0000 UTC (2 container statuses recorded)
Jul 31 13:32:53.912: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 31 13:32:53.912: INFO: 	Container node-exporter ready: true, restart count 0
Jul 31 13:32:53.912: INFO: coredns-9b6865ff9-qh2tz from kube-system started at 2019-07-29 14:55:52 +0000 UTC (1 container statuses recorded)
Jul 31 13:32:53.912: INFO: 	Container coredns ready: true, restart count 0
Jul 31 13:32:53.912: INFO: node-local-dns-4slzn from kube-system started at 2019-07-29 14:50:30 +0000 UTC (1 container statuses recorded)
Jul 31 13:32:53.912: INFO: 	Container node-cache ready: true, restart count 0
Jul 31 13:32:53.912: INFO: kube-proxy-jz7bw from kube-system started at 2019-07-29 14:49:48 +0000 UTC (1 container statuses recorded)
Jul 31 13:32:53.912: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 13:32:53.912: INFO: container-linux-update-agent-zsb6g from kube-system started at 2019-07-29 14:50:30 +0000 UTC (1 container statuses recorded)
Jul 31 13:32:53.912: INFO: 	Container update-agent ready: true, restart count 1
Jul 31 13:32:53.912: INFO: container-linux-update-operator-55d5b65b4-tgkz8 from kube-system started at 2019-07-29 14:50:33 +0000 UTC (1 container statuses recorded)
Jul 31 13:32:53.913: INFO: 	Container update-operator ready: true, restart count 0
Jul 31 13:32:53.913: INFO: sonobuoy-systemd-logs-daemon-set-adffb9aed1fd4488-8m2w7 from heptio-sonobuoy started at 2019-07-31 11:06:30 +0000 UTC (2 container statuses recorded)
Jul 31 13:32:53.913: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jul 31 13:32:53.913: INFO: 	Container systemd-logs ready: true, restart count 2
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15b68132987e4de9], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:32:55.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7537" for this suite.
Jul 31 13:33:02.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:33:07.111: INFO: namespace sched-pred-7537 deletion completed in 11.189277618s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:16.607 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:33:07.118: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-c80b39dc-39e5-47a1-94be-575b30b09423
STEP: Creating a pod to test consume configMaps
Jul 31 13:33:08.709: INFO: Waiting up to 5m0s for pod "pod-configmaps-1d65f6dc-3847-4d9c-bb54-9d49c252125e" in namespace "configmap-2262" to be "success or failure"
Jul 31 13:33:08.913: INFO: Pod "pod-configmaps-1d65f6dc-3847-4d9c-bb54-9d49c252125e": Phase="Pending", Reason="", readiness=false. Elapsed: 204.311783ms
Jul 31 13:33:11.108: INFO: Pod "pod-configmaps-1d65f6dc-3847-4d9c-bb54-9d49c252125e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.399415157s
Jul 31 13:33:13.308: INFO: Pod "pod-configmaps-1d65f6dc-3847-4d9c-bb54-9d49c252125e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.599406524s
Jul 31 13:33:15.610: INFO: Pod "pod-configmaps-1d65f6dc-3847-4d9c-bb54-9d49c252125e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.901197178s
STEP: Saw pod success
Jul 31 13:33:15.610: INFO: Pod "pod-configmaps-1d65f6dc-3847-4d9c-bb54-9d49c252125e" satisfied condition "success or failure"
Jul 31 13:33:16.210: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-configmaps-1d65f6dc-3847-4d9c-bb54-9d49c252125e container configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 13:33:18.831: INFO: Waiting for pod pod-configmaps-1d65f6dc-3847-4d9c-bb54-9d49c252125e to disappear
Jul 31 13:33:18.836: INFO: Pod pod-configmaps-1d65f6dc-3847-4d9c-bb54-9d49c252125e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:33:18.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2262" for this suite.
Jul 31 13:33:25.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:33:32.162: INFO: namespace configmap-2262 deletion completed in 13.318099346s

• [SLOW TEST:25.045 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:33:32.162: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 13:33:35.110: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:33:37.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9818" for this suite.
Jul 31 13:33:45.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:33:51.139: INFO: namespace custom-resource-definition-9818 deletion completed in 14.018783065s

• [SLOW TEST:18.977 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:33:51.141: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Jul 31 13:33:51.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 api-versions'
Jul 31 13:33:51.629: INFO: stderr: ""
Jul 31 13:33:51.629: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:33:51.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4433" for this suite.
Jul 31 13:33:58.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:34:01.048: INFO: namespace kubectl-4433 deletion completed in 9.135979118s

• [SLOW TEST:9.907 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:34:01.049: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
W0731 13:34:06.011519      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 31 13:34:06.011: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:34:06.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9638" for this suite.
Jul 31 13:34:13.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:34:19.810: INFO: namespace gc-9638 deletion completed in 13.49824041s

• [SLOW TEST:18.761 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:34:19.810: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 31 13:34:20.230: INFO: Waiting up to 5m0s for pod "downward-api-10757020-8182-401f-bd7a-a1a835993c0e" in namespace "downward-api-1540" to be "success or failure"
Jul 31 13:34:20.251: INFO: Pod "downward-api-10757020-8182-401f-bd7a-a1a835993c0e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.66971ms
Jul 31 13:34:23.009: INFO: Pod "downward-api-10757020-8182-401f-bd7a-a1a835993c0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.779102401s
Jul 31 13:34:25.114: INFO: Pod "downward-api-10757020-8182-401f-bd7a-a1a835993c0e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.883656065s
Jul 31 13:34:27.417: INFO: Pod "downward-api-10757020-8182-401f-bd7a-a1a835993c0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.186903234s
STEP: Saw pod success
Jul 31 13:34:27.417: INFO: Pod "downward-api-10757020-8182-401f-bd7a-a1a835993c0e" satisfied condition "success or failure"
Jul 31 13:34:27.424: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downward-api-10757020-8182-401f-bd7a-a1a835993c0e container dapi-container: <nil>
STEP: delete the pod
Jul 31 13:34:28.509: INFO: Waiting for pod downward-api-10757020-8182-401f-bd7a-a1a835993c0e to disappear
Jul 31 13:34:28.515: INFO: Pod downward-api-10757020-8182-401f-bd7a-a1a835993c0e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:34:28.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1540" for this suite.
Jul 31 13:34:34.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:34:41.756: INFO: namespace downward-api-1540 deletion completed in 13.223228361s

• [SLOW TEST:21.945 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:34:41.756: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-c931ed9c-0ed0-408c-b1b1-f77e4eb00c89
STEP: Creating a pod to test consume configMaps
Jul 31 13:34:46.310: INFO: Waiting up to 5m0s for pod "pod-configmaps-431f7b5d-f0ee-43a5-8410-2866b2c81f59" in namespace "configmap-4365" to be "success or failure"
Jul 31 13:34:46.610: INFO: Pod "pod-configmaps-431f7b5d-f0ee-43a5-8410-2866b2c81f59": Phase="Pending", Reason="", readiness=false. Elapsed: 300.522301ms
Jul 31 13:34:48.618: INFO: Pod "pod-configmaps-431f7b5d-f0ee-43a5-8410-2866b2c81f59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.308493648s
Jul 31 13:34:51.315: INFO: Pod "pod-configmaps-431f7b5d-f0ee-43a5-8410-2866b2c81f59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.005298008s
STEP: Saw pod success
Jul 31 13:34:51.315: INFO: Pod "pod-configmaps-431f7b5d-f0ee-43a5-8410-2866b2c81f59" satisfied condition "success or failure"
Jul 31 13:34:51.328: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-configmaps-431f7b5d-f0ee-43a5-8410-2866b2c81f59 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 13:34:51.910: INFO: Waiting for pod pod-configmaps-431f7b5d-f0ee-43a5-8410-2866b2c81f59 to disappear
Jul 31 13:34:52.110: INFO: Pod pod-configmaps-431f7b5d-f0ee-43a5-8410-2866b2c81f59 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:34:52.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4365" for this suite.
Jul 31 13:35:00.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:35:02.737: INFO: namespace configmap-4365 deletion completed in 10.325814419s

• [SLOW TEST:20.981 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:35:02.738: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-43918813-a628-47c4-a889-b069ea0f8027
STEP: Creating a pod to test consume secrets
Jul 31 13:35:03.545: INFO: Waiting up to 5m0s for pod "pod-secrets-771754ac-0e75-42d9-b696-70bb76a21505" in namespace "secrets-197" to be "success or failure"
Jul 31 13:35:03.552: INFO: Pod "pod-secrets-771754ac-0e75-42d9-b696-70bb76a21505": Phase="Pending", Reason="", readiness=false. Elapsed: 7.205399ms
Jul 31 13:35:05.814: INFO: Pod "pod-secrets-771754ac-0e75-42d9-b696-70bb76a21505": Phase="Pending", Reason="", readiness=false. Elapsed: 2.269208444s
Jul 31 13:35:08.710: INFO: Pod "pod-secrets-771754ac-0e75-42d9-b696-70bb76a21505": Phase="Pending", Reason="", readiness=false. Elapsed: 5.164810665s
Jul 31 13:35:11.214: INFO: Pod "pod-secrets-771754ac-0e75-42d9-b696-70bb76a21505": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.669023529s
STEP: Saw pod success
Jul 31 13:35:11.214: INFO: Pod "pod-secrets-771754ac-0e75-42d9-b696-70bb76a21505" satisfied condition "success or failure"
Jul 31 13:35:11.222: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-secrets-771754ac-0e75-42d9-b696-70bb76a21505 container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 13:35:11.911: INFO: Waiting for pod pod-secrets-771754ac-0e75-42d9-b696-70bb76a21505 to disappear
Jul 31 13:35:12.116: INFO: Pod pod-secrets-771754ac-0e75-42d9-b696-70bb76a21505 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:35:12.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-197" for this suite.
Jul 31 13:35:20.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:35:29.711: INFO: namespace secrets-197 deletion completed in 17.39448351s

• [SLOW TEST:26.973 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:35:29.712: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 31 13:35:46.622: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 31 13:35:46.628: INFO: Pod pod-with-poststart-http-hook still exists
Jul 31 13:35:48.628: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 31 13:35:49.011: INFO: Pod pod-with-poststart-http-hook still exists
Jul 31 13:35:50.628: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 31 13:35:50.811: INFO: Pod pod-with-poststart-http-hook still exists
Jul 31 13:35:52.628: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 31 13:35:52.718: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:35:52.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1717" for this suite.
Jul 31 13:36:14.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:36:18.412: INFO: namespace container-lifecycle-hook-1717 deletion completed in 25.678936024s

• [SLOW TEST:48.701 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:36:18.413: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 13:36:19.122: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"0ca8336d-bb88-477d-82e7-b6094e81ff32", Controller:(*bool)(0xc001e9901a), BlockOwnerDeletion:(*bool)(0xc001e9901b)}}
Jul 31 13:36:19.311: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1f53b46c-0ef0-4fb5-88ab-3713651eeaf0", Controller:(*bool)(0xc001d525da), BlockOwnerDeletion:(*bool)(0xc001d525db)}}
Jul 31 13:36:19.510: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"47e7b3c1-8494-4fbd-9743-f879fba46bbb", Controller:(*bool)(0xc001e991e2), BlockOwnerDeletion:(*bool)(0xc001e991e3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:36:24.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6456" for this suite.
Jul 31 13:36:31.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:36:35.015: INFO: namespace gc-6456 deletion completed in 10.179333104s

• [SLOW TEST:16.602 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:36:35.016: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 13:36:36.813: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:36:43.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7008" for this suite.
Jul 31 13:37:26.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:37:30.049: INFO: namespace pods-7008 deletion completed in 46.63637703s

• [SLOW TEST:55.033 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:37:30.049: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 13:37:30.461: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6be0a299-e2ed-4a5a-9c2b-dcb30f88478a" in namespace "projected-8330" to be "success or failure"
Jul 31 13:37:30.470: INFO: Pod "downwardapi-volume-6be0a299-e2ed-4a5a-9c2b-dcb30f88478a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.531599ms
Jul 31 13:37:33.423: INFO: Pod "downwardapi-volume-6be0a299-e2ed-4a5a-9c2b-dcb30f88478a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.961935322s
Jul 31 13:37:35.818: INFO: Pod "downwardapi-volume-6be0a299-e2ed-4a5a-9c2b-dcb30f88478a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.357031038s
Jul 31 13:37:38.412: INFO: Pod "downwardapi-volume-6be0a299-e2ed-4a5a-9c2b-dcb30f88478a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.950652256s
STEP: Saw pod success
Jul 31 13:37:38.412: INFO: Pod "downwardapi-volume-6be0a299-e2ed-4a5a-9c2b-dcb30f88478a" satisfied condition "success or failure"
Jul 31 13:37:39.611: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downwardapi-volume-6be0a299-e2ed-4a5a-9c2b-dcb30f88478a container client-container: <nil>
STEP: delete the pod
Jul 31 13:37:42.915: INFO: Waiting for pod downwardapi-volume-6be0a299-e2ed-4a5a-9c2b-dcb30f88478a to disappear
Jul 31 13:37:42.942: INFO: Pod downwardapi-volume-6be0a299-e2ed-4a5a-9c2b-dcb30f88478a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:37:42.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8330" for this suite.
Jul 31 13:37:49.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:37:55.252: INFO: namespace projected-8330 deletion completed in 12.299795076s

• [SLOW TEST:25.203 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:37:55.252: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 31 13:38:05.681: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 31 13:38:05.687: INFO: Pod pod-with-prestop-http-hook still exists
Jul 31 13:38:07.687: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 31 13:38:07.715: INFO: Pod pod-with-prestop-http-hook still exists
Jul 31 13:38:09.687: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 31 13:38:09.812: INFO: Pod pod-with-prestop-http-hook still exists
Jul 31 13:38:11.688: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 31 13:38:12.912: INFO: Pod pod-with-prestop-http-hook still exists
Jul 31 13:38:13.687: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 31 13:38:14.213: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:38:15.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5312" for this suite.
Jul 31 13:38:39.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:38:47.024: INFO: namespace container-lifecycle-hook-5312 deletion completed in 30.805329319s

• [SLOW TEST:51.771 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:38:47.030: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 31 13:39:02.512: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 13:39:02.617: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 13:39:04.617: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 13:39:04.915: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 13:39:06.618: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 13:39:06.915: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 13:39:08.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 13:39:09.812: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 13:39:10.617: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 13:39:11.412: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 13:39:12.617: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 13:39:12.720: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 13:39:14.617: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 13:39:14.812: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 13:39:16.619: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 13:39:17.018: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 13:39:18.617: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 13:39:18.912: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 13:39:20.617: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 13:39:20.624: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 13:39:22.617: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 13:39:23.320: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:39:24.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1434" for this suite.
Jul 31 13:39:46.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:39:56.413: INFO: namespace container-lifecycle-hook-1434 deletion completed in 31.986521452s

• [SLOW TEST:69.384 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:39:56.414: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0731 13:40:40.828365      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 31 13:40:40.828: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:40:40.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9270" for this suite.
Jul 31 13:40:55.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:41:02.014: INFO: namespace gc-9270 deletion completed in 21.173499083s

• [SLOW TEST:65.600 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:41:02.016: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:42:03.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3917" for this suite.
Jul 31 13:42:26.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:42:30.414: INFO: namespace container-probe-3917 deletion completed in 27.079814041s

• [SLOW TEST:88.398 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:42:30.414: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 31 13:42:32.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-687'
Jul 31 13:42:35.235: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 31 13:42:35.236: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Jul 31 13:42:36.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-280300738 delete deployment e2e-test-nginx-deployment --namespace=kubectl-687'
Jul 31 13:42:37.617: INFO: stderr: ""
Jul 31 13:42:37.617: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:42:37.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-687" for this suite.
Jul 31 13:42:45.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:42:54.877: INFO: namespace kubectl-687 deletion completed in 16.457072209s

• [SLOW TEST:24.463 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:42:54.889: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:43:03.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1256" for this suite.
Jul 31 13:43:58.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:44:05.720: INFO: namespace kubelet-test-1256 deletion completed in 1m1.99543002s

• [SLOW TEST:70.832 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:44:05.721: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2883
I0731 13:44:06.620000      15 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2883, replica count: 1
I0731 13:44:07.670931      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 13:44:08.671158      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 13:44:09.671318      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 13:44:10.671530      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 13:44:11.672001      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 13:44:12.672196      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 13:44:13.672434      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 13:44:14.317: INFO: Created: latency-svc-pmpcn
Jul 31 13:44:14.318: INFO: Got endpoints: latency-svc-pmpcn [344.980883ms]
Jul 31 13:44:14.822: INFO: Created: latency-svc-5nmqn
Jul 31 13:44:14.824: INFO: Created: latency-svc-2fnss
Jul 31 13:44:14.824: INFO: Created: latency-svc-5tkl7
Jul 31 13:44:14.824: INFO: Created: latency-svc-2vjqt
Jul 31 13:44:14.824: INFO: Created: latency-svc-j9s6z
Jul 31 13:44:14.824: INFO: Got endpoints: latency-svc-j9s6z [505.022779ms]
Jul 31 13:44:14.825: INFO: Got endpoints: latency-svc-2fnss [505.990161ms]
Jul 31 13:44:14.825: INFO: Got endpoints: latency-svc-5tkl7 [506.562491ms]
Jul 31 13:44:14.826: INFO: Got endpoints: latency-svc-2vjqt [505.483724ms]
Jul 31 13:44:15.014: INFO: Created: latency-svc-2kb64
Jul 31 13:44:15.015: INFO: Created: latency-svc-fv87r
Jul 31 13:44:15.015: INFO: Created: latency-svc-7lpmr
Jul 31 13:44:15.015: INFO: Created: latency-svc-mfq2m
Jul 31 13:44:15.015: INFO: Created: latency-svc-7fbxp
Jul 31 13:44:15.015: INFO: Created: latency-svc-4p47l
Jul 31 13:44:15.015: INFO: Created: latency-svc-8dgpr
Jul 31 13:44:15.016: INFO: Created: latency-svc-nbfn7
Jul 31 13:44:15.016: INFO: Created: latency-svc-5prcn
Jul 31 13:44:15.016: INFO: Created: latency-svc-t5xgh
Jul 31 13:44:15.016: INFO: Got endpoints: latency-svc-2kb64 [696.729178ms]
Jul 31 13:44:15.016: INFO: Got endpoints: latency-svc-4p47l [695.492932ms]
Jul 31 13:44:15.017: INFO: Got endpoints: latency-svc-8dgpr [695.752528ms]
Jul 31 13:44:15.017: INFO: Got endpoints: latency-svc-nbfn7 [695.846802ms]
Jul 31 13:44:15.017: INFO: Got endpoints: latency-svc-5prcn [695.742193ms]
Jul 31 13:44:15.017: INFO: Got endpoints: latency-svc-t5xgh [695.657276ms]
Jul 31 13:44:15.017: INFO: Got endpoints: latency-svc-7lpmr [696.482341ms]
Jul 31 13:44:15.017: INFO: Got endpoints: latency-svc-fv87r [697.336394ms]
Jul 31 13:44:15.017: INFO: Got endpoints: latency-svc-mfq2m [697.257669ms]
Jul 31 13:44:15.017: INFO: Got endpoints: latency-svc-7fbxp [696.255415ms]
Jul 31 13:44:15.017: INFO: Got endpoints: latency-svc-5nmqn [696.660746ms]
Jul 31 13:44:15.314: INFO: Created: latency-svc-hqbrj
Jul 31 13:44:15.314: INFO: Created: latency-svc-j9wrd
Jul 31 13:44:15.314: INFO: Created: latency-svc-q7r28
Jul 31 13:44:15.314: INFO: Created: latency-svc-zcl4r
Jul 31 13:44:15.314: INFO: Got endpoints: latency-svc-hqbrj [488.363431ms]
Jul 31 13:44:15.314: INFO: Got endpoints: latency-svc-j9wrd [489.624677ms]
Jul 31 13:44:15.315: INFO: Got endpoints: latency-svc-q7r28 [489.660133ms]
Jul 31 13:44:15.315: INFO: Got endpoints: latency-svc-zcl4r [489.232443ms]
Jul 31 13:44:15.614: INFO: Created: latency-svc-ktn8d
Jul 31 13:44:15.614: INFO: Created: latency-svc-psmct
Jul 31 13:44:15.615: INFO: Created: latency-svc-mcw2g
Jul 31 13:44:15.615: INFO: Created: latency-svc-nnl2c
Jul 31 13:44:15.615: INFO: Created: latency-svc-xdxjv
Jul 31 13:44:15.615: INFO: Created: latency-svc-hjhkr
Jul 31 13:44:15.615: INFO: Created: latency-svc-4mcz5
Jul 31 13:44:15.615: INFO: Created: latency-svc-s269q
Jul 31 13:44:15.615: INFO: Created: latency-svc-l5ntw
Jul 31 13:44:15.615: INFO: Created: latency-svc-ds8rh
Jul 31 13:44:15.615: INFO: Created: latency-svc-zlg4r
Jul 31 13:44:15.615: INFO: Got endpoints: latency-svc-ktn8d [597.850515ms]
Jul 31 13:44:15.615: INFO: Got endpoints: latency-svc-psmct [598.174497ms]
Jul 31 13:44:15.615: INFO: Got endpoints: latency-svc-mcw2g [598.357663ms]
Jul 31 13:44:15.615: INFO: Got endpoints: latency-svc-nnl2c [598.124886ms]
Jul 31 13:44:15.616: INFO: Got endpoints: latency-svc-xdxjv [598.381202ms]
Jul 31 13:44:15.616: INFO: Got endpoints: latency-svc-hjhkr [599.56553ms]
Jul 31 13:44:15.616: INFO: Got endpoints: latency-svc-4mcz5 [598.52886ms]
Jul 31 13:44:15.616: INFO: Got endpoints: latency-svc-s269q [599.495958ms]
Jul 31 13:44:15.616: INFO: Got endpoints: latency-svc-l5ntw [598.842697ms]
Jul 31 13:44:15.616: INFO: Got endpoints: latency-svc-ds8rh [598.843297ms]
Jul 31 13:44:15.616: INFO: Got endpoints: latency-svc-zlg4r [599.018166ms]
Jul 31 13:44:15.714: INFO: Created: latency-svc-sc78g
Jul 31 13:44:15.714: INFO: Got endpoints: latency-svc-sc78g [399.885015ms]
Jul 31 13:44:15.715: INFO: Created: latency-svc-cmtgp
Jul 31 13:44:15.715: INFO: Got endpoints: latency-svc-cmtgp [399.695294ms]
Jul 31 13:44:15.715: INFO: Created: latency-svc-qnfbm
Jul 31 13:44:15.715: INFO: Got endpoints: latency-svc-qnfbm [400.753898ms]
Jul 31 13:44:15.716: INFO: Created: latency-svc-9768f
Jul 31 13:44:15.716: INFO: Got endpoints: latency-svc-9768f [400.436089ms]
Jul 31 13:44:16.023: INFO: Created: latency-svc-8b6wf
Jul 31 13:44:16.023: INFO: Created: latency-svc-lvpnz
Jul 31 13:44:16.026: INFO: Created: latency-svc-dnfzq
Jul 31 13:44:16.026: INFO: Created: latency-svc-2ptrd
Jul 31 13:44:16.026: INFO: Created: latency-svc-jgz7k
Jul 31 13:44:16.026: INFO: Created: latency-svc-gtpqw
Jul 31 13:44:16.026: INFO: Created: latency-svc-nnpbf
Jul 31 13:44:16.026: INFO: Created: latency-svc-4t7jn
Jul 31 13:44:16.026: INFO: Created: latency-svc-bhgm2
Jul 31 13:44:16.026: INFO: Created: latency-svc-j8dm4
Jul 31 13:44:16.026: INFO: Created: latency-svc-tng9z
Jul 31 13:44:16.026: INFO: Got endpoints: latency-svc-8b6wf [410.864645ms]
Jul 31 13:44:16.026: INFO: Got endpoints: latency-svc-lvpnz [410.61246ms]
Jul 31 13:44:16.027: INFO: Got endpoints: latency-svc-dnfzq [411.874244ms]
Jul 31 13:44:16.027: INFO: Got endpoints: latency-svc-2ptrd [411.48634ms]
Jul 31 13:44:16.027: INFO: Got endpoints: latency-svc-jgz7k [411.531963ms]
Jul 31 13:44:16.028: INFO: Got endpoints: latency-svc-gtpqw [412.389006ms]
Jul 31 13:44:16.028: INFO: Got endpoints: latency-svc-nnpbf [411.961434ms]
Jul 31 13:44:16.028: INFO: Got endpoints: latency-svc-4t7jn [412.515467ms]
Jul 31 13:44:16.028: INFO: Got endpoints: latency-svc-bhgm2 [412.209421ms]
Jul 31 13:44:16.029: INFO: Got endpoints: latency-svc-j8dm4 [412.896144ms]
Jul 31 13:44:16.029: INFO: Got endpoints: latency-svc-tng9z [413.797182ms]
Jul 31 13:44:16.216: INFO: Created: latency-svc-zz7r7
Jul 31 13:44:16.217: INFO: Created: latency-svc-89k88
Jul 31 13:44:16.217: INFO: Created: latency-svc-6x5dl
Jul 31 13:44:16.217: INFO: Created: latency-svc-r6w86
Jul 31 13:44:16.217: INFO: Created: latency-svc-26cdb
Jul 31 13:44:16.217: INFO: Got endpoints: latency-svc-26cdb [502.100431ms]
Jul 31 13:44:16.217: INFO: Got endpoints: latency-svc-89k88 [503.129358ms]
Jul 31 13:44:16.217: INFO: Got endpoints: latency-svc-6x5dl [189.02179ms]
Jul 31 13:44:16.217: INFO: Got endpoints: latency-svc-r6w86 [188.152375ms]
Jul 31 13:44:16.314: INFO: Created: latency-svc-4sbnd
Jul 31 13:44:16.314: INFO: Created: latency-svc-mjrdr
Jul 31 13:44:16.314: INFO: Created: latency-svc-bmklc
Jul 31 13:44:16.314: INFO: Created: latency-svc-7k4dz
Jul 31 13:44:16.314: INFO: Got endpoints: latency-svc-7k4dz [286.930252ms]
Jul 31 13:44:16.315: INFO: Got endpoints: latency-svc-mjrdr [285.765372ms]
Jul 31 13:44:16.316: INFO: Got endpoints: latency-svc-bmklc [600.110509ms]
Jul 31 13:44:16.316: INFO: Got endpoints: latency-svc-zz7r7 [601.089386ms]
Jul 31 13:44:16.814: INFO: Created: latency-svc-wpgdr
Jul 31 13:44:16.814: INFO: Got endpoints: latency-svc-4sbnd [786.554905ms]
Jul 31 13:44:16.815: INFO: Created: latency-svc-6kh7q
Jul 31 13:44:16.815: INFO: Created: latency-svc-swcm5
Jul 31 13:44:16.815: INFO: Created: latency-svc-x8fjw
Jul 31 13:44:16.815: INFO: Created: latency-svc-59crr
Jul 31 13:44:16.815: INFO: Created: latency-svc-w5wn5
Jul 31 13:44:16.815: INFO: Created: latency-svc-f98xz
Jul 31 13:44:16.816: INFO: Created: latency-svc-fdgwt
Jul 31 13:44:16.816: INFO: Created: latency-svc-zx875
Jul 31 13:44:16.816: INFO: Created: latency-svc-srkzp
Jul 31 13:44:16.816: INFO: Got endpoints: latency-svc-6kh7q [598.666586ms]
Jul 31 13:44:16.816: INFO: Got endpoints: latency-svc-wpgdr [788.036298ms]
Jul 31 13:44:16.816: INFO: Got endpoints: latency-svc-swcm5 [599.241063ms]
Jul 31 13:44:16.816: INFO: Got endpoints: latency-svc-x8fjw [790.13065ms]
Jul 31 13:44:16.816: INFO: Got endpoints: latency-svc-59crr [788.301331ms]
Jul 31 13:44:16.817: INFO: Got endpoints: latency-svc-w5wn5 [790.196799ms]
Jul 31 13:44:16.817: INFO: Got endpoints: latency-svc-f98xz [789.960271ms]
Jul 31 13:44:16.817: INFO: Got endpoints: latency-svc-fdgwt [788.489185ms]
Jul 31 13:44:16.817: INFO: Got endpoints: latency-svc-zx875 [599.847407ms]
Jul 31 13:44:16.817: INFO: Got endpoints: latency-svc-srkzp [599.941358ms]
Jul 31 13:44:16.917: INFO: Created: latency-svc-b68xf
Jul 31 13:44:16.917: INFO: Created: latency-svc-8znhn
Jul 31 13:44:16.917: INFO: Got endpoints: latency-svc-8znhn [601.721388ms]
Jul 31 13:44:16.917: INFO: Created: latency-svc-4b5tf
Jul 31 13:44:16.918: INFO: Got endpoints: latency-svc-4b5tf [603.499982ms]
Jul 31 13:44:16.917: INFO: Got endpoints: latency-svc-b68xf [603.251952ms]
Jul 31 13:44:16.922: INFO: Created: latency-svc-5lkmm
Jul 31 13:44:16.922: INFO: Got endpoints: latency-svc-5lkmm [605.666744ms]
Jul 31 13:44:17.114: INFO: Created: latency-svc-vcb7w
Jul 31 13:44:17.119: INFO: Got endpoints: latency-svc-vcb7w [304.485522ms]
Jul 31 13:44:17.118: INFO: Created: latency-svc-fl84f
Jul 31 13:44:17.118: INFO: Created: latency-svc-5jtv4
Jul 31 13:44:17.119: INFO: Got endpoints: latency-svc-5jtv4 [302.584699ms]
Jul 31 13:44:17.118: INFO: Created: latency-svc-4ggpw
Jul 31 13:44:17.120: INFO: Got endpoints: latency-svc-4ggpw [303.523808ms]
Jul 31 13:44:17.118: INFO: Created: latency-svc-k5fvf
Jul 31 13:44:17.120: INFO: Got endpoints: latency-svc-k5fvf [304.253543ms]
Jul 31 13:44:17.214: INFO: Created: latency-svc-hzr74
Jul 31 13:44:17.214: INFO: Got endpoints: latency-svc-fl84f [397.934065ms]
Jul 31 13:44:17.314: INFO: Created: latency-svc-rtmzq
Jul 31 13:44:17.317: INFO: Created: latency-svc-lskxn
Jul 31 13:44:17.317: INFO: Created: latency-svc-qzplv
Jul 31 13:44:17.328: INFO: Got endpoints: latency-svc-qzplv [510.619025ms]
Jul 31 13:44:17.328: INFO: Got endpoints: latency-svc-lskxn [511.823064ms]
Jul 31 13:44:17.328: INFO: Got endpoints: latency-svc-hzr74 [510.989217ms]
Jul 31 13:44:17.516: INFO: Created: latency-svc-59sc4
Jul 31 13:44:17.516: INFO: Created: latency-svc-f5wrm
Jul 31 13:44:17.516: INFO: Created: latency-svc-27c47
Jul 31 13:44:17.516: INFO: Got endpoints: latency-svc-27c47 [699.235956ms]
Jul 31 13:44:17.516: INFO: Got endpoints: latency-svc-rtmzq [699.056829ms]
Jul 31 13:44:17.517: INFO: Got endpoints: latency-svc-f5wrm [700.094496ms]
Jul 31 13:44:17.619: INFO: Created: latency-svc-qgrkd
Jul 31 13:44:17.619: INFO: Created: latency-svc-6b42t
Jul 31 13:44:17.619: INFO: Created: latency-svc-zgh6n
Jul 31 13:44:17.619: INFO: Got endpoints: latency-svc-zgh6n [700.871419ms]
Jul 31 13:44:17.619: INFO: Got endpoints: latency-svc-6b42t [697.395916ms]
Jul 31 13:44:17.619: INFO: Got endpoints: latency-svc-59sc4 [701.769657ms]
Jul 31 13:44:17.715: INFO: Got endpoints: latency-svc-qgrkd [796.280306ms]
Jul 31 13:44:17.715: INFO: Created: latency-svc-pjcvn
Jul 31 13:44:17.715: INFO: Created: latency-svc-xcbhd
Jul 31 13:44:17.715: INFO: Got endpoints: latency-svc-xcbhd [596.20623ms]
Jul 31 13:44:17.914: INFO: Got endpoints: latency-svc-pjcvn [795.130256ms]
Jul 31 13:44:17.917: INFO: Created: latency-svc-k2vzg
Jul 31 13:44:17.917: INFO: Created: latency-svc-xm4j9
Jul 31 13:44:17.917: INFO: Created: latency-svc-xq2xz
Jul 31 13:44:17.917: INFO: Created: latency-svc-4c557
Jul 31 13:44:17.917: INFO: Created: latency-svc-swxjp
Jul 31 13:44:17.917: INFO: Created: latency-svc-mwlk4
Jul 31 13:44:17.917: INFO: Got endpoints: latency-svc-xm4j9 [400.483811ms]
Jul 31 13:44:17.918: INFO: Got endpoints: latency-svc-k2vzg [797.247204ms]
Jul 31 13:44:17.918: INFO: Got endpoints: latency-svc-mwlk4 [798.080144ms]
Jul 31 13:44:17.918: INFO: Got endpoints: latency-svc-xq2xz [590.265988ms]
Jul 31 13:44:17.918: INFO: Got endpoints: latency-svc-4c557 [590.736495ms]
Jul 31 13:44:17.919: INFO: Got endpoints: latency-svc-swxjp [590.391801ms]
Jul 31 13:44:17.919: INFO: Created: latency-svc-gd7b6
Jul 31 13:44:17.919: INFO: Got endpoints: latency-svc-gd7b6 [704.670628ms]
Jul 31 13:44:18.014: INFO: Created: latency-svc-p9kjf
Jul 31 13:44:18.219: INFO: Created: latency-svc-s6nzp
Jul 31 13:44:18.219: INFO: Got endpoints: latency-svc-p9kjf [702.379983ms]
Jul 31 13:44:18.219: INFO: Created: latency-svc-gvrsb
Jul 31 13:44:18.219: INFO: Got endpoints: latency-svc-gvrsb [702.857938ms]
Jul 31 13:44:18.219: INFO: Created: latency-svc-q5xr7
Jul 31 13:44:18.219: INFO: Got endpoints: latency-svc-q5xr7 [599.476828ms]
Jul 31 13:44:18.220: INFO: Created: latency-svc-q8tzb
Jul 31 13:44:18.220: INFO: Got endpoints: latency-svc-q8tzb [597.149585ms]
Jul 31 13:44:18.220: INFO: Created: latency-svc-sbjld
Jul 31 13:44:18.220: INFO: Got endpoints: latency-svc-sbjld [505.489129ms]
Jul 31 13:44:18.221: INFO: Created: latency-svc-pwql6
Jul 31 13:44:18.221: INFO: Got endpoints: latency-svc-pwql6 [598.668641ms]
Jul 31 13:44:18.320: INFO: Created: latency-svc-nd8ts
Jul 31 13:44:18.320: INFO: Created: latency-svc-9kdft
Jul 31 13:44:18.320: INFO: Got endpoints: latency-svc-9kdft [406.091282ms]
Jul 31 13:44:18.320: INFO: Got endpoints: latency-svc-s6nzp [604.915176ms]
Jul 31 13:44:18.414: INFO: Created: latency-svc-nbtkg
Jul 31 13:44:18.414: INFO: Created: latency-svc-88mr9
Jul 31 13:44:18.415: INFO: Created: latency-svc-cwc97
Jul 31 13:44:18.415: INFO: Created: latency-svc-n8mzd
Jul 31 13:44:18.416: INFO: Created: latency-svc-dv8t8
Jul 31 13:44:18.416: INFO: Got endpoints: latency-svc-cwc97 [497.608307ms]
Jul 31 13:44:18.416: INFO: Got endpoints: latency-svc-nd8ts [498.651118ms]
Jul 31 13:44:18.417: INFO: Got endpoints: latency-svc-88mr9 [498.207898ms]
Jul 31 13:44:18.431: INFO: Created: latency-svc-2xkq2
Jul 31 13:44:18.514: INFO: Created: latency-svc-nwfmf
Jul 31 13:44:18.514: INFO: Created: latency-svc-2lhrh
Jul 31 13:44:18.514: INFO: Created: latency-svc-88vrc
Jul 31 13:44:18.514: INFO: Got endpoints: latency-svc-n8mzd [596.13652ms]
Jul 31 13:44:18.515: INFO: Got endpoints: latency-svc-dv8t8 [596.986351ms]
Jul 31 13:44:18.518: INFO: Created: latency-svc-h2jwl
Jul 31 13:44:18.518: INFO: Created: latency-svc-gg6df
Jul 31 13:44:18.518: INFO: Created: latency-svc-qf2dz
Jul 31 13:44:18.614: INFO: Created: latency-svc-mftrz
Jul 31 13:44:18.614: INFO: Created: latency-svc-kgh7w
Jul 31 13:44:18.614: INFO: Got endpoints: latency-svc-2xkq2 [695.341491ms]
Jul 31 13:44:18.614: INFO: Created: latency-svc-r5clp
Jul 31 13:44:18.615: INFO: Created: latency-svc-z5trv
Jul 31 13:44:18.715: INFO: Created: latency-svc-d8g68
Jul 31 13:44:18.715: INFO: Got endpoints: latency-svc-nwfmf [495.917394ms]
Jul 31 13:44:18.715: INFO: Got endpoints: latency-svc-nbtkg [797.942159ms]
Jul 31 13:44:18.814: INFO: Created: latency-svc-64x9q
Jul 31 13:44:18.814: INFO: Created: latency-svc-xslvn
Jul 31 13:44:18.814: INFO: Created: latency-svc-ntpk8
Jul 31 13:44:18.824: INFO: Got endpoints: latency-svc-gg6df [604.166883ms]
Jul 31 13:44:18.825: INFO: Got endpoints: latency-svc-88vrc [605.655021ms]
Jul 31 13:44:18.826: INFO: Got endpoints: latency-svc-2lhrh [604.808817ms]
Jul 31 13:44:19.116: INFO: Created: latency-svc-xmqm9
Jul 31 13:44:19.116: INFO: Got endpoints: latency-svc-r5clp [685.977399ms]
Jul 31 13:44:19.117: INFO: Got endpoints: latency-svc-qf2dz [796.806089ms]
Jul 31 13:44:19.117: INFO: Got endpoints: latency-svc-h2jwl [796.921873ms]
Jul 31 13:44:19.118: INFO: Got endpoints: latency-svc-mftrz [898.942284ms]
Jul 31 13:44:19.118: INFO: Got endpoints: latency-svc-z5trv [897.253199ms]
Jul 31 13:44:19.118: INFO: Created: latency-svc-txmpr
Jul 31 13:44:19.316: INFO: Got endpoints: latency-svc-d8g68 [885.1643ms]
Jul 31 13:44:19.316: INFO: Got endpoints: latency-svc-kgh7w [885.60769ms]
Jul 31 13:44:19.317: INFO: Created: latency-svc-k8rc9
Jul 31 13:44:19.318: INFO: Created: latency-svc-hn82x
Jul 31 13:44:19.318: INFO: Created: latency-svc-kl5jz
Jul 31 13:44:19.318: INFO: Got endpoints: latency-svc-64x9q [803.13472ms]
Jul 31 13:44:19.318: INFO: Got endpoints: latency-svc-ntpk8 [803.927717ms]
Jul 31 13:44:19.318: INFO: Got endpoints: latency-svc-xslvn [704.430302ms]
Jul 31 13:44:19.414: INFO: Created: latency-svc-krkxk
Jul 31 13:44:19.414: INFO: Created: latency-svc-nvdvr
Jul 31 13:44:19.414: INFO: Created: latency-svc-ww8rm
Jul 31 13:44:19.414: INFO: Created: latency-svc-s8gmj
Jul 31 13:44:19.414: INFO: Got endpoints: latency-svc-txmpr [698.507553ms]
Jul 31 13:44:19.414: INFO: Created: latency-svc-gqh8w
Jul 31 13:44:19.414: INFO: Got endpoints: latency-svc-xmqm9 [698.732794ms]
Jul 31 13:44:19.514: INFO: Created: latency-svc-qw9b2
Jul 31 13:44:19.514: INFO: Got endpoints: latency-svc-k8rc9 [689.852827ms]
Jul 31 13:44:19.514: INFO: Got endpoints: latency-svc-hn82x [687.903843ms]
Jul 31 13:44:19.514: INFO: Created: latency-svc-4ggcj
Jul 31 13:44:19.514: INFO: Created: latency-svc-hwzwh
Jul 31 13:44:19.514: INFO: Created: latency-svc-pmktp
Jul 31 13:44:19.514: INFO: Created: latency-svc-56jf6
Jul 31 13:44:19.714: INFO: Created: latency-svc-4vfp5
Jul 31 13:44:19.715: INFO: Created: latency-svc-c9nlf
Jul 31 13:44:19.715: INFO: Got endpoints: latency-svc-nvdvr [596.775183ms]
Jul 31 13:44:19.715: INFO: Got endpoints: latency-svc-kl5jz [890.331615ms]
Jul 31 13:44:19.719: INFO: Got endpoints: latency-svc-ww8rm [602.258589ms]
Jul 31 13:44:19.719: INFO: Got endpoints: latency-svc-s8gmj [602.061257ms]
Jul 31 13:44:19.914: INFO: Created: latency-svc-2h9dq
Jul 31 13:44:19.914: INFO: Created: latency-svc-ndcxk
Jul 31 13:44:19.914: INFO: Got endpoints: latency-svc-56jf6 [596.184098ms]
Jul 31 13:44:19.914: INFO: Got endpoints: latency-svc-krkxk [796.263192ms]
Jul 31 13:44:19.917: INFO: Got endpoints: latency-svc-gqh8w [799.357115ms]
Jul 31 13:44:19.917: INFO: Got endpoints: latency-svc-qw9b2 [600.862883ms]
Jul 31 13:44:20.324: INFO: Created: latency-svc-ch9hs
Jul 31 13:44:20.325: INFO: Created: latency-svc-bwdhs
Jul 31 13:44:20.325: INFO: Created: latency-svc-k7nd8
Jul 31 13:44:20.325: INFO: Got endpoints: latency-svc-2h9dq [810.939108ms]
Jul 31 13:44:20.325: INFO: Got endpoints: latency-svc-4ggcj [1.008768448s]
Jul 31 13:44:20.326: INFO: Got endpoints: latency-svc-hwzwh [1.007434621s]
Jul 31 13:44:20.326: INFO: Got endpoints: latency-svc-pmktp [1.007418354s]
Jul 31 13:44:20.326: INFO: Got endpoints: latency-svc-4vfp5 [912.167015ms]
Jul 31 13:44:20.327: INFO: Got endpoints: latency-svc-c9nlf [912.896602ms]
Jul 31 13:44:20.327: INFO: Got endpoints: latency-svc-ndcxk [811.135126ms]
Jul 31 13:44:20.327: INFO: Created: latency-svc-rzjj4
Jul 31 13:44:20.327: INFO: Created: latency-svc-9lpvf
Jul 31 13:44:20.327: INFO: Got endpoints: latency-svc-9lpvf [612.754218ms]
Jul 31 13:44:20.328: INFO: Created: latency-svc-qwn74
Jul 31 13:44:20.329: INFO: Created: latency-svc-h5st4
Jul 31 13:44:20.414: INFO: Created: latency-svc-dpzsw
Jul 31 13:44:20.414: INFO: Got endpoints: latency-svc-qwn74 [496.773041ms]
Jul 31 13:44:20.414: INFO: Got endpoints: latency-svc-rzjj4 [694.55292ms]
Jul 31 13:44:20.531: INFO: Created: latency-svc-dvdpm
Jul 31 13:44:20.531: INFO: Created: latency-svc-mldzz
Jul 31 13:44:20.531: INFO: Created: latency-svc-786cg
Jul 31 13:44:20.532: INFO: Created: latency-svc-ftt6s
Jul 31 13:44:20.532: INFO: Created: latency-svc-d8z5g
Jul 31 13:44:20.532: INFO: Created: latency-svc-6jt2n
Jul 31 13:44:20.532: INFO: Created: latency-svc-fr4qs
Jul 31 13:44:20.532: INFO: Got endpoints: latency-svc-bwdhs [812.988392ms]
Jul 31 13:44:20.532: INFO: Got endpoints: latency-svc-h5st4 [615.078692ms]
Jul 31 13:44:20.719: INFO: Created: latency-svc-nf8sm
Jul 31 13:44:20.719: INFO: Got endpoints: latency-svc-6jt2n [393.953002ms]
Jul 31 13:44:20.720: INFO: Got endpoints: latency-svc-k7nd8 [805.76068ms]
Jul 31 13:44:20.720: INFO: Got endpoints: latency-svc-ch9hs [806.044072ms]
Jul 31 13:44:20.720: INFO: Got endpoints: latency-svc-dpzsw [1.005084721s]
Jul 31 13:44:20.720: INFO: Created: latency-svc-nx7xz
Jul 31 13:44:20.727: INFO: Created: latency-svc-f2gxf
Jul 31 13:44:20.814: INFO: Created: latency-svc-s6nvs
Jul 31 13:44:20.814: INFO: Created: latency-svc-cp9v4
Jul 31 13:44:20.814: INFO: Got endpoints: latency-svc-ftt6s [488.247076ms]
Jul 31 13:44:20.814: INFO: Got endpoints: latency-svc-786cg [487.84062ms]
Jul 31 13:44:21.013: INFO: Created: latency-svc-wgzqz
Jul 31 13:44:21.014: INFO: Created: latency-svc-wtcj2
Jul 31 13:44:21.013: INFO: Created: latency-svc-zfrb7
Jul 31 13:44:21.014: INFO: Got endpoints: latency-svc-dvdpm [686.707559ms]
Jul 31 13:44:21.014: INFO: Created: latency-svc-z7b2r
Jul 31 13:44:21.014: INFO: Got endpoints: latency-svc-mldzz [688.736934ms]
Jul 31 13:44:21.014: INFO: Got endpoints: latency-svc-fr4qs [687.184812ms]
Jul 31 13:44:21.014: INFO: Got endpoints: latency-svc-d8z5g [688.314286ms]
Jul 31 13:44:21.215: INFO: Got endpoints: latency-svc-cp9v4 [682.94605ms]
Jul 31 13:44:21.215: INFO: Got endpoints: latency-svc-nx7xz [801.497246ms]
Jul 31 13:44:21.215: INFO: Got endpoints: latency-svc-nf8sm [887.740262ms]
Jul 31 13:44:21.216: INFO: Got endpoints: latency-svc-f2gxf [802.272734ms]
Jul 31 13:44:21.414: INFO: Created: latency-svc-fpxpv
Jul 31 13:44:21.414: INFO: Created: latency-svc-d5g7z
Jul 31 13:44:21.415: INFO: Created: latency-svc-km5bb
Jul 31 13:44:21.415: INFO: Got endpoints: latency-svc-wtcj2 [694.53237ms]
Jul 31 13:44:21.415: INFO: Got endpoints: latency-svc-s6nvs [882.931108ms]
Jul 31 13:44:21.415: INFO: Got endpoints: latency-svc-zfrb7 [694.870279ms]
Jul 31 13:44:21.415: INFO: Got endpoints: latency-svc-z7b2r [694.698734ms]
Jul 31 13:44:21.415: INFO: Created: latency-svc-8nhf2
Jul 31 13:44:21.415: INFO: Created: latency-svc-fsftd
Jul 31 13:44:21.415: INFO: Created: latency-svc-tt9f5
Jul 31 13:44:21.620: INFO: Created: latency-svc-89qm7
Jul 31 13:44:21.620: INFO: Created: latency-svc-76jqh
Jul 31 13:44:21.620: INFO: Created: latency-svc-hrrw2
Jul 31 13:44:21.620: INFO: Created: latency-svc-gsj4n
Jul 31 13:44:21.620: INFO: Got endpoints: latency-svc-d5g7z [606.170744ms]
Jul 31 13:44:21.621: INFO: Got endpoints: latency-svc-wgzqz [902.196673ms]
Jul 31 13:44:21.621: INFO: Got endpoints: latency-svc-fpxpv [807.338624ms]
Jul 31 13:44:21.623: INFO: Got endpoints: latency-svc-km5bb [609.345719ms]
Jul 31 13:44:21.815: INFO: Created: latency-svc-xhb4j
Jul 31 13:44:21.816: INFO: Created: latency-svc-ksvnb
Jul 31 13:44:21.816: INFO: Created: latency-svc-z9m24
Jul 31 13:44:21.816: INFO: Created: latency-svc-bgvdh
Jul 31 13:44:21.816: INFO: Got endpoints: latency-svc-76jqh [600.874072ms]
Jul 31 13:44:21.816: INFO: Got endpoints: latency-svc-8nhf2 [1.001974288s]
Jul 31 13:44:21.816: INFO: Got endpoints: latency-svc-fsftd [802.229549ms]
Jul 31 13:44:21.816: INFO: Got endpoints: latency-svc-tt9f5 [790.83156ms]
Jul 31 13:44:22.014: INFO: Created: latency-svc-tnlt4
Jul 31 13:44:22.014: INFO: Created: latency-svc-z7fxj
Jul 31 13:44:22.014: INFO: Got endpoints: latency-svc-ksvnb [598.699938ms]
Jul 31 13:44:22.014: INFO: Got endpoints: latency-svc-hrrw2 [798.282791ms]
Jul 31 13:44:22.014: INFO: Got endpoints: latency-svc-gsj4n [798.966309ms]
Jul 31 13:44:22.015: INFO: Created: latency-svc-4l4tk
Jul 31 13:44:22.015: INFO: Created: latency-svc-bfjb6
Jul 31 13:44:22.015: INFO: Got endpoints: latency-svc-89qm7 [799.666499ms]
Jul 31 13:44:22.114: INFO: Created: latency-svc-9qfqp
Jul 31 13:44:22.114: INFO: Created: latency-svc-wt58l
Jul 31 13:44:22.114: INFO: Created: latency-svc-b4xvb
Jul 31 13:44:22.114: INFO: Created: latency-svc-tz5hn
Jul 31 13:44:22.115: INFO: Got endpoints: latency-svc-z9m24 [656.280358ms]
Jul 31 13:44:22.115: INFO: Got endpoints: latency-svc-bgvdh [674.941999ms]
Jul 31 13:44:22.317: INFO: Created: latency-svc-mhv76
Jul 31 13:44:22.317: INFO: Got endpoints: latency-svc-xhb4j [901.338686ms]
Jul 31 13:44:22.319: INFO: Created: latency-svc-6k7kk
Jul 31 13:44:22.319: INFO: Got endpoints: latency-svc-bfjb6 [698.152354ms]
Jul 31 13:44:22.320: INFO: Created: latency-svc-5mwk8
Jul 31 13:44:22.320: INFO: Got endpoints: latency-svc-tnlt4 [698.230909ms]
Jul 31 13:44:22.320: INFO: Got endpoints: latency-svc-z7fxj [699.43029ms]
Jul 31 13:44:22.320: INFO: Created: latency-svc-wdrbr
Jul 31 13:44:22.614: INFO: Created: latency-svc-s85j4
Jul 31 13:44:22.614: INFO: Created: latency-svc-vvmpd
Jul 31 13:44:22.614: INFO: Got endpoints: latency-svc-wt58l [789.435229ms]
Jul 31 13:44:22.615: INFO: Got endpoints: latency-svc-4l4tk [991.279748ms]
Jul 31 13:44:22.615: INFO: Got endpoints: latency-svc-b4xvb [789.03692ms]
Jul 31 13:44:22.619: INFO: Got endpoints: latency-svc-wdrbr [604.181564ms]
Jul 31 13:44:22.619: INFO: Got endpoints: latency-svc-9qfqp [790.919195ms]
Jul 31 13:44:22.619: INFO: Got endpoints: latency-svc-tz5hn [793.748684ms]
Jul 31 13:44:22.714: INFO: Created: latency-svc-xlxgg
Jul 31 13:44:22.715: INFO: Created: latency-svc-gblxz
Jul 31 13:44:22.715: INFO: Created: latency-svc-f94js
Jul 31 13:44:22.715: INFO: Created: latency-svc-jg6gt
Jul 31 13:44:22.715: INFO: Got endpoints: latency-svc-5mwk8 [699.957306ms]
Jul 31 13:44:22.715: INFO: Got endpoints: latency-svc-6k7kk [700.741436ms]
Jul 31 13:44:22.839: INFO: Created: latency-svc-fjsb5
Jul 31 13:44:22.839: INFO: Created: latency-svc-gljv7
Jul 31 13:44:22.839: INFO: Created: latency-svc-qtknq
Jul 31 13:44:22.839: INFO: Created: latency-svc-mbg5r
Jul 31 13:44:22.839: INFO: Got endpoints: latency-svc-vvmpd [724.408177ms]
Jul 31 13:44:22.839: INFO: Got endpoints: latency-svc-mhv76 [825.257163ms]
Jul 31 13:44:22.927: INFO: Got endpoints: latency-svc-gblxz [607.390525ms]
Jul 31 13:44:22.927: INFO: Got endpoints: latency-svc-s85j4 [811.670671ms]
Jul 31 13:44:23.114: INFO: Got endpoints: latency-svc-qtknq [499.153801ms]
Jul 31 13:44:23.114: INFO: Got endpoints: latency-svc-jg6gt [794.422633ms]
Jul 31 13:44:23.114: INFO: Got endpoints: latency-svc-xlxgg [797.334836ms]
Jul 31 13:44:23.114: INFO: Got endpoints: latency-svc-f94js [794.724491ms]
Jul 31 13:44:23.321: INFO: Got endpoints: latency-svc-fjsb5 [706.759987ms]
Jul 31 13:44:23.321: INFO: Got endpoints: latency-svc-gljv7 [706.195429ms]
Jul 31 13:44:23.321: INFO: Got endpoints: latency-svc-mbg5r [702.645408ms]
Jul 31 13:44:23.321: INFO: Latencies: [188.152375ms 189.02179ms 285.765372ms 286.930252ms 302.584699ms 303.523808ms 304.253543ms 304.485522ms 393.953002ms 397.934065ms 399.695294ms 399.885015ms 400.436089ms 400.483811ms 400.753898ms 406.091282ms 410.61246ms 410.864645ms 411.48634ms 411.531963ms 411.874244ms 411.961434ms 412.209421ms 412.389006ms 412.515467ms 412.896144ms 413.797182ms 487.84062ms 488.247076ms 488.363431ms 489.232443ms 489.624677ms 489.660133ms 495.917394ms 496.773041ms 497.608307ms 498.207898ms 498.651118ms 499.153801ms 502.100431ms 503.129358ms 505.022779ms 505.483724ms 505.489129ms 505.990161ms 506.562491ms 510.619025ms 510.989217ms 511.823064ms 590.265988ms 590.391801ms 590.736495ms 596.13652ms 596.184098ms 596.20623ms 596.775183ms 596.986351ms 597.149585ms 597.850515ms 598.124886ms 598.174497ms 598.357663ms 598.381202ms 598.52886ms 598.666586ms 598.668641ms 598.699938ms 598.842697ms 598.843297ms 599.018166ms 599.241063ms 599.476828ms 599.495958ms 599.56553ms 599.847407ms 599.941358ms 600.110509ms 600.862883ms 600.874072ms 601.089386ms 601.721388ms 602.061257ms 602.258589ms 603.251952ms 603.499982ms 604.166883ms 604.181564ms 604.808817ms 604.915176ms 605.655021ms 605.666744ms 606.170744ms 607.390525ms 609.345719ms 612.754218ms 615.078692ms 656.280358ms 674.941999ms 682.94605ms 685.977399ms 686.707559ms 687.184812ms 687.903843ms 688.314286ms 688.736934ms 689.852827ms 694.53237ms 694.55292ms 694.698734ms 694.870279ms 695.341491ms 695.492932ms 695.657276ms 695.742193ms 695.752528ms 695.846802ms 696.255415ms 696.482341ms 696.660746ms 696.729178ms 697.257669ms 697.336394ms 697.395916ms 698.152354ms 698.230909ms 698.507553ms 698.732794ms 699.056829ms 699.235956ms 699.43029ms 699.957306ms 700.094496ms 700.741436ms 700.871419ms 701.769657ms 702.379983ms 702.645408ms 702.857938ms 704.430302ms 704.670628ms 706.195429ms 706.759987ms 724.408177ms 786.554905ms 788.036298ms 788.301331ms 788.489185ms 789.03692ms 789.435229ms 789.960271ms 790.13065ms 790.196799ms 790.83156ms 790.919195ms 793.748684ms 794.422633ms 794.724491ms 795.130256ms 796.263192ms 796.280306ms 796.806089ms 796.921873ms 797.247204ms 797.334836ms 797.942159ms 798.080144ms 798.282791ms 798.966309ms 799.357115ms 799.666499ms 801.497246ms 802.229549ms 802.272734ms 803.13472ms 803.927717ms 805.76068ms 806.044072ms 807.338624ms 810.939108ms 811.135126ms 811.670671ms 812.988392ms 825.257163ms 882.931108ms 885.1643ms 885.60769ms 887.740262ms 890.331615ms 897.253199ms 898.942284ms 901.338686ms 902.196673ms 912.167015ms 912.896602ms 991.279748ms 1.001974288s 1.005084721s 1.007418354s 1.007434621s 1.008768448s]
Jul 31 13:44:23.321: INFO: 50 %ile: 686.707559ms
Jul 31 13:44:23.321: INFO: 90 %ile: 811.670671ms
Jul 31 13:44:23.321: INFO: 99 %ile: 1.007434621s
Jul 31 13:44:23.322: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:44:23.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2883" for this suite.
Jul 31 13:44:41.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:44:43.562: INFO: namespace svc-latency-2883 deletion completed in 20.042467527s

• [SLOW TEST:37.841 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:44:43.564: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 31 13:44:44.217: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:44:52.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2507" for this suite.
Jul 31 13:45:00.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:45:04.249: INFO: namespace init-container-2507 deletion completed in 11.506538432s

• [SLOW TEST:20.685 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:45:04.249: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 31 13:45:04.842: INFO: Waiting up to 5m0s for pod "pod-cf1dc8d1-f318-4dec-a02c-f9f3a77da2f3" in namespace "emptydir-66" to be "success or failure"
Jul 31 13:45:04.848: INFO: Pod "pod-cf1dc8d1-f318-4dec-a02c-f9f3a77da2f3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.745297ms
Jul 31 13:45:07.318: INFO: Pod "pod-cf1dc8d1-f318-4dec-a02c-f9f3a77da2f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.475388423s
Jul 31 13:45:10.636: INFO: Pod "pod-cf1dc8d1-f318-4dec-a02c-f9f3a77da2f3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.793868209s
Jul 31 13:45:12.819: INFO: Pod "pod-cf1dc8d1-f318-4dec-a02c-f9f3a77da2f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.977011152s
STEP: Saw pod success
Jul 31 13:45:12.819: INFO: Pod "pod-cf1dc8d1-f318-4dec-a02c-f9f3a77da2f3" satisfied condition "success or failure"
Jul 31 13:45:12.826: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-cf1dc8d1-f318-4dec-a02c-f9f3a77da2f3 container test-container: <nil>
STEP: delete the pod
Jul 31 13:45:16.314: INFO: Waiting for pod pod-cf1dc8d1-f318-4dec-a02c-f9f3a77da2f3 to disappear
Jul 31 13:45:16.321: INFO: Pod pod-cf1dc8d1-f318-4dec-a02c-f9f3a77da2f3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:45:16.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-66" for this suite.
Jul 31 13:45:22.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:45:38.114: INFO: namespace emptydir-66 deletion completed in 21.784615528s

• [SLOW TEST:33.865 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:45:38.115: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 31 13:45:40.415: INFO: Creating ReplicaSet my-hostname-basic-4298d5e9-5b9d-4a1c-a3a2-93267bd777bf
Jul 31 13:45:40.561: INFO: Pod name my-hostname-basic-4298d5e9-5b9d-4a1c-a3a2-93267bd777bf: Found 0 pods out of 1
Jul 31 13:45:45.731: INFO: Pod name my-hostname-basic-4298d5e9-5b9d-4a1c-a3a2-93267bd777bf: Found 1 pods out of 1
Jul 31 13:45:45.731: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4298d5e9-5b9d-4a1c-a3a2-93267bd777bf" is running
Jul 31 13:45:45.737: INFO: Pod "my-hostname-basic-4298d5e9-5b9d-4a1c-a3a2-93267bd777bf-dz4l5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-31 13:45:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-31 13:45:44 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-31 13:45:44 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-31 13:45:40 +0000 UTC Reason: Message:}])
Jul 31 13:45:45.737: INFO: Trying to dial the pod
Jul 31 13:45:51.815: INFO: Controller my-hostname-basic-4298d5e9-5b9d-4a1c-a3a2-93267bd777bf: Got expected result from replica 1 [my-hostname-basic-4298d5e9-5b9d-4a1c-a3a2-93267bd777bf-dz4l5]: "my-hostname-basic-4298d5e9-5b9d-4a1c-a3a2-93267bd777bf-dz4l5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:45:51.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3657" for this suite.
Jul 31 13:46:00.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:46:04.921: INFO: namespace replicaset-3657 deletion completed in 12.903634308s

• [SLOW TEST:26.806 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:46:04.921: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:46:56.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8664" for this suite.
Jul 31 13:47:04.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:47:11.955: INFO: namespace container-runtime-8664 deletion completed in 14.23853488s

• [SLOW TEST:67.034 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:47:11.956: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 31 13:47:12.914: INFO: Waiting up to 5m0s for pod "pod-db83c4ae-c87a-4e6a-a469-3b880141f32e" in namespace "emptydir-1747" to be "success or failure"
Jul 31 13:47:13.216: INFO: Pod "pod-db83c4ae-c87a-4e6a-a469-3b880141f32e": Phase="Pending", Reason="", readiness=false. Elapsed: 301.170536ms
Jul 31 13:47:15.519: INFO: Pod "pod-db83c4ae-c87a-4e6a-a469-3b880141f32e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.604364621s
Jul 31 13:47:17.620: INFO: Pod "pod-db83c4ae-c87a-4e6a-a469-3b880141f32e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.705182209s
Jul 31 13:47:19.816: INFO: Pod "pod-db83c4ae-c87a-4e6a-a469-3b880141f32e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.901086814s
STEP: Saw pod success
Jul 31 13:47:19.816: INFO: Pod "pod-db83c4ae-c87a-4e6a-a469-3b880141f32e" satisfied condition "success or failure"
Jul 31 13:47:19.919: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-db83c4ae-c87a-4e6a-a469-3b880141f32e container test-container: <nil>
STEP: delete the pod
Jul 31 13:47:20.517: INFO: Waiting for pod pod-db83c4ae-c87a-4e6a-a469-3b880141f32e to disappear
Jul 31 13:47:20.615: INFO: Pod pod-db83c4ae-c87a-4e6a-a469-3b880141f32e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:47:20.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1747" for this suite.
Jul 31 13:47:28.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:47:36.715: INFO: namespace emptydir-1747 deletion completed in 15.7881923s

• [SLOW TEST:24.762 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:47:36.720: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 31 13:47:37.925: INFO: Waiting up to 5m0s for pod "pod-bbc14266-babe-4d0b-9810-928f2ca36584" in namespace "emptydir-4279" to be "success or failure"
Jul 31 13:47:38.224: INFO: Pod "pod-bbc14266-babe-4d0b-9810-928f2ca36584": Phase="Pending", Reason="", readiness=false. Elapsed: 298.498675ms
Jul 31 13:47:40.414: INFO: Pod "pod-bbc14266-babe-4d0b-9810-928f2ca36584": Phase="Pending", Reason="", readiness=false. Elapsed: 2.489170393s
Jul 31 13:47:42.825: INFO: Pod "pod-bbc14266-babe-4d0b-9810-928f2ca36584": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.900251006s
STEP: Saw pod success
Jul 31 13:47:42.826: INFO: Pod "pod-bbc14266-babe-4d0b-9810-928f2ca36584" satisfied condition "success or failure"
Jul 31 13:47:42.833: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-bbc14266-babe-4d0b-9810-928f2ca36584 container test-container: <nil>
STEP: delete the pod
Jul 31 13:47:44.015: INFO: Waiting for pod pod-bbc14266-babe-4d0b-9810-928f2ca36584 to disappear
Jul 31 13:47:44.027: INFO: Pod pod-bbc14266-babe-4d0b-9810-928f2ca36584 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:47:44.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4279" for this suite.
Jul 31 13:47:50.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:47:58.661: INFO: namespace emptydir-4279 deletion completed in 14.609838383s

• [SLOW TEST:21.941 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:47:58.661: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 31 13:47:59.518: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2199957-b8f0-47b3-8ee2-f895ecd17257" in namespace "downward-api-406" to be "success or failure"
Jul 31 13:47:59.915: INFO: Pod "downwardapi-volume-d2199957-b8f0-47b3-8ee2-f895ecd17257": Phase="Pending", Reason="", readiness=false. Elapsed: 396.816869ms
Jul 31 13:48:02.119: INFO: Pod "downwardapi-volume-d2199957-b8f0-47b3-8ee2-f895ecd17257": Phase="Pending", Reason="", readiness=false. Elapsed: 2.600673109s
Jul 31 13:48:05.016: INFO: Pod "downwardapi-volume-d2199957-b8f0-47b3-8ee2-f895ecd17257": Phase="Pending", Reason="", readiness=false. Elapsed: 5.497943408s
Jul 31 13:48:07.811: INFO: Pod "downwardapi-volume-d2199957-b8f0-47b3-8ee2-f895ecd17257": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.292443182s
STEP: Saw pod success
Jul 31 13:48:07.811: INFO: Pod "downwardapi-volume-d2199957-b8f0-47b3-8ee2-f895ecd17257" satisfied condition "success or failure"
Jul 31 13:48:07.915: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod downwardapi-volume-d2199957-b8f0-47b3-8ee2-f895ecd17257 container client-container: <nil>
STEP: delete the pod
Jul 31 13:48:07.959: INFO: Waiting for pod downwardapi-volume-d2199957-b8f0-47b3-8ee2-f895ecd17257 to disappear
Jul 31 13:48:07.964: INFO: Pod downwardapi-volume-d2199957-b8f0-47b3-8ee2-f895ecd17257 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:48:07.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-406" for this suite.
Jul 31 13:48:15.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:48:19.069: INFO: namespace downward-api-406 deletion completed in 10.253146745s

• [SLOW TEST:20.408 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:48:19.072: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Jul 31 13:48:19.614: INFO: Waiting up to 5m0s for pod "client-containers-3c3b9b14-b51e-431d-b185-814ec997312f" in namespace "containers-9278" to be "success or failure"
Jul 31 13:48:19.636: INFO: Pod "client-containers-3c3b9b14-b51e-431d-b185-814ec997312f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.784227ms
Jul 31 13:48:21.917: INFO: Pod "client-containers-3c3b9b14-b51e-431d-b185-814ec997312f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.3019796s
Jul 31 13:48:24.118: INFO: Pod "client-containers-3c3b9b14-b51e-431d-b185-814ec997312f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.503097641s
STEP: Saw pod success
Jul 31 13:48:24.118: INFO: Pod "client-containers-3c3b9b14-b51e-431d-b185-814ec997312f" satisfied condition "success or failure"
Jul 31 13:48:24.315: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod client-containers-3c3b9b14-b51e-431d-b185-814ec997312f container test-container: <nil>
STEP: delete the pod
Jul 31 13:48:25.117: INFO: Waiting for pod client-containers-3c3b9b14-b51e-431d-b185-814ec997312f to disappear
Jul 31 13:48:25.219: INFO: Pod client-containers-3c3b9b14-b51e-431d-b185-814ec997312f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:48:25.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9278" for this suite.
Jul 31 13:48:32.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:48:39.815: INFO: namespace containers-9278 deletion completed in 14.586484343s

• [SLOW TEST:20.744 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 31 13:48:39.816: INFO: >>> kubeConfig: /tmp/kubeconfig-280300738
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-569c8d05-0ab6-4d53-8143-b448737c03dd
STEP: Creating a pod to test consume secrets
Jul 31 13:48:40.046: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-16776ea9-99f7-4be2-ab79-caceeec20f02" in namespace "projected-9849" to be "success or failure"
Jul 31 13:48:40.058: INFO: Pod "pod-projected-secrets-16776ea9-99f7-4be2-ab79-caceeec20f02": Phase="Pending", Reason="", readiness=false. Elapsed: 11.313831ms
Jul 31 13:48:42.320: INFO: Pod "pod-projected-secrets-16776ea9-99f7-4be2-ab79-caceeec20f02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.273486833s
Jul 31 13:48:44.716: INFO: Pod "pod-projected-secrets-16776ea9-99f7-4be2-ab79-caceeec20f02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.669752848s
Jul 31 13:48:47.222: INFO: Pod "pod-projected-secrets-16776ea9-99f7-4be2-ab79-caceeec20f02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.175950023s
STEP: Saw pod success
Jul 31 13:48:47.222: INFO: Pod "pod-projected-secrets-16776ea9-99f7-4be2-ab79-caceeec20f02" satisfied condition "success or failure"
Jul 31 13:48:47.416: INFO: Trying to get logs from node worker-2bh9r-78c4c5b4fb-czrvz pod pod-projected-secrets-16776ea9-99f7-4be2-ab79-caceeec20f02 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 31 13:48:48.041: INFO: Waiting for pod pod-projected-secrets-16776ea9-99f7-4be2-ab79-caceeec20f02 to disappear
Jul 31 13:48:48.048: INFO: Pod pod-projected-secrets-16776ea9-99f7-4be2-ab79-caceeec20f02 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 31 13:48:48.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9849" for this suite.
Jul 31 13:48:56.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 31 13:49:06.567: INFO: namespace projected-9849 deletion completed in 18.510961801s

• [SLOW TEST:26.751 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSJul 31 13:49:06.567: INFO: Running AfterSuite actions on all nodes
Jul 31 13:49:06.567: INFO: Running AfterSuite actions on node 1
Jul 31 13:49:06.567: INFO: Skipping dumping logs from cluster

Ran 215 of 4411 Specs in 9729.095 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4196 Skipped
PASS

Ginkgo ran 1 suite in 2h42m10.956977861s
Test Suite Passed
