I0814 14:03:56.187733      20 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-938832172
I0814 14:03:56.187910      20 e2e.go:241] Starting e2e run "52fe7d63-69df-43b5-b78d-ceeac478c3fc" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1565791434 - Will randomize all specs
Will run 215 of 4413 specs

Aug 14 14:03:56.485: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 14:03:56.488: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 14 14:03:56.512: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 14 14:03:56.783: INFO: 38 / 38 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 14 14:03:56.783: INFO: expected 11 pod replicas in namespace 'kube-system', 11 are Running and Ready.
Aug 14 14:03:56.783: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 14 14:03:56.793: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Aug 14 14:03:56.793: INFO: e2e test version: v1.15.2
Aug 14 14:03:56.794: INFO: kube-apiserver version: v1.15.2
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:03:56.794: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
Aug 14 14:03:57.881: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Aug 14 14:03:57.899: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1091
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 14:03:58.413: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df671319-c746-4d47-85e7-ca7ea5be29fc" in namespace "projected-1091" to be "success or failure"
Aug 14 14:03:58.419: INFO: Pod "downwardapi-volume-df671319-c746-4d47-85e7-ca7ea5be29fc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.504243ms
Aug 14 14:04:01.478: INFO: Pod "downwardapi-volume-df671319-c746-4d47-85e7-ca7ea5be29fc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.065264832s
Aug 14 14:04:03.627: INFO: Pod "downwardapi-volume-df671319-c746-4d47-85e7-ca7ea5be29fc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.213911175s
Aug 14 14:04:05.672: INFO: Pod "downwardapi-volume-df671319-c746-4d47-85e7-ca7ea5be29fc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.258580464s
Aug 14 14:04:08.001: INFO: Pod "downwardapi-volume-df671319-c746-4d47-85e7-ca7ea5be29fc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.588474574s
Aug 14 14:04:10.454: INFO: Pod "downwardapi-volume-df671319-c746-4d47-85e7-ca7ea5be29fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.041153046s
STEP: Saw pod success
Aug 14 14:04:10.454: INFO: Pod "downwardapi-volume-df671319-c746-4d47-85e7-ca7ea5be29fc" satisfied condition "success or failure"
Aug 14 14:04:10.760: INFO: Trying to get logs from node slave2 pod downwardapi-volume-df671319-c746-4d47-85e7-ca7ea5be29fc container client-container: <nil>
STEP: delete the pod
Aug 14 14:04:12.556: INFO: Waiting for pod downwardapi-volume-df671319-c746-4d47-85e7-ca7ea5be29fc to disappear
Aug 14 14:04:13.194: INFO: Pod downwardapi-volume-df671319-c746-4d47-85e7-ca7ea5be29fc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:04:13.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1091" for this suite.
Aug 14 14:04:27.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:04:29.249: INFO: namespace projected-1091 deletion completed in 16.045000984s

â€¢ [SLOW TEST:32.455 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:04:29.249: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9109
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Aug 14 14:04:31.717: INFO: Found 0 stateful pods, waiting for 3
Aug 14 14:04:42.178: INFO: Found 2 stateful pods, waiting for 3
Aug 14 14:04:52.183: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 14:04:52.183: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 14:04:52.183: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug 14 14:05:01.805: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 14:05:01.806: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 14:05:01.806: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 14:05:01.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9109 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 14:05:10.606: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 14:05:10.606: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 14:05:10.607: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 14 14:05:20.943: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 14 14:05:31.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9109 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:05:32.370: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 14 14:05:32.370: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 14:05:32.370: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 14:05:42.550: INFO: Waiting for StatefulSet statefulset-9109/ss2 to complete update
Aug 14 14:05:42.550: INFO: Waiting for Pod statefulset-9109/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 14:05:42.550: INFO: Waiting for Pod statefulset-9109/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 14:05:42.550: INFO: Waiting for Pod statefulset-9109/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 14:05:52.895: INFO: Waiting for StatefulSet statefulset-9109/ss2 to complete update
Aug 14 14:05:52.895: INFO: Waiting for Pod statefulset-9109/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 14:05:52.895: INFO: Waiting for Pod statefulset-9109/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 14:06:03.299: INFO: Waiting for StatefulSet statefulset-9109/ss2 to complete update
Aug 14 14:06:03.299: INFO: Waiting for Pod statefulset-9109/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 14:06:03.299: INFO: Waiting for Pod statefulset-9109/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 14:06:12.618: INFO: Waiting for StatefulSet statefulset-9109/ss2 to complete update
Aug 14 14:06:12.618: INFO: Waiting for Pod statefulset-9109/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 14:06:23.349: INFO: Waiting for StatefulSet statefulset-9109/ss2 to complete update
Aug 14 14:06:23.349: INFO: Waiting for Pod statefulset-9109/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 14:06:32.647: INFO: Waiting for StatefulSet statefulset-9109/ss2 to complete update
STEP: Rolling back to a previous revision
Aug 14 14:06:42.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9109 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 14:06:43.479: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 14:06:43.479: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 14:06:43.479: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 14:06:53.722: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 14 14:07:04.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9109 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:07:05.631: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 14 14:07:05.631: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 14:07:05.631: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 14:07:15.682: INFO: Waiting for StatefulSet statefulset-9109/ss2 to complete update
Aug 14 14:07:15.682: INFO: Waiting for Pod statefulset-9109/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 14 14:07:15.682: INFO: Waiting for Pod statefulset-9109/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 14 14:07:15.682: INFO: Waiting for Pod statefulset-9109/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 14 14:07:25.694: INFO: Waiting for StatefulSet statefulset-9109/ss2 to complete update
Aug 14 14:07:25.694: INFO: Waiting for Pod statefulset-9109/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 14 14:07:25.694: INFO: Waiting for Pod statefulset-9109/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 14 14:07:35.691: INFO: Waiting for StatefulSet statefulset-9109/ss2 to complete update
Aug 14 14:07:35.691: INFO: Waiting for Pod statefulset-9109/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 14 14:07:35.691: INFO: Waiting for Pod statefulset-9109/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 14 14:07:45.691: INFO: Waiting for StatefulSet statefulset-9109/ss2 to complete update
Aug 14 14:07:45.691: INFO: Waiting for Pod statefulset-9109/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 14 14:08:05.691: INFO: Deleting all statefulset in ns statefulset-9109
Aug 14 14:08:05.696: INFO: Scaling statefulset ss2 to 0
Aug 14 14:08:25.893: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 14:08:25.905: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:08:26.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9109" for this suite.
Aug 14 14:08:45.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:08:46.890: INFO: namespace statefulset-9109 deletion completed in 20.474400786s

â€¢ [SLOW TEST:257.641 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:08:46.891: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1622
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 14 14:08:58.266: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-d22e85f3-4669-4b62-af0d-6a8696ba28a9,GenerateName:,Namespace:events-1622,SelfLink:/api/v1/namespaces/events-1622/pods/send-events-d22e85f3-4669-4b62-af0d-6a8696ba28a9,UID:42a61447-680d-4a88-a059-509df4546f51,ResourceVersion:355028,Generation:0,CreationTimestamp:2019-08-14 14:08:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 873098597,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t2hsp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t2hsp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-t2hsp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00340ae20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00340ae40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:08:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:08:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:08:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:08:48 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.67,PodIP:10.151.194.234,StartTime:2019-08-14 14:08:48 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-08-14 14:08:55 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://6de9b0f872aec4a973db8a48199042712b74c1feb5a5d14babaeceda75103fb1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Aug 14 14:09:00.354: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 14 14:09:02.359: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:09:02.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1622" for this suite.
Aug 14 14:09:45.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:09:47.583: INFO: namespace events-1622 deletion completed in 44.733772208s

â€¢ [SLOW TEST:60.692 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:09:47.585: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1538
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1538
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-1538
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1538
Aug 14 14:09:49.631: INFO: Found 0 stateful pods, waiting for 1
Aug 14 14:09:59.636: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 14 14:09:59.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 14:10:00.829: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 14:10:00.829: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 14:10:00.829: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 14:10:00.833: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 14 14:10:10.987: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 14:10:10.987: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 14:10:11.493: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Aug 14 14:10:11.493: INFO: ss-0  slave1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:09:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:09:50 +0000 UTC  }]
Aug 14 14:10:11.493: INFO: ss-1          Pending         []
Aug 14 14:10:11.493: INFO: 
Aug 14 14:10:11.493: INFO: StatefulSet ss has not reached scale 3, at 2
Aug 14 14:10:12.751: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994606838s
Aug 14 14:10:13.881: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.736416301s
Aug 14 14:10:14.888: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.606378681s
Aug 14 14:10:15.892: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.600040216s
Aug 14 14:10:17.344: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.595304249s
Aug 14 14:10:18.689: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.143275632s
Aug 14 14:10:19.754: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.798227447s
Aug 14 14:10:21.022: INFO: Verifying statefulset ss doesn't scale past 3 for another 733.323517ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1538
Aug 14 14:10:22.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:10:23.466: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 14 14:10:23.466: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 14:10:23.466: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 14:10:23.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:10:24.099: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 14 14:10:24.099: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 14:10:24.099: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 14:10:24.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:10:24.702: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 14 14:10:24.702: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 14:10:24.702: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 14:10:24.722: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 14:10:24.722: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 14:10:24.722: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 14 14:10:24.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 14:10:26.085: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 14:10:26.085: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 14:10:26.085: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 14:10:26.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 14:10:27.252: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 14:10:27.252: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 14:10:27.252: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 14:10:27.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 14:10:29.221: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 14:10:29.221: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 14:10:29.221: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 14:10:29.221: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 14:10:29.388: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 14 14:10:39.599: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 14:10:39.599: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 14:10:39.599: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 14:10:39.615: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Aug 14 14:10:39.615: INFO: ss-0  slave1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:09:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:09:50 +0000 UTC  }]
Aug 14 14:10:39.615: INFO: ss-1  slave3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:11 +0000 UTC  }]
Aug 14 14:10:39.615: INFO: ss-2  slave2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  }]
Aug 14 14:10:39.615: INFO: 
Aug 14 14:10:39.615: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 14 14:10:40.720: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Aug 14 14:10:40.720: INFO: ss-0  slave1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:09:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:09:50 +0000 UTC  }]
Aug 14 14:10:40.720: INFO: ss-1  slave3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:11 +0000 UTC  }]
Aug 14 14:10:40.720: INFO: ss-2  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  }]
Aug 14 14:10:40.720: INFO: 
Aug 14 14:10:40.720: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 14 14:10:42.022: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Aug 14 14:10:42.022: INFO: ss-0  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:09:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:09:50 +0000 UTC  }]
Aug 14 14:10:42.022: INFO: ss-1  slave3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:11 +0000 UTC  }]
Aug 14 14:10:42.022: INFO: ss-2  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  }]
Aug 14 14:10:42.022: INFO: 
Aug 14 14:10:42.022: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 14 14:10:43.113: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Aug 14 14:10:43.113: INFO: ss-0  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:09:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:09:50 +0000 UTC  }]
Aug 14 14:10:43.113: INFO: ss-1  slave3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:11 +0000 UTC  }]
Aug 14 14:10:43.113: INFO: ss-2  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  }]
Aug 14 14:10:43.113: INFO: 
Aug 14 14:10:43.113: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 14 14:10:44.289: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Aug 14 14:10:44.289: INFO: ss-0  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:09:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:09:50 +0000 UTC  }]
Aug 14 14:10:44.289: INFO: ss-1  slave3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:11 +0000 UTC  }]
Aug 14 14:10:44.289: INFO: ss-2  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  }]
Aug 14 14:10:44.289: INFO: 
Aug 14 14:10:44.289: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 14 14:10:45.609: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Aug 14 14:10:45.609: INFO: ss-0  slave1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:09:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:09:50 +0000 UTC  }]
Aug 14 14:10:45.609: INFO: ss-1  slave3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:11 +0000 UTC  }]
Aug 14 14:10:45.609: INFO: ss-2  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  }]
Aug 14 14:10:45.609: INFO: 
Aug 14 14:10:45.609: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 14 14:10:46.712: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Aug 14 14:10:46.712: INFO: ss-1  slave3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:11 +0000 UTC  }]
Aug 14 14:10:46.712: INFO: ss-2  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  }]
Aug 14 14:10:46.713: INFO: 
Aug 14 14:10:46.713: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 14 14:10:47.717: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Aug 14 14:10:47.717: INFO: ss-1  slave3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:11 +0000 UTC  }]
Aug 14 14:10:47.718: INFO: 
Aug 14 14:10:47.718: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 14 14:10:48.723: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Aug 14 14:10:48.723: INFO: ss-1  slave3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 14:10:11 +0000 UTC  }]
Aug 14 14:10:48.723: INFO: 
Aug 14 14:10:48.723: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1538
Aug 14 14:10:49.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:10:49.970: INFO: rc: 1
Aug 14 14:10:49.970: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc00257b920 exit status 1 <nil> <nil> true [0xc003438228 0xc003438240 0xc003438258] [0xc003438228 0xc003438240 0xc003438258] [0xc003438238 0xc003438250] [0x9d17b0 0x9d17b0] 0xc002f39260 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Aug 14 14:10:59.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:11:00.414: INFO: rc: 1
Aug 14 14:11:00.414: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00326db60 exit status 1 <nil> <nil> true [0xc001b4cae8 0xc001b4cb60 0xc001b4cba8] [0xc001b4cae8 0xc001b4cb60 0xc001b4cba8] [0xc001b4cb48 0xc001b4cb98] [0x9d17b0 0x9d17b0] 0xc0035479e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:11:10.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:11:11.128: INFO: rc: 1
Aug 14 14:11:11.128: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0035c0690 exit status 1 <nil> <nil> true [0xc0008baf90 0xc0008bb1e0 0xc0008bb2e8] [0xc0008baf90 0xc0008bb1e0 0xc0008bb2e8] [0xc0008bb098 0xc0008bb2c0] [0x9d17b0 0x9d17b0] 0xc002bf46c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:11:21.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:11:21.540: INFO: rc: 1
Aug 14 14:11:21.540: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00326df80 exit status 1 <nil> <nil> true [0xc001b4cbb0 0xc001b4cbe0 0xc001b4cc28] [0xc001b4cbb0 0xc001b4cbe0 0xc001b4cc28] [0xc001b4cbd0 0xc001b4cc10] [0x9d17b0 0x9d17b0] 0xc003547d40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:11:31.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:11:32.202: INFO: rc: 1
Aug 14 14:11:32.202: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00257bce0 exit status 1 <nil> <nil> true [0xc003438260 0xc003438278 0xc003438290] [0xc003438260 0xc003438278 0xc003438290] [0xc003438270 0xc003438288] [0x9d17b0 0x9d17b0] 0xc002f395c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:11:42.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:11:43.833: INFO: rc: 1
Aug 14 14:11:43.833: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0035c0a20 exit status 1 <nil> <nil> true [0xc0008bb428 0xc0008bb638 0xc0008bb730] [0xc0008bb428 0xc0008bb638 0xc0008bb730] [0xc0008bb5c8 0xc0008bb6d0] [0x9d17b0 0x9d17b0] 0xc002bf4e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:11:53.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:11:54.882: INFO: rc: 1
Aug 14 14:11:54.883: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002613590 exit status 1 <nil> <nil> true [0xc0014c8130 0xc0014c8148 0xc0014c81b0] [0xc0014c8130 0xc0014c8148 0xc0014c81b0] [0xc0014c8140 0xc0014c8158] [0x9d17b0 0x9d17b0] 0xc00327b320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:12:04.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:12:06.368: INFO: rc: 1
Aug 14 14:12:06.369: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0035c0d80 exit status 1 <nil> <nil> true [0xc0008bb790 0xc0008bb8e8 0xc0008bba48] [0xc0008bb790 0xc0008bb8e8 0xc0008bba48] [0xc0008bb8c0 0xc0008bb978] [0x9d17b0 0x9d17b0] 0xc002bf5560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:12:16.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:12:17.008: INFO: rc: 1
Aug 14 14:12:17.008: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00342a360 exit status 1 <nil> <nil> true [0xc00034c710 0xc00034c9d8 0xc00034cd08] [0xc00034c710 0xc00034c9d8 0xc00034cd08] [0xc00034c8d0 0xc00034ca78] [0x9d17b0 0x9d17b0] 0xc002f382a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:12:27.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:12:28.054: INFO: rc: 1
Aug 14 14:12:28.054: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00257a300 exit status 1 <nil> <nil> true [0xc000011d10 0xc000182120 0xc000182238] [0xc000011d10 0xc000182120 0xc000182238] [0xc000182000 0xc0001821b8] [0x9d17b0 0x9d17b0] 0xc002850720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:12:38.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:12:39.740: INFO: rc: 1
Aug 14 14:12:39.740: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d30330 exit status 1 <nil> <nil> true [0xc003438018 0xc003438040 0xc003438080] [0xc003438018 0xc003438040 0xc003438080] [0xc003438038 0xc003438060] [0x9d17b0 0x9d17b0] 0xc00239c540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:12:49.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:12:50.759: INFO: rc: 1
Aug 14 14:12:50.759: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00342a720 exit status 1 <nil> <nil> true [0xc00034ce28 0xc00034d108 0xc00034d3e8] [0xc00034ce28 0xc00034d108 0xc00034d3e8] [0xc00034cee0 0xc00034d330] [0x9d17b0 0x9d17b0] 0xc002f38660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:13:00.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:13:02.075: INFO: rc: 1
Aug 14 14:13:02.075: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00342aab0 exit status 1 <nil> <nil> true [0xc00034d4a0 0xc00034d5f0 0xc00034d720] [0xc00034d4a0 0xc00034d5f0 0xc00034d720] [0xc00034d5c0 0xc00034d6b0] [0x9d17b0 0x9d17b0] 0xc002f38ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:13:12.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:13:12.732: INFO: rc: 1
Aug 14 14:13:12.732: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d30690 exit status 1 <nil> <nil> true [0xc003438098 0xc0034380e0 0xc003438108] [0xc003438098 0xc0034380e0 0xc003438108] [0xc0034380c0 0xc003438100] [0x9d17b0 0x9d17b0] 0xc00239d0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:13:22.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:13:24.044: INFO: rc: 1
Aug 14 14:13:24.044: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d30a50 exit status 1 <nil> <nil> true [0xc003438110 0xc003438148 0xc003438160] [0xc003438110 0xc003438148 0xc003438160] [0xc003438140 0xc003438158] [0x9d17b0 0x9d17b0] 0xc002bf4120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:13:34.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:13:34.763: INFO: rc: 1
Aug 14 14:13:34.763: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00342ae70 exit status 1 <nil> <nil> true [0xc00034d820 0xc00034d970 0xc00034db20] [0xc00034d820 0xc00034d970 0xc00034db20] [0xc00034d918 0xc00034dad0] [0x9d17b0 0x9d17b0] 0xc002f38f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:13:44.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:13:45.488: INFO: rc: 1
Aug 14 14:13:45.488: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00342b230 exit status 1 <nil> <nil> true [0xc00034db70 0xc00034dce8 0xc00034dd60] [0xc00034db70 0xc00034dce8 0xc00034dd60] [0xc00034dc78 0xc00034dd58] [0x9d17b0 0x9d17b0] 0xc002f392c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:13:55.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:13:55.748: INFO: rc: 1
Aug 14 14:13:55.748: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d30ea0 exit status 1 <nil> <nil> true [0xc003438178 0xc0034381a0 0xc0034381b8] [0xc003438178 0xc0034381a0 0xc0034381b8] [0xc003438198 0xc0034381b0] [0x9d17b0 0x9d17b0] 0xc002bf48a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:14:05.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:14:06.360: INFO: rc: 1
Aug 14 14:14:06.360: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00257a2d0 exit status 1 <nil> <nil> true [0xc003438018 0xc003438040 0xc003438080] [0xc003438018 0xc003438040 0xc003438080] [0xc003438038 0xc003438060] [0x9d17b0 0x9d17b0] 0xc003546300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:14:16.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:14:17.189: INFO: rc: 1
Aug 14 14:14:17.189: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d30360 exit status 1 <nil> <nil> true [0xc000011d10 0xc000182120 0xc000182238] [0xc000011d10 0xc000182120 0xc000182238] [0xc000182000 0xc0001821b8] [0x9d17b0 0x9d17b0] 0xc00239c540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:14:27.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:14:27.802: INFO: rc: 1
Aug 14 14:14:27.802: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d30720 exit status 1 <nil> <nil> true [0xc000183f38 0xc0008babc0 0xc0008bae88] [0xc000183f38 0xc0008babc0 0xc0008bae88] [0xc0008baaf8 0xc0008bad38] [0x9d17b0 0x9d17b0] 0xc00239d0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:14:37.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:14:38.250: INFO: rc: 1
Aug 14 14:14:38.250: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002e083f0 exit status 1 <nil> <nil> true [0xc00034c650 0xc00034c8d0 0xc00034ca78] [0xc00034c650 0xc00034c8d0 0xc00034ca78] [0xc00034c8c0 0xc00034c9e8] [0x9d17b0 0x9d17b0] 0xc002850720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:14:48.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:14:48.898: INFO: rc: 1
Aug 14 14:14:48.898: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d30ae0 exit status 1 <nil> <nil> true [0xc0008baf10 0xc0008bb058 0xc0008bb250] [0xc0008baf10 0xc0008bb058 0xc0008bb250] [0xc0008baf90 0xc0008bb1e0] [0x9d17b0 0x9d17b0] 0xc002bf4120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:14:58.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:14:59.130: INFO: rc: 1
Aug 14 14:14:59.130: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d30f60 exit status 1 <nil> <nil> true [0xc0008bb2c0 0xc0008bb530 0xc0008bb678] [0xc0008bb2c0 0xc0008bb530 0xc0008bb678] [0xc0008bb428 0xc0008bb638] [0x9d17b0 0x9d17b0] 0xc002bf48a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:15:09.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:15:16.412: INFO: rc: 1
Aug 14 14:15:16.412: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00342a390 exit status 1 <nil> <nil> true [0xc001b4c030 0xc001b4c090 0xc001b4c158] [0xc001b4c030 0xc001b4c090 0xc001b4c158] [0xc001b4c080 0xc001b4c120] [0x9d17b0 0x9d17b0] 0xc002f382a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:15:26.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:15:26.932: INFO: rc: 1
Aug 14 14:15:26.932: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d312c0 exit status 1 <nil> <nil> true [0xc0008bb6d0 0xc0008bb810 0xc0008bb968] [0xc0008bb6d0 0xc0008bb810 0xc0008bb968] [0xc0008bb790 0xc0008bb8e8] [0x9d17b0 0x9d17b0] 0xc002bf4fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:15:36.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:15:37.879: INFO: rc: 1
Aug 14 14:15:37.879: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d31680 exit status 1 <nil> <nil> true [0xc0008bb978 0xc0008bbb58 0xc0008bbce0] [0xc0008bb978 0xc0008bbb58 0xc0008bbce0] [0xc0008bbab0 0xc0008bbc70] [0x9d17b0 0x9d17b0] 0xc002bf56e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:15:47.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:15:49.198: INFO: rc: 1
Aug 14 14:15:49.198: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00257a690 exit status 1 <nil> <nil> true [0xc003438098 0xc0034380e0 0xc003438108] [0xc003438098 0xc0034380e0 0xc003438108] [0xc0034380c0 0xc003438100] [0x9d17b0 0x9d17b0] 0xc0035466c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Aug 14 14:15:59.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-1538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 14:15:59.623: INFO: rc: 1
Aug 14 14:15:59.623: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Aug 14 14:15:59.623: INFO: Scaling statefulset ss to 0
Aug 14 14:15:59.635: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 14 14:15:59.639: INFO: Deleting all statefulset in ns statefulset-1538
Aug 14 14:15:59.642: INFO: Scaling statefulset ss to 0
Aug 14 14:15:59.653: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 14:15:59.656: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:15:59.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1538" for this suite.
Aug 14 14:16:15.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:16:17.385: INFO: namespace statefulset-1538 deletion completed in 16.920477879s

â€¢ [SLOW TEST:389.800 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:16:17.385: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-ggpr
STEP: Creating a pod to test atomic-volume-subpath
Aug 14 14:16:19.072: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ggpr" in namespace "subpath-8189" to be "success or failure"
Aug 14 14:16:19.077: INFO: Pod "pod-subpath-test-downwardapi-ggpr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.219034ms
Aug 14 14:16:21.083: INFO: Pod "pod-subpath-test-downwardapi-ggpr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011015317s
Aug 14 14:16:23.219: INFO: Pod "pod-subpath-test-downwardapi-ggpr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.146780665s
Aug 14 14:16:25.224: INFO: Pod "pod-subpath-test-downwardapi-ggpr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.15139604s
Aug 14 14:16:27.260: INFO: Pod "pod-subpath-test-downwardapi-ggpr": Phase="Running", Reason="", readiness=true. Elapsed: 8.187366273s
Aug 14 14:16:29.267: INFO: Pod "pod-subpath-test-downwardapi-ggpr": Phase="Running", Reason="", readiness=true. Elapsed: 10.195089784s
Aug 14 14:16:31.355: INFO: Pod "pod-subpath-test-downwardapi-ggpr": Phase="Running", Reason="", readiness=true. Elapsed: 12.282531021s
Aug 14 14:16:33.359: INFO: Pod "pod-subpath-test-downwardapi-ggpr": Phase="Running", Reason="", readiness=true. Elapsed: 14.286452566s
Aug 14 14:16:35.364: INFO: Pod "pod-subpath-test-downwardapi-ggpr": Phase="Running", Reason="", readiness=true. Elapsed: 16.291772533s
Aug 14 14:16:37.369: INFO: Pod "pod-subpath-test-downwardapi-ggpr": Phase="Running", Reason="", readiness=true. Elapsed: 18.296335212s
Aug 14 14:16:39.496: INFO: Pod "pod-subpath-test-downwardapi-ggpr": Phase="Running", Reason="", readiness=true. Elapsed: 20.42410427s
Aug 14 14:16:41.501: INFO: Pod "pod-subpath-test-downwardapi-ggpr": Phase="Running", Reason="", readiness=true. Elapsed: 22.428717171s
Aug 14 14:16:43.735: INFO: Pod "pod-subpath-test-downwardapi-ggpr": Phase="Running", Reason="", readiness=true. Elapsed: 24.663138066s
Aug 14 14:16:46.018: INFO: Pod "pod-subpath-test-downwardapi-ggpr": Phase="Running", Reason="", readiness=true. Elapsed: 26.94584313s
Aug 14 14:16:48.026: INFO: Pod "pod-subpath-test-downwardapi-ggpr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.953327861s
STEP: Saw pod success
Aug 14 14:16:48.026: INFO: Pod "pod-subpath-test-downwardapi-ggpr" satisfied condition "success or failure"
Aug 14 14:16:48.031: INFO: Trying to get logs from node slave1 pod pod-subpath-test-downwardapi-ggpr container test-container-subpath-downwardapi-ggpr: <nil>
STEP: delete the pod
Aug 14 14:16:48.237: INFO: Waiting for pod pod-subpath-test-downwardapi-ggpr to disappear
Aug 14 14:16:48.243: INFO: Pod pod-subpath-test-downwardapi-ggpr no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-ggpr
Aug 14 14:16:48.243: INFO: Deleting pod "pod-subpath-test-downwardapi-ggpr" in namespace "subpath-8189"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:16:48.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8189" for this suite.
Aug 14 14:16:58.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:17:00.437: INFO: namespace subpath-8189 deletion completed in 12.183203578s

â€¢ [SLOW TEST:43.051 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:17:00.437: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6049
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 14 14:17:11.702: INFO: Successfully updated pod "labelsupdatefc4ba053-868e-4220-bd7d-a5e012a98b15"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:17:13.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6049" for this suite.
Aug 14 14:17:42.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:17:44.273: INFO: namespace projected-6049 deletion completed in 30.297580578s

â€¢ [SLOW TEST:43.835 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:17:44.273: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6352
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:17:45.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6352" for this suite.
Aug 14 14:18:14.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:18:16.738: INFO: namespace pods-6352 deletion completed in 30.806515562s

â€¢ [SLOW TEST:32.465 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:18:16.738: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8009
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 14:18:18.048: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:18:27.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8009" for this suite.
Aug 14 14:19:13.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:19:15.626: INFO: namespace pods-8009 deletion completed in 47.984268143s

â€¢ [SLOW TEST:58.888 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:19:15.628: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4106
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-4bdadca3-78bf-4b7b-9bd4-b32eab2edfeb
STEP: Creating a pod to test consume secrets
Aug 14 14:19:16.850: INFO: Waiting up to 5m0s for pod "pod-secrets-00d3cdf2-bd15-4db5-9e41-36e05fdd32b2" in namespace "secrets-4106" to be "success or failure"
Aug 14 14:19:17.057: INFO: Pod "pod-secrets-00d3cdf2-bd15-4db5-9e41-36e05fdd32b2": Phase="Pending", Reason="", readiness=false. Elapsed: 206.32522ms
Aug 14 14:19:19.064: INFO: Pod "pod-secrets-00d3cdf2-bd15-4db5-9e41-36e05fdd32b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.213988432s
Aug 14 14:19:21.071: INFO: Pod "pod-secrets-00d3cdf2-bd15-4db5-9e41-36e05fdd32b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.220420205s
Aug 14 14:19:23.154: INFO: Pod "pod-secrets-00d3cdf2-bd15-4db5-9e41-36e05fdd32b2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.303338639s
Aug 14 14:19:25.423: INFO: Pod "pod-secrets-00d3cdf2-bd15-4db5-9e41-36e05fdd32b2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.572891053s
Aug 14 14:19:27.674: INFO: Pod "pod-secrets-00d3cdf2-bd15-4db5-9e41-36e05fdd32b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.823450991s
STEP: Saw pod success
Aug 14 14:19:27.674: INFO: Pod "pod-secrets-00d3cdf2-bd15-4db5-9e41-36e05fdd32b2" satisfied condition "success or failure"
Aug 14 14:19:27.678: INFO: Trying to get logs from node slave2 pod pod-secrets-00d3cdf2-bd15-4db5-9e41-36e05fdd32b2 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 14:19:27.718: INFO: Waiting for pod pod-secrets-00d3cdf2-bd15-4db5-9e41-36e05fdd32b2 to disappear
Aug 14 14:19:27.733: INFO: Pod pod-secrets-00d3cdf2-bd15-4db5-9e41-36e05fdd32b2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:19:27.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4106" for this suite.
Aug 14 14:19:37.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:19:39.729: INFO: namespace secrets-4106 deletion completed in 11.988489519s

â€¢ [SLOW TEST:24.101 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:19:39.729: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5240
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-7234904f-dded-45dc-a754-7d0c4b0c4894
STEP: Creating a pod to test consume configMaps
Aug 14 14:19:40.985: INFO: Waiting up to 5m0s for pod "pod-configmaps-43e5ca7e-484c-489f-99e1-5858191b6839" in namespace "configmap-5240" to be "success or failure"
Aug 14 14:19:41.133: INFO: Pod "pod-configmaps-43e5ca7e-484c-489f-99e1-5858191b6839": Phase="Pending", Reason="", readiness=false. Elapsed: 146.54654ms
Aug 14 14:19:43.204: INFO: Pod "pod-configmaps-43e5ca7e-484c-489f-99e1-5858191b6839": Phase="Pending", Reason="", readiness=false. Elapsed: 2.217303268s
Aug 14 14:19:45.208: INFO: Pod "pod-configmaps-43e5ca7e-484c-489f-99e1-5858191b6839": Phase="Pending", Reason="", readiness=false. Elapsed: 4.22167078s
Aug 14 14:19:47.249: INFO: Pod "pod-configmaps-43e5ca7e-484c-489f-99e1-5858191b6839": Phase="Pending", Reason="", readiness=false. Elapsed: 6.262741209s
Aug 14 14:19:49.259: INFO: Pod "pod-configmaps-43e5ca7e-484c-489f-99e1-5858191b6839": Phase="Pending", Reason="", readiness=false. Elapsed: 8.27245443s
Aug 14 14:19:51.376: INFO: Pod "pod-configmaps-43e5ca7e-484c-489f-99e1-5858191b6839": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.38936945s
STEP: Saw pod success
Aug 14 14:19:51.376: INFO: Pod "pod-configmaps-43e5ca7e-484c-489f-99e1-5858191b6839" satisfied condition "success or failure"
Aug 14 14:19:51.380: INFO: Trying to get logs from node slave3 pod pod-configmaps-43e5ca7e-484c-489f-99e1-5858191b6839 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 14:19:51.755: INFO: Waiting for pod pod-configmaps-43e5ca7e-484c-489f-99e1-5858191b6839 to disappear
Aug 14 14:19:51.835: INFO: Pod pod-configmaps-43e5ca7e-484c-489f-99e1-5858191b6839 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:19:51.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5240" for this suite.
Aug 14 14:20:02.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:20:05.354: INFO: namespace configmap-5240 deletion completed in 13.332375842s

â€¢ [SLOW TEST:25.625 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:20:05.354: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4666
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Aug 14 14:20:14.474: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-938832172 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug 14 14:20:34.871: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:20:34.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4666" for this suite.
Aug 14 14:20:45.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:20:47.279: INFO: namespace pods-4666 deletion completed in 12.320367192s

â€¢ [SLOW TEST:41.925 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:20:47.280: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4766
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-b1aea12c-b042-409f-a63f-aebc05167f24 in namespace container-probe-4766
Aug 14 14:20:57.340: INFO: Started pod busybox-b1aea12c-b042-409f-a63f-aebc05167f24 in namespace container-probe-4766
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 14:20:57.344: INFO: Initial restart count of pod busybox-b1aea12c-b042-409f-a63f-aebc05167f24 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:24:58.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4766" for this suite.
Aug 14 14:25:09.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:25:10.769: INFO: namespace container-probe-4766 deletion completed in 12.168700538s

â€¢ [SLOW TEST:263.490 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:25:10.769: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-8x4c
STEP: Creating a pod to test atomic-volume-subpath
Aug 14 14:25:12.094: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8x4c" in namespace "subpath-1872" to be "success or failure"
Aug 14 14:25:12.353: INFO: Pod "pod-subpath-test-secret-8x4c": Phase="Pending", Reason="", readiness=false. Elapsed: 258.453976ms
Aug 14 14:25:14.534: INFO: Pod "pod-subpath-test-secret-8x4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.439907943s
Aug 14 14:25:16.539: INFO: Pod "pod-subpath-test-secret-8x4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.444586168s
Aug 14 14:25:18.545: INFO: Pod "pod-subpath-test-secret-8x4c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.450928575s
Aug 14 14:25:20.550: INFO: Pod "pod-subpath-test-secret-8x4c": Phase="Running", Reason="", readiness=true. Elapsed: 8.455657642s
Aug 14 14:25:22.555: INFO: Pod "pod-subpath-test-secret-8x4c": Phase="Running", Reason="", readiness=true. Elapsed: 10.460322654s
Aug 14 14:25:24.559: INFO: Pod "pod-subpath-test-secret-8x4c": Phase="Running", Reason="", readiness=true. Elapsed: 12.464645546s
Aug 14 14:25:26.700: INFO: Pod "pod-subpath-test-secret-8x4c": Phase="Running", Reason="", readiness=true. Elapsed: 14.605591877s
Aug 14 14:25:29.060: INFO: Pod "pod-subpath-test-secret-8x4c": Phase="Running", Reason="", readiness=true. Elapsed: 16.965564082s
Aug 14 14:25:31.067: INFO: Pod "pod-subpath-test-secret-8x4c": Phase="Running", Reason="", readiness=true. Elapsed: 18.972346002s
Aug 14 14:25:33.093: INFO: Pod "pod-subpath-test-secret-8x4c": Phase="Running", Reason="", readiness=true. Elapsed: 20.998194393s
Aug 14 14:25:35.239: INFO: Pod "pod-subpath-test-secret-8x4c": Phase="Running", Reason="", readiness=true. Elapsed: 23.144673245s
Aug 14 14:25:37.244: INFO: Pod "pod-subpath-test-secret-8x4c": Phase="Running", Reason="", readiness=true. Elapsed: 25.149906034s
Aug 14 14:25:39.315: INFO: Pod "pod-subpath-test-secret-8x4c": Phase="Running", Reason="", readiness=true. Elapsed: 27.220942595s
Aug 14 14:25:41.332: INFO: Pod "pod-subpath-test-secret-8x4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 29.237629501s
STEP: Saw pod success
Aug 14 14:25:41.332: INFO: Pod "pod-subpath-test-secret-8x4c" satisfied condition "success or failure"
Aug 14 14:25:41.336: INFO: Trying to get logs from node slave2 pod pod-subpath-test-secret-8x4c container test-container-subpath-secret-8x4c: <nil>
STEP: delete the pod
Aug 14 14:25:41.393: INFO: Waiting for pod pod-subpath-test-secret-8x4c to disappear
Aug 14 14:25:41.403: INFO: Pod pod-subpath-test-secret-8x4c no longer exists
STEP: Deleting pod pod-subpath-test-secret-8x4c
Aug 14 14:25:41.403: INFO: Deleting pod "pod-subpath-test-secret-8x4c" in namespace "subpath-1872"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:25:41.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1872" for this suite.
Aug 14 14:25:53.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:25:55.518: INFO: namespace subpath-1872 deletion completed in 14.102103281s

â€¢ [SLOW TEST:44.749 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:25:55.519: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9371
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 14 14:25:56.803: INFO: Waiting up to 5m0s for pod "pod-5fc25d3d-c8f1-4f18-bab1-16ae6bf5b919" in namespace "emptydir-9371" to be "success or failure"
Aug 14 14:25:56.811: INFO: Pod "pod-5fc25d3d-c8f1-4f18-bab1-16ae6bf5b919": Phase="Pending", Reason="", readiness=false. Elapsed: 8.462192ms
Aug 14 14:25:58.817: INFO: Pod "pod-5fc25d3d-c8f1-4f18-bab1-16ae6bf5b919": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014498602s
Aug 14 14:26:00.833: INFO: Pod "pod-5fc25d3d-c8f1-4f18-bab1-16ae6bf5b919": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02971778s
Aug 14 14:26:02.839: INFO: Pod "pod-5fc25d3d-c8f1-4f18-bab1-16ae6bf5b919": Phase="Pending", Reason="", readiness=false. Elapsed: 6.036229723s
Aug 14 14:26:05.017: INFO: Pod "pod-5fc25d3d-c8f1-4f18-bab1-16ae6bf5b919": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.214219286s
STEP: Saw pod success
Aug 14 14:26:05.017: INFO: Pod "pod-5fc25d3d-c8f1-4f18-bab1-16ae6bf5b919" satisfied condition "success or failure"
Aug 14 14:26:05.021: INFO: Trying to get logs from node slave1 pod pod-5fc25d3d-c8f1-4f18-bab1-16ae6bf5b919 container test-container: <nil>
STEP: delete the pod
Aug 14 14:26:05.501: INFO: Waiting for pod pod-5fc25d3d-c8f1-4f18-bab1-16ae6bf5b919 to disappear
Aug 14 14:26:05.511: INFO: Pod pod-5fc25d3d-c8f1-4f18-bab1-16ae6bf5b919 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:26:05.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9371" for this suite.
Aug 14 14:26:14.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:26:16.162: INFO: namespace emptydir-9371 deletion completed in 10.643104817s

â€¢ [SLOW TEST:20.642 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:26:16.162: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5859
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-babe122a-14e0-4479-9dd1-8dd52149746b
STEP: Creating secret with name s-test-opt-upd-5a587360-35a0-463b-ab94-f8f2b2c853d3
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-babe122a-14e0-4479-9dd1-8dd52149746b
STEP: Updating secret s-test-opt-upd-5a587360-35a0-463b-ab94-f8f2b2c853d3
STEP: Creating secret with name s-test-opt-create-f2fdc591-561f-4041-b5b7-aca9b29ab331
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:26:35.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5859" for this suite.
Aug 14 14:27:05.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:27:07.599: INFO: namespace secrets-5859 deletion completed in 32.272954284s

â€¢ [SLOW TEST:51.437 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:27:07.600: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4697
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 14:27:08.700: INFO: Waiting up to 5m0s for pod "downwardapi-volume-393a26dd-f1cc-45ec-9c34-7d3c025629bc" in namespace "downward-api-4697" to be "success or failure"
Aug 14 14:27:08.933: INFO: Pod "downwardapi-volume-393a26dd-f1cc-45ec-9c34-7d3c025629bc": Phase="Pending", Reason="", readiness=false. Elapsed: 232.610886ms
Aug 14 14:27:10.939: INFO: Pod "downwardapi-volume-393a26dd-f1cc-45ec-9c34-7d3c025629bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.238656703s
Aug 14 14:27:13.446: INFO: Pod "downwardapi-volume-393a26dd-f1cc-45ec-9c34-7d3c025629bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.745440032s
Aug 14 14:27:15.560: INFO: Pod "downwardapi-volume-393a26dd-f1cc-45ec-9c34-7d3c025629bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.858952575s
Aug 14 14:27:17.706: INFO: Pod "downwardapi-volume-393a26dd-f1cc-45ec-9c34-7d3c025629bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.005178816s
STEP: Saw pod success
Aug 14 14:27:17.706: INFO: Pod "downwardapi-volume-393a26dd-f1cc-45ec-9c34-7d3c025629bc" satisfied condition "success or failure"
Aug 14 14:27:17.716: INFO: Trying to get logs from node slave3 pod downwardapi-volume-393a26dd-f1cc-45ec-9c34-7d3c025629bc container client-container: <nil>
STEP: delete the pod
Aug 14 14:27:18.422: INFO: Waiting for pod downwardapi-volume-393a26dd-f1cc-45ec-9c34-7d3c025629bc to disappear
Aug 14 14:27:18.430: INFO: Pod downwardapi-volume-393a26dd-f1cc-45ec-9c34-7d3c025629bc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:27:18.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4697" for this suite.
Aug 14 14:27:26.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:27:28.303: INFO: namespace downward-api-4697 deletion completed in 9.862676676s

â€¢ [SLOW TEST:20.703 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:27:28.304: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-z499
STEP: Creating a pod to test atomic-volume-subpath
Aug 14 14:27:30.541: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-z499" in namespace "subpath-7968" to be "success or failure"
Aug 14 14:27:30.548: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Pending", Reason="", readiness=false. Elapsed: 5.953556ms
Aug 14 14:27:32.859: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.317568596s
Aug 14 14:27:35.099: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Pending", Reason="", readiness=false. Elapsed: 4.5570164s
Aug 14 14:27:37.110: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Pending", Reason="", readiness=false. Elapsed: 6.568173521s
Aug 14 14:27:39.236: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Pending", Reason="", readiness=false. Elapsed: 8.694464965s
Aug 14 14:27:41.241: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Running", Reason="", readiness=true. Elapsed: 10.69879482s
Aug 14 14:27:43.245: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Running", Reason="", readiness=true. Elapsed: 12.703490385s
Aug 14 14:27:45.389: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Running", Reason="", readiness=true. Elapsed: 14.847581932s
Aug 14 14:27:47.395: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Running", Reason="", readiness=true. Elapsed: 16.853687112s
Aug 14 14:27:49.557: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Running", Reason="", readiness=true. Elapsed: 19.015558368s
Aug 14 14:27:51.561: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Running", Reason="", readiness=true. Elapsed: 21.019401453s
Aug 14 14:27:53.565: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Running", Reason="", readiness=true. Elapsed: 23.023743472s
Aug 14 14:27:55.658: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Running", Reason="", readiness=true. Elapsed: 25.115924768s
Aug 14 14:27:57.666: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Running", Reason="", readiness=true. Elapsed: 27.123836506s
Aug 14 14:28:00.072: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Running", Reason="", readiness=true. Elapsed: 29.530091465s
Aug 14 14:28:02.076: INFO: Pod "pod-subpath-test-configmap-z499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 31.534229815s
STEP: Saw pod success
Aug 14 14:28:02.076: INFO: Pod "pod-subpath-test-configmap-z499" satisfied condition "success or failure"
Aug 14 14:28:02.161: INFO: Trying to get logs from node slave2 pod pod-subpath-test-configmap-z499 container test-container-subpath-configmap-z499: <nil>
STEP: delete the pod
Aug 14 14:28:02.295: INFO: Waiting for pod pod-subpath-test-configmap-z499 to disappear
Aug 14 14:28:02.308: INFO: Pod pod-subpath-test-configmap-z499 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-z499
Aug 14 14:28:02.308: INFO: Deleting pod "pod-subpath-test-configmap-z499" in namespace "subpath-7968"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:28:02.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7968" for this suite.
Aug 14 14:28:16.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:28:18.877: INFO: namespace subpath-7968 deletion completed in 16.356515987s

â€¢ [SLOW TEST:50.573 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:28:18.877: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3587
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 14 14:28:31.963: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4d28c66a-38af-4e87-b286-add6a5ebfb11"
Aug 14 14:28:31.963: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4d28c66a-38af-4e87-b286-add6a5ebfb11" in namespace "pods-3587" to be "terminated due to deadline exceeded"
Aug 14 14:28:31.968: INFO: Pod "pod-update-activedeadlineseconds-4d28c66a-38af-4e87-b286-add6a5ebfb11": Phase="Running", Reason="", readiness=true. Elapsed: 5.262789ms
Aug 14 14:28:34.210: INFO: Pod "pod-update-activedeadlineseconds-4d28c66a-38af-4e87-b286-add6a5ebfb11": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.247529403s
Aug 14 14:28:34.211: INFO: Pod "pod-update-activedeadlineseconds-4d28c66a-38af-4e87-b286-add6a5ebfb11" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:28:34.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3587" for this suite.
Aug 14 14:28:47.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:28:48.906: INFO: namespace pods-3587 deletion completed in 14.678853269s

â€¢ [SLOW TEST:30.029 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:28:48.907: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7769
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 14:29:01.546: INFO: Waiting up to 5m0s for pod "client-envvars-fb0130d3-b2ae-4a39-a48e-c560c8d8b5f8" in namespace "pods-7769" to be "success or failure"
Aug 14 14:29:01.551: INFO: Pod "client-envvars-fb0130d3-b2ae-4a39-a48e-c560c8d8b5f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.950962ms
Aug 14 14:29:03.610: INFO: Pod "client-envvars-fb0130d3-b2ae-4a39-a48e-c560c8d8b5f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064278925s
Aug 14 14:29:05.711: INFO: Pod "client-envvars-fb0130d3-b2ae-4a39-a48e-c560c8d8b5f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.165610504s
Aug 14 14:29:07.841: INFO: Pod "client-envvars-fb0130d3-b2ae-4a39-a48e-c560c8d8b5f8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.29500499s
Aug 14 14:29:10.301: INFO: Pod "client-envvars-fb0130d3-b2ae-4a39-a48e-c560c8d8b5f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.755208743s
STEP: Saw pod success
Aug 14 14:29:10.301: INFO: Pod "client-envvars-fb0130d3-b2ae-4a39-a48e-c560c8d8b5f8" satisfied condition "success or failure"
Aug 14 14:29:10.626: INFO: Trying to get logs from node slave1 pod client-envvars-fb0130d3-b2ae-4a39-a48e-c560c8d8b5f8 container env3cont: <nil>
STEP: delete the pod
Aug 14 14:29:11.462: INFO: Waiting for pod client-envvars-fb0130d3-b2ae-4a39-a48e-c560c8d8b5f8 to disappear
Aug 14 14:29:11.473: INFO: Pod client-envvars-fb0130d3-b2ae-4a39-a48e-c560c8d8b5f8 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:29:11.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7769" for this suite.
Aug 14 14:30:05.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:30:07.666: INFO: namespace pods-7769 deletion completed in 56.186547299s

â€¢ [SLOW TEST:78.759 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:30:07.667: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-4019
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Aug 14 14:30:09.681: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4019" to be "success or failure"
Aug 14 14:30:09.958: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 276.522906ms
Aug 14 14:30:11.962: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.280570883s
Aug 14 14:30:13.966: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28471057s
Aug 14 14:30:15.970: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.288835598s
Aug 14 14:30:18.058: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.376372639s
Aug 14 14:30:20.155: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 10.473556649s
Aug 14 14:30:22.310: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.628449593s
STEP: Saw pod success
Aug 14 14:30:22.310: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Aug 14 14:30:22.314: INFO: Trying to get logs from node slave3 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug 14 14:30:23.447: INFO: Waiting for pod pod-host-path-test to disappear
Aug 14 14:30:23.506: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:30:23.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4019" for this suite.
Aug 14 14:30:38.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:30:40.795: INFO: namespace hostpath-4019 deletion completed in 17.017689899s

â€¢ [SLOW TEST:33.128 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:30:40.796: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4380
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 14:30:43.348: INFO: Waiting up to 5m0s for pod "downwardapi-volume-33f5e8ca-05ab-4285-bbf0-f02b8b993303" in namespace "projected-4380" to be "success or failure"
Aug 14 14:30:44.342: INFO: Pod "downwardapi-volume-33f5e8ca-05ab-4285-bbf0-f02b8b993303": Phase="Pending", Reason="", readiness=false. Elapsed: 994.139559ms
Aug 14 14:30:46.412: INFO: Pod "downwardapi-volume-33f5e8ca-05ab-4285-bbf0-f02b8b993303": Phase="Pending", Reason="", readiness=false. Elapsed: 3.06411534s
Aug 14 14:30:49.256: INFO: Pod "downwardapi-volume-33f5e8ca-05ab-4285-bbf0-f02b8b993303": Phase="Pending", Reason="", readiness=false. Elapsed: 5.908157598s
Aug 14 14:30:51.549: INFO: Pod "downwardapi-volume-33f5e8ca-05ab-4285-bbf0-f02b8b993303": Phase="Pending", Reason="", readiness=false. Elapsed: 8.201040568s
Aug 14 14:30:53.555: INFO: Pod "downwardapi-volume-33f5e8ca-05ab-4285-bbf0-f02b8b993303": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.206925337s
STEP: Saw pod success
Aug 14 14:30:53.555: INFO: Pod "downwardapi-volume-33f5e8ca-05ab-4285-bbf0-f02b8b993303" satisfied condition "success or failure"
Aug 14 14:30:53.559: INFO: Trying to get logs from node slave3 pod downwardapi-volume-33f5e8ca-05ab-4285-bbf0-f02b8b993303 container client-container: <nil>
STEP: delete the pod
Aug 14 14:30:54.187: INFO: Waiting for pod downwardapi-volume-33f5e8ca-05ab-4285-bbf0-f02b8b993303 to disappear
Aug 14 14:30:54.246: INFO: Pod downwardapi-volume-33f5e8ca-05ab-4285-bbf0-f02b8b993303 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:30:54.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4380" for this suite.
Aug 14 14:31:04.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:31:07.223: INFO: namespace projected-4380 deletion completed in 12.969229386s

â€¢ [SLOW TEST:26.427 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:31:07.224: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 14 14:31:08.598: INFO: Waiting up to 5m0s for pod "pod-c59b7613-d122-4e43-a1e1-fc6ac7ae7d65" in namespace "emptydir-1412" to be "success or failure"
Aug 14 14:31:09.090: INFO: Pod "pod-c59b7613-d122-4e43-a1e1-fc6ac7ae7d65": Phase="Pending", Reason="", readiness=false. Elapsed: 491.103895ms
Aug 14 14:31:11.156: INFO: Pod "pod-c59b7613-d122-4e43-a1e1-fc6ac7ae7d65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.557370679s
Aug 14 14:31:13.186: INFO: Pod "pod-c59b7613-d122-4e43-a1e1-fc6ac7ae7d65": Phase="Pending", Reason="", readiness=false. Elapsed: 4.587092837s
Aug 14 14:31:15.363: INFO: Pod "pod-c59b7613-d122-4e43-a1e1-fc6ac7ae7d65": Phase="Pending", Reason="", readiness=false. Elapsed: 6.764686309s
Aug 14 14:31:17.413: INFO: Pod "pod-c59b7613-d122-4e43-a1e1-fc6ac7ae7d65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.814565567s
STEP: Saw pod success
Aug 14 14:31:17.413: INFO: Pod "pod-c59b7613-d122-4e43-a1e1-fc6ac7ae7d65" satisfied condition "success or failure"
Aug 14 14:31:17.416: INFO: Trying to get logs from node slave1 pod pod-c59b7613-d122-4e43-a1e1-fc6ac7ae7d65 container test-container: <nil>
STEP: delete the pod
Aug 14 14:31:17.893: INFO: Waiting for pod pod-c59b7613-d122-4e43-a1e1-fc6ac7ae7d65 to disappear
Aug 14 14:31:17.900: INFO: Pod pod-c59b7613-d122-4e43-a1e1-fc6ac7ae7d65 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:31:17.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1412" for this suite.
Aug 14 14:31:30.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:31:31.834: INFO: namespace emptydir-1412 deletion completed in 13.926738403s

â€¢ [SLOW TEST:24.610 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:31:31.834: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2039
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 14 14:31:35.004: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2039,SelfLink:/api/v1/namespaces/watch-2039/configmaps/e2e-watch-test-configmap-a,UID:3d14f5c4-bf8a-43cc-a8a1-086be212e79d,ResourceVersion:359280,Generation:0,CreationTimestamp:2019-08-14 14:31:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 14 14:31:35.005: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2039,SelfLink:/api/v1/namespaces/watch-2039/configmaps/e2e-watch-test-configmap-a,UID:3d14f5c4-bf8a-43cc-a8a1-086be212e79d,ResourceVersion:359280,Generation:0,CreationTimestamp:2019-08-14 14:31:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 14 14:31:45.065: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2039,SelfLink:/api/v1/namespaces/watch-2039/configmaps/e2e-watch-test-configmap-a,UID:3d14f5c4-bf8a-43cc-a8a1-086be212e79d,ResourceVersion:359307,Generation:0,CreationTimestamp:2019-08-14 14:31:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 14 14:31:45.066: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2039,SelfLink:/api/v1/namespaces/watch-2039/configmaps/e2e-watch-test-configmap-a,UID:3d14f5c4-bf8a-43cc-a8a1-086be212e79d,ResourceVersion:359307,Generation:0,CreationTimestamp:2019-08-14 14:31:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 14 14:31:55.864: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2039,SelfLink:/api/v1/namespaces/watch-2039/configmaps/e2e-watch-test-configmap-a,UID:3d14f5c4-bf8a-43cc-a8a1-086be212e79d,ResourceVersion:359332,Generation:0,CreationTimestamp:2019-08-14 14:31:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 14 14:31:55.864: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2039,SelfLink:/api/v1/namespaces/watch-2039/configmaps/e2e-watch-test-configmap-a,UID:3d14f5c4-bf8a-43cc-a8a1-086be212e79d,ResourceVersion:359332,Generation:0,CreationTimestamp:2019-08-14 14:31:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 14 14:32:05.874: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2039,SelfLink:/api/v1/namespaces/watch-2039/configmaps/e2e-watch-test-configmap-a,UID:3d14f5c4-bf8a-43cc-a8a1-086be212e79d,ResourceVersion:359353,Generation:0,CreationTimestamp:2019-08-14 14:31:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 14 14:32:05.874: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2039,SelfLink:/api/v1/namespaces/watch-2039/configmaps/e2e-watch-test-configmap-a,UID:3d14f5c4-bf8a-43cc-a8a1-086be212e79d,ResourceVersion:359353,Generation:0,CreationTimestamp:2019-08-14 14:31:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 14 14:32:15.905: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2039,SelfLink:/api/v1/namespaces/watch-2039/configmaps/e2e-watch-test-configmap-b,UID:22adfc97-7507-45cd-b586-5916ef43ceab,ResourceVersion:359379,Generation:0,CreationTimestamp:2019-08-14 14:32:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 14 14:32:15.905: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2039,SelfLink:/api/v1/namespaces/watch-2039/configmaps/e2e-watch-test-configmap-b,UID:22adfc97-7507-45cd-b586-5916ef43ceab,ResourceVersion:359379,Generation:0,CreationTimestamp:2019-08-14 14:32:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 14 14:32:25.932: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2039,SelfLink:/api/v1/namespaces/watch-2039/configmaps/e2e-watch-test-configmap-b,UID:22adfc97-7507-45cd-b586-5916ef43ceab,ResourceVersion:359401,Generation:0,CreationTimestamp:2019-08-14 14:32:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 14 14:32:25.932: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2039,SelfLink:/api/v1/namespaces/watch-2039/configmaps/e2e-watch-test-configmap-b,UID:22adfc97-7507-45cd-b586-5916ef43ceab,ResourceVersion:359401,Generation:0,CreationTimestamp:2019-08-14 14:32:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:32:35.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2039" for this suite.
Aug 14 14:32:46.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:32:48.259: INFO: namespace watch-2039 deletion completed in 12.319976068s

â€¢ [SLOW TEST:76.425 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:32:48.259: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1240
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 14 14:32:49.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1240'
Aug 14 14:32:58.182: INFO: stderr: ""
Aug 14 14:32:58.182: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Aug 14 14:32:58.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete pods e2e-test-nginx-pod --namespace=kubectl-1240'
Aug 14 14:33:02.877: INFO: stderr: ""
Aug 14 14:33:02.877: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:33:02.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1240" for this suite.
Aug 14 14:33:10.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:33:12.836: INFO: namespace kubectl-1240 deletion completed in 9.952689231s

â€¢ [SLOW TEST:24.577 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:33:12.836: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6385
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0814 14:33:25.325700      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 14 14:33:25.325: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:33:25.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6385" for this suite.
Aug 14 14:33:35.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:33:37.233: INFO: namespace gc-6385 deletion completed in 11.901436888s

â€¢ [SLOW TEST:24.397 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:33:37.233: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5101
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 14 14:33:38.829: INFO: Waiting up to 5m0s for pod "pod-b7626050-324f-4cf3-b542-5ddd3bbddbbe" in namespace "emptydir-5101" to be "success or failure"
Aug 14 14:33:38.849: INFO: Pod "pod-b7626050-324f-4cf3-b542-5ddd3bbddbbe": Phase="Pending", Reason="", readiness=false. Elapsed: 19.619791ms
Aug 14 14:33:40.855: INFO: Pod "pod-b7626050-324f-4cf3-b542-5ddd3bbddbbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025313113s
Aug 14 14:33:42.862: INFO: Pod "pod-b7626050-324f-4cf3-b542-5ddd3bbddbbe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032334708s
Aug 14 14:33:44.873: INFO: Pod "pod-b7626050-324f-4cf3-b542-5ddd3bbddbbe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043805362s
Aug 14 14:33:47.031: INFO: Pod "pod-b7626050-324f-4cf3-b542-5ddd3bbddbbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.202202694s
STEP: Saw pod success
Aug 14 14:33:47.032: INFO: Pod "pod-b7626050-324f-4cf3-b542-5ddd3bbddbbe" satisfied condition "success or failure"
Aug 14 14:33:47.049: INFO: Trying to get logs from node slave2 pod pod-b7626050-324f-4cf3-b542-5ddd3bbddbbe container test-container: <nil>
STEP: delete the pod
Aug 14 14:33:47.733: INFO: Waiting for pod pod-b7626050-324f-4cf3-b542-5ddd3bbddbbe to disappear
Aug 14 14:33:47.737: INFO: Pod pod-b7626050-324f-4cf3-b542-5ddd3bbddbbe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:33:47.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5101" for this suite.
Aug 14 14:34:00.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:34:01.992: INFO: namespace emptydir-5101 deletion completed in 14.248277378s

â€¢ [SLOW TEST:24.759 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:34:01.995: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3663
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-16476667-6efe-4811-bd12-1125ac59991c
STEP: Creating secret with name s-test-opt-upd-fcf7de0b-2042-4bb4-8424-40cbbe3712ea
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-16476667-6efe-4811-bd12-1125ac59991c
STEP: Updating secret s-test-opt-upd-fcf7de0b-2042-4bb4-8424-40cbbe3712ea
STEP: Creating secret with name s-test-opt-create-4d419b15-e3b3-4c8a-aa1d-37a40922e027
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:35:32.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3663" for this suite.
Aug 14 14:36:04.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:36:07.002: INFO: namespace projected-3663 deletion completed in 34.471463266s

â€¢ [SLOW TEST:125.008 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:36:07.003: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-5e3e4ffb-5228-40ec-aae5-37c80af0603e
Aug 14 14:36:09.530: INFO: Pod name my-hostname-basic-5e3e4ffb-5228-40ec-aae5-37c80af0603e: Found 0 pods out of 1
Aug 14 14:36:14.536: INFO: Pod name my-hostname-basic-5e3e4ffb-5228-40ec-aae5-37c80af0603e: Found 1 pods out of 1
Aug 14 14:36:14.536: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5e3e4ffb-5228-40ec-aae5-37c80af0603e" are running
Aug 14 14:36:16.552: INFO: Pod "my-hostname-basic-5e3e4ffb-5228-40ec-aae5-37c80af0603e-nmp8t" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-14 14:36:09 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-14 14:36:09 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-5e3e4ffb-5228-40ec-aae5-37c80af0603e]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-14 14:36:09 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-5e3e4ffb-5228-40ec-aae5-37c80af0603e]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-14 14:36:09 +0000 UTC Reason: Message:}])
Aug 14 14:36:16.552: INFO: Trying to dial the pod
Aug 14 14:36:21.832: INFO: Controller my-hostname-basic-5e3e4ffb-5228-40ec-aae5-37c80af0603e: Got expected result from replica 1 [my-hostname-basic-5e3e4ffb-5228-40ec-aae5-37c80af0603e-nmp8t]: "my-hostname-basic-5e3e4ffb-5228-40ec-aae5-37c80af0603e-nmp8t", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:36:21.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-483" for this suite.
Aug 14 14:36:37.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:36:39.583: INFO: namespace replication-controller-483 deletion completed in 17.73797388s

â€¢ [SLOW TEST:32.580 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:36:39.584: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1799
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 14 14:36:52.346: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:36:52.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1799" for this suite.
Aug 14 14:37:24.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:37:26.233: INFO: namespace replicaset-1799 deletion completed in 33.290186184s

â€¢ [SLOW TEST:46.649 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:37:26.233: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 14 14:37:35.232: INFO: Pod name wrapped-volume-race-4316819d-c304-4839-80eb-dac609701773: Found 0 pods out of 5
Aug 14 14:37:40.584: INFO: Pod name wrapped-volume-race-4316819d-c304-4839-80eb-dac609701773: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4316819d-c304-4839-80eb-dac609701773 in namespace emptydir-wrapper-1905, will wait for the garbage collector to delete the pods
Aug 14 14:38:20.330: INFO: Deleting ReplicationController wrapped-volume-race-4316819d-c304-4839-80eb-dac609701773 took: 77.828821ms
Aug 14 14:38:20.634: INFO: Terminating ReplicationController wrapped-volume-race-4316819d-c304-4839-80eb-dac609701773 pods took: 303.855435ms
STEP: Creating RC which spawns configmap-volume pods
Aug 14 14:39:14.193: INFO: Pod name wrapped-volume-race-55f52a80-7be2-4740-ad93-968f9aa6e4a1: Found 0 pods out of 5
Aug 14 14:39:19.730: INFO: Pod name wrapped-volume-race-55f52a80-7be2-4740-ad93-968f9aa6e4a1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-55f52a80-7be2-4740-ad93-968f9aa6e4a1 in namespace emptydir-wrapper-1905, will wait for the garbage collector to delete the pods
Aug 14 14:39:52.102: INFO: Deleting ReplicationController wrapped-volume-race-55f52a80-7be2-4740-ad93-968f9aa6e4a1 took: 16.433856ms
Aug 14 14:39:53.302: INFO: Terminating ReplicationController wrapped-volume-race-55f52a80-7be2-4740-ad93-968f9aa6e4a1 pods took: 1.200219763s
STEP: Creating RC which spawns configmap-volume pods
Aug 14 14:40:47.354: INFO: Pod name wrapped-volume-race-a32783ad-ec4f-432f-a42f-bd8ac2ab8aa3: Found 0 pods out of 5
Aug 14 14:40:52.552: INFO: Pod name wrapped-volume-race-a32783ad-ec4f-432f-a42f-bd8ac2ab8aa3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a32783ad-ec4f-432f-a42f-bd8ac2ab8aa3 in namespace emptydir-wrapper-1905, will wait for the garbage collector to delete the pods
Aug 14 14:41:33.064: INFO: Deleting ReplicationController wrapped-volume-race-a32783ad-ec4f-432f-a42f-bd8ac2ab8aa3 took: 67.173075ms
Aug 14 14:41:34.667: INFO: Terminating ReplicationController wrapped-volume-race-a32783ad-ec4f-432f-a42f-bd8ac2ab8aa3 pods took: 1.603424957s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:42:34.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1905" for this suite.
Aug 14 14:43:12.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:43:14.414: INFO: namespace emptydir-wrapper-1905 deletion completed in 39.860142312s

â€¢ [SLOW TEST:348.181 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:43:14.416: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6698
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Aug 14 14:43:21.934: INFO: Pod pod-hostip-be5fe3b0-714e-4343-9ae8-40bab7d623b8 has hostIP: 192.168.202.59
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:43:21.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6698" for this suite.
Aug 14 14:43:48.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:43:49.739: INFO: namespace pods-6698 deletion completed in 27.799581594s

â€¢ [SLOW TEST:35.323 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:43:49.748: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1085
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 14 14:43:51.374: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:44:07.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1085" for this suite.
Aug 14 14:44:20.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:44:22.721: INFO: namespace init-container-1085 deletion completed in 14.986091146s

â€¢ [SLOW TEST:32.973 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:44:22.722: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5343
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 14 14:44:23.703: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 14 14:44:23.824: INFO: Waiting for terminating namespaces to be deleted...
Aug 14 14:44:23.894: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Aug 14 14:44:23.904: INFO: dns-autoscaler-6b9bbb69f4-v77pg from kube-system started at 2019-08-13 07:04:20 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.905: INFO: 	Container autoscaler ready: true, restart count 0
Aug 14 14:44:23.905: INFO: kube-apiserver-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:44:23.905: INFO: kube-controller-manager-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:44:23.905: INFO: istio-egressgateway-68856cf966-k7975 from istio-system started at 2019-08-14 11:49:09 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.905: INFO: 	Container egressgateway ready: true, restart count 0
Aug 14 14:44:23.905: INFO: kube-scheduler-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:44:23.905: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-zx5nv from heptio-sonobuoy started at 2019-08-14 14:03:36 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:23.905: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 14:44:23.905: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 14 14:44:23.905: INFO: coredns-9c98d6cbb-nz84n from kube-system started at 2019-08-13 01:20:04 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.905: INFO: 	Container coredns ready: true, restart count 0
Aug 14 14:44:23.905: INFO: node-exporter-ht9c6 from monitoring started at 2019-08-13 01:20:52 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.905: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 14:44:23.905: INFO: vpa-updater-557c96bf46-lhlk7 from kube-system started at 2019-08-13 07:04:16 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.905: INFO: 	Container updater ready: true, restart count 0
Aug 14 14:44:23.905: INFO: istio-policy-5c69497f5-m2kph from istio-system started at 2019-08-13 07:22:06 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:23.905: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:44:23.905: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:44:23.905: INFO: resource-reserver-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:44:23.905: INFO: nginx-proxy-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:44:23.905: INFO: calico-node-qjwhs from kube-system started at 2019-08-13 01:17:48 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.905: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 14:44:23.905: INFO: metrics-server-5cb9b96667-8wn6n from kube-system started at 2019-08-13 01:21:42 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.905: INFO: 	Container metrics-server ready: true, restart count 0
Aug 14 14:44:23.905: INFO: prometheus-adapter-6798664dcf-2jb4b from monitoring started at 2019-08-13 01:25:35 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.905: INFO: 	Container prometheus-adapter ready: true, restart count 10
Aug 14 14:44:23.905: INFO: istio-ingressgateway-798c45bf9c-qqqcw from istio-system started at 2019-08-13 07:04:19 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.905: INFO: 	Container ingressgateway ready: true, restart count 0
Aug 14 14:44:23.905: INFO: fluentd-zqgvd from monitoring started at 2019-08-13 01:21:56 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.905: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 14:44:23.905: INFO: kube-proxy-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:44:23.905: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Aug 14 14:44:23.914: INFO: node-exporter-mqllj from monitoring started at 2019-08-13 01:20:53 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.914: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 14:44:23.914: INFO: calico-node-ltbzp from kube-system started at 2019-08-13 01:17:46 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.914: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 14:44:23.914: INFO: kube-scheduler-master2 from kube-system started at 2019-08-14 01:15:06 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.914: INFO: 	Container kube-scheduler ready: true, restart count 1
Aug 14 14:44:23.914: INFO: nginx-proxy-master2 from kube-system started at 2019-08-13 07:53:47 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.914: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 14:44:23.914: INFO: prometheus-5c8cd4644d-jvrsq from monitoring started at 2019-08-13 01:27:28 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:23.914: INFO: 	Container configmap-reload ready: false, restart count 0
Aug 14 14:44:23.914: INFO: 	Container prometheus ready: false, restart count 0
Aug 14 14:44:23.914: INFO: fluentd-c2sf9 from monitoring started at 2019-08-13 01:21:56 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.914: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 14:44:23.914: INFO: resource-reserver-master2 from kube-system started at 2019-08-13 09:05:46 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.914: INFO: 	Container sleep-forever ready: true, restart count 1
Aug 14 14:44:23.914: INFO: kube-apiserver-master2 from kube-system started at 2019-08-13 09:05:26 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.914: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 14 14:44:23.914: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-2t2pw from heptio-sonobuoy started at 2019-08-14 14:03:37 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:23.914: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 14:44:23.915: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 14 14:44:23.915: INFO: kube-proxy-master2 from kube-system started at 2019-08-14 01:13:46 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.915: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 14:44:23.915: INFO: kube-controller-manager-master2 from kube-system started at 2019-08-14 01:13:46 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:23.915: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 14:44:23.915: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Aug 14 14:44:24.049: INFO: vpa-admission-controller-77f89cb6d9-fhnf6 from kube-system started at 2019-08-13 01:26:09 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.049: INFO: 	Container admission-controller ready: true, restart count 0
Aug 14 14:44:24.049: INFO: istio-ingressgateway-798c45bf9c-8gqfd from istio-system started at 2019-08-13 01:45:35 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.049: INFO: 	Container ingressgateway ready: true, restart count 0
Aug 14 14:44:24.049: INFO: blackbox-exporter-679b8bc8c-wktmk from monitoring started at 2019-08-13 07:04:16 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.049: INFO: 	Container blackbox-exporter ready: true, restart count 0
Aug 14 14:44:24.049: INFO: kube-apiserver-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:44:24.049: INFO: fluentd-7nzvz from monitoring started at 2019-08-13 01:21:56 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.049: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 14:44:24.049: INFO: vpa-recommender-658cb5b88b-8fsxl from kube-system started at 2019-08-13 01:26:06 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.049: INFO: 	Container recommender ready: true, restart count 0
Aug 14 14:44:24.049: INFO: kube-state-metrics-7f59b96454-g872c from monitoring started at 2019-08-13 07:04:16 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.049: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 14 14:44:24.049: INFO: resource-reserver-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:44:24.049: INFO: prometheus-5c8cd4644d-97xk7 from monitoring started at 2019-08-13 07:04:15 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.049: INFO: 	Container configmap-reload ready: false, restart count 0
Aug 14 14:44:24.049: INFO: 	Container prometheus ready: false, restart count 0
Aug 14 14:44:24.049: INFO: istio-policy-5c69497f5-ztbzd from istio-system started at 2019-08-13 06:02:25 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.049: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:44:24.049: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:44:24.049: INFO: istio-egressgateway-68856cf966-f2pbv from istio-system started at 2019-08-13 07:40:44 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.049: INFO: 	Container egressgateway ready: true, restart count 0
Aug 14 14:44:24.049: INFO: kube-controller-manager-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:44:24.049: INFO: kube-scheduler-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:44:24.049: INFO: calico-node-dzqhc from kube-system started at 2019-08-13 01:17:48 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.049: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 14:44:24.049: INFO: coredns-9c98d6cbb-ncrr2 from kube-system started at 2019-08-13 01:20:32 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.049: INFO: 	Container coredns ready: true, restart count 0
Aug 14 14:44:24.049: INFO: node-exporter-xjj7z from monitoring started at 2019-08-13 01:20:53 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.049: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 14:44:24.049: INFO: nginx-proxy-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:44:24.049: INFO: kube-proxy-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:44:24.049: INFO: alertmanager-67c75747cd-5f5g9 from monitoring started at 2019-08-13 01:21:11 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.049: INFO: 	Container alertmanager ready: true, restart count 0
Aug 14 14:44:24.049: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-t75nt from heptio-sonobuoy started at 2019-08-14 14:03:37 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.049: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 14:44:24.049: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 14 14:44:24.049: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Aug 14 14:44:24.058: INFO: trigger-d85ff74f-r87hh from kube-system started at 2019-08-13 01:22:17 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.058: INFO: 	Container trigger ready: true, restart count 0
Aug 14 14:44:24.058: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-f5ljw from heptio-sonobuoy started at 2019-08-14 14:03:36 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.058: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 14:44:24.058: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 14 14:44:24.058: INFO: fluentd-kmfgj from monitoring started at 2019-08-13 01:21:55 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.058: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 14:44:24.058: INFO: istio-egressgateway-68856cf966-5ksfj from istio-system started at 2019-08-13 01:23:51 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.058: INFO: 	Container egressgateway ready: true, restart count 0
Aug 14 14:44:24.058: INFO: db-daemon-server-7859f8794b-n69wx from kube-system started at 2019-08-13 01:24:34 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.058: INFO: 	Container db-daemon-server ready: true, restart count 0
Aug 14 14:44:24.058: INFO: istio-telemetry-7fb5cd9cb4-k8bs9 from istio-system started at 2019-08-13 04:00:41 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.058: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:44:24.058: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:44:24.058: INFO: kube-proxy-slave1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:44:24.058: INFO: istio-telemetry-7fb5cd9cb4-rvc4k from istio-system started at 2019-08-13 01:41:01 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.058: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:44:24.058: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:44:24.058: INFO: calico-node-jgcdn from kube-system started at 2019-08-13 01:17:49 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.058: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 14:44:24.058: INFO: istio-ingressgateway-798c45bf9c-cr97n from istio-system started at 2019-08-13 01:40:49 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.058: INFO: 	Container ingressgateway ready: true, restart count 0
Aug 14 14:44:24.058: INFO: istio-policy-5c69497f5-l7vkr from istio-system started at 2019-08-13 01:41:01 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.058: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:44:24.058: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:44:24.058: INFO: istio-citadel-6b64f55c84-vkx5j from istio-system started at 2019-08-13 01:23:54 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.058: INFO: 	Container citadel ready: true, restart count 0
Aug 14 14:44:24.058: INFO: istio-statsd-prom-bridge-c98c48fd5-m9l8x from istio-system started at 2019-08-13 01:23:53 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.058: INFO: 	Container statsd-prom-bridge ready: true, restart count 0
Aug 14 14:44:24.058: INFO: node-exporter-crjnb from monitoring started at 2019-08-13 01:20:54 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.058: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 14:44:24.058: INFO: 
Logging pods the kubelet thinks is on node slave2 before test
Aug 14 14:44:24.068: INFO: kube-proxy-slave2 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:44:24.068: INFO: calico-node-csv54 from kube-system started at 2019-08-13 01:17:48 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.068: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 14:44:24.068: INFO: istio-policy-5c69497f5-rfhzl from istio-system started at 2019-08-13 01:23:52 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.068: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:44:24.068: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:44:24.068: INFO: istio-pilot-57bf44f4f7-5hs9t from istio-system started at 2019-08-13 01:23:53 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.068: INFO: 	Container discovery ready: true, restart count 0
Aug 14 14:44:24.069: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:44:24.069: INFO: istio-egressgateway-68856cf966-vqqz4 from istio-system started at 2019-08-13 01:26:00 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.069: INFO: 	Container egressgateway ready: true, restart count 0
Aug 14 14:44:24.069: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-mng6c from heptio-sonobuoy started at 2019-08-14 14:03:38 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 14:44:24.069: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 14 14:44:24.069: INFO: tiller-deploy-cdd5d96ff-qw744 from kube-system started at 2019-08-13 01:19:35 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.069: INFO: 	Container tiller ready: true, restart count 0
Aug 14 14:44:24.069: INFO: istio-telemetry-7fb5cd9cb4-9wrct from istio-system started at 2019-08-13 01:42:03 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.069: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:44:24.069: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:44:24.069: INFO: node-exporter-zj7rc from monitoring started at 2019-08-13 01:20:54 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.069: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 14:44:24.069: INFO: fluentd-v22vb from monitoring started at 2019-08-13 01:21:56 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.069: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 14:44:24.069: INFO: istio-ingressgateway-798c45bf9c-cg9np from istio-system started at 2019-08-13 01:50:41 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.069: INFO: 	Container ingressgateway ready: true, restart count 0
Aug 14 14:44:24.069: INFO: 
Logging pods the kubelet thinks is on node slave3 before test
Aug 14 14:44:24.082: INFO: kube-proxy-slave3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:44:24.082: INFO: node-exporter-tbv9g from monitoring started at 2019-08-13 01:20:54 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.082: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 14:44:24.082: INFO: istio-telemetry-7fb5cd9cb4-t2qmf from istio-system started at 2019-08-13 06:11:07 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.082: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:44:24.082: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:44:24.082: INFO: fluentd-jdlm5 from monitoring started at 2019-08-13 01:21:55 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.082: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 14:44:24.082: INFO: istio-ingressgateway-798c45bf9c-rt89k from istio-system started at 2019-08-13 01:23:52 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.082: INFO: 	Container ingressgateway ready: true, restart count 0
Aug 14 14:44:24.082: INFO: istio-telemetry-7fb5cd9cb4-5ttsx from istio-system started at 2019-08-13 01:23:52 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.082: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:44:24.082: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:44:24.082: INFO: istio-tracing-64f74595f-s2g9w from istio-system started at 2019-08-13 01:23:54 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.082: INFO: 	Container jaeger ready: true, restart count 0
Aug 14 14:44:24.082: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-sttpd from heptio-sonobuoy started at 2019-08-14 14:03:35 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.082: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 14:44:24.082: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 14 14:44:24.082: INFO: istio-sidecar-injector-8886f58d8-64v9m from istio-system started at 2019-08-13 01:23:53 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.082: INFO: 	Container sidecar-injector-webhook ready: true, restart count 0
Aug 14 14:44:24.082: INFO: istio-policy-5c69497f5-nblx2 from istio-system started at 2019-08-13 04:31:07 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.082: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:44:24.082: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:44:24.082: INFO: sonobuoy-e2e-job-a99adffb6ed94bd6 from heptio-sonobuoy started at 2019-08-14 14:03:35 +0000 UTC (2 container statuses recorded)
Aug 14 14:44:24.082: INFO: 	Container e2e ready: true, restart count 0
Aug 14 14:44:24.082: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 14:44:24.082: INFO: calico-node-zwdcq from kube-system started at 2019-08-13 01:17:47 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.082: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 14:44:24.082: INFO: calico-kube-controllers-56f8894747-j4gdw from kube-system started at 2019-08-13 01:18:13 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.082: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 14 14:44:24.082: INFO: istio-egressgateway-68856cf966-nczfg from istio-system started at 2019-08-13 01:26:00 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.082: INFO: 	Container egressgateway ready: true, restart count 0
Aug 14 14:44:24.082: INFO: istio-galley-5f97c5c497-cfvs2 from istio-system started at 2019-08-13 07:47:20 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.082: INFO: 	Container validator ready: true, restart count 1
Aug 14 14:44:24.082: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-14 14:03:22 +0000 UTC (1 container statuses recorded)
Aug 14 14:44:24.082: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a88c123e-305f-471a-8751-ba77dbda3727 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-a88c123e-305f-471a-8751-ba77dbda3727 off the node slave3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a88c123e-305f-471a-8751-ba77dbda3727
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:44:44.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5343" for this suite.
Aug 14 14:45:04.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:45:06.466: INFO: namespace sched-pred-5343 deletion completed in 22.19486137s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

â€¢ [SLOW TEST:43.745 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:45:06.469: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5545
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Aug 14 14:45:08.841: INFO: Waiting up to 5m0s for pod "client-containers-3c94c739-0613-41cd-a2e3-f37a1f8a5197" in namespace "containers-5545" to be "success or failure"
Aug 14 14:45:08.848: INFO: Pod "client-containers-3c94c739-0613-41cd-a2e3-f37a1f8a5197": Phase="Pending", Reason="", readiness=false. Elapsed: 6.783301ms
Aug 14 14:45:10.853: INFO: Pod "client-containers-3c94c739-0613-41cd-a2e3-f37a1f8a5197": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012015596s
Aug 14 14:45:13.079: INFO: Pod "client-containers-3c94c739-0613-41cd-a2e3-f37a1f8a5197": Phase="Pending", Reason="", readiness=false. Elapsed: 4.237378925s
Aug 14 14:45:15.086: INFO: Pod "client-containers-3c94c739-0613-41cd-a2e3-f37a1f8a5197": Phase="Pending", Reason="", readiness=false. Elapsed: 6.244438616s
Aug 14 14:45:17.106: INFO: Pod "client-containers-3c94c739-0613-41cd-a2e3-f37a1f8a5197": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.264637789s
STEP: Saw pod success
Aug 14 14:45:17.106: INFO: Pod "client-containers-3c94c739-0613-41cd-a2e3-f37a1f8a5197" satisfied condition "success or failure"
Aug 14 14:45:17.306: INFO: Trying to get logs from node slave1 pod client-containers-3c94c739-0613-41cd-a2e3-f37a1f8a5197 container test-container: <nil>
STEP: delete the pod
Aug 14 14:45:17.787: INFO: Waiting for pod client-containers-3c94c739-0613-41cd-a2e3-f37a1f8a5197 to disappear
Aug 14 14:45:17.834: INFO: Pod client-containers-3c94c739-0613-41cd-a2e3-f37a1f8a5197 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:45:17.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5545" for this suite.
Aug 14 14:45:30.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:45:32.118: INFO: namespace containers-5545 deletion completed in 14.276918244s

â€¢ [SLOW TEST:25.649 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:45:32.119: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-381/configmap-test-d8890565-6a6b-49d7-b8e0-f039ba5d824b
STEP: Creating a pod to test consume configMaps
Aug 14 14:45:35.109: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac37f22d-839e-4f52-ab68-f2614f73d573" in namespace "configmap-381" to be "success or failure"
Aug 14 14:45:35.334: INFO: Pod "pod-configmaps-ac37f22d-839e-4f52-ab68-f2614f73d573": Phase="Pending", Reason="", readiness=false. Elapsed: 224.744785ms
Aug 14 14:45:37.338: INFO: Pod "pod-configmaps-ac37f22d-839e-4f52-ab68-f2614f73d573": Phase="Pending", Reason="", readiness=false. Elapsed: 2.22950344s
Aug 14 14:45:39.344: INFO: Pod "pod-configmaps-ac37f22d-839e-4f52-ab68-f2614f73d573": Phase="Pending", Reason="", readiness=false. Elapsed: 4.234763648s
Aug 14 14:45:41.760: INFO: Pod "pod-configmaps-ac37f22d-839e-4f52-ab68-f2614f73d573": Phase="Pending", Reason="", readiness=false. Elapsed: 6.6510928s
Aug 14 14:45:44.114: INFO: Pod "pod-configmaps-ac37f22d-839e-4f52-ab68-f2614f73d573": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.005326556s
STEP: Saw pod success
Aug 14 14:45:44.114: INFO: Pod "pod-configmaps-ac37f22d-839e-4f52-ab68-f2614f73d573" satisfied condition "success or failure"
Aug 14 14:45:44.118: INFO: Trying to get logs from node slave2 pod pod-configmaps-ac37f22d-839e-4f52-ab68-f2614f73d573 container env-test: <nil>
STEP: delete the pod
Aug 14 14:45:44.163: INFO: Waiting for pod pod-configmaps-ac37f22d-839e-4f52-ab68-f2614f73d573 to disappear
Aug 14 14:45:44.169: INFO: Pod pod-configmaps-ac37f22d-839e-4f52-ab68-f2614f73d573 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:45:44.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-381" for this suite.
Aug 14 14:45:52.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:45:55.384: INFO: namespace configmap-381 deletion completed in 11.209122432s

â€¢ [SLOW TEST:23.265 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:45:55.386: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-970
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 14 14:45:57.523: INFO: Waiting up to 5m0s for pod "pod-b923bd34-6ac7-4476-aab4-0a449bfa50c6" in namespace "emptydir-970" to be "success or failure"
Aug 14 14:45:57.622: INFO: Pod "pod-b923bd34-6ac7-4476-aab4-0a449bfa50c6": Phase="Pending", Reason="", readiness=false. Elapsed: 98.634658ms
Aug 14 14:45:59.696: INFO: Pod "pod-b923bd34-6ac7-4476-aab4-0a449bfa50c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.172760529s
Aug 14 14:46:01.901: INFO: Pod "pod-b923bd34-6ac7-4476-aab4-0a449bfa50c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.378254122s
Aug 14 14:46:03.906: INFO: Pod "pod-b923bd34-6ac7-4476-aab4-0a449bfa50c6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.383205813s
Aug 14 14:46:05.912: INFO: Pod "pod-b923bd34-6ac7-4476-aab4-0a449bfa50c6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.388942701s
Aug 14 14:46:07.924: INFO: Pod "pod-b923bd34-6ac7-4476-aab4-0a449bfa50c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.400750129s
STEP: Saw pod success
Aug 14 14:46:07.924: INFO: Pod "pod-b923bd34-6ac7-4476-aab4-0a449bfa50c6" satisfied condition "success or failure"
Aug 14 14:46:08.161: INFO: Trying to get logs from node slave3 pod pod-b923bd34-6ac7-4476-aab4-0a449bfa50c6 container test-container: <nil>
STEP: delete the pod
Aug 14 14:46:08.419: INFO: Waiting for pod pod-b923bd34-6ac7-4476-aab4-0a449bfa50c6 to disappear
Aug 14 14:46:08.429: INFO: Pod pod-b923bd34-6ac7-4476-aab4-0a449bfa50c6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:46:08.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-970" for this suite.
Aug 14 14:46:18.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:46:20.786: INFO: namespace emptydir-970 deletion completed in 12.338684665s

â€¢ [SLOW TEST:25.400 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:46:20.786: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8790
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 14 14:46:22.610: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 14 14:46:22.623: INFO: Waiting for terminating namespaces to be deleted...
Aug 14 14:46:22.627: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Aug 14 14:46:22.640: INFO: istio-egressgateway-68856cf966-k7975 from istio-system started at 2019-08-14 11:49:09 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.640: INFO: 	Container egressgateway ready: true, restart count 0
Aug 14 14:46:22.640: INFO: dns-autoscaler-6b9bbb69f4-v77pg from kube-system started at 2019-08-13 07:04:20 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.640: INFO: 	Container autoscaler ready: true, restart count 0
Aug 14 14:46:22.640: INFO: kube-apiserver-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:46:22.640: INFO: kube-controller-manager-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:46:22.640: INFO: coredns-9c98d6cbb-nz84n from kube-system started at 2019-08-13 01:20:04 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.640: INFO: 	Container coredns ready: true, restart count 0
Aug 14 14:46:22.640: INFO: node-exporter-ht9c6 from monitoring started at 2019-08-13 01:20:52 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.640: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 14:46:22.640: INFO: vpa-updater-557c96bf46-lhlk7 from kube-system started at 2019-08-13 07:04:16 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.640: INFO: 	Container updater ready: true, restart count 0
Aug 14 14:46:22.640: INFO: istio-policy-5c69497f5-m2kph from istio-system started at 2019-08-13 07:22:06 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.640: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:46:22.640: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:46:22.640: INFO: kube-scheduler-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:46:22.640: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-zx5nv from heptio-sonobuoy started at 2019-08-14 14:03:36 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.640: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 14:46:22.640: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 14 14:46:22.640: INFO: calico-node-qjwhs from kube-system started at 2019-08-13 01:17:48 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.640: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 14:46:22.640: INFO: metrics-server-5cb9b96667-8wn6n from kube-system started at 2019-08-13 01:21:42 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.640: INFO: 	Container metrics-server ready: true, restart count 0
Aug 14 14:46:22.640: INFO: prometheus-adapter-6798664dcf-2jb4b from monitoring started at 2019-08-13 01:25:35 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.640: INFO: 	Container prometheus-adapter ready: true, restart count 10
Aug 14 14:46:22.640: INFO: istio-ingressgateway-798c45bf9c-qqqcw from istio-system started at 2019-08-13 07:04:19 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.640: INFO: 	Container ingressgateway ready: true, restart count 0
Aug 14 14:46:22.640: INFO: resource-reserver-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:46:22.640: INFO: nginx-proxy-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:46:22.640: INFO: fluentd-zqgvd from monitoring started at 2019-08-13 01:21:56 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.640: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 14:46:22.640: INFO: kube-proxy-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:46:22.640: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Aug 14 14:46:22.648: INFO: kube-proxy-master2 from kube-system started at 2019-08-14 01:13:46 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.648: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 14:46:22.648: INFO: kube-controller-manager-master2 from kube-system started at 2019-08-14 01:13:46 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.648: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 14:46:22.648: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-2t2pw from heptio-sonobuoy started at 2019-08-14 14:03:37 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.648: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 14:46:22.649: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 14 14:46:22.649: INFO: calico-node-ltbzp from kube-system started at 2019-08-13 01:17:46 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.649: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 14:46:22.649: INFO: node-exporter-mqllj from monitoring started at 2019-08-13 01:20:53 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.649: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 14:46:22.649: INFO: kube-scheduler-master2 from kube-system started at 2019-08-14 01:15:06 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.649: INFO: 	Container kube-scheduler ready: true, restart count 1
Aug 14 14:46:22.649: INFO: kube-apiserver-master2 from kube-system started at 2019-08-13 09:05:26 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.649: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 14 14:46:22.649: INFO: nginx-proxy-master2 from kube-system started at 2019-08-13 07:53:47 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.649: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 14:46:22.649: INFO: prometheus-5c8cd4644d-jvrsq from monitoring started at 2019-08-13 01:27:28 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.649: INFO: 	Container configmap-reload ready: false, restart count 0
Aug 14 14:46:22.649: INFO: 	Container prometheus ready: false, restart count 0
Aug 14 14:46:22.649: INFO: fluentd-c2sf9 from monitoring started at 2019-08-13 01:21:56 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.649: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 14:46:22.649: INFO: resource-reserver-master2 from kube-system started at 2019-08-13 09:05:46 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.649: INFO: 	Container sleep-forever ready: true, restart count 1
Aug 14 14:46:22.649: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Aug 14 14:46:22.660: INFO: nginx-proxy-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:46:22.660: INFO: kube-proxy-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:46:22.660: INFO: alertmanager-67c75747cd-5f5g9 from monitoring started at 2019-08-13 01:21:11 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.660: INFO: 	Container alertmanager ready: true, restart count 0
Aug 14 14:46:22.660: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-t75nt from heptio-sonobuoy started at 2019-08-14 14:03:37 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.660: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 14:46:22.660: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 14 14:46:22.660: INFO: vpa-admission-controller-77f89cb6d9-fhnf6 from kube-system started at 2019-08-13 01:26:09 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.660: INFO: 	Container admission-controller ready: true, restart count 0
Aug 14 14:46:22.660: INFO: istio-ingressgateway-798c45bf9c-8gqfd from istio-system started at 2019-08-13 01:45:35 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.660: INFO: 	Container ingressgateway ready: true, restart count 0
Aug 14 14:46:22.660: INFO: blackbox-exporter-679b8bc8c-wktmk from monitoring started at 2019-08-13 07:04:16 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.660: INFO: 	Container blackbox-exporter ready: true, restart count 0
Aug 14 14:46:22.660: INFO: kube-apiserver-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:46:22.660: INFO: fluentd-7nzvz from monitoring started at 2019-08-13 01:21:56 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.660: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 14:46:22.660: INFO: vpa-recommender-658cb5b88b-8fsxl from kube-system started at 2019-08-13 01:26:06 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.660: INFO: 	Container recommender ready: true, restart count 0
Aug 14 14:46:22.660: INFO: kube-state-metrics-7f59b96454-g872c from monitoring started at 2019-08-13 07:04:16 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.660: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 14 14:46:22.660: INFO: resource-reserver-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:46:22.660: INFO: prometheus-5c8cd4644d-97xk7 from monitoring started at 2019-08-13 07:04:15 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.660: INFO: 	Container configmap-reload ready: false, restart count 0
Aug 14 14:46:22.660: INFO: 	Container prometheus ready: false, restart count 0
Aug 14 14:46:22.660: INFO: istio-policy-5c69497f5-ztbzd from istio-system started at 2019-08-13 06:02:25 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.660: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:46:22.660: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:46:22.660: INFO: istio-egressgateway-68856cf966-f2pbv from istio-system started at 2019-08-13 07:40:44 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.660: INFO: 	Container egressgateway ready: true, restart count 0
Aug 14 14:46:22.660: INFO: kube-controller-manager-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:46:22.660: INFO: kube-scheduler-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:46:22.660: INFO: calico-node-dzqhc from kube-system started at 2019-08-13 01:17:48 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.660: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 14:46:22.660: INFO: coredns-9c98d6cbb-ncrr2 from kube-system started at 2019-08-13 01:20:32 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.660: INFO: 	Container coredns ready: true, restart count 0
Aug 14 14:46:22.660: INFO: node-exporter-xjj7z from monitoring started at 2019-08-13 01:20:53 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.660: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 14:46:22.660: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Aug 14 14:46:22.670: INFO: db-daemon-server-7859f8794b-n69wx from kube-system started at 2019-08-13 01:24:34 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.670: INFO: 	Container db-daemon-server ready: true, restart count 0
Aug 14 14:46:22.670: INFO: istio-telemetry-7fb5cd9cb4-k8bs9 from istio-system started at 2019-08-13 04:00:41 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.670: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:46:22.670: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:46:22.670: INFO: kube-proxy-slave1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:46:22.670: INFO: istio-egressgateway-68856cf966-5ksfj from istio-system started at 2019-08-13 01:23:51 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.670: INFO: 	Container egressgateway ready: true, restart count 0
Aug 14 14:46:22.670: INFO: calico-node-jgcdn from kube-system started at 2019-08-13 01:17:49 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.670: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 14:46:22.670: INFO: istio-telemetry-7fb5cd9cb4-rvc4k from istio-system started at 2019-08-13 01:41:01 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.670: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:46:22.670: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:46:22.670: INFO: istio-ingressgateway-798c45bf9c-cr97n from istio-system started at 2019-08-13 01:40:49 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.671: INFO: 	Container ingressgateway ready: true, restart count 0
Aug 14 14:46:22.671: INFO: istio-policy-5c69497f5-l7vkr from istio-system started at 2019-08-13 01:41:01 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.671: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:46:22.671: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:46:22.671: INFO: istio-statsd-prom-bridge-c98c48fd5-m9l8x from istio-system started at 2019-08-13 01:23:53 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.671: INFO: 	Container statsd-prom-bridge ready: true, restart count 0
Aug 14 14:46:22.671: INFO: istio-citadel-6b64f55c84-vkx5j from istio-system started at 2019-08-13 01:23:54 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.671: INFO: 	Container citadel ready: true, restart count 0
Aug 14 14:46:22.671: INFO: node-exporter-crjnb from monitoring started at 2019-08-13 01:20:54 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.671: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 14:46:22.671: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-f5ljw from heptio-sonobuoy started at 2019-08-14 14:03:36 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.671: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 14:46:22.671: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 14 14:46:22.671: INFO: fluentd-kmfgj from monitoring started at 2019-08-13 01:21:55 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.671: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 14:46:22.671: INFO: trigger-d85ff74f-r87hh from kube-system started at 2019-08-13 01:22:17 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.671: INFO: 	Container trigger ready: true, restart count 0
Aug 14 14:46:22.671: INFO: 
Logging pods the kubelet thinks is on node slave2 before test
Aug 14 14:46:22.680: INFO: kube-proxy-slave2 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:46:22.680: INFO: calico-node-csv54 from kube-system started at 2019-08-13 01:17:48 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.680: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 14:46:22.680: INFO: istio-policy-5c69497f5-rfhzl from istio-system started at 2019-08-13 01:23:52 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.680: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:46:22.680: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:46:22.680: INFO: istio-pilot-57bf44f4f7-5hs9t from istio-system started at 2019-08-13 01:23:53 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.680: INFO: 	Container discovery ready: true, restart count 0
Aug 14 14:46:22.680: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:46:22.680: INFO: istio-egressgateway-68856cf966-vqqz4 from istio-system started at 2019-08-13 01:26:00 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.680: INFO: 	Container egressgateway ready: true, restart count 0
Aug 14 14:46:22.680: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-mng6c from heptio-sonobuoy started at 2019-08-14 14:03:38 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.680: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 14:46:22.680: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 14 14:46:22.680: INFO: tiller-deploy-cdd5d96ff-qw744 from kube-system started at 2019-08-13 01:19:35 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.680: INFO: 	Container tiller ready: true, restart count 0
Aug 14 14:46:22.680: INFO: istio-telemetry-7fb5cd9cb4-9wrct from istio-system started at 2019-08-13 01:42:03 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.680: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:46:22.680: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:46:22.680: INFO: node-exporter-zj7rc from monitoring started at 2019-08-13 01:20:54 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.680: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 14:46:22.680: INFO: fluentd-v22vb from monitoring started at 2019-08-13 01:21:56 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.680: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 14:46:22.680: INFO: istio-ingressgateway-798c45bf9c-cg9np from istio-system started at 2019-08-13 01:50:41 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.680: INFO: 	Container ingressgateway ready: true, restart count 0
Aug 14 14:46:22.680: INFO: 
Logging pods the kubelet thinks is on node slave3 before test
Aug 14 14:46:22.694: INFO: node-exporter-tbv9g from monitoring started at 2019-08-13 01:20:54 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.694: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 14:46:22.694: INFO: istio-telemetry-7fb5cd9cb4-t2qmf from istio-system started at 2019-08-13 06:11:07 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.694: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:46:22.694: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:46:22.694: INFO: kube-proxy-slave3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 14:46:22.694: INFO: istio-ingressgateway-798c45bf9c-rt89k from istio-system started at 2019-08-13 01:23:52 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.694: INFO: 	Container ingressgateway ready: true, restart count 0
Aug 14 14:46:22.694: INFO: istio-telemetry-7fb5cd9cb4-5ttsx from istio-system started at 2019-08-13 01:23:52 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.694: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:46:22.694: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:46:22.694: INFO: istio-tracing-64f74595f-s2g9w from istio-system started at 2019-08-13 01:23:54 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.694: INFO: 	Container jaeger ready: true, restart count 0
Aug 14 14:46:22.694: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-sttpd from heptio-sonobuoy started at 2019-08-14 14:03:35 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.694: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 14:46:22.694: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 14 14:46:22.694: INFO: fluentd-jdlm5 from monitoring started at 2019-08-13 01:21:55 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.694: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 14:46:22.694: INFO: istio-policy-5c69497f5-nblx2 from istio-system started at 2019-08-13 04:31:07 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.694: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 14:46:22.694: INFO: 	Container mixer ready: true, restart count 0
Aug 14 14:46:22.694: INFO: sonobuoy-e2e-job-a99adffb6ed94bd6 from heptio-sonobuoy started at 2019-08-14 14:03:35 +0000 UTC (2 container statuses recorded)
Aug 14 14:46:22.694: INFO: 	Container e2e ready: true, restart count 0
Aug 14 14:46:22.694: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 14:46:22.694: INFO: istio-sidecar-injector-8886f58d8-64v9m from istio-system started at 2019-08-13 01:23:53 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.694: INFO: 	Container sidecar-injector-webhook ready: true, restart count 0
Aug 14 14:46:22.695: INFO: calico-kube-controllers-56f8894747-j4gdw from kube-system started at 2019-08-13 01:18:13 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.695: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 14 14:46:22.695: INFO: istio-egressgateway-68856cf966-nczfg from istio-system started at 2019-08-13 01:26:00 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.695: INFO: 	Container egressgateway ready: true, restart count 0
Aug 14 14:46:22.695: INFO: istio-galley-5f97c5c497-cfvs2 from istio-system started at 2019-08-13 07:47:20 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.695: INFO: 	Container validator ready: true, restart count 1
Aug 14 14:46:22.695: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-14 14:03:22 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.695: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 14 14:46:22.695: INFO: calico-node-zwdcq from kube-system started at 2019-08-13 01:17:47 +0000 UTC (1 container statuses recorded)
Aug 14 14:46:22.695: INFO: 	Container calico-node ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15bad154dd74499d], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:46:23.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8790" for this suite.
Aug 14 14:46:34.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:46:35.821: INFO: namespace sched-pred-8790 deletion completed in 11.923808306s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

â€¢ [SLOW TEST:15.035 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:46:35.822: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7524
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 14 14:46:45.282: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:46:45.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7524" for this suite.
Aug 14 14:46:56.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:46:57.978: INFO: namespace container-runtime-7524 deletion completed in 12.364078854s

â€¢ [SLOW TEST:22.157 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:46:57.979: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6056
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-e97685c9-6496-4a1e-8888-ac057fd060fc in namespace container-probe-6056
Aug 14 14:47:09.647: INFO: Started pod test-webserver-e97685c9-6496-4a1e-8888-ac057fd060fc in namespace container-probe-6056
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 14:47:09.653: INFO: Initial restart count of pod test-webserver-e97685c9-6496-4a1e-8888-ac057fd060fc is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:51:11.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6056" for this suite.
Aug 14 14:51:22.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:51:23.905: INFO: namespace container-probe-6056 deletion completed in 11.941530262s

â€¢ [SLOW TEST:265.926 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:51:23.905: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9031
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-efa167ad-6100-464c-9b34-aec03c67aa23
STEP: Creating a pod to test consume secrets
Aug 14 14:51:24.867: INFO: Waiting up to 5m0s for pod "pod-secrets-e2c6198b-389f-4959-b2b7-138ea77ba326" in namespace "secrets-9031" to be "success or failure"
Aug 14 14:51:24.875: INFO: Pod "pod-secrets-e2c6198b-389f-4959-b2b7-138ea77ba326": Phase="Pending", Reason="", readiness=false. Elapsed: 8.122271ms
Aug 14 14:51:26.880: INFO: Pod "pod-secrets-e2c6198b-389f-4959-b2b7-138ea77ba326": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012700755s
Aug 14 14:51:28.952: INFO: Pod "pod-secrets-e2c6198b-389f-4959-b2b7-138ea77ba326": Phase="Pending", Reason="", readiness=false. Elapsed: 4.085382909s
Aug 14 14:51:30.959: INFO: Pod "pod-secrets-e2c6198b-389f-4959-b2b7-138ea77ba326": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091721658s
Aug 14 14:51:33.170: INFO: Pod "pod-secrets-e2c6198b-389f-4959-b2b7-138ea77ba326": Phase="Pending", Reason="", readiness=false. Elapsed: 8.303002805s
Aug 14 14:51:35.656: INFO: Pod "pod-secrets-e2c6198b-389f-4959-b2b7-138ea77ba326": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.789207044s
STEP: Saw pod success
Aug 14 14:51:35.656: INFO: Pod "pod-secrets-e2c6198b-389f-4959-b2b7-138ea77ba326" satisfied condition "success or failure"
Aug 14 14:51:35.660: INFO: Trying to get logs from node slave3 pod pod-secrets-e2c6198b-389f-4959-b2b7-138ea77ba326 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 14:51:36.197: INFO: Waiting for pod pod-secrets-e2c6198b-389f-4959-b2b7-138ea77ba326 to disappear
Aug 14 14:51:36.211: INFO: Pod pod-secrets-e2c6198b-389f-4959-b2b7-138ea77ba326 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:51:36.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9031" for this suite.
Aug 14 14:51:48.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:51:50.822: INFO: namespace secrets-9031 deletion completed in 14.602536197s

â€¢ [SLOW TEST:26.917 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:51:50.822: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5701
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Aug 14 14:51:52.656: INFO: created pod pod-service-account-defaultsa
Aug 14 14:51:52.657: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 14 14:51:52.671: INFO: created pod pod-service-account-mountsa
Aug 14 14:51:52.671: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 14 14:51:52.697: INFO: created pod pod-service-account-nomountsa
Aug 14 14:51:52.697: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 14 14:51:52.707: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 14 14:51:52.707: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 14 14:51:52.977: INFO: created pod pod-service-account-mountsa-mountspec
Aug 14 14:51:52.977: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 14 14:51:53.471: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 14 14:51:53.471: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 14 14:51:54.071: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 14 14:51:54.071: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 14 14:51:54.475: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 14 14:51:54.475: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 14 14:51:54.854: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 14 14:51:54.854: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:51:54.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5701" for this suite.
Aug 14 14:52:18.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:52:20.030: INFO: namespace svcaccounts-5701 deletion completed in 24.757741801s

â€¢ [SLOW TEST:29.208 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:52:20.031: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Aug 14 14:52:21.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 create -f - --namespace=kubectl-9604'
Aug 14 14:52:28.879: INFO: stderr: ""
Aug 14 14:52:28.879: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 14:52:28.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9604'
Aug 14 14:52:29.750: INFO: stderr: ""
Aug 14 14:52:29.750: INFO: stdout: "update-demo-nautilus-qt4g5 update-demo-nautilus-r265t "
Aug 14 14:52:29.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-qt4g5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9604'
Aug 14 14:52:31.561: INFO: stderr: ""
Aug 14 14:52:31.561: INFO: stdout: ""
Aug 14 14:52:31.561: INFO: update-demo-nautilus-qt4g5 is created but not running
Aug 14 14:52:36.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9604'
Aug 14 14:52:38.308: INFO: stderr: ""
Aug 14 14:52:38.308: INFO: stdout: "update-demo-nautilus-qt4g5 update-demo-nautilus-r265t "
Aug 14 14:52:38.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-qt4g5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9604'
Aug 14 14:52:38.908: INFO: stderr: ""
Aug 14 14:52:38.908: INFO: stdout: "true"
Aug 14 14:52:38.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-qt4g5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9604'
Aug 14 14:52:40.078: INFO: stderr: ""
Aug 14 14:52:40.078: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 14:52:40.078: INFO: validating pod update-demo-nautilus-qt4g5
Aug 14 14:52:40.597: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 14:52:40.597: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 14:52:40.597: INFO: update-demo-nautilus-qt4g5 is verified up and running
Aug 14 14:52:40.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-r265t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9604'
Aug 14 14:52:40.943: INFO: stderr: ""
Aug 14 14:52:40.943: INFO: stdout: "true"
Aug 14 14:52:40.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-r265t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9604'
Aug 14 14:52:41.640: INFO: stderr: ""
Aug 14 14:52:41.640: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 14:52:41.640: INFO: validating pod update-demo-nautilus-r265t
Aug 14 14:52:41.646: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 14:52:41.646: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 14:52:41.646: INFO: update-demo-nautilus-r265t is verified up and running
STEP: scaling down the replication controller
Aug 14 14:52:41.648: INFO: scanned /root for discovery docs: <nil>
Aug 14 14:52:41.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9604'
Aug 14 14:52:44.706: INFO: stderr: ""
Aug 14 14:52:44.706: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 14:52:44.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9604'
Aug 14 14:52:45.395: INFO: stderr: ""
Aug 14 14:52:45.395: INFO: stdout: "update-demo-nautilus-qt4g5 update-demo-nautilus-r265t "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 14 14:52:50.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9604'
Aug 14 14:52:50.936: INFO: stderr: ""
Aug 14 14:52:50.936: INFO: stdout: "update-demo-nautilus-qt4g5 update-demo-nautilus-r265t "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 14 14:52:55.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9604'
Aug 14 14:52:56.706: INFO: stderr: ""
Aug 14 14:52:56.706: INFO: stdout: "update-demo-nautilus-qt4g5 "
Aug 14 14:52:56.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-qt4g5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9604'
Aug 14 14:52:57.470: INFO: stderr: ""
Aug 14 14:52:57.470: INFO: stdout: "true"
Aug 14 14:52:57.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-qt4g5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9604'
Aug 14 14:52:58.570: INFO: stderr: ""
Aug 14 14:52:58.570: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 14:52:58.570: INFO: validating pod update-demo-nautilus-qt4g5
Aug 14 14:52:58.697: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 14:52:58.697: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 14:52:58.697: INFO: update-demo-nautilus-qt4g5 is verified up and running
STEP: scaling up the replication controller
Aug 14 14:52:58.699: INFO: scanned /root for discovery docs: <nil>
Aug 14 14:52:58.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9604'
Aug 14 14:53:00.641: INFO: stderr: ""
Aug 14 14:53:00.641: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 14:53:00.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9604'
Aug 14 14:53:01.186: INFO: stderr: ""
Aug 14 14:53:01.186: INFO: stdout: "update-demo-nautilus-94sxh update-demo-nautilus-qt4g5 "
Aug 14 14:53:01.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-94sxh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9604'
Aug 14 14:53:01.907: INFO: stderr: ""
Aug 14 14:53:01.907: INFO: stdout: ""
Aug 14 14:53:01.907: INFO: update-demo-nautilus-94sxh is created but not running
Aug 14 14:53:06.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9604'
Aug 14 14:53:07.810: INFO: stderr: ""
Aug 14 14:53:07.810: INFO: stdout: "update-demo-nautilus-94sxh update-demo-nautilus-qt4g5 "
Aug 14 14:53:07.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-94sxh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9604'
Aug 14 14:53:08.097: INFO: stderr: ""
Aug 14 14:53:08.097: INFO: stdout: "true"
Aug 14 14:53:08.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-94sxh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9604'
Aug 14 14:53:08.852: INFO: stderr: ""
Aug 14 14:53:08.852: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 14:53:08.852: INFO: validating pod update-demo-nautilus-94sxh
Aug 14 14:53:08.858: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 14:53:08.858: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 14:53:08.858: INFO: update-demo-nautilus-94sxh is verified up and running
Aug 14 14:53:08.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-qt4g5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9604'
Aug 14 14:53:09.467: INFO: stderr: ""
Aug 14 14:53:09.467: INFO: stdout: "true"
Aug 14 14:53:09.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-qt4g5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9604'
Aug 14 14:53:09.872: INFO: stderr: ""
Aug 14 14:53:09.872: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 14:53:09.872: INFO: validating pod update-demo-nautilus-qt4g5
Aug 14 14:53:09.877: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 14:53:09.877: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 14:53:09.877: INFO: update-demo-nautilus-qt4g5 is verified up and running
STEP: using delete to clean up resources
Aug 14 14:53:09.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete --grace-period=0 --force -f - --namespace=kubectl-9604'
Aug 14 14:53:10.146: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 14:53:10.146: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 14 14:53:10.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9604'
Aug 14 14:53:11.908: INFO: stderr: "No resources found.\n"
Aug 14 14:53:11.908: INFO: stdout: ""
Aug 14 14:53:11.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -l name=update-demo --namespace=kubectl-9604 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 14 14:53:13.719: INFO: stderr: ""
Aug 14 14:53:13.719: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:53:13.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9604" for this suite.
Aug 14 14:53:25.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:53:27.455: INFO: namespace kubectl-9604 deletion completed in 13.729892324s

â€¢ [SLOW TEST:67.424 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:53:27.456: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 14:53:29.026: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0111cf6b-e93e-4313-89a2-75b94d84af34" in namespace "downward-api-230" to be "success or failure"
Aug 14 14:53:29.172: INFO: Pod "downwardapi-volume-0111cf6b-e93e-4313-89a2-75b94d84af34": Phase="Pending", Reason="", readiness=false. Elapsed: 145.755309ms
Aug 14 14:53:31.181: INFO: Pod "downwardapi-volume-0111cf6b-e93e-4313-89a2-75b94d84af34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.155453162s
Aug 14 14:53:33.393: INFO: Pod "downwardapi-volume-0111cf6b-e93e-4313-89a2-75b94d84af34": Phase="Pending", Reason="", readiness=false. Elapsed: 4.3669691s
Aug 14 14:53:35.398: INFO: Pod "downwardapi-volume-0111cf6b-e93e-4313-89a2-75b94d84af34": Phase="Pending", Reason="", readiness=false. Elapsed: 6.372012768s
Aug 14 14:53:37.447: INFO: Pod "downwardapi-volume-0111cf6b-e93e-4313-89a2-75b94d84af34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.421598158s
STEP: Saw pod success
Aug 14 14:53:37.448: INFO: Pod "downwardapi-volume-0111cf6b-e93e-4313-89a2-75b94d84af34" satisfied condition "success or failure"
Aug 14 14:53:37.714: INFO: Trying to get logs from node slave1 pod downwardapi-volume-0111cf6b-e93e-4313-89a2-75b94d84af34 container client-container: <nil>
STEP: delete the pod
Aug 14 14:53:38.247: INFO: Waiting for pod downwardapi-volume-0111cf6b-e93e-4313-89a2-75b94d84af34 to disappear
Aug 14 14:53:38.588: INFO: Pod downwardapi-volume-0111cf6b-e93e-4313-89a2-75b94d84af34 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:53:38.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-230" for this suite.
Aug 14 14:53:49.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:53:51.556: INFO: namespace downward-api-230 deletion completed in 12.96228812s

â€¢ [SLOW TEST:24.100 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:53:51.556: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5759
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-a5013814-958d-4dca-ac56-dd0e2148d876
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:53:52.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5759" for this suite.
Aug 14 14:54:02.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:54:04.675: INFO: namespace configmap-5759 deletion completed in 12.163139242s

â€¢ [SLOW TEST:13.119 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:54:04.676: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8007
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-09f1d087-ad6d-4c3e-9161-a3058f35bfb1
STEP: Creating a pod to test consume configMaps
Aug 14 14:54:06.322: INFO: Waiting up to 5m0s for pod "pod-configmaps-3622eb8a-be41-4b6c-bf2c-47878046bc14" in namespace "configmap-8007" to be "success or failure"
Aug 14 14:54:06.515: INFO: Pod "pod-configmaps-3622eb8a-be41-4b6c-bf2c-47878046bc14": Phase="Pending", Reason="", readiness=false. Elapsed: 193.172396ms
Aug 14 14:54:08.541: INFO: Pod "pod-configmaps-3622eb8a-be41-4b6c-bf2c-47878046bc14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.218897485s
Aug 14 14:54:10.642: INFO: Pod "pod-configmaps-3622eb8a-be41-4b6c-bf2c-47878046bc14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.319769086s
Aug 14 14:54:12.759: INFO: Pod "pod-configmaps-3622eb8a-be41-4b6c-bf2c-47878046bc14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.436773035s
Aug 14 14:54:14.907: INFO: Pod "pod-configmaps-3622eb8a-be41-4b6c-bf2c-47878046bc14": Phase="Pending", Reason="", readiness=false. Elapsed: 8.585509731s
Aug 14 14:54:17.010: INFO: Pod "pod-configmaps-3622eb8a-be41-4b6c-bf2c-47878046bc14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.688004462s
STEP: Saw pod success
Aug 14 14:54:17.010: INFO: Pod "pod-configmaps-3622eb8a-be41-4b6c-bf2c-47878046bc14" satisfied condition "success or failure"
Aug 14 14:54:17.016: INFO: Trying to get logs from node slave2 pod pod-configmaps-3622eb8a-be41-4b6c-bf2c-47878046bc14 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 14:54:17.264: INFO: Waiting for pod pod-configmaps-3622eb8a-be41-4b6c-bf2c-47878046bc14 to disappear
Aug 14 14:54:17.271: INFO: Pod pod-configmaps-3622eb8a-be41-4b6c-bf2c-47878046bc14 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:54:17.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8007" for this suite.
Aug 14 14:54:25.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:54:27.365: INFO: namespace configmap-8007 deletion completed in 10.087233802s

â€¢ [SLOW TEST:22.689 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:54:27.366: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4312
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Aug 14 14:54:45.017: INFO: 10 pods remaining
Aug 14 14:54:45.017: INFO: 10 pods has nil DeletionTimestamp
Aug 14 14:54:45.018: INFO: 
Aug 14 14:54:46.106: INFO: 10 pods remaining
Aug 14 14:54:46.106: INFO: 0 pods has nil DeletionTimestamp
Aug 14 14:54:46.106: INFO: 
Aug 14 14:54:47.714: INFO: 7 pods remaining
Aug 14 14:54:47.714: INFO: 0 pods has nil DeletionTimestamp
Aug 14 14:54:47.714: INFO: 
Aug 14 14:54:49.829: INFO: 0 pods remaining
Aug 14 14:54:49.829: INFO: 0 pods has nil DeletionTimestamp
Aug 14 14:54:49.829: INFO: 
STEP: Gathering metrics
W0814 14:54:52.608988      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 14 14:54:52.609: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:54:52.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4312" for this suite.
Aug 14 14:55:11.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:55:13.476: INFO: namespace gc-4312 deletion completed in 19.996416691s

â€¢ [SLOW TEST:46.110 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:55:13.477: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9073
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9073
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Aug 14 14:55:15.503: INFO: Found 0 stateful pods, waiting for 3
Aug 14 14:55:25.508: INFO: Found 2 stateful pods, waiting for 3
Aug 14 14:55:35.508: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 14:55:35.508: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 14:55:35.508: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug 14 14:55:45.510: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 14:55:45.510: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 14:55:45.510: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 14 14:55:45.626: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 14 14:55:55.886: INFO: Updating stateful set ss2
Aug 14 14:55:56.026: INFO: Waiting for Pod statefulset-9073/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Aug 14 14:56:07.574: INFO: Found 2 stateful pods, waiting for 3
Aug 14 14:56:17.579: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 14:56:17.580: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 14:56:17.580: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug 14 14:56:27.579: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 14:56:27.579: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 14:56:27.579: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 14 14:56:27.762: INFO: Updating stateful set ss2
Aug 14 14:56:27.783: INFO: Waiting for Pod statefulset-9073/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 14:56:38.163: INFO: Updating stateful set ss2
Aug 14 14:56:38.179: INFO: Waiting for StatefulSet statefulset-9073/ss2 to complete update
Aug 14 14:56:38.179: INFO: Waiting for Pod statefulset-9073/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 14:56:48.250: INFO: Waiting for StatefulSet statefulset-9073/ss2 to complete update
Aug 14 14:56:48.250: INFO: Waiting for Pod statefulset-9073/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 14:56:58.317: INFO: Waiting for StatefulSet statefulset-9073/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 14 14:57:08.191: INFO: Deleting all statefulset in ns statefulset-9073
Aug 14 14:57:08.196: INFO: Scaling statefulset ss2 to 0
Aug 14 14:57:38.244: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 14:57:38.248: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:57:38.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9073" for this suite.
Aug 14 14:57:54.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:57:56.640: INFO: namespace statefulset-9073 deletion completed in 18.233265099s

â€¢ [SLOW TEST:163.163 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:57:56.640: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3684
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Aug 14 14:57:57.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 create -f - --namespace=kubectl-3684'
Aug 14 14:57:59.371: INFO: stderr: ""
Aug 14 14:57:59.371: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 14 14:58:00.380: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 14:58:00.380: INFO: Found 0 / 1
Aug 14 14:58:01.498: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 14:58:01.498: INFO: Found 0 / 1
Aug 14 14:58:02.378: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 14:58:02.378: INFO: Found 0 / 1
Aug 14 14:58:03.376: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 14:58:03.376: INFO: Found 0 / 1
Aug 14 14:58:04.522: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 14:58:04.522: INFO: Found 0 / 1
Aug 14 14:58:05.671: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 14:58:05.671: INFO: Found 0 / 1
Aug 14 14:58:06.471: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 14:58:06.471: INFO: Found 0 / 1
Aug 14 14:58:07.778: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 14:58:07.778: INFO: Found 1 / 1
Aug 14 14:58:07.778: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 14 14:58:07.782: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 14:58:07.782: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 14 14:58:07.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 patch pod redis-master-zrckc --namespace=kubectl-3684 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 14 14:58:09.243: INFO: stderr: ""
Aug 14 14:58:09.243: INFO: stdout: "pod/redis-master-zrckc patched\n"
STEP: checking annotations
Aug 14 14:58:09.250: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 14:58:09.250: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:58:09.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3684" for this suite.
Aug 14 14:58:37.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:58:38.993: INFO: namespace kubectl-3684 deletion completed in 29.736939525s

â€¢ [SLOW TEST:42.352 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:58:38.994: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4210
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-4210/configmap-test-264f198a-79bf-43be-926f-607b2fb70037
STEP: Creating a pod to test consume configMaps
Aug 14 14:58:41.113: INFO: Waiting up to 5m0s for pod "pod-configmaps-6c2d8ba0-d5f3-48f7-bfa1-b185ece5ed4e" in namespace "configmap-4210" to be "success or failure"
Aug 14 14:58:41.122: INFO: Pod "pod-configmaps-6c2d8ba0-d5f3-48f7-bfa1-b185ece5ed4e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.469631ms
Aug 14 14:58:43.677: INFO: Pod "pod-configmaps-6c2d8ba0-d5f3-48f7-bfa1-b185ece5ed4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.563326165s
Aug 14 14:58:45.682: INFO: Pod "pod-configmaps-6c2d8ba0-d5f3-48f7-bfa1-b185ece5ed4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.568214137s
Aug 14 14:58:47.758: INFO: Pod "pod-configmaps-6c2d8ba0-d5f3-48f7-bfa1-b185ece5ed4e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.644459763s
Aug 14 14:58:49.776: INFO: Pod "pod-configmaps-6c2d8ba0-d5f3-48f7-bfa1-b185ece5ed4e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.662026062s
Aug 14 14:58:51.841: INFO: Pod "pod-configmaps-6c2d8ba0-d5f3-48f7-bfa1-b185ece5ed4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.727278277s
STEP: Saw pod success
Aug 14 14:58:51.841: INFO: Pod "pod-configmaps-6c2d8ba0-d5f3-48f7-bfa1-b185ece5ed4e" satisfied condition "success or failure"
Aug 14 14:58:51.845: INFO: Trying to get logs from node slave1 pod pod-configmaps-6c2d8ba0-d5f3-48f7-bfa1-b185ece5ed4e container env-test: <nil>
STEP: delete the pod
Aug 14 14:58:51.900: INFO: Waiting for pod pod-configmaps-6c2d8ba0-d5f3-48f7-bfa1-b185ece5ed4e to disappear
Aug 14 14:58:52.122: INFO: Pod pod-configmaps-6c2d8ba0-d5f3-48f7-bfa1-b185ece5ed4e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:58:52.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4210" for this suite.
Aug 14 14:59:02.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 14:59:06.077: INFO: namespace configmap-4210 deletion completed in 13.948703449s

â€¢ [SLOW TEST:27.083 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 14:59:06.079: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8256
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8256
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 14 14:59:07.232: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 14 14:59:52.524: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.32.45:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8256 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 14:59:52.524: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 14:59:52.724: INFO: Found all expected endpoints: [netserver-0]
Aug 14 14:59:52.814: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.51.23:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8256 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 14:59:52.814: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 14:59:53.093: INFO: Found all expected endpoints: [netserver-1]
Aug 14 14:59:53.097: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.194.14:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8256 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 14:59:53.097: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 14:59:53.333: INFO: Found all expected endpoints: [netserver-2]
Aug 14 14:59:53.338: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.208.50:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8256 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 14:59:53.338: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 14:59:53.700: INFO: Found all expected endpoints: [netserver-3]
Aug 14 14:59:53.847: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.49.243:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8256 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 14:59:53.847: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 14:59:54.111: INFO: Found all expected endpoints: [netserver-4]
Aug 14 14:59:54.120: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.161.86:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8256 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 14:59:54.120: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 14:59:54.426: INFO: Found all expected endpoints: [netserver-5]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 14:59:54.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8256" for this suite.
Aug 14 15:00:40.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:00:42.584: INFO: namespace pod-network-test-8256 deletion completed in 48.151422752s

â€¢ [SLOW TEST:96.506 seconds]
[sig-network] Networking
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:00:42.585: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4716
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4716
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 14 15:00:45.769: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 14 15:01:30.283: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.49.245 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4716 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:01:30.283: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:01:31.473: INFO: Found all expected endpoints: [netserver-0]
Aug 14 15:01:31.523: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.208.51 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4716 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:01:31.523: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:01:32.701: INFO: Found all expected endpoints: [netserver-1]
Aug 14 15:01:32.706: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.51.33 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4716 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:01:32.706: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:01:33.907: INFO: Found all expected endpoints: [netserver-2]
Aug 14 15:01:33.911: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.161.87 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4716 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:01:33.911: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:01:35.068: INFO: Found all expected endpoints: [netserver-3]
Aug 14 15:01:35.072: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.32.46 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4716 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:01:35.072: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:01:36.242: INFO: Found all expected endpoints: [netserver-4]
Aug 14 15:01:36.273: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.194.15 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4716 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:01:36.273: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:01:37.676: INFO: Found all expected endpoints: [netserver-5]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:01:37.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4716" for this suite.
Aug 14 15:02:19.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:02:21.627: INFO: namespace pod-network-test-4716 deletion completed in 43.943640608s

â€¢ [SLOW TEST:99.042 seconds]
[sig-network] Networking
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:02:21.627: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1186
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-b212a7e2-f35d-4af3-8a96-5a12ab7942ce
STEP: Creating a pod to test consume secrets
Aug 14 15:02:23.461: INFO: Waiting up to 5m0s for pod "pod-secrets-3cbebd24-4172-4f2b-a5e5-1163cc21fbaf" in namespace "secrets-1186" to be "success or failure"
Aug 14 15:02:23.548: INFO: Pod "pod-secrets-3cbebd24-4172-4f2b-a5e5-1163cc21fbaf": Phase="Pending", Reason="", readiness=false. Elapsed: 86.498164ms
Aug 14 15:02:25.553: INFO: Pod "pod-secrets-3cbebd24-4172-4f2b-a5e5-1163cc21fbaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091464227s
Aug 14 15:02:27.558: INFO: Pod "pod-secrets-3cbebd24-4172-4f2b-a5e5-1163cc21fbaf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.096103078s
Aug 14 15:02:29.821: INFO: Pod "pod-secrets-3cbebd24-4172-4f2b-a5e5-1163cc21fbaf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.359182893s
Aug 14 15:02:31.950: INFO: Pod "pod-secrets-3cbebd24-4172-4f2b-a5e5-1163cc21fbaf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.488173134s
Aug 14 15:02:34.105: INFO: Pod "pod-secrets-3cbebd24-4172-4f2b-a5e5-1163cc21fbaf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.643610953s
Aug 14 15:02:36.144: INFO: Pod "pod-secrets-3cbebd24-4172-4f2b-a5e5-1163cc21fbaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.682944097s
STEP: Saw pod success
Aug 14 15:02:36.145: INFO: Pod "pod-secrets-3cbebd24-4172-4f2b-a5e5-1163cc21fbaf" satisfied condition "success or failure"
Aug 14 15:02:36.152: INFO: Trying to get logs from node slave3 pod pod-secrets-3cbebd24-4172-4f2b-a5e5-1163cc21fbaf container secret-env-test: <nil>
STEP: delete the pod
Aug 14 15:02:36.879: INFO: Waiting for pod pod-secrets-3cbebd24-4172-4f2b-a5e5-1163cc21fbaf to disappear
Aug 14 15:02:36.882: INFO: Pod pod-secrets-3cbebd24-4172-4f2b-a5e5-1163cc21fbaf no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:02:36.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1186" for this suite.
Aug 14 15:02:51.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:02:53.113: INFO: namespace secrets-1186 deletion completed in 16.225227096s

â€¢ [SLOW TEST:31.487 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:02:53.114: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4651
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:03:05.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4651" for this suite.
Aug 14 15:03:15.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:03:17.478: INFO: namespace watch-4651 deletion completed in 11.772415973s

â€¢ [SLOW TEST:24.364 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:03:17.479: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1409
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Aug 14 15:03:19.346: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-938832172 proxy --unix-socket=/tmp/kubectl-proxy-unix338256027/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:03:19.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1409" for this suite.
Aug 14 15:03:30.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:03:31.936: INFO: namespace kubectl-1409 deletion completed in 12.458073352s

â€¢ [SLOW TEST:14.457 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:03:31.937: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4299
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 14 15:03:33.144: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4299,SelfLink:/api/v1/namespaces/watch-4299/configmaps/e2e-watch-test-watch-closed,UID:bf58a1c2-bb09-4a6b-bc21-71430b905f85,ResourceVersion:366986,Generation:0,CreationTimestamp:2019-08-14 15:03:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 14 15:03:33.145: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4299,SelfLink:/api/v1/namespaces/watch-4299/configmaps/e2e-watch-test-watch-closed,UID:bf58a1c2-bb09-4a6b-bc21-71430b905f85,ResourceVersion:366987,Generation:0,CreationTimestamp:2019-08-14 15:03:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 14 15:03:33.287: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4299,SelfLink:/api/v1/namespaces/watch-4299/configmaps/e2e-watch-test-watch-closed,UID:bf58a1c2-bb09-4a6b-bc21-71430b905f85,ResourceVersion:366988,Generation:0,CreationTimestamp:2019-08-14 15:03:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 14 15:03:33.287: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4299,SelfLink:/api/v1/namespaces/watch-4299/configmaps/e2e-watch-test-watch-closed,UID:bf58a1c2-bb09-4a6b-bc21-71430b905f85,ResourceVersion:366989,Generation:0,CreationTimestamp:2019-08-14 15:03:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:03:33.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4299" for this suite.
Aug 14 15:03:43.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:03:50.527: INFO: namespace watch-4299 deletion completed in 17.233450059s

â€¢ [SLOW TEST:18.590 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:03:50.527: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5775
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-5775
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5775 to expose endpoints map[]
Aug 14 15:04:01.480: INFO: successfully validated that service multi-endpoint-test in namespace services-5775 exposes endpoints map[] (758.879181ms elapsed)
STEP: Creating pod pod1 in namespace services-5775
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5775 to expose endpoints map[pod1:[100]]
Aug 14 15:04:06.712: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.688768783s elapsed, will retry)
Aug 14 15:04:13.116: INFO: successfully validated that service multi-endpoint-test in namespace services-5775 exposes endpoints map[pod1:[100]] (11.092308994s elapsed)
STEP: Creating pod pod2 in namespace services-5775
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5775 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 14 15:04:17.724: INFO: Unexpected endpoints: found map[f4bb97b6-5ee4-4a51-9cc6-b5fa4b08fe18:[100]], expected map[pod1:[100] pod2:[101]] (4.542772964s elapsed, will retry)
Aug 14 15:04:21.020: INFO: successfully validated that service multi-endpoint-test in namespace services-5775 exposes endpoints map[pod1:[100] pod2:[101]] (7.838798114s elapsed)
STEP: Deleting pod pod1 in namespace services-5775
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5775 to expose endpoints map[pod2:[101]]
Aug 14 15:04:21.339: INFO: successfully validated that service multi-endpoint-test in namespace services-5775 exposes endpoints map[pod2:[101]] (228.895178ms elapsed)
STEP: Deleting pod pod2 in namespace services-5775
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5775 to expose endpoints map[]
Aug 14 15:04:22.652: INFO: successfully validated that service multi-endpoint-test in namespace services-5775 exposes endpoints map[] (1.251660734s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:04:24.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5775" for this suite.
Aug 14 15:04:34.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:04:37.082: INFO: namespace services-5775 deletion completed in 12.650075216s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

â€¢ [SLOW TEST:46.555 seconds]
[sig-network] Services
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:04:37.083: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-691
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Aug 14 15:04:37.895: INFO: Waiting up to 5m0s for pod "client-containers-70a20549-962a-4089-ac54-abb60ca2f2a2" in namespace "containers-691" to be "success or failure"
Aug 14 15:04:37.912: INFO: Pod "client-containers-70a20549-962a-4089-ac54-abb60ca2f2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.168744ms
Aug 14 15:04:39.917: INFO: Pod "client-containers-70a20549-962a-4089-ac54-abb60ca2f2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022456564s
Aug 14 15:04:41.921: INFO: Pod "client-containers-70a20549-962a-4089-ac54-abb60ca2f2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02660382s
Aug 14 15:04:43.926: INFO: Pod "client-containers-70a20549-962a-4089-ac54-abb60ca2f2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031046302s
Aug 14 15:04:46.317: INFO: Pod "client-containers-70a20549-962a-4089-ac54-abb60ca2f2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.422129472s
Aug 14 15:04:48.321: INFO: Pod "client-containers-70a20549-962a-4089-ac54-abb60ca2f2a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.426583981s
STEP: Saw pod success
Aug 14 15:04:48.322: INFO: Pod "client-containers-70a20549-962a-4089-ac54-abb60ca2f2a2" satisfied condition "success or failure"
Aug 14 15:04:48.325: INFO: Trying to get logs from node slave3 pod client-containers-70a20549-962a-4089-ac54-abb60ca2f2a2 container test-container: <nil>
STEP: delete the pod
Aug 14 15:04:48.600: INFO: Waiting for pod client-containers-70a20549-962a-4089-ac54-abb60ca2f2a2 to disappear
Aug 14 15:04:48.610: INFO: Pod client-containers-70a20549-962a-4089-ac54-abb60ca2f2a2 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:04:48.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-691" for this suite.
Aug 14 15:04:59.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:05:02.255: INFO: namespace containers-691 deletion completed in 13.638397052s

â€¢ [SLOW TEST:25.173 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:05:02.256: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Aug 14 15:05:04.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 cluster-info'
Aug 14 15:05:15.597: INFO: stderr: ""
Aug 14 15:05:15.598: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.150.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://10.150.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.150.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\x1b[0;32mtrigger\x1b[0m is running at \x1b[0;33mhttps://10.150.0.1:443/api/v1/namespaces/kube-system/services/trigger/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:05:15.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5166" for this suite.
Aug 14 15:05:24.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:05:26.258: INFO: namespace kubectl-5166 deletion completed in 10.24905922s

â€¢ [SLOW TEST:24.002 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:05:26.258: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5772
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 15:05:28.253: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 14 15:05:33.359: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 14 15:05:37.474: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 14 15:05:39.546: INFO: Creating deployment "test-rollover-deployment"
Aug 14 15:05:39.798: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 14 15:05:43.808: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 14 15:05:43.845: INFO: Ensure that both replica sets have 1 created replica
Aug 14 15:05:43.854: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 14 15:05:43.958: INFO: Updating deployment test-rollover-deployment
Aug 14 15:05:43.958: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 14 15:05:46.328: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 14 15:05:46.468: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 14 15:05:47.562: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 15:05:47.562: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:0, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391944, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391940, loc:(*time.Location)(0x80bfa40)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-rollover-deployment-854595fc44\""}}, CollisionCount:(*int32)(nil)}
Aug 14 15:05:49.811: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 15:05:49.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391947, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391940, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:05:52.332: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 15:05:52.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391947, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391940, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:05:53.575: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 15:05:53.575: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391947, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391940, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:05:55.573: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 15:05:55.573: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391954, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391940, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:05:57.575: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 15:05:57.575: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391954, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391940, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:05:59.570: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 15:05:59.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391954, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391940, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:06:02.906: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 15:06:02.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391954, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391940, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:06:03.572: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 15:06:03.572: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391954, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391940, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:06:05.968: INFO: 
Aug 14 15:06:05.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391941, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391965, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701391940, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:06:07.593: INFO: 
Aug 14 15:06:07.593: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 14 15:06:07.914: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-5772,SelfLink:/apis/apps/v1/namespaces/deployment-5772/deployments/test-rollover-deployment,UID:608c1203-4c77-4f5c-a2eb-ed601d1c34f8,ResourceVersion:367669,Generation:2,CreationTimestamp:2019-08-14 15:05:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-14 15:05:41 +0000 UTC 2019-08-14 15:05:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-14 15:06:07 +0000 UTC 2019-08-14 15:05:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 14 15:06:08.055: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-5772,SelfLink:/apis/apps/v1/namespaces/deployment-5772/replicasets/test-rollover-deployment-854595fc44,UID:d50ed57f-ef45-42d6-8e95-94d8952adb89,ResourceVersion:367653,Generation:2,CreationTimestamp:2019-08-14 15:05:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 608c1203-4c77-4f5c-a2eb-ed601d1c34f8 0xc001bcfd77 0xc001bcfd78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 14 15:06:08.055: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 14 15:06:08.055: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-5772,SelfLink:/apis/apps/v1/namespaces/deployment-5772/replicasets/test-rollover-controller,UID:8a1b7ec3-c901-4dea-a713-048767226890,ResourceVersion:367664,Generation:2,CreationTimestamp:2019-08-14 15:05:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 608c1203-4c77-4f5c-a2eb-ed601d1c34f8 0xc001bcfca7 0xc001bcfca8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 14 15:06:08.055: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-5772,SelfLink:/apis/apps/v1/namespaces/deployment-5772/replicasets/test-rollover-deployment-9b8b997cf,UID:bbeefa29-389e-48a4-8993-6fe914d3e549,ResourceVersion:367588,Generation:2,CreationTimestamp:2019-08-14 15:05:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 608c1203-4c77-4f5c-a2eb-ed601d1c34f8 0xc001bcfe40 0xc001bcfe41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 14 15:06:08.151: INFO: Pod "test-rollover-deployment-854595fc44-g4gbk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-g4gbk,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-5772,SelfLink:/api/v1/namespaces/deployment-5772/pods/test-rollover-deployment-854595fc44-g4gbk,UID:919bd83d-ab71-4b0b-a471-42e6b223e480,ResourceVersion:367625,Generation:0,CreationTimestamp:2019-08-14 15:05:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 d50ed57f-ef45-42d6-8e95-94d8952adb89 0xc0030ffb57 0xc0030ffb58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bj4jx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bj4jx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-bj4jx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030ffbd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030ffbf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:05:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:05:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:05:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:05:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.67,PodIP:10.151.194.21,StartTime:2019-08-14 15:05:46 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-14 15:05:53 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://60f9dd59ec1757541a5264e1eee0cd85b7a829dd153ba1070324f54072a97a7f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:06:08.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5772" for this suite.
Aug 14 15:06:22.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:06:24.971: INFO: namespace deployment-5772 deletion completed in 16.673404937s

â€¢ [SLOW TEST:58.713 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:06:24.971: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Aug 14 15:06:27.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 create -f - --namespace=kubectl-5112'
Aug 14 15:06:29.492: INFO: stderr: ""
Aug 14 15:06:29.492: INFO: stdout: "pod/pause created\n"
Aug 14 15:06:29.492: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 14 15:06:29.492: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5112" to be "running and ready"
Aug 14 15:06:29.802: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 309.038982ms
Aug 14 15:06:31.806: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.31375548s
Aug 14 15:06:33.903: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.410662634s
Aug 14 15:06:35.907: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.414892916s
Aug 14 15:06:38.261: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.768830694s
Aug 14 15:06:40.267: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 10.774430521s
Aug 14 15:06:40.267: INFO: Pod "pause" satisfied condition "running and ready"
Aug 14 15:06:40.267: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 14 15:06:40.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 label pods pause testing-label=testing-label-value --namespace=kubectl-5112'
Aug 14 15:06:40.665: INFO: stderr: ""
Aug 14 15:06:40.665: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 14 15:06:40.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pod pause -L testing-label --namespace=kubectl-5112'
Aug 14 15:06:41.380: INFO: stderr: ""
Aug 14 15:06:41.380: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          12s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 14 15:06:41.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 label pods pause testing-label- --namespace=kubectl-5112'
Aug 14 15:06:42.696: INFO: stderr: ""
Aug 14 15:06:42.696: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 14 15:06:42.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pod pause -L testing-label --namespace=kubectl-5112'
Aug 14 15:06:43.384: INFO: stderr: ""
Aug 14 15:06:43.384: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          14s   \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Aug 14 15:06:43.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete --grace-period=0 --force -f - --namespace=kubectl-5112'
Aug 14 15:06:43.975: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 15:06:43.975: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 14 15:06:43.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get rc,svc -l name=pause --no-headers --namespace=kubectl-5112'
Aug 14 15:06:44.703: INFO: stderr: "No resources found.\n"
Aug 14 15:06:44.703: INFO: stdout: ""
Aug 14 15:06:44.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -l name=pause --namespace=kubectl-5112 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 14 15:06:44.851: INFO: stderr: ""
Aug 14 15:06:44.851: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:06:44.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5112" for this suite.
Aug 14 15:06:55.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:06:57.009: INFO: namespace kubectl-5112 deletion completed in 12.151570699s

â€¢ [SLOW TEST:32.038 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:06:57.012: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-3144
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3144 to expose endpoints map[]
Aug 14 15:06:59.002: INFO: Get endpoints failed (4.122031ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Aug 14 15:07:00.007: INFO: successfully validated that service endpoint-test2 in namespace services-3144 exposes endpoints map[] (1.009496538s elapsed)
STEP: Creating pod pod1 in namespace services-3144
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3144 to expose endpoints map[pod1:[80]]
Aug 14 15:07:05.242: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (5.041044812s elapsed, will retry)
Aug 14 15:07:09.507: INFO: successfully validated that service endpoint-test2 in namespace services-3144 exposes endpoints map[pod1:[80]] (9.306420442s elapsed)
STEP: Creating pod pod2 in namespace services-3144
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3144 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 14 15:07:13.775: INFO: Unexpected endpoints: found map[1f1e02ff-bf63-4a54-b169-38a8055f3ee2:[80]], expected map[pod1:[80] pod2:[80]] (4.216963807s elapsed, will retry)
Aug 14 15:07:17.881: INFO: successfully validated that service endpoint-test2 in namespace services-3144 exposes endpoints map[pod1:[80] pod2:[80]] (8.323065335s elapsed)
STEP: Deleting pod pod1 in namespace services-3144
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3144 to expose endpoints map[pod2:[80]]
Aug 14 15:07:18.934: INFO: successfully validated that service endpoint-test2 in namespace services-3144 exposes endpoints map[pod2:[80]] (1.03830609s elapsed)
STEP: Deleting pod pod2 in namespace services-3144
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3144 to expose endpoints map[]
Aug 14 15:07:19.282: INFO: successfully validated that service endpoint-test2 in namespace services-3144 exposes endpoints map[] (265.777782ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:07:20.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3144" for this suite.
Aug 14 15:07:52.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:07:54.346: INFO: namespace services-3144 deletion completed in 34.266714109s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

â€¢ [SLOW TEST:57.334 seconds]
[sig-network] Services
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:07:54.346: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7606
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 14 15:07:55.223: INFO: Waiting up to 5m0s for pod "pod-6a2f4233-e3ff-4f7a-8759-d59fe8a57673" in namespace "emptydir-7606" to be "success or failure"
Aug 14 15:07:55.227: INFO: Pod "pod-6a2f4233-e3ff-4f7a-8759-d59fe8a57673": Phase="Pending", Reason="", readiness=false. Elapsed: 3.965142ms
Aug 14 15:07:57.232: INFO: Pod "pod-6a2f4233-e3ff-4f7a-8759-d59fe8a57673": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008709866s
Aug 14 15:07:59.781: INFO: Pod "pod-6a2f4233-e3ff-4f7a-8759-d59fe8a57673": Phase="Pending", Reason="", readiness=false. Elapsed: 4.558331224s
Aug 14 15:08:01.785: INFO: Pod "pod-6a2f4233-e3ff-4f7a-8759-d59fe8a57673": Phase="Pending", Reason="", readiness=false. Elapsed: 6.562367813s
Aug 14 15:08:03.796: INFO: Pod "pod-6a2f4233-e3ff-4f7a-8759-d59fe8a57673": Phase="Pending", Reason="", readiness=false. Elapsed: 8.573140561s
Aug 14 15:08:05.903: INFO: Pod "pod-6a2f4233-e3ff-4f7a-8759-d59fe8a57673": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.679949653s
STEP: Saw pod success
Aug 14 15:08:05.903: INFO: Pod "pod-6a2f4233-e3ff-4f7a-8759-d59fe8a57673" satisfied condition "success or failure"
Aug 14 15:08:05.908: INFO: Trying to get logs from node slave1 pod pod-6a2f4233-e3ff-4f7a-8759-d59fe8a57673 container test-container: <nil>
STEP: delete the pod
Aug 14 15:08:06.311: INFO: Waiting for pod pod-6a2f4233-e3ff-4f7a-8759-d59fe8a57673 to disappear
Aug 14 15:08:06.584: INFO: Pod pod-6a2f4233-e3ff-4f7a-8759-d59fe8a57673 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:08:06.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7606" for this suite.
Aug 14 15:08:17.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:08:19.235: INFO: namespace emptydir-7606 deletion completed in 12.508125143s

â€¢ [SLOW TEST:24.889 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:08:19.236: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 14 15:08:21.746: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3741,SelfLink:/api/v1/namespaces/watch-3741/configmaps/e2e-watch-test-label-changed,UID:7176604d-50d4-4f50-9973-5a7cf70604ea,ResourceVersion:368172,Generation:0,CreationTimestamp:2019-08-14 15:08:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 14 15:08:21.746: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3741,SelfLink:/api/v1/namespaces/watch-3741/configmaps/e2e-watch-test-label-changed,UID:7176604d-50d4-4f50-9973-5a7cf70604ea,ResourceVersion:368173,Generation:0,CreationTimestamp:2019-08-14 15:08:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 14 15:08:21.746: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3741,SelfLink:/api/v1/namespaces/watch-3741/configmaps/e2e-watch-test-label-changed,UID:7176604d-50d4-4f50-9973-5a7cf70604ea,ResourceVersion:368174,Generation:0,CreationTimestamp:2019-08-14 15:08:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 14 15:08:32.822: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3741,SelfLink:/api/v1/namespaces/watch-3741/configmaps/e2e-watch-test-label-changed,UID:7176604d-50d4-4f50-9973-5a7cf70604ea,ResourceVersion:368200,Generation:0,CreationTimestamp:2019-08-14 15:08:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 14 15:08:32.822: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3741,SelfLink:/api/v1/namespaces/watch-3741/configmaps/e2e-watch-test-label-changed,UID:7176604d-50d4-4f50-9973-5a7cf70604ea,ResourceVersion:368202,Generation:0,CreationTimestamp:2019-08-14 15:08:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Aug 14 15:08:32.823: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3741,SelfLink:/api/v1/namespaces/watch-3741/configmaps/e2e-watch-test-label-changed,UID:7176604d-50d4-4f50-9973-5a7cf70604ea,ResourceVersion:368203,Generation:0,CreationTimestamp:2019-08-14 15:08:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:08:32.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3741" for this suite.
Aug 14 15:08:42.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:08:43.923: INFO: namespace watch-3741 deletion completed in 10.48505424s

â€¢ [SLOW TEST:24.686 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:08:43.923: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9942
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 14 15:08:57.680: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:08:58.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9942" for this suite.
Aug 14 15:09:16.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:09:18.386: INFO: namespace container-runtime-9942 deletion completed in 20.172627816s

â€¢ [SLOW TEST:34.463 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:09:18.387: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-152
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 14 15:09:20.907: INFO: Waiting up to 5m0s for pod "pod-85645cd9-7c82-4dd8-a983-66ebcfeb5b89" in namespace "emptydir-152" to be "success or failure"
Aug 14 15:09:21.066: INFO: Pod "pod-85645cd9-7c82-4dd8-a983-66ebcfeb5b89": Phase="Pending", Reason="", readiness=false. Elapsed: 159.013389ms
Aug 14 15:09:23.083: INFO: Pod "pod-85645cd9-7c82-4dd8-a983-66ebcfeb5b89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.17647003s
Aug 14 15:09:25.162: INFO: Pod "pod-85645cd9-7c82-4dd8-a983-66ebcfeb5b89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.255139666s
Aug 14 15:09:27.377: INFO: Pod "pod-85645cd9-7c82-4dd8-a983-66ebcfeb5b89": Phase="Pending", Reason="", readiness=false. Elapsed: 6.470767678s
Aug 14 15:09:29.941: INFO: Pod "pod-85645cd9-7c82-4dd8-a983-66ebcfeb5b89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.033855547s
STEP: Saw pod success
Aug 14 15:09:29.941: INFO: Pod "pod-85645cd9-7c82-4dd8-a983-66ebcfeb5b89" satisfied condition "success or failure"
Aug 14 15:09:29.945: INFO: Trying to get logs from node slave3 pod pod-85645cd9-7c82-4dd8-a983-66ebcfeb5b89 container test-container: <nil>
STEP: delete the pod
Aug 14 15:09:31.018: INFO: Waiting for pod pod-85645cd9-7c82-4dd8-a983-66ebcfeb5b89 to disappear
Aug 14 15:09:31.163: INFO: Pod pod-85645cd9-7c82-4dd8-a983-66ebcfeb5b89 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:09:31.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-152" for this suite.
Aug 14 15:09:43.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:09:45.490: INFO: namespace emptydir-152 deletion completed in 14.321118206s

â€¢ [SLOW TEST:27.103 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:09:45.491: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 15:09:47.179: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39bc3347-da29-490c-b75d-0c8deb38a1e9" in namespace "downward-api-5797" to be "success or failure"
Aug 14 15:09:47.437: INFO: Pod "downwardapi-volume-39bc3347-da29-490c-b75d-0c8deb38a1e9": Phase="Pending", Reason="", readiness=false. Elapsed: 257.826531ms
Aug 14 15:09:49.441: INFO: Pod "downwardapi-volume-39bc3347-da29-490c-b75d-0c8deb38a1e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.262511909s
Aug 14 15:09:51.448: INFO: Pod "downwardapi-volume-39bc3347-da29-490c-b75d-0c8deb38a1e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.269120033s
Aug 14 15:09:53.547: INFO: Pod "downwardapi-volume-39bc3347-da29-490c-b75d-0c8deb38a1e9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.368317469s
Aug 14 15:09:55.552: INFO: Pod "downwardapi-volume-39bc3347-da29-490c-b75d-0c8deb38a1e9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.373289955s
Aug 14 15:09:57.556: INFO: Pod "downwardapi-volume-39bc3347-da29-490c-b75d-0c8deb38a1e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.37743356s
STEP: Saw pod success
Aug 14 15:09:57.556: INFO: Pod "downwardapi-volume-39bc3347-da29-490c-b75d-0c8deb38a1e9" satisfied condition "success or failure"
Aug 14 15:09:57.559: INFO: Trying to get logs from node slave1 pod downwardapi-volume-39bc3347-da29-490c-b75d-0c8deb38a1e9 container client-container: <nil>
STEP: delete the pod
Aug 14 15:09:57.841: INFO: Waiting for pod downwardapi-volume-39bc3347-da29-490c-b75d-0c8deb38a1e9 to disappear
Aug 14 15:09:57.852: INFO: Pod downwardapi-volume-39bc3347-da29-490c-b75d-0c8deb38a1e9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:09:57.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5797" for this suite.
Aug 14 15:10:09.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:10:11.612: INFO: namespace downward-api-5797 deletion completed in 13.753886717s

â€¢ [SLOW TEST:26.121 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:10:11.613: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1010
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-af474015-33a2-4c43-8370-10613da05cf1
STEP: Creating a pod to test consume secrets
Aug 14 15:10:15.943: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-51eb1a7e-7351-4d68-bedf-ae8bb1db3065" in namespace "projected-1010" to be "success or failure"
Aug 14 15:10:15.975: INFO: Pod "pod-projected-secrets-51eb1a7e-7351-4d68-bedf-ae8bb1db3065": Phase="Pending", Reason="", readiness=false. Elapsed: 32.197252ms
Aug 14 15:10:17.980: INFO: Pod "pod-projected-secrets-51eb1a7e-7351-4d68-bedf-ae8bb1db3065": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03675893s
Aug 14 15:10:19.989: INFO: Pod "pod-projected-secrets-51eb1a7e-7351-4d68-bedf-ae8bb1db3065": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045525181s
Aug 14 15:10:22.124: INFO: Pod "pod-projected-secrets-51eb1a7e-7351-4d68-bedf-ae8bb1db3065": Phase="Pending", Reason="", readiness=false. Elapsed: 6.180619853s
Aug 14 15:10:24.287: INFO: Pod "pod-projected-secrets-51eb1a7e-7351-4d68-bedf-ae8bb1db3065": Phase="Pending", Reason="", readiness=false. Elapsed: 8.343852922s
Aug 14 15:10:26.539: INFO: Pod "pod-projected-secrets-51eb1a7e-7351-4d68-bedf-ae8bb1db3065": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.596005117s
STEP: Saw pod success
Aug 14 15:10:26.539: INFO: Pod "pod-projected-secrets-51eb1a7e-7351-4d68-bedf-ae8bb1db3065" satisfied condition "success or failure"
Aug 14 15:10:26.543: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-51eb1a7e-7351-4d68-bedf-ae8bb1db3065 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 14 15:10:27.314: INFO: Waiting for pod pod-projected-secrets-51eb1a7e-7351-4d68-bedf-ae8bb1db3065 to disappear
Aug 14 15:10:27.319: INFO: Pod pod-projected-secrets-51eb1a7e-7351-4d68-bedf-ae8bb1db3065 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:10:27.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1010" for this suite.
Aug 14 15:10:37.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:10:40.397: INFO: namespace projected-1010 deletion completed in 13.070687295s

â€¢ [SLOW TEST:28.785 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:10:40.398: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 14 15:10:42.842: INFO: PodSpec: initContainers in spec.initContainers
Aug 14 15:11:46.943: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1ecbb2cc-e5d6-4ebc-a7ef-9a2309b76eef", GenerateName:"", Namespace:"init-container-4336", SelfLink:"/api/v1/namespaces/init-container-4336/pods/pod-init-1ecbb2cc-e5d6-4ebc-a7ef-9a2309b76eef", UID:"3e3778d0-fc04-4084-a05b-e624678e1323", ResourceVersion:"368876", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63701392242, loc:(*time.Location)(0x80bfa40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"842906887"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-qzgq7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002da2640), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qzgq7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qzgq7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qzgq7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000fa4698), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"slave3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0032f8120), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000fa4720)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000fa4740)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000fa4748), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000fa474c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701392244, loc:(*time.Location)(0x80bfa40)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701392244, loc:(*time.Location)(0x80bfa40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701392244, loc:(*time.Location)(0x80bfa40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701392243, loc:(*time.Location)(0x80bfa40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.202.67", PodIP:"10.151.194.25", StartTime:(*v1.Time)(0xc002ab8b20), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000f31110)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000f311f0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://b0697eaaa86257d6387140bef0dd14471a3f2a64f2600c2b05aa178b596f2254"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002ab8b60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002ab8b40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:11:46.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4336" for this suite.
Aug 14 15:12:15.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:12:17.406: INFO: namespace init-container-4336 deletion completed in 30.446660905s

â€¢ [SLOW TEST:97.008 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:12:17.407: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6136
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-40d5c395-6d53-4a12-921e-ec27ce80da0e
STEP: Creating a pod to test consume secrets
Aug 14 15:12:19.132: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-45a106bf-370c-4f2b-8d8c-67aa39fb312d" in namespace "projected-6136" to be "success or failure"
Aug 14 15:12:19.143: INFO: Pod "pod-projected-secrets-45a106bf-370c-4f2b-8d8c-67aa39fb312d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.94017ms
Aug 14 15:12:21.147: INFO: Pod "pod-projected-secrets-45a106bf-370c-4f2b-8d8c-67aa39fb312d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015625686s
Aug 14 15:12:23.386: INFO: Pod "pod-projected-secrets-45a106bf-370c-4f2b-8d8c-67aa39fb312d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.254540714s
Aug 14 15:12:25.531: INFO: Pod "pod-projected-secrets-45a106bf-370c-4f2b-8d8c-67aa39fb312d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.398903741s
Aug 14 15:12:27.622: INFO: Pod "pod-projected-secrets-45a106bf-370c-4f2b-8d8c-67aa39fb312d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.490410299s
STEP: Saw pod success
Aug 14 15:12:27.622: INFO: Pod "pod-projected-secrets-45a106bf-370c-4f2b-8d8c-67aa39fb312d" satisfied condition "success or failure"
Aug 14 15:12:27.679: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-45a106bf-370c-4f2b-8d8c-67aa39fb312d container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 14 15:12:28.140: INFO: Waiting for pod pod-projected-secrets-45a106bf-370c-4f2b-8d8c-67aa39fb312d to disappear
Aug 14 15:12:28.152: INFO: Pod pod-projected-secrets-45a106bf-370c-4f2b-8d8c-67aa39fb312d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:12:28.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6136" for this suite.
Aug 14 15:12:40.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:12:42.337: INFO: namespace projected-6136 deletion completed in 14.17827479s

â€¢ [SLOW TEST:24.930 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:12:42.337: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Aug 14 15:13:26.544: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:13:26.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0814 15:13:26.544269      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-5423" for this suite.
Aug 14 15:13:52.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:13:54.819: INFO: namespace gc-5423 deletion completed in 28.142217435s

â€¢ [SLOW TEST:72.482 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:13:54.820: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 14 15:13:57.065: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-2990,SelfLink:/api/v1/namespaces/watch-2990/configmaps/e2e-watch-test-resource-version,UID:17376a8f-f5d1-44d5-81c3-ca36b709c5fc,ResourceVersion:369490,Generation:0,CreationTimestamp:2019-08-14 15:13:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 14 15:13:57.065: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-2990,SelfLink:/api/v1/namespaces/watch-2990/configmaps/e2e-watch-test-resource-version,UID:17376a8f-f5d1-44d5-81c3-ca36b709c5fc,ResourceVersion:369491,Generation:0,CreationTimestamp:2019-08-14 15:13:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:13:57.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2990" for this suite.
Aug 14 15:14:07.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:14:10.282: INFO: namespace watch-2990 deletion completed in 13.187431251s

â€¢ [SLOW TEST:15.463 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:14:10.283: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6485
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 14 15:14:21.017: INFO: Successfully updated pod "labelsupdatefe771a28-8d95-42e8-a87c-44deaf129b8e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:14:23.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6485" for this suite.
Aug 14 15:14:53.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:14:55.178: INFO: namespace downward-api-6485 deletion completed in 32.127864632s

â€¢ [SLOW TEST:44.895 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:14:55.179: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6564
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 14 15:14:55.771: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:15:11.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6564" for this suite.
Aug 14 15:15:21.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:15:23.552: INFO: namespace pods-6564 deletion completed in 11.962088724s

â€¢ [SLOW TEST:28.373 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:15:23.552: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-af63aec3-5077-45fc-b20d-299e114aadb1
STEP: Creating a pod to test consume configMaps
Aug 14 15:15:25.478: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-affa9173-79f9-423a-b2b5-ed06b2d74e04" in namespace "projected-7506" to be "success or failure"
Aug 14 15:15:25.673: INFO: Pod "pod-projected-configmaps-affa9173-79f9-423a-b2b5-ed06b2d74e04": Phase="Pending", Reason="", readiness=false. Elapsed: 194.32827ms
Aug 14 15:15:27.678: INFO: Pod "pod-projected-configmaps-affa9173-79f9-423a-b2b5-ed06b2d74e04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.199349884s
Aug 14 15:15:29.693: INFO: Pod "pod-projected-configmaps-affa9173-79f9-423a-b2b5-ed06b2d74e04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.214365629s
Aug 14 15:15:31.697: INFO: Pod "pod-projected-configmaps-affa9173-79f9-423a-b2b5-ed06b2d74e04": Phase="Pending", Reason="", readiness=false. Elapsed: 6.218872386s
Aug 14 15:15:33.792: INFO: Pod "pod-projected-configmaps-affa9173-79f9-423a-b2b5-ed06b2d74e04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.313976357s
STEP: Saw pod success
Aug 14 15:15:33.792: INFO: Pod "pod-projected-configmaps-affa9173-79f9-423a-b2b5-ed06b2d74e04" satisfied condition "success or failure"
Aug 14 15:15:33.797: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-affa9173-79f9-423a-b2b5-ed06b2d74e04 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 15:15:34.209: INFO: Waiting for pod pod-projected-configmaps-affa9173-79f9-423a-b2b5-ed06b2d74e04 to disappear
Aug 14 15:15:34.213: INFO: Pod pod-projected-configmaps-affa9173-79f9-423a-b2b5-ed06b2d74e04 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:15:34.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7506" for this suite.
Aug 14 15:15:44.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:15:46.405: INFO: namespace projected-7506 deletion completed in 12.185624898s

â€¢ [SLOW TEST:22.852 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:15:46.405: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-227
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-hkv4
STEP: Creating a pod to test atomic-volume-subpath
Aug 14 15:15:49.894: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hkv4" in namespace "subpath-227" to be "success or failure"
Aug 14 15:15:50.310: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Pending", Reason="", readiness=false. Elapsed: 415.583911ms
Aug 14 15:15:52.314: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.419995253s
Aug 14 15:15:54.847: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.952541343s
Aug 14 15:15:56.867: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.973006057s
Aug 14 15:15:58.872: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Running", Reason="", readiness=true. Elapsed: 8.978329083s
Aug 14 15:16:00.895: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Running", Reason="", readiness=true. Elapsed: 11.001071288s
Aug 14 15:16:02.902: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Running", Reason="", readiness=true. Elapsed: 13.008188866s
Aug 14 15:16:05.209: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Running", Reason="", readiness=true. Elapsed: 15.315462401s
Aug 14 15:16:07.214: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Running", Reason="", readiness=true. Elapsed: 17.320368156s
Aug 14 15:16:09.288: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Running", Reason="", readiness=true. Elapsed: 19.393634715s
Aug 14 15:16:11.292: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Running", Reason="", readiness=true. Elapsed: 21.398433791s
Aug 14 15:16:13.298: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Running", Reason="", readiness=true. Elapsed: 23.404454891s
Aug 14 15:16:15.505: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Running", Reason="", readiness=true. Elapsed: 25.610684542s
Aug 14 15:16:17.510: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Running", Reason="", readiness=true. Elapsed: 27.616370017s
Aug 14 15:16:19.641: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Running", Reason="", readiness=true. Elapsed: 29.746771275s
Aug 14 15:16:21.710: INFO: Pod "pod-subpath-test-configmap-hkv4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 31.815923832s
STEP: Saw pod success
Aug 14 15:16:21.710: INFO: Pod "pod-subpath-test-configmap-hkv4" satisfied condition "success or failure"
Aug 14 15:16:21.715: INFO: Trying to get logs from node slave3 pod pod-subpath-test-configmap-hkv4 container test-container-subpath-configmap-hkv4: <nil>
STEP: delete the pod
Aug 14 15:16:22.195: INFO: Waiting for pod pod-subpath-test-configmap-hkv4 to disappear
Aug 14 15:16:22.198: INFO: Pod pod-subpath-test-configmap-hkv4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hkv4
Aug 14 15:16:22.198: INFO: Deleting pod "pod-subpath-test-configmap-hkv4" in namespace "subpath-227"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:16:22.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-227" for this suite.
Aug 14 15:16:32.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:16:34.188: INFO: namespace subpath-227 deletion completed in 11.981308213s

â€¢ [SLOW TEST:47.783 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:16:34.188: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 15:16:35.181: INFO: Waiting up to 5m0s for pod "downwardapi-volume-773bf7ca-0cb5-42f0-b551-9a30ce4a8a07" in namespace "projected-6622" to be "success or failure"
Aug 14 15:16:35.564: INFO: Pod "downwardapi-volume-773bf7ca-0cb5-42f0-b551-9a30ce4a8a07": Phase="Pending", Reason="", readiness=false. Elapsed: 382.364757ms
Aug 14 15:16:37.569: INFO: Pod "downwardapi-volume-773bf7ca-0cb5-42f0-b551-9a30ce4a8a07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.387452346s
Aug 14 15:16:39.593: INFO: Pod "downwardapi-volume-773bf7ca-0cb5-42f0-b551-9a30ce4a8a07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.411609842s
Aug 14 15:16:41.639: INFO: Pod "downwardapi-volume-773bf7ca-0cb5-42f0-b551-9a30ce4a8a07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.458351539s
Aug 14 15:16:43.866: INFO: Pod "downwardapi-volume-773bf7ca-0cb5-42f0-b551-9a30ce4a8a07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.685107739s
STEP: Saw pod success
Aug 14 15:16:43.866: INFO: Pod "downwardapi-volume-773bf7ca-0cb5-42f0-b551-9a30ce4a8a07" satisfied condition "success or failure"
Aug 14 15:16:43.872: INFO: Trying to get logs from node slave1 pod downwardapi-volume-773bf7ca-0cb5-42f0-b551-9a30ce4a8a07 container client-container: <nil>
STEP: delete the pod
Aug 14 15:16:44.292: INFO: Waiting for pod downwardapi-volume-773bf7ca-0cb5-42f0-b551-9a30ce4a8a07 to disappear
Aug 14 15:16:44.445: INFO: Pod downwardapi-volume-773bf7ca-0cb5-42f0-b551-9a30ce4a8a07 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:16:44.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6622" for this suite.
Aug 14 15:16:54.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:16:56.743: INFO: namespace projected-6622 deletion completed in 12.283568338s

â€¢ [SLOW TEST:22.554 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:16:56.743: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9302
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 15:16:58.537: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:17:05.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9302" for this suite.
Aug 14 15:17:16.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:17:17.964: INFO: namespace custom-resource-definition-9302 deletion completed in 12.072215786s

â€¢ [SLOW TEST:21.221 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:17:17.965: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-623
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Aug 14 15:17:19.235: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Aug 14 15:17:19.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 create -f - --namespace=kubectl-623'
Aug 14 15:17:31.049: INFO: stderr: ""
Aug 14 15:17:31.049: INFO: stdout: "service/redis-slave created\n"
Aug 14 15:17:31.049: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Aug 14 15:17:31.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 create -f - --namespace=kubectl-623'
Aug 14 15:17:31.934: INFO: stderr: ""
Aug 14 15:17:31.934: INFO: stdout: "service/redis-master created\n"
Aug 14 15:17:31.934: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 14 15:17:31.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 create -f - --namespace=kubectl-623'
Aug 14 15:17:33.172: INFO: stderr: ""
Aug 14 15:17:33.172: INFO: stdout: "service/frontend created\n"
Aug 14 15:17:33.173: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Aug 14 15:17:33.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 create -f - --namespace=kubectl-623'
Aug 14 15:17:34.247: INFO: stderr: ""
Aug 14 15:17:34.247: INFO: stdout: "deployment.apps/frontend created\n"
Aug 14 15:17:34.247: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 14 15:17:34.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 create -f - --namespace=kubectl-623'
Aug 14 15:17:35.605: INFO: stderr: ""
Aug 14 15:17:35.605: INFO: stdout: "deployment.apps/redis-master created\n"
Aug 14 15:17:35.605: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Aug 14 15:17:35.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 create -f - --namespace=kubectl-623'
Aug 14 15:17:37.619: INFO: stderr: ""
Aug 14 15:17:37.619: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Aug 14 15:17:37.619: INFO: Waiting for all frontend pods to be Running.
Aug 14 15:17:47.670: INFO: Waiting for frontend to serve content.
Aug 14 15:17:52.852: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Aug 14 15:17:57.878: INFO: Trying to add a new entry to the guestbook.
Aug 14 15:17:57.896: INFO: Verifying that added entry can be retrieved.
Aug 14 15:17:57.914: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Aug 14 15:18:02.932: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Aug 14 15:18:08.045: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Aug 14 15:18:13.064: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Aug 14 15:18:18.082: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Aug 14 15:18:23.101: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Aug 14 15:18:28.248: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Aug 14 15:18:33.274: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Aug 14 15:18:38.291: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Aug 14 15:18:43.307: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Aug 14 15:18:48.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete --grace-period=0 --force -f - --namespace=kubectl-623'
Aug 14 15:18:49.062: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 15:18:49.062: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug 14 15:18:49.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete --grace-period=0 --force -f - --namespace=kubectl-623'
Aug 14 15:18:50.621: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 15:18:50.621: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 14 15:18:50.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete --grace-period=0 --force -f - --namespace=kubectl-623'
Aug 14 15:18:51.569: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 15:18:51.569: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 14 15:18:51.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete --grace-period=0 --force -f - --namespace=kubectl-623'
Aug 14 15:18:51.935: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 15:18:51.935: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 14 15:18:51.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete --grace-period=0 --force -f - --namespace=kubectl-623'
Aug 14 15:18:53.063: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 15:18:53.063: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 14 15:18:53.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete --grace-period=0 --force -f - --namespace=kubectl-623'
Aug 14 15:18:54.231: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 15:18:54.231: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:18:54.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-623" for this suite.
Aug 14 15:19:33.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:19:35.625: INFO: namespace kubectl-623 deletion completed in 40.765084996s

â€¢ [SLOW TEST:137.660 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:19:35.625: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9284
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:19:46.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9284" for this suite.
Aug 14 15:19:56.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:19:58.179: INFO: namespace emptydir-wrapper-9284 deletion completed in 12.159487107s

â€¢ [SLOW TEST:22.554 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:19:58.180: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3262
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9611
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9072
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:20:43.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3262" for this suite.
Aug 14 15:20:53.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:20:55.642: INFO: namespace namespaces-3262 deletion completed in 11.845229084s
STEP: Destroying namespace "nsdeletetest-9611" for this suite.
Aug 14 15:20:55.649: INFO: Namespace nsdeletetest-9611 was already deleted
STEP: Destroying namespace "nsdeletetest-9072" for this suite.
Aug 14 15:21:05.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:21:08.452: INFO: namespace nsdeletetest-9072 deletion completed in 12.802906027s

â€¢ [SLOW TEST:70.273 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:21:08.452: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 14 15:21:10.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-4489'
Aug 14 15:21:11.213: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 14 15:21:11.213: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Aug 14 15:21:15.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4489'
Aug 14 15:21:16.900: INFO: stderr: ""
Aug 14 15:21:16.900: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:21:16.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4489" for this suite.
Aug 14 15:21:43.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:21:46.042: INFO: namespace kubectl-4489 deletion completed in 28.420594141s

â€¢ [SLOW TEST:37.590 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:21:46.042: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3113
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 14 15:21:47.864: INFO: Waiting up to 5m0s for pod "downward-api-1c2a2038-2187-4b3a-934d-117721a2517b" in namespace "downward-api-3113" to be "success or failure"
Aug 14 15:21:47.872: INFO: Pod "downward-api-1c2a2038-2187-4b3a-934d-117721a2517b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.364748ms
Aug 14 15:21:49.877: INFO: Pod "downward-api-1c2a2038-2187-4b3a-934d-117721a2517b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013526948s
Aug 14 15:21:52.104: INFO: Pod "downward-api-1c2a2038-2187-4b3a-934d-117721a2517b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.240013441s
Aug 14 15:21:54.117: INFO: Pod "downward-api-1c2a2038-2187-4b3a-934d-117721a2517b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.253288576s
Aug 14 15:21:56.251: INFO: Pod "downward-api-1c2a2038-2187-4b3a-934d-117721a2517b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.387609822s
Aug 14 15:21:58.256: INFO: Pod "downward-api-1c2a2038-2187-4b3a-934d-117721a2517b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.392320262s
STEP: Saw pod success
Aug 14 15:21:58.256: INFO: Pod "downward-api-1c2a2038-2187-4b3a-934d-117721a2517b" satisfied condition "success or failure"
Aug 14 15:21:58.260: INFO: Trying to get logs from node slave2 pod downward-api-1c2a2038-2187-4b3a-934d-117721a2517b container dapi-container: <nil>
STEP: delete the pod
Aug 14 15:21:58.638: INFO: Waiting for pod downward-api-1c2a2038-2187-4b3a-934d-117721a2517b to disappear
Aug 14 15:21:58.642: INFO: Pod downward-api-1c2a2038-2187-4b3a-934d-117721a2517b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:21:58.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3113" for this suite.
Aug 14 15:22:08.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:22:11.215: INFO: namespace downward-api-3113 deletion completed in 12.566895269s

â€¢ [SLOW TEST:25.173 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:22:11.217: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3904
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:23:12.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3904" for this suite.
Aug 14 15:23:40.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:23:42.383: INFO: namespace container-probe-3904 deletion completed in 30.188037977s

â€¢ [SLOW TEST:91.166 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:23:42.383: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2385
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Aug 14 15:23:45.248: INFO: Waiting up to 5m0s for pod "var-expansion-475462b3-4d8c-4a78-afb4-e7a686c9f5ea" in namespace "var-expansion-2385" to be "success or failure"
Aug 14 15:23:45.256: INFO: Pod "var-expansion-475462b3-4d8c-4a78-afb4-e7a686c9f5ea": Phase="Pending", Reason="", readiness=false. Elapsed: 7.051573ms
Aug 14 15:23:47.386: INFO: Pod "var-expansion-475462b3-4d8c-4a78-afb4-e7a686c9f5ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137301591s
Aug 14 15:23:49.516: INFO: Pod "var-expansion-475462b3-4d8c-4a78-afb4-e7a686c9f5ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.267292641s
Aug 14 15:23:52.181: INFO: Pod "var-expansion-475462b3-4d8c-4a78-afb4-e7a686c9f5ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.932495618s
STEP: Saw pod success
Aug 14 15:23:52.181: INFO: Pod "var-expansion-475462b3-4d8c-4a78-afb4-e7a686c9f5ea" satisfied condition "success or failure"
Aug 14 15:23:52.243: INFO: Trying to get logs from node slave1 pod var-expansion-475462b3-4d8c-4a78-afb4-e7a686c9f5ea container dapi-container: <nil>
STEP: delete the pod
Aug 14 15:23:52.543: INFO: Waiting for pod var-expansion-475462b3-4d8c-4a78-afb4-e7a686c9f5ea to disappear
Aug 14 15:23:52.551: INFO: Pod var-expansion-475462b3-4d8c-4a78-afb4-e7a686c9f5ea no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:23:52.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2385" for this suite.
Aug 14 15:24:02.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:24:04.458: INFO: namespace var-expansion-2385 deletion completed in 11.898897047s

â€¢ [SLOW TEST:22.075 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:24:04.459: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2384
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2384.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2384.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2384.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2384.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2384.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2384.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2384.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2384.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2384.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2384.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2384.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2384.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2384.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 110.65.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.65.110_udp@PTR;check="$$(dig +tcp +noall +answer +search 110.65.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.65.110_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2384.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2384.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2384.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2384.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2384.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2384.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2384.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2384.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2384.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2384.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2384.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2384.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2384.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 110.65.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.65.110_udp@PTR;check="$$(dig +tcp +noall +answer +search 110.65.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.65.110_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 15:24:21.572: INFO: Unable to read wheezy_udp@dns-test-service.dns-2384.svc.cluster.local from pod dns-2384/dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530: the server could not find the requested resource (get pods dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530)
Aug 14 15:24:21.579: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2384.svc.cluster.local from pod dns-2384/dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530: the server could not find the requested resource (get pods dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530)
Aug 14 15:24:21.594: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2384.svc.cluster.local from pod dns-2384/dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530: the server could not find the requested resource (get pods dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530)
Aug 14 15:24:21.599: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2384.svc.cluster.local from pod dns-2384/dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530: the server could not find the requested resource (get pods dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530)
Aug 14 15:24:21.647: INFO: Unable to read jessie_udp@dns-test-service.dns-2384.svc.cluster.local from pod dns-2384/dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530: the server could not find the requested resource (get pods dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530)
Aug 14 15:24:21.651: INFO: Unable to read jessie_tcp@dns-test-service.dns-2384.svc.cluster.local from pod dns-2384/dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530: the server could not find the requested resource (get pods dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530)
Aug 14 15:24:21.657: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2384.svc.cluster.local from pod dns-2384/dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530: the server could not find the requested resource (get pods dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530)
Aug 14 15:24:21.825: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2384.svc.cluster.local from pod dns-2384/dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530: the server could not find the requested resource (get pods dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530)
Aug 14 15:24:21.851: INFO: Lookups using dns-2384/dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530 failed for: [wheezy_udp@dns-test-service.dns-2384.svc.cluster.local wheezy_tcp@dns-test-service.dns-2384.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2384.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2384.svc.cluster.local jessie_udp@dns-test-service.dns-2384.svc.cluster.local jessie_tcp@dns-test-service.dns-2384.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2384.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2384.svc.cluster.local]

Aug 14 15:24:26.972: INFO: DNS probes using dns-2384/dns-test-50280bc7-5f46-4f76-9ff7-eff6b6aa2530 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:24:28.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2384" for this suite.
Aug 14 15:24:41.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:24:42.969: INFO: namespace dns-2384 deletion completed in 13.986997742s

â€¢ [SLOW TEST:38.510 seconds]
[sig-network] DNS
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:24:42.970: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3303
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Aug 14 15:24:44.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 --namespace=kubectl-3303 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Aug 14 15:24:56.032: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Aug 14 15:24:56.032: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:24:58.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3303" for this suite.
Aug 14 15:25:10.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:25:12.366: INFO: namespace kubectl-3303 deletion completed in 14.320368322s

â€¢ [SLOW TEST:29.396 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:25:12.366: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 14 15:25:14.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-5444'
Aug 14 15:25:15.444: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 14 15:25:15.444: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Aug 14 15:25:17.735: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-8hqvh]
Aug 14 15:25:17.735: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-8hqvh" in namespace "kubectl-5444" to be "running and ready"
Aug 14 15:25:17.739: INFO: Pod "e2e-test-nginx-rc-8hqvh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.950938ms
Aug 14 15:25:19.743: INFO: Pod "e2e-test-nginx-rc-8hqvh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007707902s
Aug 14 15:25:21.832: INFO: Pod "e2e-test-nginx-rc-8hqvh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.09633089s
Aug 14 15:25:23.957: INFO: Pod "e2e-test-nginx-rc-8hqvh": Phase="Running", Reason="", readiness=true. Elapsed: 6.221324405s
Aug 14 15:25:23.957: INFO: Pod "e2e-test-nginx-rc-8hqvh" satisfied condition "running and ready"
Aug 14 15:25:23.957: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-8hqvh]
Aug 14 15:25:23.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 logs rc/e2e-test-nginx-rc --namespace=kubectl-5444'
Aug 14 15:25:24.405: INFO: stderr: ""
Aug 14 15:25:24.405: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Aug 14 15:25:24.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete rc e2e-test-nginx-rc --namespace=kubectl-5444'
Aug 14 15:25:25.202: INFO: stderr: ""
Aug 14 15:25:25.202: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:25:25.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5444" for this suite.
Aug 14 15:25:53.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:25:55.687: INFO: namespace kubectl-5444 deletion completed in 30.463990009s

â€¢ [SLOW TEST:43.321 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:25:55.687: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5599
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 14 15:25:56.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5599'
Aug 14 15:25:57.326: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 14 15:25:57.326: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Aug 14 15:25:59.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete deployment e2e-test-nginx-deployment --namespace=kubectl-5599'
Aug 14 15:25:59.736: INFO: stderr: ""
Aug 14 15:25:59.736: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:25:59.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5599" for this suite.
Aug 14 15:26:12.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:26:14.468: INFO: namespace kubectl-5599 deletion completed in 14.726359298s

â€¢ [SLOW TEST:18.782 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:26:14.480: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8412
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 14 15:26:28.337: INFO: Successfully updated pod "annotationupdate537bb364-c2c1-49c1-959b-343de9bf85e3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:26:30.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8412" for this suite.
Aug 14 15:27:01.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:27:02.796: INFO: namespace downward-api-8412 deletion completed in 32.398861992s

â€¢ [SLOW TEST:48.316 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:27:02.796: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7603
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-e12e9e65-2190-479e-91d1-466e23d136a7
STEP: Creating a pod to test consume secrets
Aug 14 15:27:04.800: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3c79bf64-5c80-4b9f-ac3a-f355c5128d83" in namespace "projected-7603" to be "success or failure"
Aug 14 15:27:04.808: INFO: Pod "pod-projected-secrets-3c79bf64-5c80-4b9f-ac3a-f355c5128d83": Phase="Pending", Reason="", readiness=false. Elapsed: 8.596766ms
Aug 14 15:27:06.879: INFO: Pod "pod-projected-secrets-3c79bf64-5c80-4b9f-ac3a-f355c5128d83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078693146s
Aug 14 15:27:09.104: INFO: Pod "pod-projected-secrets-3c79bf64-5c80-4b9f-ac3a-f355c5128d83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.3040572s
Aug 14 15:27:11.108: INFO: Pod "pod-projected-secrets-3c79bf64-5c80-4b9f-ac3a-f355c5128d83": Phase="Pending", Reason="", readiness=false. Elapsed: 6.308225669s
Aug 14 15:27:13.116: INFO: Pod "pod-projected-secrets-3c79bf64-5c80-4b9f-ac3a-f355c5128d83": Phase="Pending", Reason="", readiness=false. Elapsed: 8.316221081s
Aug 14 15:27:15.305: INFO: Pod "pod-projected-secrets-3c79bf64-5c80-4b9f-ac3a-f355c5128d83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.504804284s
STEP: Saw pod success
Aug 14 15:27:15.305: INFO: Pod "pod-projected-secrets-3c79bf64-5c80-4b9f-ac3a-f355c5128d83" satisfied condition "success or failure"
Aug 14 15:27:15.467: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-3c79bf64-5c80-4b9f-ac3a-f355c5128d83 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 14 15:27:16.295: INFO: Waiting for pod pod-projected-secrets-3c79bf64-5c80-4b9f-ac3a-f355c5128d83 to disappear
Aug 14 15:27:16.481: INFO: Pod pod-projected-secrets-3c79bf64-5c80-4b9f-ac3a-f355c5128d83 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:27:16.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7603" for this suite.
Aug 14 15:27:27.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:27:30.051: INFO: namespace projected-7603 deletion completed in 13.284608968s

â€¢ [SLOW TEST:27.255 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:27:30.052: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Aug 14 15:27:30.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 create -f - --namespace=kubectl-3968'
Aug 14 15:27:37.201: INFO: stderr: ""
Aug 14 15:27:37.201: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Aug 14 15:27:38.206: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:27:38.206: INFO: Found 0 / 1
Aug 14 15:27:39.411: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:27:39.411: INFO: Found 0 / 1
Aug 14 15:27:40.322: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:27:40.322: INFO: Found 0 / 1
Aug 14 15:27:41.206: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:27:41.206: INFO: Found 0 / 1
Aug 14 15:27:42.373: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:27:42.374: INFO: Found 0 / 1
Aug 14 15:27:43.303: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:27:43.303: INFO: Found 0 / 1
Aug 14 15:27:44.291: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:27:44.291: INFO: Found 1 / 1
Aug 14 15:27:44.291: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 14 15:27:44.296: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:27:44.296: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Aug 14 15:27:44.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 logs redis-master-s69j5 redis-master --namespace=kubectl-3968'
Aug 14 15:27:44.944: INFO: stderr: ""
Aug 14 15:27:44.944: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Aug 15:27:42.630 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Aug 15:27:42.630 # Server started, Redis version 3.2.12\n1:M 14 Aug 15:27:42.630 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Aug 15:27:42.630 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Aug 14 15:27:44.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 log redis-master-s69j5 redis-master --namespace=kubectl-3968 --tail=1'
Aug 14 15:27:45.384: INFO: stderr: ""
Aug 14 15:27:45.384: INFO: stdout: "1:M 14 Aug 15:27:42.630 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Aug 14 15:27:45.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 log redis-master-s69j5 redis-master --namespace=kubectl-3968 --limit-bytes=1'
Aug 14 15:27:45.767: INFO: stderr: ""
Aug 14 15:27:45.767: INFO: stdout: " "
STEP: exposing timestamps
Aug 14 15:27:45.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 log redis-master-s69j5 redis-master --namespace=kubectl-3968 --tail=1 --timestamps'
Aug 14 15:27:46.480: INFO: stderr: ""
Aug 14 15:27:46.480: INFO: stdout: "2019-08-14T15:27:42.631466929Z 1:M 14 Aug 15:27:42.630 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Aug 14 15:27:48.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 log redis-master-s69j5 redis-master --namespace=kubectl-3968 --since=1s'
Aug 14 15:27:49.435: INFO: stderr: ""
Aug 14 15:27:49.435: INFO: stdout: ""
Aug 14 15:27:49.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 log redis-master-s69j5 redis-master --namespace=kubectl-3968 --since=24h'
Aug 14 15:27:50.093: INFO: stderr: ""
Aug 14 15:27:50.093: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Aug 15:27:42.630 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Aug 15:27:42.630 # Server started, Redis version 3.2.12\n1:M 14 Aug 15:27:42.630 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Aug 15:27:42.630 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Aug 14 15:27:50.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete --grace-period=0 --force -f - --namespace=kubectl-3968'
Aug 14 15:27:50.965: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 15:27:50.965: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Aug 14 15:27:50.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get rc,svc -l name=nginx --no-headers --namespace=kubectl-3968'
Aug 14 15:27:52.794: INFO: stderr: "No resources found.\n"
Aug 14 15:27:52.794: INFO: stdout: ""
Aug 14 15:27:52.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -l name=nginx --namespace=kubectl-3968 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 14 15:27:53.744: INFO: stderr: ""
Aug 14 15:27:53.744: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:27:53.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3968" for this suite.
Aug 14 15:28:04.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:28:08.629: INFO: namespace kubectl-3968 deletion completed in 14.877578004s

â€¢ [SLOW TEST:38.578 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:28:08.630: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9065
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 14 15:28:21.631: INFO: Successfully updated pod "pod-update-5449fb39-8208-4149-8ecc-5ca9adeee17d"
STEP: verifying the updated pod is in kubernetes
Aug 14 15:28:21.644: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:28:21.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9065" for this suite.
Aug 14 15:28:51.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:28:53.770: INFO: namespace pods-9065 deletion completed in 32.118114357s

â€¢ [SLOW TEST:45.140 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:28:53.772: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9652
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:29:07.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9652" for this suite.
Aug 14 15:29:53.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:29:55.340: INFO: namespace kubelet-test-9652 deletion completed in 48.204479281s

â€¢ [SLOW TEST:61.568 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:29:55.340: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5989
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:30:07.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5989" for this suite.
Aug 14 15:30:51.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:30:53.350: INFO: namespace kubelet-test-5989 deletion completed in 46.206419253s

â€¢ [SLOW TEST:58.010 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:30:53.351: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8008
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-dbe60888-4be2-40de-8014-7ff2e3ff7899
STEP: Creating a pod to test consume configMaps
Aug 14 15:30:55.828: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6cb29ba0-9b93-48ef-9354-62044c9cdd9d" in namespace "projected-8008" to be "success or failure"
Aug 14 15:30:55.832: INFO: Pod "pod-projected-configmaps-6cb29ba0-9b93-48ef-9354-62044c9cdd9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.557867ms
Aug 14 15:30:58.015: INFO: Pod "pod-projected-configmaps-6cb29ba0-9b93-48ef-9354-62044c9cdd9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187576499s
Aug 14 15:31:00.137: INFO: Pod "pod-projected-configmaps-6cb29ba0-9b93-48ef-9354-62044c9cdd9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.309363263s
Aug 14 15:31:02.876: INFO: Pod "pod-projected-configmaps-6cb29ba0-9b93-48ef-9354-62044c9cdd9d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.048350851s
Aug 14 15:31:05.210: INFO: Pod "pod-projected-configmaps-6cb29ba0-9b93-48ef-9354-62044c9cdd9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.382792193s
STEP: Saw pod success
Aug 14 15:31:05.210: INFO: Pod "pod-projected-configmaps-6cb29ba0-9b93-48ef-9354-62044c9cdd9d" satisfied condition "success or failure"
Aug 14 15:31:05.438: INFO: Trying to get logs from node slave3 pod pod-projected-configmaps-6cb29ba0-9b93-48ef-9354-62044c9cdd9d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 15:31:06.338: INFO: Waiting for pod pod-projected-configmaps-6cb29ba0-9b93-48ef-9354-62044c9cdd9d to disappear
Aug 14 15:31:06.829: INFO: Pod pod-projected-configmaps-6cb29ba0-9b93-48ef-9354-62044c9cdd9d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:31:06.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8008" for this suite.
Aug 14 15:31:21.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:31:23.384: INFO: namespace projected-8008 deletion completed in 16.516865496s

â€¢ [SLOW TEST:30.033 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:31:23.384: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5720
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 14 15:31:24.817: INFO: Number of nodes with available pods: 0
Aug 14 15:31:24.817: INFO: Node master1 is running more than one daemon pod
Aug 14 15:31:28.410: INFO: Number of nodes with available pods: 0
Aug 14 15:31:28.410: INFO: Node master1 is running more than one daemon pod
Aug 14 15:31:29.235: INFO: Number of nodes with available pods: 0
Aug 14 15:31:29.235: INFO: Node master1 is running more than one daemon pod
Aug 14 15:31:31.089: INFO: Number of nodes with available pods: 0
Aug 14 15:31:31.091: INFO: Node master1 is running more than one daemon pod
Aug 14 15:31:32.400: INFO: Number of nodes with available pods: 0
Aug 14 15:31:32.400: INFO: Node master1 is running more than one daemon pod
Aug 14 15:31:33.040: INFO: Number of nodes with available pods: 0
Aug 14 15:31:33.042: INFO: Node master1 is running more than one daemon pod
Aug 14 15:31:34.437: INFO: Number of nodes with available pods: 0
Aug 14 15:31:34.437: INFO: Node master1 is running more than one daemon pod
Aug 14 15:31:36.546: INFO: Number of nodes with available pods: 4
Aug 14 15:31:36.546: INFO: Node master1 is running more than one daemon pod
Aug 14 15:31:37.274: INFO: Number of nodes with available pods: 4
Aug 14 15:31:37.274: INFO: Node master1 is running more than one daemon pod
Aug 14 15:31:37.829: INFO: Number of nodes with available pods: 5
Aug 14 15:31:37.829: INFO: Node master2 is running more than one daemon pod
Aug 14 15:31:38.829: INFO: Number of nodes with available pods: 6
Aug 14 15:31:38.829: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 14 15:31:41.398: INFO: Number of nodes with available pods: 5
Aug 14 15:31:41.398: INFO: Node master2 is running more than one daemon pod
Aug 14 15:31:43.544: INFO: Number of nodes with available pods: 5
Aug 14 15:31:43.544: INFO: Node master2 is running more than one daemon pod
Aug 14 15:31:44.530: INFO: Number of nodes with available pods: 5
Aug 14 15:31:44.530: INFO: Node master2 is running more than one daemon pod
Aug 14 15:31:45.544: INFO: Number of nodes with available pods: 5
Aug 14 15:31:45.544: INFO: Node master2 is running more than one daemon pod
Aug 14 15:31:46.636: INFO: Number of nodes with available pods: 5
Aug 14 15:31:46.636: INFO: Node master2 is running more than one daemon pod
Aug 14 15:31:47.492: INFO: Number of nodes with available pods: 5
Aug 14 15:31:47.492: INFO: Node master2 is running more than one daemon pod
Aug 14 15:31:48.409: INFO: Number of nodes with available pods: 5
Aug 14 15:31:48.409: INFO: Node master2 is running more than one daemon pod
Aug 14 15:31:49.614: INFO: Number of nodes with available pods: 5
Aug 14 15:31:49.614: INFO: Node master2 is running more than one daemon pod
Aug 14 15:31:51.295: INFO: Number of nodes with available pods: 5
Aug 14 15:31:51.295: INFO: Node master2 is running more than one daemon pod
Aug 14 15:31:51.875: INFO: Number of nodes with available pods: 5
Aug 14 15:31:51.875: INFO: Node master2 is running more than one daemon pod
Aug 14 15:31:52.416: INFO: Number of nodes with available pods: 5
Aug 14 15:31:52.416: INFO: Node master2 is running more than one daemon pod
Aug 14 15:31:53.415: INFO: Number of nodes with available pods: 6
Aug 14 15:31:53.415: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5720, will wait for the garbage collector to delete the pods
Aug 14 15:31:53.684: INFO: Deleting DaemonSet.extensions daemon-set took: 165.682166ms
Aug 14 15:31:54.684: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000280315s
Aug 14 15:32:17.090: INFO: Number of nodes with available pods: 0
Aug 14 15:32:17.090: INFO: Number of running nodes: 0, number of available pods: 0
Aug 14 15:32:17.095: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5720/daemonsets","resourceVersion":"373563"},"items":null}

Aug 14 15:32:17.352: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5720/pods","resourceVersion":"373563"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:32:17.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5720" for this suite.
Aug 14 15:32:33.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:32:35.449: INFO: namespace daemonsets-5720 deletion completed in 18.052644283s

â€¢ [SLOW TEST:72.065 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:32:35.449: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1985
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 14 15:32:36.847: INFO: Waiting up to 5m0s for pod "pod-96d2b1a2-f88f-45e8-a4c0-1c41f4bc8d96" in namespace "emptydir-1985" to be "success or failure"
Aug 14 15:32:37.192: INFO: Pod "pod-96d2b1a2-f88f-45e8-a4c0-1c41f4bc8d96": Phase="Pending", Reason="", readiness=false. Elapsed: 344.795627ms
Aug 14 15:32:39.229: INFO: Pod "pod-96d2b1a2-f88f-45e8-a4c0-1c41f4bc8d96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.381541655s
Aug 14 15:32:41.871: INFO: Pod "pod-96d2b1a2-f88f-45e8-a4c0-1c41f4bc8d96": Phase="Pending", Reason="", readiness=false. Elapsed: 5.023405586s
Aug 14 15:32:43.879: INFO: Pod "pod-96d2b1a2-f88f-45e8-a4c0-1c41f4bc8d96": Phase="Pending", Reason="", readiness=false. Elapsed: 7.031877402s
Aug 14 15:32:45.883: INFO: Pod "pod-96d2b1a2-f88f-45e8-a4c0-1c41f4bc8d96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.035705688s
STEP: Saw pod success
Aug 14 15:32:45.883: INFO: Pod "pod-96d2b1a2-f88f-45e8-a4c0-1c41f4bc8d96" satisfied condition "success or failure"
Aug 14 15:32:45.887: INFO: Trying to get logs from node slave1 pod pod-96d2b1a2-f88f-45e8-a4c0-1c41f4bc8d96 container test-container: <nil>
STEP: delete the pod
Aug 14 15:32:46.046: INFO: Waiting for pod pod-96d2b1a2-f88f-45e8-a4c0-1c41f4bc8d96 to disappear
Aug 14 15:32:46.050: INFO: Pod pod-96d2b1a2-f88f-45e8-a4c0-1c41f4bc8d96 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:32:46.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1985" for this suite.
Aug 14 15:32:58.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:33:00.010: INFO: namespace emptydir-1985 deletion completed in 13.951322071s

â€¢ [SLOW TEST:24.561 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:33:00.011: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-133
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:33:12.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-133" for this suite.
Aug 14 15:34:04.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:34:06.959: INFO: namespace kubelet-test-133 deletion completed in 54.678832999s

â€¢ [SLOW TEST:66.948 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:34:06.960: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4987
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 15:34:09.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 create -f - --namespace=kubectl-4987'
Aug 14 15:34:10.641: INFO: stderr: ""
Aug 14 15:34:10.641: INFO: stdout: "replicationcontroller/redis-master created\n"
Aug 14 15:34:10.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 create -f - --namespace=kubectl-4987'
Aug 14 15:34:12.215: INFO: stderr: ""
Aug 14 15:34:12.215: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 14 15:34:13.488: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:34:13.488: INFO: Found 0 / 1
Aug 14 15:34:14.315: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:34:14.315: INFO: Found 0 / 1
Aug 14 15:34:15.236: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:34:15.236: INFO: Found 0 / 1
Aug 14 15:34:16.237: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:34:16.237: INFO: Found 0 / 1
Aug 14 15:34:17.228: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:34:17.228: INFO: Found 0 / 1
Aug 14 15:34:18.221: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:34:18.221: INFO: Found 0 / 1
Aug 14 15:34:19.221: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:34:19.221: INFO: Found 0 / 1
Aug 14 15:34:20.321: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:34:20.321: INFO: Found 0 / 1
Aug 14 15:34:21.411: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:34:21.411: INFO: Found 1 / 1
Aug 14 15:34:21.411: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 14 15:34:21.419: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 15:34:21.419: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 14 15:34:21.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 describe pod redis-master-r4vnw --namespace=kubectl-4987'
Aug 14 15:34:22.074: INFO: stderr: ""
Aug 14 15:34:22.074: INFO: stdout: "Name:           redis-master-r4vnw\nNamespace:      kubectl-4987\nPriority:       0\nNode:           slave3/192.168.202.67\nStart Time:     Wed, 14 Aug 2019 15:34:11 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             10.151.194.35\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://737e48ea686ee026f5c7c4cd8876c43f3c82a52fb72f3282738cbecf997d5a80\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 14 Aug 2019 15:34:19 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-6vxmp (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-6vxmp:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-6vxmp\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  11s   default-scheduler  Successfully assigned kubectl-4987/redis-master-r4vnw to slave3\n  Normal  Pulled     6s    kubelet, slave3    Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    4s    kubelet, slave3    Created container redis-master\n  Normal  Started    3s    kubelet, slave3    Started container redis-master\n"
Aug 14 15:34:22.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 describe rc redis-master --namespace=kubectl-4987'
Aug 14 15:34:22.785: INFO: stderr: ""
Aug 14 15:34:22.785: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4987\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  11s   replication-controller  Created pod: redis-master-r4vnw\n"
Aug 14 15:34:22.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 describe service redis-master --namespace=kubectl-4987'
Aug 14 15:34:23.029: INFO: stderr: ""
Aug 14 15:34:23.029: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4987\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.150.143.87\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.151.194.35:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 14 15:34:23.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 describe node master1'
Aug 14 15:34:24.033: INFO: stderr: ""
Aug 14 15:34:24.033: INFO: stdout: "Name:               master1\nRoles:              master,monitor,node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=true\n                    node-role.kubernetes.io/monitor=true\n                    node-role.kubernetes.io/node=true\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\nCreationTimestamp:  Tue, 13 Aug 2019 01:15:02 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 14 Aug 2019 15:33:54 +0000   Tue, 13 Aug 2019 01:15:02 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 14 Aug 2019 15:33:54 +0000   Tue, 13 Aug 2019 01:15:02 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 14 Aug 2019 15:33:54 +0000   Tue, 13 Aug 2019 01:15:02 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 14 Aug 2019 15:33:54 +0000   Tue, 13 Aug 2019 01:16:36 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.202.53\n  Hostname:    master1\nCapacity:\n cpu:                4\n ephemeral-storage:  206357460Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             8167856Ki\n pods:               50\nAllocatable:\n cpu:                3\n ephemeral-storage:  195871700Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             6643568Ki\n pods:               50\nSystem Info:\n Machine ID:                 a87d37f9c4d74e9eb672b97b5dd0c818\n System UUID:                F87EA38D-8FAB-4C84-87D9-4C01E757A1F8\n Boot ID:                    466178bc-9f24-4dc6-bee5-618c7acf55d9\n Kernel Version:             4.15.0-45-generic\n OS Image:                   Ubuntu 18.04.2 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.14.3\n Kube-Proxy Version:         v1.14.3\nNon-terminated Pods:         (18 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-zx5nv    0 (0%)        0 (0%)      0 (0%)           0 (0%)         90m\n  istio-system               istio-egressgateway-68856cf966-k7975                       10m (0%)      0 (0%)      0 (0%)           0 (0%)         3h45m\n  istio-system               istio-ingressgateway-798c45bf9c-qqqcw                      10m (0%)      0 (0%)      0 (0%)           0 (0%)         32h\n  istio-system               istio-policy-5c69497f5-m2kph                               20m (0%)      0 (0%)      0 (0%)           0 (0%)         32h\n  kube-system                calico-node-qjwhs                                          150m (5%)     1 (33%)     64Mi (0%)        4Gi (63%)      38h\n  kube-system                coredns-9c98d6cbb-nz84n                                    100m (3%)     0 (0%)      70Mi (1%)        170Mi (2%)     38h\n  kube-system                dns-autoscaler-6b9bbb69f4-v77pg                            20m (0%)      0 (0%)      10Mi (0%)        0 (0%)         32h\n  kube-system                kube-apiserver-master1                                     100m (3%)     2 (66%)     256Mi (3%)       6Gi (94%)      14h\n  kube-system                kube-controller-manager-master1                            100m (3%)     2 (66%)     100Mi (1%)       6Gi (94%)      14h\n  kube-system                kube-proxy-master1                                         150m (5%)     500m (16%)  64M (0%)         2G (29%)       14h\n  kube-system                kube-scheduler-master1                                     80m (2%)      2 (66%)     170Mi (2%)       6Gi (94%)      14h\n  kube-system                metrics-server-5cb9b96667-8wn6n                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         38h\n  kube-system                nginx-proxy-master1                                        25m (0%)      300m (10%)  32M (0%)         512M (7%)      38h\n  kube-system                resource-reserver-master1                                  800m (26%)    800m (26%)  512Mi (7%)       512Mi (7%)     38h\n  kube-system                vpa-updater-557c96bf46-lhlk7                               50m (1%)      200m (6%)   500Mi (7%)       1000Mi (15%)   32h\n  monitoring                 fluentd-zqgvd                                              0 (0%)        0 (0%)      200Mi (3%)       0 (0%)         38h\n  monitoring                 node-exporter-ht9c6                                        100m (3%)     200m (6%)   30Mi (0%)        50Mi (0%)      38h\n  monitoring                 prometheus-adapter-6798664dcf-2jb4b                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         38h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1715m (57%)      9 (300%)\n  memory             2051638Ki (30%)  27295365Ki (410%)\n  ephemeral-storage  0 (0%)           0 (0%)\nEvents:              <none>\n"
Aug 14 15:34:24.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 describe namespace kubectl-4987'
Aug 14 15:34:24.742: INFO: stderr: ""
Aug 14 15:34:24.742: INFO: stdout: "Name:         kubectl-4987\nLabels:       e2e-framework=kubectl\n              e2e-run=52fe7d63-69df-43b5-b78d-ceeac478c3fc\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:34:24.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4987" for this suite.
Aug 14 15:34:59.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:35:00.998: INFO: namespace kubectl-4987 deletion completed in 36.250241915s

â€¢ [SLOW TEST:54.038 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:35:00.999: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1913
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 15:35:02.305: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f96ae24-78db-4adf-9fb9-9e21bdc56f5b" in namespace "projected-1913" to be "success or failure"
Aug 14 15:35:02.312: INFO: Pod "downwardapi-volume-6f96ae24-78db-4adf-9fb9-9e21bdc56f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.137342ms
Aug 14 15:35:04.524: INFO: Pod "downwardapi-volume-6f96ae24-78db-4adf-9fb9-9e21bdc56f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.21823342s
Aug 14 15:35:06.819: INFO: Pod "downwardapi-volume-6f96ae24-78db-4adf-9fb9-9e21bdc56f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.513304316s
Aug 14 15:35:08.829: INFO: Pod "downwardapi-volume-6f96ae24-78db-4adf-9fb9-9e21bdc56f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.523821298s
Aug 14 15:35:10.833: INFO: Pod "downwardapi-volume-6f96ae24-78db-4adf-9fb9-9e21bdc56f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.528171606s
Aug 14 15:35:12.840: INFO: Pod "downwardapi-volume-6f96ae24-78db-4adf-9fb9-9e21bdc56f5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.534475137s
STEP: Saw pod success
Aug 14 15:35:12.840: INFO: Pod "downwardapi-volume-6f96ae24-78db-4adf-9fb9-9e21bdc56f5b" satisfied condition "success or failure"
Aug 14 15:35:12.846: INFO: Trying to get logs from node slave1 pod downwardapi-volume-6f96ae24-78db-4adf-9fb9-9e21bdc56f5b container client-container: <nil>
STEP: delete the pod
Aug 14 15:35:12.992: INFO: Waiting for pod downwardapi-volume-6f96ae24-78db-4adf-9fb9-9e21bdc56f5b to disappear
Aug 14 15:35:13.000: INFO: Pod downwardapi-volume-6f96ae24-78db-4adf-9fb9-9e21bdc56f5b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:35:13.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1913" for this suite.
Aug 14 15:35:23.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:35:25.442: INFO: namespace projected-1913 deletion completed in 12.434264893s

â€¢ [SLOW TEST:24.444 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:35:25.443: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-7012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Aug 14 15:35:26.948: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Aug 14 15:35:28.247: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 14 15:35:31.959: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:35:33.964: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:35:36.055: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:35:38.027: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:35:39.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:35:41.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:35:43.964: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:35:46.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:35:47.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:35:49.964: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:35:51.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:35:53.964: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:35:55.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:35:58.227: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:35:59.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:36:02.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:36:03.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393729, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701393728, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:36:08.084: INFO: Waited 1.528784732s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:36:14.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7012" for this suite.
Aug 14 15:36:28.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:36:30.506: INFO: namespace aggregator-7012 deletion completed in 16.444562111s

â€¢ [SLOW TEST:65.064 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:36:30.508: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2087
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 15:36:32.788: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 14 15:36:33.148: INFO: Number of nodes with available pods: 0
Aug 14 15:36:33.148: INFO: Node master1 is running more than one daemon pod
Aug 14 15:36:37.392: INFO: Number of nodes with available pods: 0
Aug 14 15:36:37.394: INFO: Node master1 is running more than one daemon pod
Aug 14 15:36:39.143: INFO: Number of nodes with available pods: 0
Aug 14 15:36:39.143: INFO: Node master1 is running more than one daemon pod
Aug 14 15:36:40.715: INFO: Number of nodes with available pods: 0
Aug 14 15:36:40.715: INFO: Node master1 is running more than one daemon pod
Aug 14 15:36:42.637: INFO: Number of nodes with available pods: 0
Aug 14 15:36:42.637: INFO: Node master1 is running more than one daemon pod
Aug 14 15:36:44.071: INFO: Number of nodes with available pods: 0
Aug 14 15:36:44.071: INFO: Node master1 is running more than one daemon pod
Aug 14 15:36:45.249: INFO: Number of nodes with available pods: 0
Aug 14 15:36:45.249: INFO: Node master1 is running more than one daemon pod
Aug 14 15:36:48.292: INFO: Number of nodes with available pods: 1
Aug 14 15:36:48.292: INFO: Node master1 is running more than one daemon pod
Aug 14 15:36:50.226: INFO: Number of nodes with available pods: 4
Aug 14 15:36:50.226: INFO: Node master1 is running more than one daemon pod
Aug 14 15:36:51.267: INFO: Number of nodes with available pods: 6
Aug 14 15:36:51.267: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 14 15:36:51.499: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:51.499: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:51.499: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:51.499: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:51.499: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:51.499: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:53.550: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:53.550: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:53.550: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:53.550: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:53.550: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:53.550: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:54.169: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:54.169: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:54.169: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:54.169: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:54.169: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:54.169: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:54.866: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:54.866: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:54.866: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:54.866: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:54.866: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:54.866: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:55.710: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:55.710: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:55.710: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:55.710: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:55.710: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:55.710: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:56.951: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:56.951: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:56.951: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:56.951: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:56.951: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:56.951: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:57.861: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:57.861: INFO: Pod daemon-set-28vqr is not available
Aug 14 15:36:57.861: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:57.861: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:57.861: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:57.861: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:57.861: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:58.852: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:58.852: INFO: Pod daemon-set-28vqr is not available
Aug 14 15:36:58.852: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:58.852: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:58.852: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:58.852: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:58.852: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:59.712: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:59.712: INFO: Pod daemon-set-28vqr is not available
Aug 14 15:36:59.712: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:59.712: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:59.712: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:59.712: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:36:59.712: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:00.773: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:00.773: INFO: Pod daemon-set-28vqr is not available
Aug 14 15:37:00.773: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:00.773: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:00.773: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:00.773: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:00.773: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:01.706: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:01.706: INFO: Pod daemon-set-28vqr is not available
Aug 14 15:37:01.706: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:01.706: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:01.706: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:01.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:01.706: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:02.717: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:02.717: INFO: Pod daemon-set-28vqr is not available
Aug 14 15:37:02.717: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:02.717: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:02.717: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:02.717: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:02.717: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:03.706: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:03.706: INFO: Pod daemon-set-28vqr is not available
Aug 14 15:37:03.706: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:03.706: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:03.706: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:03.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:03.706: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:04.707: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:04.707: INFO: Pod daemon-set-28vqr is not available
Aug 14 15:37:04.707: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:04.707: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:04.707: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:04.707: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:04.707: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:05.816: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:05.816: INFO: Pod daemon-set-28vqr is not available
Aug 14 15:37:05.816: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:05.816: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:05.816: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:05.816: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:05.816: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:06.710: INFO: Wrong image for pod: daemon-set-28vqr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:06.710: INFO: Pod daemon-set-28vqr is not available
Aug 14 15:37:06.710: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:06.710: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:06.710: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:06.710: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:06.710: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:07.706: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:07.706: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:07.706: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:07.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:07.706: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:08.830: INFO: Pod daemon-set-4k9v7 is not available
Aug 14 15:37:08.830: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:08.830: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:08.830: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:08.830: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:08.830: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:09.735: INFO: Pod daemon-set-4k9v7 is not available
Aug 14 15:37:09.735: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:09.735: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:09.735: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:09.735: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:09.735: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:11.119: INFO: Pod daemon-set-4k9v7 is not available
Aug 14 15:37:11.119: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:11.119: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:11.119: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:11.119: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:11.119: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:11.894: INFO: Pod daemon-set-4k9v7 is not available
Aug 14 15:37:11.895: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:11.895: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:11.895: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:11.895: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:11.895: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:12.804: INFO: Pod daemon-set-4k9v7 is not available
Aug 14 15:37:12.804: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:12.804: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:12.804: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:12.804: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:12.804: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:13.791: INFO: Pod daemon-set-4k9v7 is not available
Aug 14 15:37:13.791: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:13.791: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:13.791: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:13.791: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:13.791: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:14.708: INFO: Pod daemon-set-4k9v7 is not available
Aug 14 15:37:14.708: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:14.708: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:14.708: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:14.708: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:14.708: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:16.480: INFO: Pod daemon-set-4k9v7 is not available
Aug 14 15:37:16.480: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:16.480: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:16.480: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:16.480: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:16.480: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:17.706: INFO: Pod daemon-set-4k9v7 is not available
Aug 14 15:37:17.706: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:17.706: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:17.706: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:17.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:17.706: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:18.985: INFO: Pod daemon-set-4k9v7 is not available
Aug 14 15:37:18.985: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:18.985: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:18.985: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:18.985: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:18.985: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:19.845: INFO: Pod daemon-set-4k9v7 is not available
Aug 14 15:37:19.845: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:19.845: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:19.845: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:19.845: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:19.845: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:20.753: INFO: Pod daemon-set-4k9v7 is not available
Aug 14 15:37:20.753: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:20.753: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:20.753: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:20.753: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:20.753: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:22.023: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:22.023: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:22.023: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:22.023: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:22.023: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:22.811: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:22.811: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:22.811: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:22.811: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:22.811: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:23.708: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:23.708: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:23.708: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:23.708: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:23.708: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:24.707: INFO: Wrong image for pod: daemon-set-5ddng. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:24.707: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:24.707: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:24.707: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:24.707: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:25.916: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:25.916: INFO: Pod daemon-set-jd8b6 is not available
Aug 14 15:37:25.916: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:25.916: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:25.916: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:26.816: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:26.816: INFO: Pod daemon-set-jd8b6 is not available
Aug 14 15:37:26.816: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:26.816: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:26.816: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:27.705: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:27.705: INFO: Pod daemon-set-jd8b6 is not available
Aug 14 15:37:27.705: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:27.705: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:27.705: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:28.710: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:28.710: INFO: Pod daemon-set-jd8b6 is not available
Aug 14 15:37:28.710: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:28.710: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:28.710: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:30.073: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:30.073: INFO: Pod daemon-set-jd8b6 is not available
Aug 14 15:37:30.073: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:30.073: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:30.073: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:30.860: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:30.860: INFO: Pod daemon-set-jd8b6 is not available
Aug 14 15:37:30.860: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:30.860: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:30.860: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:31.705: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:31.706: INFO: Pod daemon-set-jd8b6 is not available
Aug 14 15:37:31.706: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:31.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:31.706: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:32.724: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:32.724: INFO: Pod daemon-set-jd8b6 is not available
Aug 14 15:37:32.724: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:32.724: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:32.724: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:33.706: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:33.706: INFO: Pod daemon-set-jd8b6 is not available
Aug 14 15:37:33.706: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:33.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:33.706: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:35.687: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:35.687: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:35.687: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:35.687: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:36.802: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:36.803: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:36.803: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:36.803: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:37.715: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:37.715: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:37.715: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:37.715: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:38.718: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:38.718: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:38.718: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:38.718: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:39.706: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:39.706: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:39.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:39.706: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:40.707: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:40.707: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:40.707: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:40.707: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:41.709: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:41.709: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:41.709: INFO: Pod daemon-set-s9hcc is not available
Aug 14 15:37:41.709: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:41.709: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:42.706: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:42.706: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:42.706: INFO: Pod daemon-set-s9hcc is not available
Aug 14 15:37:42.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:42.706: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:43.708: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:43.708: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:43.708: INFO: Pod daemon-set-s9hcc is not available
Aug 14 15:37:43.708: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:43.708: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:44.710: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:44.710: INFO: Wrong image for pod: daemon-set-s9hcc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:44.710: INFO: Pod daemon-set-s9hcc is not available
Aug 14 15:37:44.710: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:44.710: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:45.944: INFO: Pod daemon-set-8btcz is not available
Aug 14 15:37:45.945: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:45.945: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:45.945: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:46.740: INFO: Pod daemon-set-8btcz is not available
Aug 14 15:37:46.740: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:46.740: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:46.740: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:47.709: INFO: Pod daemon-set-8btcz is not available
Aug 14 15:37:47.709: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:47.709: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:47.709: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:49.251: INFO: Pod daemon-set-8btcz is not available
Aug 14 15:37:49.251: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:49.251: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:49.251: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:49.756: INFO: Pod daemon-set-8btcz is not available
Aug 14 15:37:49.757: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:49.757: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:49.757: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:50.712: INFO: Pod daemon-set-8btcz is not available
Aug 14 15:37:50.712: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:50.712: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:50.712: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:52.344: INFO: Pod daemon-set-8btcz is not available
Aug 14 15:37:52.344: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:52.344: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:52.344: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:53.093: INFO: Pod daemon-set-8btcz is not available
Aug 14 15:37:53.093: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:53.093: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:53.093: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:53.841: INFO: Pod daemon-set-8btcz is not available
Aug 14 15:37:53.841: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:53.841: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:53.841: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:54.742: INFO: Pod daemon-set-8btcz is not available
Aug 14 15:37:54.742: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:54.742: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:54.742: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:55.707: INFO: Pod daemon-set-8btcz is not available
Aug 14 15:37:55.707: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:55.707: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:55.707: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:56.762: INFO: Pod daemon-set-8btcz is not available
Aug 14 15:37:56.762: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:56.762: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:56.762: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:57.971: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:57.971: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:57.971: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:58.711: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:58.711: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:58.711: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:59.727: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:59.727: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:37:59.727: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:00.708: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:00.708: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:00.708: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:01.706: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:01.707: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:01.707: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:02.968: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:02.968: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:02.968: INFO: Wrong image for pod: daemon-set-zzdzr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:02.968: INFO: Pod daemon-set-zzdzr is not available
Aug 14 15:38:04.028: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:04.029: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:04.789: INFO: Pod daemon-set-c7grs is not available
Aug 14 15:38:04.789: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:04.789: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:05.707: INFO: Pod daemon-set-c7grs is not available
Aug 14 15:38:05.707: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:05.707: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:06.708: INFO: Pod daemon-set-c7grs is not available
Aug 14 15:38:06.708: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:06.708: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:07.759: INFO: Pod daemon-set-c7grs is not available
Aug 14 15:38:07.759: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:07.759: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:08.907: INFO: Pod daemon-set-c7grs is not available
Aug 14 15:38:08.907: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:08.907: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:09.917: INFO: Pod daemon-set-c7grs is not available
Aug 14 15:38:09.917: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:09.917: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:10.706: INFO: Pod daemon-set-c7grs is not available
Aug 14 15:38:10.706: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:10.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:12.045: INFO: Pod daemon-set-c7grs is not available
Aug 14 15:38:12.045: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:12.045: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:12.706: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:12.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:14.410: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:14.410: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:15.016: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:15.016: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:15.706: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:15.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:16.706: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:16.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:17.787: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:17.787: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:18.712: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:18.712: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:19.812: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:19.812: INFO: Pod daemon-set-gj72b is not available
Aug 14 15:38:19.812: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:21.073: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:21.073: INFO: Pod daemon-set-gj72b is not available
Aug 14 15:38:21.073: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:21.932: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:21.932: INFO: Pod daemon-set-gj72b is not available
Aug 14 15:38:21.932: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:22.706: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:22.706: INFO: Pod daemon-set-gj72b is not available
Aug 14 15:38:22.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:23.709: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:23.709: INFO: Pod daemon-set-gj72b is not available
Aug 14 15:38:23.709: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:25.058: INFO: Wrong image for pod: daemon-set-gj72b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:25.058: INFO: Pod daemon-set-gj72b is not available
Aug 14 15:38:25.058: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:25.887: INFO: Pod daemon-set-68sz7 is not available
Aug 14 15:38:25.887: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:26.878: INFO: Pod daemon-set-68sz7 is not available
Aug 14 15:38:26.878: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:27.710: INFO: Pod daemon-set-68sz7 is not available
Aug 14 15:38:27.710: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:28.709: INFO: Pod daemon-set-68sz7 is not available
Aug 14 15:38:28.709: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:29.713: INFO: Pod daemon-set-68sz7 is not available
Aug 14 15:38:29.713: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:30.706: INFO: Pod daemon-set-68sz7 is not available
Aug 14 15:38:30.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:31.707: INFO: Pod daemon-set-68sz7 is not available
Aug 14 15:38:31.707: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:32.706: INFO: Pod daemon-set-68sz7 is not available
Aug 14 15:38:32.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:33.706: INFO: Pod daemon-set-68sz7 is not available
Aug 14 15:38:33.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:34.710: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:35.818: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:36.907: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:37.794: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:38.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:39.705: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:39.705: INFO: Pod daemon-set-x9tpg is not available
Aug 14 15:38:40.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:40.706: INFO: Pod daemon-set-x9tpg is not available
Aug 14 15:38:41.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:41.706: INFO: Pod daemon-set-x9tpg is not available
Aug 14 15:38:42.707: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:42.707: INFO: Pod daemon-set-x9tpg is not available
Aug 14 15:38:43.746: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:43.746: INFO: Pod daemon-set-x9tpg is not available
Aug 14 15:38:44.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:44.706: INFO: Pod daemon-set-x9tpg is not available
Aug 14 15:38:45.705: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:45.705: INFO: Pod daemon-set-x9tpg is not available
Aug 14 15:38:46.727: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:46.727: INFO: Pod daemon-set-x9tpg is not available
Aug 14 15:38:47.707: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:47.707: INFO: Pod daemon-set-x9tpg is not available
Aug 14 15:38:48.709: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:48.709: INFO: Pod daemon-set-x9tpg is not available
Aug 14 15:38:49.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:49.707: INFO: Pod daemon-set-x9tpg is not available
Aug 14 15:38:50.706: INFO: Wrong image for pod: daemon-set-x9tpg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 15:38:50.706: INFO: Pod daemon-set-x9tpg is not available
Aug 14 15:38:52.244: INFO: Pod daemon-set-ldttz is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 14 15:38:52.265: INFO: Number of nodes with available pods: 5
Aug 14 15:38:52.265: INFO: Node slave3 is running more than one daemon pod
Aug 14 15:38:53.277: INFO: Number of nodes with available pods: 5
Aug 14 15:38:53.277: INFO: Node slave3 is running more than one daemon pod
Aug 14 15:38:54.276: INFO: Number of nodes with available pods: 5
Aug 14 15:38:54.276: INFO: Node slave3 is running more than one daemon pod
Aug 14 15:38:55.276: INFO: Number of nodes with available pods: 5
Aug 14 15:38:55.276: INFO: Node slave3 is running more than one daemon pod
Aug 14 15:38:56.293: INFO: Number of nodes with available pods: 5
Aug 14 15:38:56.294: INFO: Node slave3 is running more than one daemon pod
Aug 14 15:38:57.284: INFO: Number of nodes with available pods: 5
Aug 14 15:38:57.284: INFO: Node slave3 is running more than one daemon pod
Aug 14 15:38:58.279: INFO: Number of nodes with available pods: 5
Aug 14 15:38:58.279: INFO: Node slave3 is running more than one daemon pod
Aug 14 15:38:59.381: INFO: Number of nodes with available pods: 6
Aug 14 15:38:59.381: INFO: Number of running nodes: 6, number of available pods: 6
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2087, will wait for the garbage collector to delete the pods
Aug 14 15:38:59.705: INFO: Deleting DaemonSet.extensions daemon-set took: 198.955727ms
Aug 14 15:39:00.606: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.25943ms
Aug 14 15:39:15.133: INFO: Number of nodes with available pods: 0
Aug 14 15:39:15.133: INFO: Number of running nodes: 0, number of available pods: 0
Aug 14 15:39:15.234: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2087/daemonsets","resourceVersion":"375193"},"items":null}

Aug 14 15:39:15.240: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2087/pods","resourceVersion":"375194"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:39:15.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2087" for this suite.
Aug 14 15:39:31.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:39:33.072: INFO: namespace daemonsets-2087 deletion completed in 17.800726813s

â€¢ [SLOW TEST:182.564 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:39:33.072: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 15:39:35.092: INFO: Create a RollingUpdate DaemonSet
Aug 14 15:39:35.099: INFO: Check that daemon pods launch on every node of the cluster
Aug 14 15:39:35.262: INFO: Number of nodes with available pods: 0
Aug 14 15:39:35.262: INFO: Node master1 is running more than one daemon pod
Aug 14 15:39:38.543: INFO: Number of nodes with available pods: 0
Aug 14 15:39:38.543: INFO: Node master1 is running more than one daemon pod
Aug 14 15:39:40.879: INFO: Number of nodes with available pods: 0
Aug 14 15:39:40.879: INFO: Node master1 is running more than one daemon pod
Aug 14 15:39:41.514: INFO: Number of nodes with available pods: 0
Aug 14 15:39:41.514: INFO: Node master1 is running more than one daemon pod
Aug 14 15:39:43.234: INFO: Number of nodes with available pods: 0
Aug 14 15:39:43.234: INFO: Node master1 is running more than one daemon pod
Aug 14 15:39:43.781: INFO: Number of nodes with available pods: 0
Aug 14 15:39:43.781: INFO: Node master1 is running more than one daemon pod
Aug 14 15:39:45.503: INFO: Number of nodes with available pods: 0
Aug 14 15:39:45.503: INFO: Node master1 is running more than one daemon pod
Aug 14 15:39:48.240: INFO: Number of nodes with available pods: 2
Aug 14 15:39:48.240: INFO: Node master2 is running more than one daemon pod
Aug 14 15:39:48.274: INFO: Number of nodes with available pods: 3
Aug 14 15:39:48.274: INFO: Node master2 is running more than one daemon pod
Aug 14 15:39:49.793: INFO: Number of nodes with available pods: 3
Aug 14 15:39:49.793: INFO: Node master2 is running more than one daemon pod
Aug 14 15:39:50.555: INFO: Number of nodes with available pods: 5
Aug 14 15:39:50.555: INFO: Node master2 is running more than one daemon pod
Aug 14 15:39:51.272: INFO: Number of nodes with available pods: 5
Aug 14 15:39:51.272: INFO: Node master2 is running more than one daemon pod
Aug 14 15:39:52.273: INFO: Number of nodes with available pods: 6
Aug 14 15:39:52.273: INFO: Number of running nodes: 6, number of available pods: 6
Aug 14 15:39:52.273: INFO: Update the DaemonSet to trigger a rollout
Aug 14 15:39:52.341: INFO: Updating DaemonSet daemon-set
Aug 14 15:40:03.017: INFO: Roll back the DaemonSet before rollout is complete
Aug 14 15:40:03.259: INFO: Updating DaemonSet daemon-set
Aug 14 15:40:03.259: INFO: Make sure DaemonSet rollback is complete
Aug 14 15:40:03.266: INFO: Wrong image for pod: daemon-set-h499j. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 14 15:40:03.266: INFO: Pod daemon-set-h499j is not available
Aug 14 15:40:04.986: INFO: Wrong image for pod: daemon-set-h499j. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 14 15:40:04.986: INFO: Pod daemon-set-h499j is not available
Aug 14 15:40:05.618: INFO: Wrong image for pod: daemon-set-h499j. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 14 15:40:05.618: INFO: Pod daemon-set-h499j is not available
Aug 14 15:40:06.742: INFO: Pod daemon-set-pfj4x is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8609, will wait for the garbage collector to delete the pods
Aug 14 15:40:09.904: INFO: Deleting DaemonSet.extensions daemon-set took: 1.617825126s
Aug 14 15:40:11.804: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.900278496s
Aug 14 15:40:31.623: INFO: Number of nodes with available pods: 0
Aug 14 15:40:31.623: INFO: Number of running nodes: 0, number of available pods: 0
Aug 14 15:40:31.627: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8609/daemonsets","resourceVersion":"375617"},"items":null}

Aug 14 15:40:31.630: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8609/pods","resourceVersion":"375617"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:40:31.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8609" for this suite.
Aug 14 15:40:45.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:40:47.564: INFO: namespace daemonsets-8609 deletion completed in 15.897330354s

â€¢ [SLOW TEST:74.492 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:40:47.564: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-3229ca7b-8001-4084-ad50-a2de7689578e
STEP: Creating a pod to test consume configMaps
Aug 14 15:40:49.672: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2e68af46-35e9-45b0-ba36-c105e1308736" in namespace "projected-846" to be "success or failure"
Aug 14 15:40:49.898: INFO: Pod "pod-projected-configmaps-2e68af46-35e9-45b0-ba36-c105e1308736": Phase="Pending", Reason="", readiness=false. Elapsed: 225.215616ms
Aug 14 15:40:51.904: INFO: Pod "pod-projected-configmaps-2e68af46-35e9-45b0-ba36-c105e1308736": Phase="Pending", Reason="", readiness=false. Elapsed: 2.231710024s
Aug 14 15:40:53.911: INFO: Pod "pod-projected-configmaps-2e68af46-35e9-45b0-ba36-c105e1308736": Phase="Pending", Reason="", readiness=false. Elapsed: 4.238132097s
Aug 14 15:40:55.917: INFO: Pod "pod-projected-configmaps-2e68af46-35e9-45b0-ba36-c105e1308736": Phase="Pending", Reason="", readiness=false. Elapsed: 6.245011362s
Aug 14 15:40:57.985: INFO: Pod "pod-projected-configmaps-2e68af46-35e9-45b0-ba36-c105e1308736": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.312720566s
STEP: Saw pod success
Aug 14 15:40:57.985: INFO: Pod "pod-projected-configmaps-2e68af46-35e9-45b0-ba36-c105e1308736" satisfied condition "success or failure"
Aug 14 15:40:58.282: INFO: Trying to get logs from node slave3 pod pod-projected-configmaps-2e68af46-35e9-45b0-ba36-c105e1308736 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 15:40:58.973: INFO: Waiting for pod pod-projected-configmaps-2e68af46-35e9-45b0-ba36-c105e1308736 to disappear
Aug 14 15:40:58.984: INFO: Pod pod-projected-configmaps-2e68af46-35e9-45b0-ba36-c105e1308736 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:40:58.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-846" for this suite.
Aug 14 15:41:10.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:41:12.624: INFO: namespace projected-846 deletion completed in 13.258167062s

â€¢ [SLOW TEST:25.060 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:41:12.625: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4476
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-1dcd99e7-94ae-43f4-9ca6-51506182aff6
STEP: Creating a pod to test consume secrets
Aug 14 15:41:14.971: INFO: Waiting up to 5m0s for pod "pod-secrets-dc003bcc-75f0-4f04-8358-37e8ac299038" in namespace "secrets-4476" to be "success or failure"
Aug 14 15:41:14.991: INFO: Pod "pod-secrets-dc003bcc-75f0-4f04-8358-37e8ac299038": Phase="Pending", Reason="", readiness=false. Elapsed: 19.622868ms
Aug 14 15:41:17.048: INFO: Pod "pod-secrets-dc003bcc-75f0-4f04-8358-37e8ac299038": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076889335s
Aug 14 15:41:19.112: INFO: Pod "pod-secrets-dc003bcc-75f0-4f04-8358-37e8ac299038": Phase="Pending", Reason="", readiness=false. Elapsed: 4.140820337s
Aug 14 15:41:21.176: INFO: Pod "pod-secrets-dc003bcc-75f0-4f04-8358-37e8ac299038": Phase="Pending", Reason="", readiness=false. Elapsed: 6.20497941s
Aug 14 15:41:23.181: INFO: Pod "pod-secrets-dc003bcc-75f0-4f04-8358-37e8ac299038": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.209963164s
STEP: Saw pod success
Aug 14 15:41:23.181: INFO: Pod "pod-secrets-dc003bcc-75f0-4f04-8358-37e8ac299038" satisfied condition "success or failure"
Aug 14 15:41:23.185: INFO: Trying to get logs from node slave1 pod pod-secrets-dc003bcc-75f0-4f04-8358-37e8ac299038 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 15:41:23.431: INFO: Waiting for pod pod-secrets-dc003bcc-75f0-4f04-8358-37e8ac299038 to disappear
Aug 14 15:41:23.436: INFO: Pod pod-secrets-dc003bcc-75f0-4f04-8358-37e8ac299038 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:41:23.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4476" for this suite.
Aug 14 15:41:33.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:41:35.643: INFO: namespace secrets-4476 deletion completed in 12.196769399s

â€¢ [SLOW TEST:23.018 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:41:35.643: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 14 15:41:37.515: INFO: Waiting up to 5m0s for pod "pod-2cc88828-5a58-4605-ac89-d92c2e3f23bb" in namespace "emptydir-382" to be "success or failure"
Aug 14 15:41:37.752: INFO: Pod "pod-2cc88828-5a58-4605-ac89-d92c2e3f23bb": Phase="Pending", Reason="", readiness=false. Elapsed: 237.078066ms
Aug 14 15:41:39.759: INFO: Pod "pod-2cc88828-5a58-4605-ac89-d92c2e3f23bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.24412508s
Aug 14 15:41:41.886: INFO: Pod "pod-2cc88828-5a58-4605-ac89-d92c2e3f23bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.371533805s
Aug 14 15:41:43.982: INFO: Pod "pod-2cc88828-5a58-4605-ac89-d92c2e3f23bb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.46713554s
Aug 14 15:41:45.998: INFO: Pod "pod-2cc88828-5a58-4605-ac89-d92c2e3f23bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.483002248s
STEP: Saw pod success
Aug 14 15:41:45.998: INFO: Pod "pod-2cc88828-5a58-4605-ac89-d92c2e3f23bb" satisfied condition "success or failure"
Aug 14 15:41:46.003: INFO: Trying to get logs from node slave2 pod pod-2cc88828-5a58-4605-ac89-d92c2e3f23bb container test-container: <nil>
STEP: delete the pod
Aug 14 15:41:46.890: INFO: Waiting for pod pod-2cc88828-5a58-4605-ac89-d92c2e3f23bb to disappear
Aug 14 15:41:47.063: INFO: Pod pod-2cc88828-5a58-4605-ac89-d92c2e3f23bb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:41:47.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-382" for this suite.
Aug 14 15:41:57.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:41:59.461: INFO: namespace emptydir-382 deletion completed in 12.09664719s

â€¢ [SLOW TEST:23.818 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:41:59.461: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5216
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 15:42:25.443: INFO: Container started at 2019-08-14 15:42:06 +0000 UTC, pod became ready at 2019-08-14 15:42:24 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:42:25.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5216" for this suite.
Aug 14 15:42:53.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:42:55.453: INFO: namespace container-probe-5216 deletion completed in 30.003841483s

â€¢ [SLOW TEST:55.992 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:42:55.454: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3691
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 15:42:56.482: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef93144d-4a85-49f4-85ff-4c34ea603963" in namespace "projected-3691" to be "success or failure"
Aug 14 15:42:56.871: INFO: Pod "downwardapi-volume-ef93144d-4a85-49f4-85ff-4c34ea603963": Phase="Pending", Reason="", readiness=false. Elapsed: 388.877356ms
Aug 14 15:42:58.880: INFO: Pod "downwardapi-volume-ef93144d-4a85-49f4-85ff-4c34ea603963": Phase="Pending", Reason="", readiness=false. Elapsed: 2.39824686s
Aug 14 15:43:01.275: INFO: Pod "downwardapi-volume-ef93144d-4a85-49f4-85ff-4c34ea603963": Phase="Pending", Reason="", readiness=false. Elapsed: 4.793466889s
Aug 14 15:43:03.280: INFO: Pod "downwardapi-volume-ef93144d-4a85-49f4-85ff-4c34ea603963": Phase="Pending", Reason="", readiness=false. Elapsed: 6.797568193s
Aug 14 15:43:05.402: INFO: Pod "downwardapi-volume-ef93144d-4a85-49f4-85ff-4c34ea603963": Phase="Pending", Reason="", readiness=false. Elapsed: 8.919978466s
Aug 14 15:43:07.408: INFO: Pod "downwardapi-volume-ef93144d-4a85-49f4-85ff-4c34ea603963": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.926360576s
STEP: Saw pod success
Aug 14 15:43:07.408: INFO: Pod "downwardapi-volume-ef93144d-4a85-49f4-85ff-4c34ea603963" satisfied condition "success or failure"
Aug 14 15:43:07.415: INFO: Trying to get logs from node slave1 pod downwardapi-volume-ef93144d-4a85-49f4-85ff-4c34ea603963 container client-container: <nil>
STEP: delete the pod
Aug 14 15:43:07.762: INFO: Waiting for pod downwardapi-volume-ef93144d-4a85-49f4-85ff-4c34ea603963 to disappear
Aug 14 15:43:07.767: INFO: Pod downwardapi-volume-ef93144d-4a85-49f4-85ff-4c34ea603963 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:43:07.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3691" for this suite.
Aug 14 15:43:16.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:43:17.873: INFO: namespace projected-3691 deletion completed in 10.095638959s

â€¢ [SLOW TEST:22.419 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:43:17.873: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8953
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-089c8f28-e299-425f-a761-44686e7ccaba
STEP: Creating a pod to test consume configMaps
Aug 14 15:43:18.979: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bdbd3c02-0d8d-4c26-8c0a-26871d571bf4" in namespace "projected-8953" to be "success or failure"
Aug 14 15:43:19.177: INFO: Pod "pod-projected-configmaps-bdbd3c02-0d8d-4c26-8c0a-26871d571bf4": Phase="Pending", Reason="", readiness=false. Elapsed: 197.421106ms
Aug 14 15:43:21.211: INFO: Pod "pod-projected-configmaps-bdbd3c02-0d8d-4c26-8c0a-26871d571bf4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.232237561s
Aug 14 15:43:23.780: INFO: Pod "pod-projected-configmaps-bdbd3c02-0d8d-4c26-8c0a-26871d571bf4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.800861393s
Aug 14 15:43:25.785: INFO: Pod "pod-projected-configmaps-bdbd3c02-0d8d-4c26-8c0a-26871d571bf4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.805445912s
Aug 14 15:43:27.796: INFO: Pod "pod-projected-configmaps-bdbd3c02-0d8d-4c26-8c0a-26871d571bf4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.816425571s
STEP: Saw pod success
Aug 14 15:43:27.796: INFO: Pod "pod-projected-configmaps-bdbd3c02-0d8d-4c26-8c0a-26871d571bf4" satisfied condition "success or failure"
Aug 14 15:43:27.803: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-bdbd3c02-0d8d-4c26-8c0a-26871d571bf4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 15:43:27.847: INFO: Waiting for pod pod-projected-configmaps-bdbd3c02-0d8d-4c26-8c0a-26871d571bf4 to disappear
Aug 14 15:43:27.852: INFO: Pod pod-projected-configmaps-bdbd3c02-0d8d-4c26-8c0a-26871d571bf4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:43:27.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8953" for this suite.
Aug 14 15:43:36.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:43:37.871: INFO: namespace projected-8953 deletion completed in 10.009664385s

â€¢ [SLOW TEST:19.998 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:43:37.871: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1492
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 15:43:39.258: INFO: (0) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.398453ms)
Aug 14 15:43:39.264: INFO: (1) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.743645ms)
Aug 14 15:43:39.269: INFO: (2) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.406193ms)
Aug 14 15:43:39.274: INFO: (3) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.131013ms)
Aug 14 15:43:39.278: INFO: (4) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.06684ms)
Aug 14 15:43:39.282: INFO: (5) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.149115ms)
Aug 14 15:43:39.286: INFO: (6) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.172982ms)
Aug 14 15:43:39.291: INFO: (7) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.225613ms)
Aug 14 15:43:39.296: INFO: (8) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.965235ms)
Aug 14 15:43:39.299: INFO: (9) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.902646ms)
Aug 14 15:43:39.304: INFO: (10) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.427844ms)
Aug 14 15:43:39.309: INFO: (11) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.656251ms)
Aug 14 15:43:39.314: INFO: (12) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.541988ms)
Aug 14 15:43:39.318: INFO: (13) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.146659ms)
Aug 14 15:43:39.323: INFO: (14) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.643978ms)
Aug 14 15:43:39.328: INFO: (15) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.478471ms)
Aug 14 15:43:39.334: INFO: (16) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.273376ms)
Aug 14 15:43:39.338: INFO: (17) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.651817ms)
Aug 14 15:43:39.344: INFO: (18) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.406787ms)
Aug 14 15:43:39.348: INFO: (19) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.189155ms)
[AfterEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:43:39.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1492" for this suite.
Aug 14 15:43:49.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:43:50.267: INFO: namespace proxy-1492 deletion completed in 10.911719687s

â€¢ [SLOW TEST:12.395 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:43:50.267: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1663
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Aug 14 15:43:51.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 api-versions'
Aug 14 15:43:52.070: INFO: stderr: ""
Aug 14 15:43:52.070: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.istio.io/v1alpha1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling.k8s.io/v1beta1\nautoscaling.k8s.io/v1beta2\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\nconfig.istio.io/v1alpha2\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncustom.metrics.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.istio.io/v1alpha3\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nrbac.istio.io/v1alpha1\nrds.iop.com/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:43:52.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1663" for this suite.
Aug 14 15:44:02.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:44:04.579: INFO: namespace kubectl-1663 deletion completed in 12.501483234s

â€¢ [SLOW TEST:14.312 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:44:04.579: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-167
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 15:44:05.381: INFO: Waiting up to 5m0s for pod "downwardapi-volume-584b2a28-3c95-4c49-959c-f6fdf0f43f9c" in namespace "downward-api-167" to be "success or failure"
Aug 14 15:44:05.762: INFO: Pod "downwardapi-volume-584b2a28-3c95-4c49-959c-f6fdf0f43f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 380.638867ms
Aug 14 15:44:07.766: INFO: Pod "downwardapi-volume-584b2a28-3c95-4c49-959c-f6fdf0f43f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.385048007s
Aug 14 15:44:09.781: INFO: Pod "downwardapi-volume-584b2a28-3c95-4c49-959c-f6fdf0f43f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.400014594s
Aug 14 15:44:12.040: INFO: Pod "downwardapi-volume-584b2a28-3c95-4c49-959c-f6fdf0f43f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.65894615s
Aug 14 15:44:14.258: INFO: Pod "downwardapi-volume-584b2a28-3c95-4c49-959c-f6fdf0f43f9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.877000286s
STEP: Saw pod success
Aug 14 15:44:14.269: INFO: Pod "downwardapi-volume-584b2a28-3c95-4c49-959c-f6fdf0f43f9c" satisfied condition "success or failure"
Aug 14 15:44:14.456: INFO: Trying to get logs from node slave3 pod downwardapi-volume-584b2a28-3c95-4c49-959c-f6fdf0f43f9c container client-container: <nil>
STEP: delete the pod
Aug 14 15:44:15.448: INFO: Waiting for pod downwardapi-volume-584b2a28-3c95-4c49-959c-f6fdf0f43f9c to disappear
Aug 14 15:44:15.458: INFO: Pod downwardapi-volume-584b2a28-3c95-4c49-959c-f6fdf0f43f9c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:44:15.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-167" for this suite.
Aug 14 15:44:25.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:44:27.573: INFO: namespace downward-api-167 deletion completed in 12.095059985s

â€¢ [SLOW TEST:22.994 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:44:27.573: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-7531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 14 15:44:52.227: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7531 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:44:52.227: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:44:52.721: INFO: Exec stderr: ""
Aug 14 15:44:52.721: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7531 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:44:52.721: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:44:52.901: INFO: Exec stderr: ""
Aug 14 15:44:52.901: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7531 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:44:52.901: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:44:53.106: INFO: Exec stderr: ""
Aug 14 15:44:53.106: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7531 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:44:53.106: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:44:53.342: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 14 15:44:53.342: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7531 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:44:53.342: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:44:53.540: INFO: Exec stderr: ""
Aug 14 15:44:53.540: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7531 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:44:53.540: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:44:53.726: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 14 15:44:53.726: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7531 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:44:53.727: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:44:53.895: INFO: Exec stderr: ""
Aug 14 15:44:53.895: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7531 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:44:53.895: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:44:54.086: INFO: Exec stderr: ""
Aug 14 15:44:54.086: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7531 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:44:54.086: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:44:54.245: INFO: Exec stderr: ""
Aug 14 15:44:54.245: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7531 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 15:44:54.245: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 15:44:55.039: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:44:55.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7531" for this suite.
Aug 14 15:45:53.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:45:55.816: INFO: namespace e2e-kubelet-etc-hosts-7531 deletion completed in 1m0.600493352s

â€¢ [SLOW TEST:88.243 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:45:55.817: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-924
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 15:45:56.259: INFO: Creating deployment "nginx-deployment"
Aug 14 15:45:56.703: INFO: Waiting for observed generation 1
Aug 14 15:45:59.526: INFO: Waiting for all required pods to come up
Aug 14 15:46:01.527: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 14 15:46:20.768: INFO: Waiting for deployment "nginx-deployment" to complete
Aug 14 15:46:20.771: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:10, UpdatedReplicas:10, ReadyReplicas:9, AvailableReplicas:9, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701394375, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701394375, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701394380, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701394356, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"nginx-deployment-7b8c6f4498\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 15:46:22.780: INFO: Updating deployment "nginx-deployment" with a non-existent image
Aug 14 15:46:22.798: INFO: Updating deployment nginx-deployment
Aug 14 15:46:22.798: INFO: Waiting for observed generation 2
Aug 14 15:46:25.462: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 14 15:46:28.110: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 14 15:46:28.862: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 14 15:46:29.894: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 14 15:46:29.894: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 14 15:46:29.903: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 14 15:46:30.274: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Aug 14 15:46:30.274: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Aug 14 15:46:31.484: INFO: Updating deployment nginx-deployment
Aug 14 15:46:31.484: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Aug 14 15:46:31.628: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 14 15:46:37.079: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 14 15:46:38.125: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-924,SelfLink:/apis/apps/v1/namespaces/deployment-924/deployments/nginx-deployment,UID:4e25735c-bb29-40d1-929a-64545dedb025,ResourceVersion:377208,Generation:3,CreationTimestamp:2019-08-14 15:45:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:21,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-08-14 15:46:31 +0000 UTC 2019-08-14 15:46:31 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-14 15:46:37 +0000 UTC 2019-08-14 15:45:56 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Aug 14 15:46:38.683: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-924,SelfLink:/apis/apps/v1/namespaces/deployment-924/replicasets/nginx-deployment-55fb7cb77f,UID:368428ae-c858-45fe-a3a2-831c63836715,ResourceVersion:377195,Generation:3,CreationTimestamp:2019-08-14 15:46:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4e25735c-bb29-40d1-929a-64545dedb025 0xc001fe4b87 0xc001fe4b88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 14 15:46:38.683: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Aug 14 15:46:38.683: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-924,SelfLink:/apis/apps/v1/namespaces/deployment-924/replicasets/nginx-deployment-7b8c6f4498,UID:6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1,ResourceVersion:377211,Generation:3,CreationTimestamp:2019-08-14 15:45:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4e25735c-bb29-40d1-929a-64545dedb025 0xc001fe4c57 0xc001fe4c58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Aug 14 15:46:39.444: INFO: Pod "nginx-deployment-55fb7cb77f-6lljf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6lljf,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-55fb7cb77f-6lljf,UID:b07a55b0-dd7e-4b7f-9e83-9e4535d7629d,ResourceVersion:377082,Generation:0,CreationTimestamp:2019-08-14 15:46:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 368428ae-c858-45fe-a3a2-831c63836715 0xc0035781d7 0xc0035781d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003578250} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003578270}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:26 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.53,PodIP:,StartTime:2019-08-14 15:46:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.444: INFO: Pod "nginx-deployment-55fb7cb77f-6pzrn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6pzrn,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-55fb7cb77f-6pzrn,UID:0b052f92-e25f-4818-8a73-23e5a183d52f,ResourceVersion:377191,Generation:0,CreationTimestamp:2019-08-14 15:46:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 368428ae-c858-45fe-a3a2-831c63836715 0xc003578340 0xc003578341}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003578470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035784c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.59,PodIP:,StartTime:2019-08-14 15:46:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.444: INFO: Pod "nginx-deployment-55fb7cb77f-bnddn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-bnddn,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-55fb7cb77f-bnddn,UID:9e841a5c-32f6-4e3b-ba49-673c7ba1a508,ResourceVersion:377059,Generation:0,CreationTimestamp:2019-08-14 15:46:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 368428ae-c858-45fe-a3a2-831c63836715 0xc0035785d0 0xc0035785d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035786e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003578700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.55,PodIP:,StartTime:2019-08-14 15:46:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.445: INFO: Pod "nginx-deployment-55fb7cb77f-csqqj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-csqqj,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-55fb7cb77f-csqqj,UID:2220223b-009a-43bf-a6b2-9df9889c264e,ResourceVersion:377058,Generation:0,CreationTimestamp:2019-08-14 15:46:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 368428ae-c858-45fe-a3a2-831c63836715 0xc003578820 0xc003578821}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003578920} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003578940}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.67,PodIP:,StartTime:2019-08-14 15:46:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.445: INFO: Pod "nginx-deployment-55fb7cb77f-glg2t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-glg2t,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-55fb7cb77f-glg2t,UID:3a773eb2-15dd-47bc-8a1d-9871a73a44db,ResourceVersion:377190,Generation:0,CreationTimestamp:2019-08-14 15:46:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 368428ae-c858-45fe-a3a2-831c63836715 0xc003578ac0 0xc003578ac1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003578b60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003578b80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:33 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.67,PodIP:,StartTime:2019-08-14 15:46:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.445: INFO: Pod "nginx-deployment-55fb7cb77f-l7cng" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-l7cng,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-55fb7cb77f-l7cng,UID:e52c070f-56ab-4b0f-96ef-4c73f35511bc,ResourceVersion:377177,Generation:0,CreationTimestamp:2019-08-14 15:46:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 368428ae-c858-45fe-a3a2-831c63836715 0xc003578ca0 0xc003578ca1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003578d70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003578da0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.445: INFO: Pod "nginx-deployment-55fb7cb77f-lb4pc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-lb4pc,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-55fb7cb77f-lb4pc,UID:1f584087-56dc-429b-ad79-078523f677d8,ResourceVersion:377202,Generation:0,CreationTimestamp:2019-08-14 15:46:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 368428ae-c858-45fe-a3a2-831c63836715 0xc003578ee0 0xc003578ee1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003578ff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003579010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.55,PodIP:,StartTime:2019-08-14 15:46:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.445: INFO: Pod "nginx-deployment-55fb7cb77f-mb5c8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mb5c8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-55fb7cb77f-mb5c8,UID:d5b36b5c-1d90-4bc9-8043-93476eda8692,ResourceVersion:377163,Generation:0,CreationTimestamp:2019-08-14 15:46:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 368428ae-c858-45fe-a3a2-831c63836715 0xc003579110 0xc003579111}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003579190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035791b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.446: INFO: Pod "nginx-deployment-55fb7cb77f-p7nv6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-p7nv6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-55fb7cb77f-p7nv6,UID:f2cb467d-84c2-4051-9b5c-37525b6f4e58,ResourceVersion:377221,Generation:0,CreationTimestamp:2019-08-14 15:46:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 368428ae-c858-45fe-a3a2-831c63836715 0xc0035793b0 0xc0035793b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003579430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003579450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.59,PodIP:,StartTime:2019-08-14 15:46:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.446: INFO: Pod "nginx-deployment-55fb7cb77f-rw2np" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rw2np,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-55fb7cb77f-rw2np,UID:5e58c7d9-816d-4689-baec-fb866f880041,ResourceVersion:377175,Generation:0,CreationTimestamp:2019-08-14 15:46:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 368428ae-c858-45fe-a3a2-831c63836715 0xc0035795f0 0xc0035795f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035796e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003579720}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:32 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.70,PodIP:,StartTime:2019-08-14 15:46:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.446: INFO: Pod "nginx-deployment-55fb7cb77f-srkhw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-srkhw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-55fb7cb77f-srkhw,UID:67b5c0b9-2850-434a-adfa-b6c8d4279100,ResourceVersion:377094,Generation:0,CreationTimestamp:2019-08-14 15:46:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 368428ae-c858-45fe-a3a2-831c63836715 0xc003579890 0xc003579891}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035799d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035799f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:26 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.83,PodIP:,StartTime:2019-08-14 15:46:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.446: INFO: Pod "nginx-deployment-55fb7cb77f-st8p9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-st8p9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-55fb7cb77f-st8p9,UID:5a8f7460-9f60-4377-be15-9ed12023eb89,ResourceVersion:377183,Generation:0,CreationTimestamp:2019-08-14 15:46:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 368428ae-c858-45fe-a3a2-831c63836715 0xc003579ac0 0xc003579ac1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003579b40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003579b60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:33 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.55,PodIP:,StartTime:2019-08-14 15:46:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.446: INFO: Pod "nginx-deployment-55fb7cb77f-xrl7v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-xrl7v,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-55fb7cb77f-xrl7v,UID:18f7f29b-54e3-4057-b361-f880cbd64888,ResourceVersion:377150,Generation:0,CreationTimestamp:2019-08-14 15:46:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 368428ae-c858-45fe-a3a2-831c63836715 0xc003579cd0 0xc003579cd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003579e90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003579eb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:23 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.59,PodIP:,StartTime:2019-08-14 15:46:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.446: INFO: Pod "nginx-deployment-7b8c6f4498-2sc6f" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-2sc6f,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-2sc6f,UID:af9c1abe-383f-4d2d-a5ca-b05c41378995,ResourceVersion:376989,Generation:0,CreationTimestamp:2019-08-14 15:45:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc003579f90 0xc003579f91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0005ad940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0005ad980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:45:59 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.55,PodIP:10.151.49.24,StartTime:2019-08-14 15:46:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 15:46:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d54cd5150dfc4ec83d1bea818659b11232bc3d77c555e917037205009966c895}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.447: INFO: Pod "nginx-deployment-7b8c6f4498-4dmpg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4dmpg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-4dmpg,UID:e8981e5d-8687-44b3-81ba-fe6c3cf0c9c6,ResourceVersion:377178,Generation:0,CreationTimestamp:2019-08-14 15:46:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc0005adde7 0xc0005adde8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002bae110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002bae130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.447: INFO: Pod "nginx-deployment-7b8c6f4498-59clp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-59clp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-59clp,UID:af32bf0a-eda3-47d1-b9e4-2be248cd2e60,ResourceVersion:377186,Generation:0,CreationTimestamp:2019-08-14 15:46:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc002bae210 0xc002bae211}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002bae350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002bae370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.447: INFO: Pod "nginx-deployment-7b8c6f4498-7h69n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7h69n,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-7h69n,UID:b45549f9-bf33-4fab-84c4-42340c55ad35,ResourceVersion:377182,Generation:0,CreationTimestamp:2019-08-14 15:46:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc002bae4b0 0xc002bae4b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002bae5d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002bae5f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:33 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.59,PodIP:,StartTime:2019-08-14 15:46:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.447: INFO: Pod "nginx-deployment-7b8c6f4498-95mn9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-95mn9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-95mn9,UID:353e63d7-8c53-45da-870f-0288b88252ec,ResourceVersion:377174,Generation:0,CreationTimestamp:2019-08-14 15:46:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc002bae7b7 0xc002bae7b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002bae930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002bae950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:32 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.67,PodIP:,StartTime:2019-08-14 15:46:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.447: INFO: Pod "nginx-deployment-7b8c6f4498-brwjc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-brwjc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-brwjc,UID:0f430ddc-d2e8-4d27-87d4-b4c4c050d6ff,ResourceVersion:377209,Generation:0,CreationTimestamp:2019-08-14 15:46:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc002baead7 0xc002baead8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002baebb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002baebd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:33 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.67,PodIP:,StartTime:2019-08-14 15:46:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.447: INFO: Pod "nginx-deployment-7b8c6f4498-cn775" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cn775,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-cn775,UID:4ce14389-1e5d-4ea2-857c-894a067b9ed7,ResourceVersion:377167,Generation:0,CreationTimestamp:2019-08-14 15:46:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc002baed97 0xc002baed98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002baee70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002baee90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.447: INFO: Pod "nginx-deployment-7b8c6f4498-dnkn4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dnkn4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-dnkn4,UID:8403650a-e8ed-4a6b-a49a-0453219633eb,ResourceVersion:376969,Generation:0,CreationTimestamp:2019-08-14 15:45:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc002baef70 0xc002baef71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002baeff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002baf010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:45:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:45:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.53,PodIP:10.151.161.93,StartTime:2019-08-14 15:45:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 15:46:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2ceb6190daa9d52bf09d6e214c7d1ae6ef430a5094c0fd64dca275bfea1885cf}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.448: INFO: Pod "nginx-deployment-7b8c6f4498-dvngv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dvngv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-dvngv,UID:b9a76ad4-1d4d-4f80-901e-ab5eb0b293f4,ResourceVersion:376961,Generation:0,CreationTimestamp:2019-08-14 15:45:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc002baf0e7 0xc002baf0e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002baf160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002baf180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:45:59 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.59,PodIP:10.151.51.63,StartTime:2019-08-14 15:46:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 15:46:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://04b07c10366528b47852e734a03fd6140c5b39b2a50e41bf1e7983c0516781bd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.448: INFO: Pod "nginx-deployment-7b8c6f4498-fgqc4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fgqc4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-fgqc4,UID:1218b11e-6a09-4536-a5c1-d65ba2a38b86,ResourceVersion:376931,Generation:0,CreationTimestamp:2019-08-14 15:45:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc002baf257 0xc002baf258}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002baf2d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002baf2f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:45:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:45:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.55,PodIP:10.151.49.23,StartTime:2019-08-14 15:45:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 15:46:05 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://5f90d51337eb74963a2c28f40cc8141fe7a61f403a7e70d72846bcbb08008f1d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.448: INFO: Pod "nginx-deployment-7b8c6f4498-gnljj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gnljj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-gnljj,UID:ea5d693b-a658-4c16-800a-d9a9cd293764,ResourceVersion:377225,Generation:0,CreationTimestamp:2019-08-14 15:46:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc002baf3c7 0xc002baf3c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002baf440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002baf460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.67,PodIP:,StartTime:2019-08-14 15:46:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.448: INFO: Pod "nginx-deployment-7b8c6f4498-gsvhk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gsvhk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-gsvhk,UID:3dbb2d87-b9a5-4c0b-95bb-3ff7454ee161,ResourceVersion:377158,Generation:0,CreationTimestamp:2019-08-14 15:46:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc002baf527 0xc002baf528}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002baf5a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002baf5c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.448: INFO: Pod "nginx-deployment-7b8c6f4498-hlzj8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hlzj8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-hlzj8,UID:4dd61a5e-a145-4019-b91e-b53376f33df6,ResourceVersion:377176,Generation:0,CreationTimestamp:2019-08-14 15:46:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc002baf640 0xc002baf641}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002baf760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002baf780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.448: INFO: Pod "nginx-deployment-7b8c6f4498-jz5w4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jz5w4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-jz5w4,UID:8fd934dd-b375-4f02-a20e-431f1bd0df75,ResourceVersion:376984,Generation:0,CreationTimestamp:2019-08-14 15:45:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc002baf840 0xc002baf841}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002baf8e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002baf900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:45:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:45:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.83,PodIP:10.151.208.58,StartTime:2019-08-14 15:45:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 15:46:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://bae6da80537731ec6261c12b6cee8fcb61c0d4023ce41317a61cd8a4b222885e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.449: INFO: Pod "nginx-deployment-7b8c6f4498-l5sqz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-l5sqz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-l5sqz,UID:0e6ff135-2dbf-4d1c-a45b-531d1d24bb1d,ResourceVersion:377204,Generation:0,CreationTimestamp:2019-08-14 15:46:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc002bafaa7 0xc002bafaa8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002bafb30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002bafb50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.53,PodIP:,StartTime:2019-08-14 15:46:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.449: INFO: Pod "nginx-deployment-7b8c6f4498-lk8pb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-lk8pb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-lk8pb,UID:4439632c-9203-4330-a5b4-ac47ad80e9d0,ResourceVersion:377168,Generation:0,CreationTimestamp:2019-08-14 15:46:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc002bafc97 0xc002bafc98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002bafd50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002bafde0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.449: INFO: Pod "nginx-deployment-7b8c6f4498-spt4k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-spt4k,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-spt4k,UID:9a25c798-9ee1-4a84-9406-044f2b26ee4d,ResourceVersion:376960,Generation:0,CreationTimestamp:2019-08-14 15:45:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc002bafe80 0xc002bafe81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002baff40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002baff60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:45:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:45:58 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.70,PodIP:10.151.32.52,StartTime:2019-08-14 15:45:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 15:46:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4ae5af375d9a1d47e8d1d40bdc88b9af1084102561d96dd66055a228865c039d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.449: INFO: Pod "nginx-deployment-7b8c6f4498-t9bct" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-t9bct,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-t9bct,UID:62f95b29-c119-4b8c-92a7-063dfa22b8bf,ResourceVersion:376958,Generation:0,CreationTimestamp:2019-08-14 15:45:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc0004c22d7 0xc0004c22d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0004c2850} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0004c2a80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:45:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:45:56 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.67,PodIP:10.151.194.42,StartTime:2019-08-14 15:45:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 15:46:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://690f945d49d0f550b7d190f2ed2cd072596e175fe63c41a81728fe6369247486}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.449: INFO: Pod "nginx-deployment-7b8c6f4498-vl7hq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vl7hq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-vl7hq,UID:2d096b77-f586-47bc-9149-697ea761e3fc,ResourceVersion:376976,Generation:0,CreationTimestamp:2019-08-14 15:45:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc0004c3007 0xc0004c3008}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0004c3860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0004c3880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:45:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:45:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.59,PodIP:10.151.51.61,StartTime:2019-08-14 15:45:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 15:46:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://512ddf75aeb8a798ab2347e45f76db4a7c5e3477c35a58c878d425b4cc4b11ff}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 15:46:39.450: INFO: Pod "nginx-deployment-7b8c6f4498-zk8bb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zk8bb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-924,SelfLink:/api/v1/namespaces/deployment-924/pods/nginx-deployment-7b8c6f4498-zk8bb,UID:16eb0842-cd99-4f29-bb93-6a98c17e85db,ResourceVersion:377203,Generation:0,CreationTimestamp:2019-08-14 15:46:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 6a0bb1ec-69e7-48e2-9c8f-9dc7f62355a1 0xc0004c3b87 0xc0004c3b88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-524rg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-524rg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-524rg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000374260} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000374680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 15:46:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.83,PodIP:,StartTime:2019-08-14 15:46:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:46:39.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-924" for this suite.
Aug 14 15:48:41.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:48:42.848: INFO: namespace deployment-924 deletion completed in 2m2.423095055s

â€¢ [SLOW TEST:167.031 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:48:42.848: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8706
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 14 15:48:44.928: INFO: Waiting up to 5m0s for pod "downward-api-377f9e83-de30-4a2d-91e0-76ab9cac6df9" in namespace "downward-api-8706" to be "success or failure"
Aug 14 15:48:44.940: INFO: Pod "downward-api-377f9e83-de30-4a2d-91e0-76ab9cac6df9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.054739ms
Aug 14 15:48:46.949: INFO: Pod "downward-api-377f9e83-de30-4a2d-91e0-76ab9cac6df9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021741812s
Aug 14 15:48:49.261: INFO: Pod "downward-api-377f9e83-de30-4a2d-91e0-76ab9cac6df9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.333206974s
Aug 14 15:48:51.266: INFO: Pod "downward-api-377f9e83-de30-4a2d-91e0-76ab9cac6df9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.33860162s
Aug 14 15:48:53.271: INFO: Pod "downward-api-377f9e83-de30-4a2d-91e0-76ab9cac6df9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.343105493s
STEP: Saw pod success
Aug 14 15:48:53.271: INFO: Pod "downward-api-377f9e83-de30-4a2d-91e0-76ab9cac6df9" satisfied condition "success or failure"
Aug 14 15:48:53.278: INFO: Trying to get logs from node slave2 pod downward-api-377f9e83-de30-4a2d-91e0-76ab9cac6df9 container dapi-container: <nil>
STEP: delete the pod
Aug 14 15:48:53.356: INFO: Waiting for pod downward-api-377f9e83-de30-4a2d-91e0-76ab9cac6df9 to disappear
Aug 14 15:48:53.361: INFO: Pod downward-api-377f9e83-de30-4a2d-91e0-76ab9cac6df9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:48:53.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8706" for this suite.
Aug 14 15:49:03.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:49:05.737: INFO: namespace downward-api-8706 deletion completed in 12.368628792s

â€¢ [SLOW TEST:22.889 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:49:05.737: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4116
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 14 15:49:07.414: INFO: Waiting up to 5m0s for pod "pod-44016834-49ae-4dd0-a738-d315914395c1" in namespace "emptydir-4116" to be "success or failure"
Aug 14 15:49:07.419: INFO: Pod "pod-44016834-49ae-4dd0-a738-d315914395c1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.103793ms
Aug 14 15:49:09.516: INFO: Pod "pod-44016834-49ae-4dd0-a738-d315914395c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.101393422s
Aug 14 15:49:11.793: INFO: Pod "pod-44016834-49ae-4dd0-a738-d315914395c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.378719352s
Aug 14 15:49:14.277: INFO: Pod "pod-44016834-49ae-4dd0-a738-d315914395c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.863158938s
Aug 14 15:49:16.282: INFO: Pod "pod-44016834-49ae-4dd0-a738-d315914395c1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.86747609s
Aug 14 15:49:18.289: INFO: Pod "pod-44016834-49ae-4dd0-a738-d315914395c1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.874526892s
Aug 14 15:49:20.309: INFO: Pod "pod-44016834-49ae-4dd0-a738-d315914395c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.894565564s
STEP: Saw pod success
Aug 14 15:49:20.309: INFO: Pod "pod-44016834-49ae-4dd0-a738-d315914395c1" satisfied condition "success or failure"
Aug 14 15:49:20.313: INFO: Trying to get logs from node slave3 pod pod-44016834-49ae-4dd0-a738-d315914395c1 container test-container: <nil>
STEP: delete the pod
Aug 14 15:49:20.820: INFO: Waiting for pod pod-44016834-49ae-4dd0-a738-d315914395c1 to disappear
Aug 14 15:49:20.826: INFO: Pod pod-44016834-49ae-4dd0-a738-d315914395c1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:49:20.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4116" for this suite.
Aug 14 15:49:29.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:49:31.569: INFO: namespace emptydir-4116 deletion completed in 10.736191999s

â€¢ [SLOW TEST:25.832 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:49:31.569: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9819
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Aug 14 15:49:32.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 create -f - --namespace=kubectl-9819'
Aug 14 15:49:41.852: INFO: stderr: ""
Aug 14 15:49:41.852: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 15:49:41.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9819'
Aug 14 15:49:42.464: INFO: stderr: ""
Aug 14 15:49:42.464: INFO: stdout: "update-demo-nautilus-gl7gk update-demo-nautilus-gnb8c "
Aug 14 15:49:42.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-gl7gk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9819'
Aug 14 15:49:43.370: INFO: stderr: ""
Aug 14 15:49:43.370: INFO: stdout: ""
Aug 14 15:49:43.370: INFO: update-demo-nautilus-gl7gk is created but not running
Aug 14 15:49:48.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9819'
Aug 14 15:49:49.160: INFO: stderr: ""
Aug 14 15:49:49.160: INFO: stdout: "update-demo-nautilus-gl7gk update-demo-nautilus-gnb8c "
Aug 14 15:49:49.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-gl7gk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9819'
Aug 14 15:49:49.615: INFO: stderr: ""
Aug 14 15:49:49.615: INFO: stdout: "true"
Aug 14 15:49:49.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-gl7gk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9819'
Aug 14 15:49:50.578: INFO: stderr: ""
Aug 14 15:49:50.578: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 15:49:50.578: INFO: validating pod update-demo-nautilus-gl7gk
Aug 14 15:49:50.585: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 15:49:50.585: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 15:49:50.585: INFO: update-demo-nautilus-gl7gk is verified up and running
Aug 14 15:49:50.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-gnb8c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9819'
Aug 14 15:49:52.000: INFO: stderr: ""
Aug 14 15:49:52.000: INFO: stdout: "true"
Aug 14 15:49:52.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-gnb8c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9819'
Aug 14 15:49:52.164: INFO: stderr: ""
Aug 14 15:49:52.164: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 15:49:52.164: INFO: validating pod update-demo-nautilus-gnb8c
Aug 14 15:49:52.171: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 15:49:52.171: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 15:49:52.171: INFO: update-demo-nautilus-gnb8c is verified up and running
STEP: rolling-update to new replication controller
Aug 14 15:49:52.173: INFO: scanned /root for discovery docs: <nil>
Aug 14 15:49:52.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-9819'
Aug 14 15:50:27.110: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 14 15:50:27.110: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 15:50:27.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9819'
Aug 14 15:50:28.180: INFO: stderr: ""
Aug 14 15:50:28.180: INFO: stdout: "update-demo-kitten-sm54w update-demo-kitten-w5jcs "
Aug 14 15:50:28.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-kitten-sm54w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9819'
Aug 14 15:50:28.817: INFO: stderr: ""
Aug 14 15:50:28.817: INFO: stdout: "true"
Aug 14 15:50:28.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-kitten-sm54w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9819'
Aug 14 15:50:30.130: INFO: stderr: ""
Aug 14 15:50:30.130: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 14 15:50:30.130: INFO: validating pod update-demo-kitten-sm54w
Aug 14 15:50:30.136: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 14 15:50:30.136: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 14 15:50:30.136: INFO: update-demo-kitten-sm54w is verified up and running
Aug 14 15:50:30.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-kitten-w5jcs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9819'
Aug 14 15:50:30.762: INFO: stderr: ""
Aug 14 15:50:30.762: INFO: stdout: "true"
Aug 14 15:50:30.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-kitten-w5jcs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9819'
Aug 14 15:50:31.010: INFO: stderr: ""
Aug 14 15:50:31.010: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 14 15:50:31.010: INFO: validating pod update-demo-kitten-w5jcs
Aug 14 15:50:31.016: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 14 15:50:31.016: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 14 15:50:31.016: INFO: update-demo-kitten-w5jcs is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:50:31.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9819" for this suite.
Aug 14 15:51:03.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:51:06.100: INFO: namespace kubectl-9819 deletion completed in 35.077085571s

â€¢ [SLOW TEST:94.532 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:51:06.105: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9583
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9583.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9583.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 15:51:21.817: INFO: DNS probes using dns-9583/dns-test-0ed68f86-6677-40c3-9ea2-3c3cb29d2604 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:51:22.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9583" for this suite.
Aug 14 15:51:32.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:51:34.824: INFO: namespace dns-9583 deletion completed in 12.43603707s

â€¢ [SLOW TEST:28.719 seconds]
[sig-network] DNS
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:51:34.829: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6314
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 14 15:51:36.831: INFO: Waiting up to 5m0s for pod "pod-6001dcb6-c0cc-467c-b950-0c43a3eadc69" in namespace "emptydir-6314" to be "success or failure"
Aug 14 15:51:36.880: INFO: Pod "pod-6001dcb6-c0cc-467c-b950-0c43a3eadc69": Phase="Pending", Reason="", readiness=false. Elapsed: 49.168475ms
Aug 14 15:51:38.904: INFO: Pod "pod-6001dcb6-c0cc-467c-b950-0c43a3eadc69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073748805s
Aug 14 15:51:41.135: INFO: Pod "pod-6001dcb6-c0cc-467c-b950-0c43a3eadc69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.303825732s
Aug 14 15:51:43.168: INFO: Pod "pod-6001dcb6-c0cc-467c-b950-0c43a3eadc69": Phase="Pending", Reason="", readiness=false. Elapsed: 6.337026094s
Aug 14 15:51:45.257: INFO: Pod "pod-6001dcb6-c0cc-467c-b950-0c43a3eadc69": Phase="Pending", Reason="", readiness=false. Elapsed: 8.426545794s
Aug 14 15:51:47.285: INFO: Pod "pod-6001dcb6-c0cc-467c-b950-0c43a3eadc69": Phase="Pending", Reason="", readiness=false. Elapsed: 10.45421525s
Aug 14 15:51:49.290: INFO: Pod "pod-6001dcb6-c0cc-467c-b950-0c43a3eadc69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.4589328s
STEP: Saw pod success
Aug 14 15:51:49.290: INFO: Pod "pod-6001dcb6-c0cc-467c-b950-0c43a3eadc69" satisfied condition "success or failure"
Aug 14 15:51:49.636: INFO: Trying to get logs from node slave3 pod pod-6001dcb6-c0cc-467c-b950-0c43a3eadc69 container test-container: <nil>
STEP: delete the pod
Aug 14 15:51:50.050: INFO: Waiting for pod pod-6001dcb6-c0cc-467c-b950-0c43a3eadc69 to disappear
Aug 14 15:51:50.148: INFO: Pod pod-6001dcb6-c0cc-467c-b950-0c43a3eadc69 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:51:50.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6314" for this suite.
Aug 14 15:52:00.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:52:03.383: INFO: namespace emptydir-6314 deletion completed in 13.229333933s

â€¢ [SLOW TEST:28.554 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:52:03.384: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2498
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-560cc917-2c44-4c8b-bb8a-f3906f4f69e2
STEP: Creating a pod to test consume configMaps
Aug 14 15:52:06.491: INFO: Waiting up to 5m0s for pod "pod-configmaps-f03aa88a-0bb0-442d-a4cd-4cca28a0243e" in namespace "configmap-2498" to be "success or failure"
Aug 14 15:52:06.511: INFO: Pod "pod-configmaps-f03aa88a-0bb0-442d-a4cd-4cca28a0243e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.547932ms
Aug 14 15:52:08.519: INFO: Pod "pod-configmaps-f03aa88a-0bb0-442d-a4cd-4cca28a0243e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027382046s
Aug 14 15:52:10.857: INFO: Pod "pod-configmaps-f03aa88a-0bb0-442d-a4cd-4cca28a0243e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.365538697s
Aug 14 15:52:12.880: INFO: Pod "pod-configmaps-f03aa88a-0bb0-442d-a4cd-4cca28a0243e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.388466303s
Aug 14 15:52:15.012: INFO: Pod "pod-configmaps-f03aa88a-0bb0-442d-a4cd-4cca28a0243e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.520321472s
Aug 14 15:52:17.137: INFO: Pod "pod-configmaps-f03aa88a-0bb0-442d-a4cd-4cca28a0243e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.645156306s
Aug 14 15:52:19.726: INFO: Pod "pod-configmaps-f03aa88a-0bb0-442d-a4cd-4cca28a0243e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.235084773s
STEP: Saw pod success
Aug 14 15:52:19.727: INFO: Pod "pod-configmaps-f03aa88a-0bb0-442d-a4cd-4cca28a0243e" satisfied condition "success or failure"
Aug 14 15:52:19.732: INFO: Trying to get logs from node slave1 pod pod-configmaps-f03aa88a-0bb0-442d-a4cd-4cca28a0243e container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 15:52:19.782: INFO: Waiting for pod pod-configmaps-f03aa88a-0bb0-442d-a4cd-4cca28a0243e to disappear
Aug 14 15:52:19.792: INFO: Pod pod-configmaps-f03aa88a-0bb0-442d-a4cd-4cca28a0243e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:52:19.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2498" for this suite.
Aug 14 15:52:31.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:52:33.700: INFO: namespace configmap-2498 deletion completed in 13.903295199s

â€¢ [SLOW TEST:30.317 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:52:33.701: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3091
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-a0efa151-661c-4310-acfb-53eee5b26285
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:52:35.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3091" for this suite.
Aug 14 15:52:45.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:52:47.036: INFO: namespace secrets-3091 deletion completed in 11.8285472s

â€¢ [SLOW TEST:13.335 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:52:47.037: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9372
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 15:52:49.226: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:52:58.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9372" for this suite.
Aug 14 15:53:56.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:53:57.947: INFO: namespace pods-9372 deletion completed in 59.888326198s

â€¢ [SLOW TEST:70.910 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:53:57.948: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4149
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:54:12.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4149" for this suite.
Aug 14 15:54:24.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:54:26.859: INFO: namespace kubelet-test-4149 deletion completed in 14.164273735s

â€¢ [SLOW TEST:28.911 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:54:26.860: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1035
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-fc430fa0-5588-46a2-8b96-f3cd1ec23e43
STEP: Creating secret with name secret-projected-all-test-volume-75108f98-521d-45d4-9a69-252b7e025870
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 14 15:54:29.366: INFO: Waiting up to 5m0s for pod "projected-volume-a6176eef-95ac-46f2-a6da-f0a9b3fe0914" in namespace "projected-1035" to be "success or failure"
Aug 14 15:54:29.507: INFO: Pod "projected-volume-a6176eef-95ac-46f2-a6da-f0a9b3fe0914": Phase="Pending", Reason="", readiness=false. Elapsed: 141.147007ms
Aug 14 15:54:31.630: INFO: Pod "projected-volume-a6176eef-95ac-46f2-a6da-f0a9b3fe0914": Phase="Pending", Reason="", readiness=false. Elapsed: 2.264122935s
Aug 14 15:54:33.870: INFO: Pod "projected-volume-a6176eef-95ac-46f2-a6da-f0a9b3fe0914": Phase="Pending", Reason="", readiness=false. Elapsed: 4.50352767s
Aug 14 15:54:35.875: INFO: Pod "projected-volume-a6176eef-95ac-46f2-a6da-f0a9b3fe0914": Phase="Pending", Reason="", readiness=false. Elapsed: 6.509197387s
Aug 14 15:54:38.117: INFO: Pod "projected-volume-a6176eef-95ac-46f2-a6da-f0a9b3fe0914": Phase="Pending", Reason="", readiness=false. Elapsed: 8.750573353s
Aug 14 15:54:40.515: INFO: Pod "projected-volume-a6176eef-95ac-46f2-a6da-f0a9b3fe0914": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.148937076s
STEP: Saw pod success
Aug 14 15:54:40.515: INFO: Pod "projected-volume-a6176eef-95ac-46f2-a6da-f0a9b3fe0914" satisfied condition "success or failure"
Aug 14 15:54:40.520: INFO: Trying to get logs from node slave1 pod projected-volume-a6176eef-95ac-46f2-a6da-f0a9b3fe0914 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 14 15:54:40.825: INFO: Waiting for pod projected-volume-a6176eef-95ac-46f2-a6da-f0a9b3fe0914 to disappear
Aug 14 15:54:40.831: INFO: Pod projected-volume-a6176eef-95ac-46f2-a6da-f0a9b3fe0914 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:54:40.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1035" for this suite.
Aug 14 15:54:53.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:54:56.878: INFO: namespace projected-1035 deletion completed in 16.038626391s

â€¢ [SLOW TEST:30.019 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:54:56.879: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7341
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:54:58.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7341" for this suite.
Aug 14 15:55:09.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:55:11.363: INFO: namespace services-7341 deletion completed in 12.759974198s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

â€¢ [SLOW TEST:14.484 seconds]
[sig-network] Services
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:55:11.363: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4214
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-a48c9e2b-785b-4671-95b2-4f8ae849d9c7
STEP: Creating a pod to test consume secrets
Aug 14 15:55:13.440: INFO: Waiting up to 5m0s for pod "pod-secrets-ab86d7c5-8bc8-4c18-b93f-6a52c0bc40c4" in namespace "secrets-4214" to be "success or failure"
Aug 14 15:55:13.448: INFO: Pod "pod-secrets-ab86d7c5-8bc8-4c18-b93f-6a52c0bc40c4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016536ms
Aug 14 15:55:15.454: INFO: Pod "pod-secrets-ab86d7c5-8bc8-4c18-b93f-6a52c0bc40c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013285721s
Aug 14 15:55:17.478: INFO: Pod "pod-secrets-ab86d7c5-8bc8-4c18-b93f-6a52c0bc40c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038006279s
Aug 14 15:55:20.112: INFO: Pod "pod-secrets-ab86d7c5-8bc8-4c18-b93f-6a52c0bc40c4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.671648589s
Aug 14 15:55:22.286: INFO: Pod "pod-secrets-ab86d7c5-8bc8-4c18-b93f-6a52c0bc40c4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.845479754s
Aug 14 15:55:24.480: INFO: Pod "pod-secrets-ab86d7c5-8bc8-4c18-b93f-6a52c0bc40c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.039279992s
STEP: Saw pod success
Aug 14 15:55:24.480: INFO: Pod "pod-secrets-ab86d7c5-8bc8-4c18-b93f-6a52c0bc40c4" satisfied condition "success or failure"
Aug 14 15:55:24.679: INFO: Trying to get logs from node slave2 pod pod-secrets-ab86d7c5-8bc8-4c18-b93f-6a52c0bc40c4 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 15:55:25.404: INFO: Waiting for pod pod-secrets-ab86d7c5-8bc8-4c18-b93f-6a52c0bc40c4 to disappear
Aug 14 15:55:25.414: INFO: Pod pod-secrets-ab86d7c5-8bc8-4c18-b93f-6a52c0bc40c4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:55:25.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4214" for this suite.
Aug 14 15:55:39.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:55:41.909: INFO: namespace secrets-4214 deletion completed in 16.489531215s

â€¢ [SLOW TEST:30.546 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:55:41.909: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3870
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-1e4a6ffb-cc59-451f-92b4-3c2298c10916
STEP: Creating a pod to test consume secrets
Aug 14 15:55:44.558: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-57229ff6-2aed-45cd-a238-3b374d91c5e8" in namespace "projected-3870" to be "success or failure"
Aug 14 15:55:45.015: INFO: Pod "pod-projected-secrets-57229ff6-2aed-45cd-a238-3b374d91c5e8": Phase="Pending", Reason="", readiness=false. Elapsed: 456.681626ms
Aug 14 15:55:47.149: INFO: Pod "pod-projected-secrets-57229ff6-2aed-45cd-a238-3b374d91c5e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.591220037s
Aug 14 15:55:49.173: INFO: Pod "pod-projected-secrets-57229ff6-2aed-45cd-a238-3b374d91c5e8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.614458968s
Aug 14 15:55:51.178: INFO: Pod "pod-projected-secrets-57229ff6-2aed-45cd-a238-3b374d91c5e8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.619357138s
Aug 14 15:55:53.634: INFO: Pod "pod-projected-secrets-57229ff6-2aed-45cd-a238-3b374d91c5e8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.075495892s
Aug 14 15:55:55.664: INFO: Pod "pod-projected-secrets-57229ff6-2aed-45cd-a238-3b374d91c5e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.105599271s
STEP: Saw pod success
Aug 14 15:55:55.664: INFO: Pod "pod-projected-secrets-57229ff6-2aed-45cd-a238-3b374d91c5e8" satisfied condition "success or failure"
Aug 14 15:55:55.855: INFO: Trying to get logs from node slave3 pod pod-projected-secrets-57229ff6-2aed-45cd-a238-3b374d91c5e8 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 15:55:56.217: INFO: Waiting for pod pod-projected-secrets-57229ff6-2aed-45cd-a238-3b374d91c5e8 to disappear
Aug 14 15:55:56.582: INFO: Pod pod-projected-secrets-57229ff6-2aed-45cd-a238-3b374d91c5e8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:55:56.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3870" for this suite.
Aug 14 15:56:07.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:56:09.159: INFO: namespace projected-3870 deletion completed in 12.571287605s

â€¢ [SLOW TEST:27.251 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:56:09.160: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-981
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Aug 14 15:56:22.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec pod-sharedvolume-63ddf40e-ce4c-42db-ac2d-317c9089d5cb -c busybox-main-container --namespace=emptydir-981 -- cat /usr/share/volumeshare/shareddata.txt'
Aug 14 15:56:25.067: INFO: stderr: ""
Aug 14 15:56:25.067: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:56:25.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-981" for this suite.
Aug 14 15:56:37.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:56:40.287: INFO: namespace emptydir-981 deletion completed in 15.173000988s

â€¢ [SLOW TEST:31.127 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:56:40.289: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9144
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-053e21b3-e811-420c-af33-1ca6cbaf345b
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:56:57.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9144" for this suite.
Aug 14 15:57:29.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:57:31.789: INFO: namespace configmap-9144 deletion completed in 34.334951532s

â€¢ [SLOW TEST:51.500 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:57:31.790: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9849
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9849.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9849.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9849.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9849.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 15:57:49.863: INFO: DNS probes using dns-test-1c5bc806-d79d-4f43-b753-869080c5999a succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9849.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9849.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9849.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9849.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 15:58:04.939: INFO: File wheezy_udp@dns-test-service-3.dns-9849.svc.cluster.local from pod  dns-9849/dns-test-be87435a-4f62-460d-a6d8-849a87e54648 contains '' instead of 'bar.example.com.'
Aug 14 15:58:04.945: INFO: Lookups using dns-9849/dns-test-be87435a-4f62-460d-a6d8-849a87e54648 failed for: [wheezy_udp@dns-test-service-3.dns-9849.svc.cluster.local]

Aug 14 15:58:09.955: INFO: DNS probes using dns-test-be87435a-4f62-460d-a6d8-849a87e54648 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9849.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9849.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9849.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9849.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 15:58:28.970: INFO: DNS probes using dns-test-dbeaf1bf-09e4-46c2-8c9b-0a80c437d022 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:58:29.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9849" for this suite.
Aug 14 15:58:44.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:58:45.974: INFO: namespace dns-9849 deletion completed in 15.878241175s

â€¢ [SLOW TEST:74.184 seconds]
[sig-network] DNS
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:58:45.975: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6662
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Aug 14 15:58:47.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 create -f - --namespace=kubectl-6662'
Aug 14 15:58:48.376: INFO: stderr: ""
Aug 14 15:58:48.377: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 15:58:48.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6662'
Aug 14 15:58:49.075: INFO: stderr: ""
Aug 14 15:58:49.075: INFO: stdout: "update-demo-nautilus-jm44z update-demo-nautilus-nf8qs "
Aug 14 15:58:49.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-jm44z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6662'
Aug 14 15:58:50.723: INFO: stderr: ""
Aug 14 15:58:50.723: INFO: stdout: ""
Aug 14 15:58:50.723: INFO: update-demo-nautilus-jm44z is created but not running
Aug 14 15:58:55.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6662'
Aug 14 15:58:56.477: INFO: stderr: ""
Aug 14 15:58:56.477: INFO: stdout: "update-demo-nautilus-jm44z update-demo-nautilus-nf8qs "
Aug 14 15:58:56.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-jm44z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6662'
Aug 14 15:58:57.417: INFO: stderr: ""
Aug 14 15:58:57.417: INFO: stdout: ""
Aug 14 15:58:57.417: INFO: update-demo-nautilus-jm44z is created but not running
Aug 14 15:59:02.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6662'
Aug 14 15:59:02.968: INFO: stderr: ""
Aug 14 15:59:02.968: INFO: stdout: "update-demo-nautilus-jm44z update-demo-nautilus-nf8qs "
Aug 14 15:59:02.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-jm44z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6662'
Aug 14 15:59:03.853: INFO: stderr: ""
Aug 14 15:59:03.853: INFO: stdout: "true"
Aug 14 15:59:03.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-jm44z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6662'
Aug 14 15:59:04.621: INFO: stderr: ""
Aug 14 15:59:04.621: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 15:59:04.621: INFO: validating pod update-demo-nautilus-jm44z
Aug 14 15:59:04.630: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 15:59:04.630: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 15:59:04.630: INFO: update-demo-nautilus-jm44z is verified up and running
Aug 14 15:59:04.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-nf8qs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6662'
Aug 14 15:59:06.237: INFO: stderr: ""
Aug 14 15:59:06.238: INFO: stdout: "true"
Aug 14 15:59:06.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods update-demo-nautilus-nf8qs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6662'
Aug 14 15:59:07.505: INFO: stderr: ""
Aug 14 15:59:07.505: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 15:59:07.505: INFO: validating pod update-demo-nautilus-nf8qs
Aug 14 15:59:07.511: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 15:59:07.511: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 15:59:07.511: INFO: update-demo-nautilus-nf8qs is verified up and running
STEP: using delete to clean up resources
Aug 14 15:59:07.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete --grace-period=0 --force -f - --namespace=kubectl-6662'
Aug 14 15:59:07.981: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 15:59:07.981: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 14 15:59:07.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6662'
Aug 14 15:59:10.962: INFO: stderr: "No resources found.\n"
Aug 14 15:59:10.962: INFO: stdout: ""
Aug 14 15:59:10.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -l name=update-demo --namespace=kubectl-6662 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 14 15:59:12.322: INFO: stderr: ""
Aug 14 15:59:12.322: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:59:12.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6662" for this suite.
Aug 14 15:59:22.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:59:24.411: INFO: namespace kubectl-6662 deletion completed in 12.071176939s

â€¢ [SLOW TEST:38.437 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 15:59:24.413: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3655
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4770
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6455
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 15:59:42.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3655" for this suite.
Aug 14 15:59:51.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 15:59:52.991: INFO: namespace namespaces-3655 deletion completed in 10.208769543s
STEP: Destroying namespace "nsdeletetest-4770" for this suite.
Aug 14 15:59:52.996: INFO: Namespace nsdeletetest-4770 was already deleted
STEP: Destroying namespace "nsdeletetest-6455" for this suite.
Aug 14 16:00:01.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:00:02.840: INFO: namespace nsdeletetest-6455 deletion completed in 9.84433236s

â€¢ [SLOW TEST:38.428 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:00:02.841: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5723
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-9e3d1eec-1058-4051-82e8-4f699af0e70e
STEP: Creating configMap with name cm-test-opt-upd-ea17c4b1-45af-4cc4-a1ef-879df15d75b7
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9e3d1eec-1058-4051-82e8-4f699af0e70e
STEP: Updating configmap cm-test-opt-upd-ea17c4b1-45af-4cc4-a1ef-879df15d75b7
STEP: Creating configMap with name cm-test-opt-create-f9a234fd-bda0-43a9-808a-5abd04fba7d8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:01:52.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5723" for this suite.
Aug 14 16:02:22.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:02:25.068: INFO: namespace projected-5723 deletion completed in 32.648836135s

â€¢ [SLOW TEST:142.228 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:02:25.070: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5047
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Aug 14 16:02:27.511: INFO: Waiting up to 5m0s for pod "var-expansion-2926ea9f-7b3c-4c57-8d19-6e702bac3c96" in namespace "var-expansion-5047" to be "success or failure"
Aug 14 16:02:27.522: INFO: Pod "var-expansion-2926ea9f-7b3c-4c57-8d19-6e702bac3c96": Phase="Pending", Reason="", readiness=false. Elapsed: 10.562378ms
Aug 14 16:02:29.527: INFO: Pod "var-expansion-2926ea9f-7b3c-4c57-8d19-6e702bac3c96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015524229s
Aug 14 16:02:31.712: INFO: Pod "var-expansion-2926ea9f-7b3c-4c57-8d19-6e702bac3c96": Phase="Pending", Reason="", readiness=false. Elapsed: 4.200393312s
Aug 14 16:02:33.730: INFO: Pod "var-expansion-2926ea9f-7b3c-4c57-8d19-6e702bac3c96": Phase="Pending", Reason="", readiness=false. Elapsed: 6.218469105s
Aug 14 16:02:36.009: INFO: Pod "var-expansion-2926ea9f-7b3c-4c57-8d19-6e702bac3c96": Phase="Pending", Reason="", readiness=false. Elapsed: 8.496959956s
Aug 14 16:02:38.038: INFO: Pod "var-expansion-2926ea9f-7b3c-4c57-8d19-6e702bac3c96": Phase="Pending", Reason="", readiness=false. Elapsed: 10.526642813s
Aug 14 16:02:40.043: INFO: Pod "var-expansion-2926ea9f-7b3c-4c57-8d19-6e702bac3c96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.53157105s
STEP: Saw pod success
Aug 14 16:02:40.043: INFO: Pod "var-expansion-2926ea9f-7b3c-4c57-8d19-6e702bac3c96" satisfied condition "success or failure"
Aug 14 16:02:40.047: INFO: Trying to get logs from node slave1 pod var-expansion-2926ea9f-7b3c-4c57-8d19-6e702bac3c96 container dapi-container: <nil>
STEP: delete the pod
Aug 14 16:02:40.510: INFO: Waiting for pod var-expansion-2926ea9f-7b3c-4c57-8d19-6e702bac3c96 to disappear
Aug 14 16:02:40.518: INFO: Pod var-expansion-2926ea9f-7b3c-4c57-8d19-6e702bac3c96 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:02:40.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5047" for this suite.
Aug 14 16:02:51.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:02:53.399: INFO: namespace var-expansion-5047 deletion completed in 12.874514647s

â€¢ [SLOW TEST:28.329 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:02:53.399: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7113
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 14 16:03:03.764: INFO: Successfully updated pod "annotationupdate95aba53f-1458-4b61-a0db-94c7e3bb48ee"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:03:05.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7113" for this suite.
Aug 14 16:03:34.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:03:36.795: INFO: namespace projected-7113 deletion completed in 30.920940225s

â€¢ [SLOW TEST:43.396 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:03:36.800: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2157
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Aug 14 16:03:56.934: INFO: 5 pods remaining
Aug 14 16:03:56.934: INFO: 5 pods has nil DeletionTimestamp
Aug 14 16:03:56.934: INFO: 
STEP: Gathering metrics
Aug 14 16:04:03.593: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0814 16:04:03.593877      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:04:03.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2157" for this suite.
Aug 14 16:04:35.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:04:37.553: INFO: namespace gc-2157 deletion completed in 32.858760929s

â€¢ [SLOW TEST:60.752 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:04:37.553: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8968
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 14 16:04:51.243: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:04:51.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8968" for this suite.
Aug 14 16:05:03.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:05:05.647: INFO: namespace container-runtime-8968 deletion completed in 14.346281603s

â€¢ [SLOW TEST:28.094 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:05:05.648: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-125
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 14 16:05:07.764: INFO: Waiting up to 5m0s for pod "downward-api-40a6a48b-ac50-4728-85d1-98024c571099" in namespace "downward-api-125" to be "success or failure"
Aug 14 16:05:08.428: INFO: Pod "downward-api-40a6a48b-ac50-4728-85d1-98024c571099": Phase="Pending", Reason="", readiness=false. Elapsed: 664.057864ms
Aug 14 16:05:10.433: INFO: Pod "downward-api-40a6a48b-ac50-4728-85d1-98024c571099": Phase="Pending", Reason="", readiness=false. Elapsed: 2.669499686s
Aug 14 16:05:12.494: INFO: Pod "downward-api-40a6a48b-ac50-4728-85d1-98024c571099": Phase="Pending", Reason="", readiness=false. Elapsed: 4.729833724s
Aug 14 16:05:14.500: INFO: Pod "downward-api-40a6a48b-ac50-4728-85d1-98024c571099": Phase="Pending", Reason="", readiness=false. Elapsed: 6.735823999s
Aug 14 16:05:16.504: INFO: Pod "downward-api-40a6a48b-ac50-4728-85d1-98024c571099": Phase="Pending", Reason="", readiness=false. Elapsed: 8.739845284s
Aug 14 16:05:18.560: INFO: Pod "downward-api-40a6a48b-ac50-4728-85d1-98024c571099": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.79598699s
STEP: Saw pod success
Aug 14 16:05:18.560: INFO: Pod "downward-api-40a6a48b-ac50-4728-85d1-98024c571099" satisfied condition "success or failure"
Aug 14 16:05:18.925: INFO: Trying to get logs from node slave1 pod downward-api-40a6a48b-ac50-4728-85d1-98024c571099 container dapi-container: <nil>
STEP: delete the pod
Aug 14 16:05:19.368: INFO: Waiting for pod downward-api-40a6a48b-ac50-4728-85d1-98024c571099 to disappear
Aug 14 16:05:19.411: INFO: Pod downward-api-40a6a48b-ac50-4728-85d1-98024c571099 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:05:19.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-125" for this suite.
Aug 14 16:05:29.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:05:31.234: INFO: namespace downward-api-125 deletion completed in 11.815522393s

â€¢ [SLOW TEST:25.587 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:05:31.237: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1423
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 14 16:05:55.827: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 16:05:55.837: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 16:05:57.838: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 16:05:57.843: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 16:05:59.838: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 16:06:00.049: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 16:06:01.838: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 16:06:02.029: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 16:06:03.838: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 16:06:03.843: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 16:06:05.838: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 16:06:05.844: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 16:06:07.838: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 16:06:07.886: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 16:06:09.838: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 16:06:10.133: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 16:06:11.838: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 16:06:11.842: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:06:11.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1423" for this suite.
Aug 14 16:06:40.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:06:42.195: INFO: namespace container-lifecycle-hook-1423 deletion completed in 30.335756222s

â€¢ [SLOW TEST:70.958 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:06:42.195: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1841
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 16:06:44.083: INFO: (0) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.597675ms)
Aug 14 16:06:44.088: INFO: (1) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.226273ms)
Aug 14 16:06:44.094: INFO: (2) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.535238ms)
Aug 14 16:06:44.100: INFO: (3) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.804252ms)
Aug 14 16:06:44.105: INFO: (4) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.287662ms)
Aug 14 16:06:44.111: INFO: (5) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.784149ms)
Aug 14 16:06:44.116: INFO: (6) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.927207ms)
Aug 14 16:06:44.121: INFO: (7) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.723641ms)
Aug 14 16:06:44.126: INFO: (8) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.772055ms)
Aug 14 16:06:44.131: INFO: (9) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.311023ms)
Aug 14 16:06:44.198: INFO: (10) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 67.079644ms)
Aug 14 16:06:44.203: INFO: (11) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.892747ms)
Aug 14 16:06:44.207: INFO: (12) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.28651ms)
Aug 14 16:06:44.212: INFO: (13) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.454206ms)
Aug 14 16:06:44.217: INFO: (14) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.887546ms)
Aug 14 16:06:44.221: INFO: (15) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.459846ms)
Aug 14 16:06:44.227: INFO: (16) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.053567ms)
Aug 14 16:06:44.232: INFO: (17) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.224917ms)
Aug 14 16:06:44.236: INFO: (18) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.449427ms)
Aug 14 16:06:44.241: INFO: (19) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.286729ms)
[AfterEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:06:44.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1841" for this suite.
Aug 14 16:06:54.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:06:54.796: INFO: namespace proxy-1841 deletion completed in 10.548872854s

â€¢ [SLOW TEST:12.601 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:06:54.796: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8363
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-8045226d-38e8-41f5-a38d-962a40b92e42
STEP: Creating a pod to test consume configMaps
Aug 14 16:06:56.807: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2262efc5-b7e0-48ad-b0c4-1a4c30ed70e4" in namespace "projected-8363" to be "success or failure"
Aug 14 16:06:56.828: INFO: Pod "pod-projected-configmaps-2262efc5-b7e0-48ad-b0c4-1a4c30ed70e4": Phase="Pending", Reason="", readiness=false. Elapsed: 21.012689ms
Aug 14 16:06:58.833: INFO: Pod "pod-projected-configmaps-2262efc5-b7e0-48ad-b0c4-1a4c30ed70e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026039961s
Aug 14 16:07:00.837: INFO: Pod "pod-projected-configmaps-2262efc5-b7e0-48ad-b0c4-1a4c30ed70e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030530492s
Aug 14 16:07:03.283: INFO: Pod "pod-projected-configmaps-2262efc5-b7e0-48ad-b0c4-1a4c30ed70e4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.476608249s
Aug 14 16:07:05.506: INFO: Pod "pod-projected-configmaps-2262efc5-b7e0-48ad-b0c4-1a4c30ed70e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.698661645s
STEP: Saw pod success
Aug 14 16:07:05.506: INFO: Pod "pod-projected-configmaps-2262efc5-b7e0-48ad-b0c4-1a4c30ed70e4" satisfied condition "success or failure"
Aug 14 16:07:05.658: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-2262efc5-b7e0-48ad-b0c4-1a4c30ed70e4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 16:07:06.018: INFO: Waiting for pod pod-projected-configmaps-2262efc5-b7e0-48ad-b0c4-1a4c30ed70e4 to disappear
Aug 14 16:07:06.026: INFO: Pod pod-projected-configmaps-2262efc5-b7e0-48ad-b0c4-1a4c30ed70e4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:07:06.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8363" for this suite.
Aug 14 16:07:14.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:07:16.618: INFO: namespace projected-8363 deletion completed in 10.586158734s

â€¢ [SLOW TEST:21.822 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:07:16.618: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1989
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-19025e6d-db45-4011-8753-1f2fc0d88a6e in namespace container-probe-1989
Aug 14 16:07:26.853: INFO: Started pod liveness-19025e6d-db45-4011-8753-1f2fc0d88a6e in namespace container-probe-1989
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 16:07:26.856: INFO: Initial restart count of pod liveness-19025e6d-db45-4011-8753-1f2fc0d88a6e is 0
Aug 14 16:07:43.789: INFO: Restart count of pod container-probe-1989/liveness-19025e6d-db45-4011-8753-1f2fc0d88a6e is now 1 (16.932668423s elapsed)
Aug 14 16:08:04.491: INFO: Restart count of pod container-probe-1989/liveness-19025e6d-db45-4011-8753-1f2fc0d88a6e is now 2 (37.634615713s elapsed)
Aug 14 16:08:23.266: INFO: Restart count of pod container-probe-1989/liveness-19025e6d-db45-4011-8753-1f2fc0d88a6e is now 3 (56.410076129s elapsed)
Aug 14 16:08:43.770: INFO: Restart count of pod container-probe-1989/liveness-19025e6d-db45-4011-8753-1f2fc0d88a6e is now 4 (1m16.913696237s elapsed)
Aug 14 16:09:43.295: INFO: Restart count of pod container-probe-1989/liveness-19025e6d-db45-4011-8753-1f2fc0d88a6e is now 5 (2m16.438542752s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:09:43.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1989" for this suite.
Aug 14 16:09:56.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:09:58.170: INFO: namespace container-probe-1989 deletion completed in 14.197058462s

â€¢ [SLOW TEST:161.552 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:09:58.170: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 14 16:09:59.221: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 14 16:10:04.310: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:10:05.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9140" for this suite.
Aug 14 16:10:18.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:10:20.525: INFO: namespace replication-controller-9140 deletion completed in 14.657587606s

â€¢ [SLOW TEST:22.354 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:10:20.525: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 16:10:21.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 version'
Aug 14 16:10:21.709: INFO: stderr: ""
Aug 14 16:10:21.709: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.2\", GitCommit:\"f6278300bebbb750328ac16ee6dd3aa7d3549568\", GitTreeState:\"clean\", BuildDate:\"2019-08-05T09:23:26Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.2\", GitCommit:\"f6278300bebbb750328ac16ee6dd3aa7d3549568\", GitTreeState:\"clean\", BuildDate:\"2019-08-05T09:15:22Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:10:21.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8968" for this suite.
Aug 14 16:10:30.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:10:32.511: INFO: namespace kubectl-8968 deletion completed in 10.574054096s

â€¢ [SLOW TEST:11.986 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:10:32.512: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9142
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 14 16:10:33.414: INFO: Waiting up to 5m0s for pod "pod-86989cd9-9f0e-4273-82dc-dcc19417d554" in namespace "emptydir-9142" to be "success or failure"
Aug 14 16:10:33.827: INFO: Pod "pod-86989cd9-9f0e-4273-82dc-dcc19417d554": Phase="Pending", Reason="", readiness=false. Elapsed: 412.104532ms
Aug 14 16:10:35.892: INFO: Pod "pod-86989cd9-9f0e-4273-82dc-dcc19417d554": Phase="Pending", Reason="", readiness=false. Elapsed: 2.477140308s
Aug 14 16:10:37.896: INFO: Pod "pod-86989cd9-9f0e-4273-82dc-dcc19417d554": Phase="Pending", Reason="", readiness=false. Elapsed: 4.481344972s
Aug 14 16:10:40.011: INFO: Pod "pod-86989cd9-9f0e-4273-82dc-dcc19417d554": Phase="Pending", Reason="", readiness=false. Elapsed: 6.596171909s
Aug 14 16:10:42.139: INFO: Pod "pod-86989cd9-9f0e-4273-82dc-dcc19417d554": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.724832858s
STEP: Saw pod success
Aug 14 16:10:42.140: INFO: Pod "pod-86989cd9-9f0e-4273-82dc-dcc19417d554" satisfied condition "success or failure"
Aug 14 16:10:42.146: INFO: Trying to get logs from node slave2 pod pod-86989cd9-9f0e-4273-82dc-dcc19417d554 container test-container: <nil>
STEP: delete the pod
Aug 14 16:10:42.618: INFO: Waiting for pod pod-86989cd9-9f0e-4273-82dc-dcc19417d554 to disappear
Aug 14 16:10:42.626: INFO: Pod pod-86989cd9-9f0e-4273-82dc-dcc19417d554 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:10:42.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9142" for this suite.
Aug 14 16:10:52.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:10:54.415: INFO: namespace emptydir-9142 deletion completed in 11.783213987s

â€¢ [SLOW TEST:21.903 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:10:54.417: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4228
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 16:10:55.271: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Aug 14 16:10:58.635: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:10:58.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4228" for this suite.
Aug 14 16:11:15.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:11:17.690: INFO: namespace replication-controller-4228 deletion completed in 18.555504266s

â€¢ [SLOW TEST:23.273 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:11:17.691: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1856
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 14 16:11:36.523: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 16:11:36.529: INFO: Pod pod-with-poststart-http-hook still exists
Aug 14 16:11:38.529: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 16:11:38.535: INFO: Pod pod-with-poststart-http-hook still exists
Aug 14 16:11:40.529: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 16:11:40.706: INFO: Pod pod-with-poststart-http-hook still exists
Aug 14 16:11:42.529: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 16:11:42.534: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:11:42.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1856" for this suite.
Aug 14 16:12:10.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:12:12.488: INFO: namespace container-lifecycle-hook-1856 deletion completed in 29.947025167s

â€¢ [SLOW TEST:54.797 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:12:12.488: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-c573625e-0077-4fd3-961e-f2a0a8a5c8d6
STEP: Creating a pod to test consume configMaps
Aug 14 16:12:14.782: INFO: Waiting up to 5m0s for pod "pod-configmaps-b3ce9fc8-4051-46bf-91d2-2f08cd4e8294" in namespace "configmap-620" to be "success or failure"
Aug 14 16:12:14.917: INFO: Pod "pod-configmaps-b3ce9fc8-4051-46bf-91d2-2f08cd4e8294": Phase="Pending", Reason="", readiness=false. Elapsed: 135.261564ms
Aug 14 16:12:17.037: INFO: Pod "pod-configmaps-b3ce9fc8-4051-46bf-91d2-2f08cd4e8294": Phase="Pending", Reason="", readiness=false. Elapsed: 2.254907274s
Aug 14 16:12:19.042: INFO: Pod "pod-configmaps-b3ce9fc8-4051-46bf-91d2-2f08cd4e8294": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259500813s
Aug 14 16:12:21.341: INFO: Pod "pod-configmaps-b3ce9fc8-4051-46bf-91d2-2f08cd4e8294": Phase="Pending", Reason="", readiness=false. Elapsed: 6.559028166s
Aug 14 16:12:23.793: INFO: Pod "pod-configmaps-b3ce9fc8-4051-46bf-91d2-2f08cd4e8294": Phase="Pending", Reason="", readiness=false. Elapsed: 9.010545798s
Aug 14 16:12:25.828: INFO: Pod "pod-configmaps-b3ce9fc8-4051-46bf-91d2-2f08cd4e8294": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.045606941s
STEP: Saw pod success
Aug 14 16:12:25.828: INFO: Pod "pod-configmaps-b3ce9fc8-4051-46bf-91d2-2f08cd4e8294" satisfied condition "success or failure"
Aug 14 16:12:25.832: INFO: Trying to get logs from node slave1 pod pod-configmaps-b3ce9fc8-4051-46bf-91d2-2f08cd4e8294 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 16:12:26.198: INFO: Waiting for pod pod-configmaps-b3ce9fc8-4051-46bf-91d2-2f08cd4e8294 to disappear
Aug 14 16:12:26.206: INFO: Pod pod-configmaps-b3ce9fc8-4051-46bf-91d2-2f08cd4e8294 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:12:26.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-620" for this suite.
Aug 14 16:12:36.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:12:38.761: INFO: namespace configmap-620 deletion completed in 12.544533918s

â€¢ [SLOW TEST:26.273 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:12:38.761: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6026
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 14 16:12:41.347: INFO: Waiting up to 5m0s for pod "pod-55763e58-ff17-404f-b8af-02a87f263fe8" in namespace "emptydir-6026" to be "success or failure"
Aug 14 16:12:41.360: INFO: Pod "pod-55763e58-ff17-404f-b8af-02a87f263fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.514865ms
Aug 14 16:12:43.365: INFO: Pod "pod-55763e58-ff17-404f-b8af-02a87f263fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017280944s
Aug 14 16:12:45.517: INFO: Pod "pod-55763e58-ff17-404f-b8af-02a87f263fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.169767874s
Aug 14 16:12:47.705: INFO: Pod "pod-55763e58-ff17-404f-b8af-02a87f263fe8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.357580385s
Aug 14 16:12:49.710: INFO: Pod "pod-55763e58-ff17-404f-b8af-02a87f263fe8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.36222262s
STEP: Saw pod success
Aug 14 16:12:49.710: INFO: Pod "pod-55763e58-ff17-404f-b8af-02a87f263fe8" satisfied condition "success or failure"
Aug 14 16:12:49.713: INFO: Trying to get logs from node slave2 pod pod-55763e58-ff17-404f-b8af-02a87f263fe8 container test-container: <nil>
STEP: delete the pod
Aug 14 16:12:49.981: INFO: Waiting for pod pod-55763e58-ff17-404f-b8af-02a87f263fe8 to disappear
Aug 14 16:12:49.986: INFO: Pod pod-55763e58-ff17-404f-b8af-02a87f263fe8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:12:49.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6026" for this suite.
Aug 14 16:13:00.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:13:01.949: INFO: namespace emptydir-6026 deletion completed in 11.95458552s

â€¢ [SLOW TEST:23.188 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:13:01.950: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8601
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-35216d4e-0216-40c0-a061-d70abd46b836
STEP: Creating a pod to test consume secrets
Aug 14 16:13:04.372: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6aa432c0-d8fd-4a6e-aad5-2014342831ba" in namespace "projected-8601" to be "success or failure"
Aug 14 16:13:04.543: INFO: Pod "pod-projected-secrets-6aa432c0-d8fd-4a6e-aad5-2014342831ba": Phase="Pending", Reason="", readiness=false. Elapsed: 171.218863ms
Aug 14 16:13:06.549: INFO: Pod "pod-projected-secrets-6aa432c0-d8fd-4a6e-aad5-2014342831ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.17730385s
Aug 14 16:13:08.560: INFO: Pod "pod-projected-secrets-6aa432c0-d8fd-4a6e-aad5-2014342831ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.187934013s
Aug 14 16:13:10.565: INFO: Pod "pod-projected-secrets-6aa432c0-d8fd-4a6e-aad5-2014342831ba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.193099725s
Aug 14 16:13:12.570: INFO: Pod "pod-projected-secrets-6aa432c0-d8fd-4a6e-aad5-2014342831ba": Phase="Pending", Reason="", readiness=false. Elapsed: 8.197741196s
Aug 14 16:13:14.615: INFO: Pod "pod-projected-secrets-6aa432c0-d8fd-4a6e-aad5-2014342831ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.24363179s
STEP: Saw pod success
Aug 14 16:13:14.616: INFO: Pod "pod-projected-secrets-6aa432c0-d8fd-4a6e-aad5-2014342831ba" satisfied condition "success or failure"
Aug 14 16:13:14.621: INFO: Trying to get logs from node slave3 pod pod-projected-secrets-6aa432c0-d8fd-4a6e-aad5-2014342831ba container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 14 16:13:15.389: INFO: Waiting for pod pod-projected-secrets-6aa432c0-d8fd-4a6e-aad5-2014342831ba to disappear
Aug 14 16:13:15.394: INFO: Pod pod-projected-secrets-6aa432c0-d8fd-4a6e-aad5-2014342831ba no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:13:15.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8601" for this suite.
Aug 14 16:13:26.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:13:28.254: INFO: namespace projected-8601 deletion completed in 12.349626108s

â€¢ [SLOW TEST:26.304 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:13:28.255: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6517
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-b10ed21a-eb99-4b87-9ca7-639e2353a8d7 in namespace container-probe-6517
Aug 14 16:13:39.679: INFO: Started pod liveness-b10ed21a-eb99-4b87-9ca7-639e2353a8d7 in namespace container-probe-6517
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 16:13:39.684: INFO: Initial restart count of pod liveness-b10ed21a-eb99-4b87-9ca7-639e2353a8d7 is 0
Aug 14 16:14:01.206: INFO: Restart count of pod container-probe-6517/liveness-b10ed21a-eb99-4b87-9ca7-639e2353a8d7 is now 1 (21.522028497s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:14:01.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6517" for this suite.
Aug 14 16:14:21.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:14:23.079: INFO: namespace container-probe-6517 deletion completed in 20.905625023s

â€¢ [SLOW TEST:54.825 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:14:23.080: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:14:26.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8400" for this suite.
Aug 14 16:14:38.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:14:41.865: INFO: namespace kubelet-test-8400 deletion completed in 15.406631765s

â€¢ [SLOW TEST:18.785 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:14:41.866: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7372
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-7372/secret-test-4b79713c-a2f4-4cdc-88b4-ddcf6ed51545
STEP: Creating a pod to test consume secrets
Aug 14 16:14:43.478: INFO: Waiting up to 5m0s for pod "pod-configmaps-7faee71a-5d26-480a-9c9c-c7b4e89281b8" in namespace "secrets-7372" to be "success or failure"
Aug 14 16:14:43.485: INFO: Pod "pod-configmaps-7faee71a-5d26-480a-9c9c-c7b4e89281b8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.49883ms
Aug 14 16:14:45.491: INFO: Pod "pod-configmaps-7faee71a-5d26-480a-9c9c-c7b4e89281b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013078083s
Aug 14 16:14:47.495: INFO: Pod "pod-configmaps-7faee71a-5d26-480a-9c9c-c7b4e89281b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017050873s
Aug 14 16:14:49.654: INFO: Pod "pod-configmaps-7faee71a-5d26-480a-9c9c-c7b4e89281b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.17622094s
Aug 14 16:14:51.667: INFO: Pod "pod-configmaps-7faee71a-5d26-480a-9c9c-c7b4e89281b8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.189640694s
Aug 14 16:14:53.676: INFO: Pod "pod-configmaps-7faee71a-5d26-480a-9c9c-c7b4e89281b8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.197895292s
Aug 14 16:14:55.785: INFO: Pod "pod-configmaps-7faee71a-5d26-480a-9c9c-c7b4e89281b8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.306915893s
Aug 14 16:14:57.816: INFO: Pod "pod-configmaps-7faee71a-5d26-480a-9c9c-c7b4e89281b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.337960135s
STEP: Saw pod success
Aug 14 16:14:57.816: INFO: Pod "pod-configmaps-7faee71a-5d26-480a-9c9c-c7b4e89281b8" satisfied condition "success or failure"
Aug 14 16:14:57.822: INFO: Trying to get logs from node slave3 pod pod-configmaps-7faee71a-5d26-480a-9c9c-c7b4e89281b8 container env-test: <nil>
STEP: delete the pod
Aug 14 16:14:58.308: INFO: Waiting for pod pod-configmaps-7faee71a-5d26-480a-9c9c-c7b4e89281b8 to disappear
Aug 14 16:14:58.316: INFO: Pod pod-configmaps-7faee71a-5d26-480a-9c9c-c7b4e89281b8 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:14:58.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7372" for this suite.
Aug 14 16:15:14.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:15:16.540: INFO: namespace secrets-7372 deletion completed in 18.217920006s

â€¢ [SLOW TEST:34.674 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:15:16.540: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7011
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-c4d38fe8-a1a4-4252-af37-8e251cb941a1
STEP: Creating a pod to test consume configMaps
Aug 14 16:15:17.654: INFO: Waiting up to 5m0s for pod "pod-configmaps-d79fec79-243c-47f9-8c8a-d1e8b84d2498" in namespace "configmap-7011" to be "success or failure"
Aug 14 16:15:17.659: INFO: Pod "pod-configmaps-d79fec79-243c-47f9-8c8a-d1e8b84d2498": Phase="Pending", Reason="", readiness=false. Elapsed: 4.486119ms
Aug 14 16:15:19.663: INFO: Pod "pod-configmaps-d79fec79-243c-47f9-8c8a-d1e8b84d2498": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009186154s
Aug 14 16:15:21.782: INFO: Pod "pod-configmaps-d79fec79-243c-47f9-8c8a-d1e8b84d2498": Phase="Pending", Reason="", readiness=false. Elapsed: 4.127552432s
Aug 14 16:15:23.791: INFO: Pod "pod-configmaps-d79fec79-243c-47f9-8c8a-d1e8b84d2498": Phase="Pending", Reason="", readiness=false. Elapsed: 6.136699107s
Aug 14 16:15:26.239: INFO: Pod "pod-configmaps-d79fec79-243c-47f9-8c8a-d1e8b84d2498": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.584645082s
STEP: Saw pod success
Aug 14 16:15:26.239: INFO: Pod "pod-configmaps-d79fec79-243c-47f9-8c8a-d1e8b84d2498" satisfied condition "success or failure"
Aug 14 16:15:26.445: INFO: Trying to get logs from node slave1 pod pod-configmaps-d79fec79-243c-47f9-8c8a-d1e8b84d2498 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 16:15:27.094: INFO: Waiting for pod pod-configmaps-d79fec79-243c-47f9-8c8a-d1e8b84d2498 to disappear
Aug 14 16:15:27.120: INFO: Pod pod-configmaps-d79fec79-243c-47f9-8c8a-d1e8b84d2498 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:15:27.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7011" for this suite.
Aug 14 16:15:37.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:15:39.734: INFO: namespace configmap-7011 deletion completed in 12.607081506s

â€¢ [SLOW TEST:23.194 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:15:39.734: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2008
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 16:15:41.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e54c4992-e603-4f78-91e9-1488e2f991a4" in namespace "downward-api-2008" to be "success or failure"
Aug 14 16:15:41.718: INFO: Pod "downwardapi-volume-e54c4992-e603-4f78-91e9-1488e2f991a4": Phase="Pending", Reason="", readiness=false. Elapsed: 278.522535ms
Aug 14 16:15:43.804: INFO: Pod "downwardapi-volume-e54c4992-e603-4f78-91e9-1488e2f991a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.364305724s
Aug 14 16:15:45.835: INFO: Pod "downwardapi-volume-e54c4992-e603-4f78-91e9-1488e2f991a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.395597346s
Aug 14 16:15:47.840: INFO: Pod "downwardapi-volume-e54c4992-e603-4f78-91e9-1488e2f991a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.400413216s
STEP: Saw pod success
Aug 14 16:15:47.840: INFO: Pod "downwardapi-volume-e54c4992-e603-4f78-91e9-1488e2f991a4" satisfied condition "success or failure"
Aug 14 16:15:47.848: INFO: Trying to get logs from node slave2 pod downwardapi-volume-e54c4992-e603-4f78-91e9-1488e2f991a4 container client-container: <nil>
STEP: delete the pod
Aug 14 16:15:47.889: INFO: Waiting for pod downwardapi-volume-e54c4992-e603-4f78-91e9-1488e2f991a4 to disappear
Aug 14 16:15:47.895: INFO: Pod downwardapi-volume-e54c4992-e603-4f78-91e9-1488e2f991a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:15:47.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2008" for this suite.
Aug 14 16:15:58.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:15:59.750: INFO: namespace downward-api-2008 deletion completed in 11.848855701s

â€¢ [SLOW TEST:20.016 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:15:59.750: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4488
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Aug 14 16:16:01.617: INFO: Waiting up to 5m0s for pod "client-containers-330a52cf-49eb-430b-9c5f-e00b3a814da1" in namespace "containers-4488" to be "success or failure"
Aug 14 16:16:01.621: INFO: Pod "client-containers-330a52cf-49eb-430b-9c5f-e00b3a814da1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.563349ms
Aug 14 16:16:03.728: INFO: Pod "client-containers-330a52cf-49eb-430b-9c5f-e00b3a814da1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110973446s
Aug 14 16:16:05.842: INFO: Pod "client-containers-330a52cf-49eb-430b-9c5f-e00b3a814da1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.22569108s
Aug 14 16:16:07.849: INFO: Pod "client-containers-330a52cf-49eb-430b-9c5f-e00b3a814da1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.232346524s
Aug 14 16:16:09.855: INFO: Pod "client-containers-330a52cf-49eb-430b-9c5f-e00b3a814da1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.238617657s
Aug 14 16:16:11.920: INFO: Pod "client-containers-330a52cf-49eb-430b-9c5f-e00b3a814da1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.302841081s
STEP: Saw pod success
Aug 14 16:16:11.920: INFO: Pod "client-containers-330a52cf-49eb-430b-9c5f-e00b3a814da1" satisfied condition "success or failure"
Aug 14 16:16:12.285: INFO: Trying to get logs from node slave3 pod client-containers-330a52cf-49eb-430b-9c5f-e00b3a814da1 container test-container: <nil>
STEP: delete the pod
Aug 14 16:16:13.367: INFO: Waiting for pod client-containers-330a52cf-49eb-430b-9c5f-e00b3a814da1 to disappear
Aug 14 16:16:13.418: INFO: Pod client-containers-330a52cf-49eb-430b-9c5f-e00b3a814da1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:16:13.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4488" for this suite.
Aug 14 16:16:23.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:16:25.540: INFO: namespace containers-4488 deletion completed in 12.012237765s

â€¢ [SLOW TEST:25.789 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:16:25.540: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-583
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-583
STEP: Creating statefulset with conflicting port in namespace statefulset-583
STEP: Waiting until pod test-pod will start running in namespace statefulset-583
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-583
Aug 14 16:16:41.569: INFO: Observed stateful pod in namespace: statefulset-583, name: ss-0, uid: 508aec30-0c60-491e-b7fd-502d3751125d, status phase: Pending. Waiting for statefulset controller to delete.
Aug 14 16:16:41.984: INFO: Observed stateful pod in namespace: statefulset-583, name: ss-0, uid: 508aec30-0c60-491e-b7fd-502d3751125d, status phase: Failed. Waiting for statefulset controller to delete.
Aug 14 16:16:42.240: INFO: Observed stateful pod in namespace: statefulset-583, name: ss-0, uid: 508aec30-0c60-491e-b7fd-502d3751125d, status phase: Failed. Waiting for statefulset controller to delete.
Aug 14 16:16:42.428: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-583
STEP: Removing pod with conflicting port in namespace statefulset-583
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-583 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 14 16:16:57.795: INFO: Deleting all statefulset in ns statefulset-583
Aug 14 16:16:57.804: INFO: Scaling statefulset ss to 0
Aug 14 16:17:18.050: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 16:17:18.054: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:17:18.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-583" for this suite.
Aug 14 16:17:32.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:17:34.468: INFO: namespace statefulset-583 deletion completed in 15.983604604s

â€¢ [SLOW TEST:68.928 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:17:34.469: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 16:17:36.375: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99045b2a-2682-4580-9a2f-ccb34c7e22d4" in namespace "downward-api-1742" to be "success or failure"
Aug 14 16:17:36.390: INFO: Pod "downwardapi-volume-99045b2a-2682-4580-9a2f-ccb34c7e22d4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.480038ms
Aug 14 16:17:38.394: INFO: Pod "downwardapi-volume-99045b2a-2682-4580-9a2f-ccb34c7e22d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01861775s
Aug 14 16:17:40.447: INFO: Pod "downwardapi-volume-99045b2a-2682-4580-9a2f-ccb34c7e22d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07104953s
Aug 14 16:17:42.452: INFO: Pod "downwardapi-volume-99045b2a-2682-4580-9a2f-ccb34c7e22d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076245164s
Aug 14 16:17:44.538: INFO: Pod "downwardapi-volume-99045b2a-2682-4580-9a2f-ccb34c7e22d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.162504147s
STEP: Saw pod success
Aug 14 16:17:44.538: INFO: Pod "downwardapi-volume-99045b2a-2682-4580-9a2f-ccb34c7e22d4" satisfied condition "success or failure"
Aug 14 16:17:44.542: INFO: Trying to get logs from node slave1 pod downwardapi-volume-99045b2a-2682-4580-9a2f-ccb34c7e22d4 container client-container: <nil>
STEP: delete the pod
Aug 14 16:17:45.341: INFO: Waiting for pod downwardapi-volume-99045b2a-2682-4580-9a2f-ccb34c7e22d4 to disappear
Aug 14 16:17:45.346: INFO: Pod downwardapi-volume-99045b2a-2682-4580-9a2f-ccb34c7e22d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:17:45.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1742" for this suite.
Aug 14 16:17:53.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:17:55.274: INFO: namespace downward-api-1742 deletion completed in 9.92232287s

â€¢ [SLOW TEST:20.806 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:17:55.275: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4619
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 16:17:59.705: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 14 16:17:59.855: INFO: Number of nodes with available pods: 0
Aug 14 16:17:59.855: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 14 16:18:00.165: INFO: Number of nodes with available pods: 0
Aug 14 16:18:00.165: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:01.501: INFO: Number of nodes with available pods: 0
Aug 14 16:18:01.501: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:02.232: INFO: Number of nodes with available pods: 0
Aug 14 16:18:02.232: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:03.169: INFO: Number of nodes with available pods: 0
Aug 14 16:18:03.169: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:04.199: INFO: Number of nodes with available pods: 0
Aug 14 16:18:04.199: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:05.890: INFO: Number of nodes with available pods: 0
Aug 14 16:18:05.890: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:06.724: INFO: Number of nodes with available pods: 0
Aug 14 16:18:06.724: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:07.484: INFO: Number of nodes with available pods: 0
Aug 14 16:18:07.485: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:08.170: INFO: Number of nodes with available pods: 0
Aug 14 16:18:08.170: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:09.298: INFO: Number of nodes with available pods: 0
Aug 14 16:18:09.298: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:10.393: INFO: Number of nodes with available pods: 0
Aug 14 16:18:10.393: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:11.213: INFO: Number of nodes with available pods: 1
Aug 14 16:18:11.213: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 14 16:18:11.247: INFO: Number of nodes with available pods: 1
Aug 14 16:18:11.247: INFO: Number of running nodes: 0, number of available pods: 1
Aug 14 16:18:12.309: INFO: Number of nodes with available pods: 0
Aug 14 16:18:12.309: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 14 16:18:12.324: INFO: Number of nodes with available pods: 0
Aug 14 16:18:12.324: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:13.484: INFO: Number of nodes with available pods: 0
Aug 14 16:18:13.484: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:14.783: INFO: Number of nodes with available pods: 0
Aug 14 16:18:14.783: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:15.328: INFO: Number of nodes with available pods: 0
Aug 14 16:18:15.328: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:16.486: INFO: Number of nodes with available pods: 0
Aug 14 16:18:16.487: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:17.330: INFO: Number of nodes with available pods: 0
Aug 14 16:18:17.330: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:18.329: INFO: Number of nodes with available pods: 0
Aug 14 16:18:18.329: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:19.418: INFO: Number of nodes with available pods: 0
Aug 14 16:18:19.418: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:20.333: INFO: Number of nodes with available pods: 0
Aug 14 16:18:20.333: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:21.331: INFO: Number of nodes with available pods: 0
Aug 14 16:18:21.331: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:22.328: INFO: Number of nodes with available pods: 0
Aug 14 16:18:22.328: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:23.328: INFO: Number of nodes with available pods: 0
Aug 14 16:18:23.328: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:24.461: INFO: Number of nodes with available pods: 0
Aug 14 16:18:24.461: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:25.561: INFO: Number of nodes with available pods: 0
Aug 14 16:18:25.561: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:26.467: INFO: Number of nodes with available pods: 0
Aug 14 16:18:26.467: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:27.363: INFO: Number of nodes with available pods: 0
Aug 14 16:18:27.363: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:28.925: INFO: Number of nodes with available pods: 0
Aug 14 16:18:28.925: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:30.766: INFO: Number of nodes with available pods: 0
Aug 14 16:18:30.766: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:31.341: INFO: Number of nodes with available pods: 0
Aug 14 16:18:31.341: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:32.329: INFO: Number of nodes with available pods: 0
Aug 14 16:18:32.329: INFO: Node master1 is running more than one daemon pod
Aug 14 16:18:33.378: INFO: Number of nodes with available pods: 1
Aug 14 16:18:33.378: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4619, will wait for the garbage collector to delete the pods
Aug 14 16:18:33.820: INFO: Deleting DaemonSet.extensions daemon-set took: 173.243633ms
Aug 14 16:18:34.021: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.315202ms
Aug 14 16:18:41.820: INFO: Number of nodes with available pods: 0
Aug 14 16:18:41.820: INFO: Number of running nodes: 0, number of available pods: 0
Aug 14 16:18:41.824: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4619/daemonsets","resourceVersion":"385090"},"items":null}

Aug 14 16:18:41.827: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4619/pods","resourceVersion":"385090"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:18:42.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4619" for this suite.
Aug 14 16:18:54.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:18:56.068: INFO: namespace daemonsets-4619 deletion completed in 13.845502825s

â€¢ [SLOW TEST:60.794 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:18:56.069: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8476
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-c4902b34-f8cc-431d-ae7a-73bb8ff4665d
STEP: Creating a pod to test consume secrets
Aug 14 16:18:57.928: INFO: Waiting up to 5m0s for pod "pod-secrets-e6a62083-f027-4c51-b4ac-70c0854e303b" in namespace "secrets-8476" to be "success or failure"
Aug 14 16:18:57.933: INFO: Pod "pod-secrets-e6a62083-f027-4c51-b4ac-70c0854e303b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.565996ms
Aug 14 16:18:59.937: INFO: Pod "pod-secrets-e6a62083-f027-4c51-b4ac-70c0854e303b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008994464s
Aug 14 16:19:02.043: INFO: Pod "pod-secrets-e6a62083-f027-4c51-b4ac-70c0854e303b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.114763698s
Aug 14 16:19:04.387: INFO: Pod "pod-secrets-e6a62083-f027-4c51-b4ac-70c0854e303b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.459114712s
Aug 14 16:19:06.407: INFO: Pod "pod-secrets-e6a62083-f027-4c51-b4ac-70c0854e303b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.47915292s
Aug 14 16:19:08.413: INFO: Pod "pod-secrets-e6a62083-f027-4c51-b4ac-70c0854e303b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.484637242s
STEP: Saw pod success
Aug 14 16:19:08.413: INFO: Pod "pod-secrets-e6a62083-f027-4c51-b4ac-70c0854e303b" satisfied condition "success or failure"
Aug 14 16:19:08.416: INFO: Trying to get logs from node slave2 pod pod-secrets-e6a62083-f027-4c51-b4ac-70c0854e303b container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 16:19:08.860: INFO: Waiting for pod pod-secrets-e6a62083-f027-4c51-b4ac-70c0854e303b to disappear
Aug 14 16:19:08.867: INFO: Pod pod-secrets-e6a62083-f027-4c51-b4ac-70c0854e303b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:19:08.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8476" for this suite.
Aug 14 16:19:21.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:19:23.830: INFO: namespace secrets-8476 deletion completed in 14.462346601s

â€¢ [SLOW TEST:27.761 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:19:23.831: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4659
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 14 16:19:47.134: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 16:19:47.140: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 16:19:49.140: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 16:19:49.284: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 16:19:51.140: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 16:19:51.145: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 16:19:53.141: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 16:19:53.145: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 16:19:55.140: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 16:19:55.145: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 16:19:57.140: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 16:19:57.145: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 16:19:59.140: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 16:19:59.334: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 16:20:01.140: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 16:20:01.145: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 16:20:03.140: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 16:20:03.508: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 16:20:05.140: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 16:20:05.804: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 16:20:07.140: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 16:20:07.145: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 16:20:09.140: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 16:20:09.302: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 16:20:11.140: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 16:20:11.346: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:20:11.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4659" for this suite.
Aug 14 16:20:42.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:20:44.545: INFO: namespace container-lifecycle-hook-4659 deletion completed in 33.18877766s

â€¢ [SLOW TEST:80.714 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:20:44.545: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2757
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Aug 14 16:20:47.475: INFO: Waiting up to 5m0s for pod "client-containers-68c4e1a7-1ae2-454e-aa56-2df82140bd92" in namespace "containers-2757" to be "success or failure"
Aug 14 16:20:47.789: INFO: Pod "client-containers-68c4e1a7-1ae2-454e-aa56-2df82140bd92": Phase="Pending", Reason="", readiness=false. Elapsed: 314.093421ms
Aug 14 16:20:49.795: INFO: Pod "client-containers-68c4e1a7-1ae2-454e-aa56-2df82140bd92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.319539622s
Aug 14 16:20:52.263: INFO: Pod "client-containers-68c4e1a7-1ae2-454e-aa56-2df82140bd92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.7880734s
Aug 14 16:20:54.323: INFO: Pod "client-containers-68c4e1a7-1ae2-454e-aa56-2df82140bd92": Phase="Pending", Reason="", readiness=false. Elapsed: 6.847595691s
Aug 14 16:20:56.370: INFO: Pod "client-containers-68c4e1a7-1ae2-454e-aa56-2df82140bd92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.895380469s
STEP: Saw pod success
Aug 14 16:20:56.370: INFO: Pod "client-containers-68c4e1a7-1ae2-454e-aa56-2df82140bd92" satisfied condition "success or failure"
Aug 14 16:20:56.376: INFO: Trying to get logs from node slave1 pod client-containers-68c4e1a7-1ae2-454e-aa56-2df82140bd92 container test-container: <nil>
STEP: delete the pod
Aug 14 16:20:57.719: INFO: Waiting for pod client-containers-68c4e1a7-1ae2-454e-aa56-2df82140bd92 to disappear
Aug 14 16:20:57.723: INFO: Pod client-containers-68c4e1a7-1ae2-454e-aa56-2df82140bd92 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:20:57.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2757" for this suite.
Aug 14 16:21:11.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:21:14.483: INFO: namespace containers-2757 deletion completed in 16.727357482s

â€¢ [SLOW TEST:29.939 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:21:14.484: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6557
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:21:28.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6557" for this suite.
Aug 14 16:22:00.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:22:02.089: INFO: namespace replication-controller-6557 deletion completed in 33.869058478s

â€¢ [SLOW TEST:47.606 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:22:02.090: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-811
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Aug 14 16:22:04.566: INFO: namespace kubectl-811
Aug 14 16:22:04.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 create -f - --namespace=kubectl-811'
Aug 14 16:22:16.091: INFO: stderr: ""
Aug 14 16:22:16.092: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 14 16:22:17.448: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 16:22:17.448: INFO: Found 0 / 1
Aug 14 16:22:18.102: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 16:22:18.102: INFO: Found 0 / 1
Aug 14 16:22:19.168: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 16:22:19.169: INFO: Found 0 / 1
Aug 14 16:22:20.166: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 16:22:20.166: INFO: Found 0 / 1
Aug 14 16:22:21.099: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 16:22:21.100: INFO: Found 0 / 1
Aug 14 16:22:22.500: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 16:22:22.500: INFO: Found 0 / 1
Aug 14 16:22:23.219: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 16:22:23.219: INFO: Found 0 / 1
Aug 14 16:22:24.472: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 16:22:24.472: INFO: Found 0 / 1
Aug 14 16:22:25.144: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 16:22:25.144: INFO: Found 0 / 1
Aug 14 16:22:26.226: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 16:22:26.226: INFO: Found 0 / 1
Aug 14 16:22:27.097: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 16:22:27.097: INFO: Found 1 / 1
Aug 14 16:22:27.097: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 14 16:22:27.107: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 16:22:27.107: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 14 16:22:27.107: INFO: wait on redis-master startup in kubectl-811 
Aug 14 16:22:27.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 logs redis-master-t4pf4 redis-master --namespace=kubectl-811'
Aug 14 16:22:28.261: INFO: stderr: ""
Aug 14 16:22:28.261: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Aug 16:22:24.775 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Aug 16:22:24.775 # Server started, Redis version 3.2.12\n1:M 14 Aug 16:22:24.775 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Aug 16:22:24.775 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Aug 14 16:22:28.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-811'
Aug 14 16:22:29.870: INFO: stderr: ""
Aug 14 16:22:29.870: INFO: stdout: "service/rm2 exposed\n"
Aug 14 16:22:29.913: INFO: Service rm2 in namespace kubectl-811 found.
STEP: exposing service
Aug 14 16:22:31.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-811'
Aug 14 16:22:33.728: INFO: stderr: ""
Aug 14 16:22:33.728: INFO: stdout: "service/rm3 exposed\n"
Aug 14 16:22:33.803: INFO: Service rm3 in namespace kubectl-811 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:22:35.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-811" for this suite.
Aug 14 16:23:12.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:23:14.817: INFO: namespace kubectl-811 deletion completed in 38.99955314s

â€¢ [SLOW TEST:72.727 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:23:14.817: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 16:23:17.007: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ab4ff59-858f-4165-beeb-707b5b51f2d5" in namespace "projected-6159" to be "success or failure"
Aug 14 16:23:17.016: INFO: Pod "downwardapi-volume-8ab4ff59-858f-4165-beeb-707b5b51f2d5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.071659ms
Aug 14 16:23:19.083: INFO: Pod "downwardapi-volume-8ab4ff59-858f-4165-beeb-707b5b51f2d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076045802s
Aug 14 16:23:21.481: INFO: Pod "downwardapi-volume-8ab4ff59-858f-4165-beeb-707b5b51f2d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.473779734s
Aug 14 16:23:23.603: INFO: Pod "downwardapi-volume-8ab4ff59-858f-4165-beeb-707b5b51f2d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.595932201s
Aug 14 16:23:25.612: INFO: Pod "downwardapi-volume-8ab4ff59-858f-4165-beeb-707b5b51f2d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.605135993s
STEP: Saw pod success
Aug 14 16:23:25.612: INFO: Pod "downwardapi-volume-8ab4ff59-858f-4165-beeb-707b5b51f2d5" satisfied condition "success or failure"
Aug 14 16:23:25.617: INFO: Trying to get logs from node slave1 pod downwardapi-volume-8ab4ff59-858f-4165-beeb-707b5b51f2d5 container client-container: <nil>
STEP: delete the pod
Aug 14 16:23:26.192: INFO: Waiting for pod downwardapi-volume-8ab4ff59-858f-4165-beeb-707b5b51f2d5 to disappear
Aug 14 16:23:26.247: INFO: Pod downwardapi-volume-8ab4ff59-858f-4165-beeb-707b5b51f2d5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:23:26.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6159" for this suite.
Aug 14 16:23:37.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:23:40.707: INFO: namespace projected-6159 deletion completed in 14.241295958s

â€¢ [SLOW TEST:25.890 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:23:40.707: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-5760
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-5760
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5760
STEP: Deleting pre-stop pod
Aug 14 16:24:06.735: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:24:06.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5760" for this suite.
Aug 14 16:24:44.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:24:46.596: INFO: namespace prestop-5760 deletion completed in 39.833461063s

â€¢ [SLOW TEST:65.889 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:24:46.597: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-782
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 16:24:48.895: INFO: Creating ReplicaSet my-hostname-basic-bdccabd8-3e34-41e5-a7c1-9526288b8c1f
Aug 14 16:24:50.077: INFO: Pod name my-hostname-basic-bdccabd8-3e34-41e5-a7c1-9526288b8c1f: Found 1 pods out of 1
Aug 14 16:24:50.077: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-bdccabd8-3e34-41e5-a7c1-9526288b8c1f" is running
Aug 14 16:24:58.324: INFO: Pod "my-hostname-basic-bdccabd8-3e34-41e5-a7c1-9526288b8c1f-mpbj6" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-14 16:24:49 +0000 UTC Reason: Message:}])
Aug 14 16:24:58.324: INFO: Trying to dial the pod
Aug 14 16:25:03.690: INFO: Controller my-hostname-basic-bdccabd8-3e34-41e5-a7c1-9526288b8c1f: Got expected result from replica 1 [my-hostname-basic-bdccabd8-3e34-41e5-a7c1-9526288b8c1f-mpbj6]: "my-hostname-basic-bdccabd8-3e34-41e5-a7c1-9526288b8c1f-mpbj6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:25:03.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-782" for this suite.
Aug 14 16:25:19.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:25:21.686: INFO: namespace replicaset-782 deletion completed in 17.989823858s

â€¢ [SLOW TEST:35.090 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:25:21.691: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4649
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 14 16:25:23.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4649'
Aug 14 16:25:24.469: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 14 16:25:24.469: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Aug 14 16:25:24.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete jobs e2e-test-nginx-job --namespace=kubectl-4649'
Aug 14 16:25:26.304: INFO: stderr: ""
Aug 14 16:25:26.304: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:25:26.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4649" for this suite.
Aug 14 16:25:37.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:25:39.563: INFO: namespace kubectl-4649 deletion completed in 12.924354206s

â€¢ [SLOW TEST:17.872 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:25:39.564: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-925
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 14 16:25:41.749: INFO: Number of nodes with available pods: 0
Aug 14 16:25:41.749: INFO: Node master1 is running more than one daemon pod
Aug 14 16:25:43.829: INFO: Number of nodes with available pods: 0
Aug 14 16:25:43.829: INFO: Node master1 is running more than one daemon pod
Aug 14 16:25:46.715: INFO: Number of nodes with available pods: 0
Aug 14 16:25:46.715: INFO: Node master1 is running more than one daemon pod
Aug 14 16:25:47.666: INFO: Number of nodes with available pods: 0
Aug 14 16:25:47.666: INFO: Node master1 is running more than one daemon pod
Aug 14 16:25:48.465: INFO: Number of nodes with available pods: 0
Aug 14 16:25:48.465: INFO: Node master1 is running more than one daemon pod
Aug 14 16:25:49.923: INFO: Number of nodes with available pods: 0
Aug 14 16:25:49.925: INFO: Node master1 is running more than one daemon pod
Aug 14 16:25:52.154: INFO: Number of nodes with available pods: 0
Aug 14 16:25:52.155: INFO: Node master1 is running more than one daemon pod
Aug 14 16:25:53.994: INFO: Number of nodes with available pods: 0
Aug 14 16:25:53.994: INFO: Node master1 is running more than one daemon pod
Aug 14 16:25:56.539: INFO: Number of nodes with available pods: 1
Aug 14 16:25:56.539: INFO: Node master1 is running more than one daemon pod
Aug 14 16:25:58.323: INFO: Number of nodes with available pods: 4
Aug 14 16:25:58.323: INFO: Node master1 is running more than one daemon pod
Aug 14 16:25:59.813: INFO: Number of nodes with available pods: 5
Aug 14 16:25:59.813: INFO: Node master2 is running more than one daemon pod
Aug 14 16:26:00.863: INFO: Number of nodes with available pods: 6
Aug 14 16:26:00.863: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 14 16:26:01.038: INFO: Number of nodes with available pods: 5
Aug 14 16:26:01.038: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:02.062: INFO: Number of nodes with available pods: 5
Aug 14 16:26:02.062: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:03.208: INFO: Number of nodes with available pods: 5
Aug 14 16:26:03.208: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:04.307: INFO: Number of nodes with available pods: 5
Aug 14 16:26:04.307: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:05.131: INFO: Number of nodes with available pods: 5
Aug 14 16:26:05.131: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:06.050: INFO: Number of nodes with available pods: 5
Aug 14 16:26:06.050: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:07.108: INFO: Number of nodes with available pods: 5
Aug 14 16:26:07.108: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:08.474: INFO: Number of nodes with available pods: 5
Aug 14 16:26:08.474: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:09.048: INFO: Number of nodes with available pods: 5
Aug 14 16:26:09.048: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:10.050: INFO: Number of nodes with available pods: 5
Aug 14 16:26:10.051: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:11.148: INFO: Number of nodes with available pods: 5
Aug 14 16:26:11.148: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:12.055: INFO: Number of nodes with available pods: 5
Aug 14 16:26:12.055: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:13.053: INFO: Number of nodes with available pods: 5
Aug 14 16:26:13.053: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:14.051: INFO: Number of nodes with available pods: 5
Aug 14 16:26:14.051: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:15.774: INFO: Number of nodes with available pods: 5
Aug 14 16:26:15.774: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:16.225: INFO: Number of nodes with available pods: 5
Aug 14 16:26:16.225: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:17.615: INFO: Number of nodes with available pods: 5
Aug 14 16:26:17.615: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:18.051: INFO: Number of nodes with available pods: 5
Aug 14 16:26:18.051: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:19.967: INFO: Number of nodes with available pods: 5
Aug 14 16:26:19.967: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:20.977: INFO: Number of nodes with available pods: 5
Aug 14 16:26:20.977: INFO: Node master3 is running more than one daemon pod
Aug 14 16:26:21.332: INFO: Number of nodes with available pods: 6
Aug 14 16:26:21.332: INFO: Number of running nodes: 6, number of available pods: 6
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-925, will wait for the garbage collector to delete the pods
Aug 14 16:26:21.599: INFO: Deleting DaemonSet.extensions daemon-set took: 207.508447ms
Aug 14 16:26:23.299: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.700191318s
Aug 14 16:26:41.903: INFO: Number of nodes with available pods: 0
Aug 14 16:26:41.903: INFO: Number of running nodes: 0, number of available pods: 0
Aug 14 16:26:41.906: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-925/daemonsets","resourceVersion":"386842"},"items":null}

Aug 14 16:26:41.910: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-925/pods","resourceVersion":"386842"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:26:41.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-925" for this suite.
Aug 14 16:26:58.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:26:59.879: INFO: namespace daemonsets-925 deletion completed in 17.919876081s

â€¢ [SLOW TEST:80.315 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:26:59.879: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-2204
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2204
I0814 16:27:01.528325      20 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2204, replica count: 1
I0814 16:27:02.578730      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 0 pending, 1 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:27:03.578923      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:27:04.579105      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:27:05.579279      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:27:06.579451      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:27:07.579654      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:27:08.579842      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:27:09.580015      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:27:10.580193      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 14 16:27:10.947: INFO: Created: latency-svc-6nxdp
Aug 14 16:27:11.108: INFO: Got endpoints: latency-svc-6nxdp [427.661422ms]
Aug 14 16:27:11.514: INFO: Created: latency-svc-g6mt8
Aug 14 16:27:12.055: INFO: Created: latency-svc-hk9b4
Aug 14 16:27:12.078: INFO: Got endpoints: latency-svc-g6mt8 [970.383714ms]
Aug 14 16:27:12.084: INFO: Got endpoints: latency-svc-hk9b4 [975.680017ms]
Aug 14 16:27:12.120: INFO: Created: latency-svc-mxtq7
Aug 14 16:27:12.593: INFO: Got endpoints: latency-svc-mxtq7 [1.485741801s]
Aug 14 16:27:12.597: INFO: Created: latency-svc-rx29d
Aug 14 16:27:12.607: INFO: Got endpoints: latency-svc-rx29d [1.499331276s]
Aug 14 16:27:12.613: INFO: Created: latency-svc-hzw9z
Aug 14 16:27:13.324: INFO: Got endpoints: latency-svc-hzw9z [2.215588562s]
Aug 14 16:27:13.602: INFO: Created: latency-svc-z88w8
Aug 14 16:27:13.603: INFO: Created: latency-svc-qcsdw
Aug 14 16:27:13.606: INFO: Created: latency-svc-f26r4
Aug 14 16:27:14.313: INFO: Got endpoints: latency-svc-z88w8 [3.204726718s]
Aug 14 16:27:14.313: INFO: Got endpoints: latency-svc-f26r4 [3.20516854s]
Aug 14 16:27:14.313: INFO: Got endpoints: latency-svc-qcsdw [3.20495809s]
Aug 14 16:27:14.318: INFO: Created: latency-svc-btqf2
Aug 14 16:27:14.318: INFO: Created: latency-svc-kznwz
Aug 14 16:27:14.649: INFO: Got endpoints: latency-svc-btqf2 [3.541390577s]
Aug 14 16:27:14.649: INFO: Got endpoints: latency-svc-kznwz [3.541488048s]
Aug 14 16:27:14.908: INFO: Created: latency-svc-c5mvt
Aug 14 16:27:15.311: INFO: Got endpoints: latency-svc-c5mvt [4.203492104s]
Aug 14 16:27:15.334: INFO: Created: latency-svc-7jk5v
Aug 14 16:27:15.632: INFO: Got endpoints: latency-svc-7jk5v [4.523623462s]
Aug 14 16:27:15.645: INFO: Created: latency-svc-nprnt
Aug 14 16:27:15.645: INFO: Created: latency-svc-prj6z
Aug 14 16:27:15.901: INFO: Created: latency-svc-5z5pk
Aug 14 16:27:16.123: INFO: Got endpoints: latency-svc-nprnt [5.015407471s]
Aug 14 16:27:16.124: INFO: Got endpoints: latency-svc-5z5pk [5.015954685s]
Aug 14 16:27:16.124: INFO: Got endpoints: latency-svc-prj6z [5.016206656s]
Aug 14 16:27:16.362: INFO: Created: latency-svc-kmbxg
Aug 14 16:27:16.715: INFO: Got endpoints: latency-svc-kmbxg [4.637098443s]
Aug 14 16:27:16.724: INFO: Created: latency-svc-kqmdn
Aug 14 16:27:16.874: INFO: Got endpoints: latency-svc-kqmdn [4.78984519s]
Aug 14 16:27:16.875: INFO: Created: latency-svc-7fmbf
Aug 14 16:27:17.422: INFO: Created: latency-svc-mjdgv
Aug 14 16:27:17.434: INFO: Got endpoints: latency-svc-7fmbf [4.840770939s]
Aug 14 16:27:18.012: INFO: Got endpoints: latency-svc-mjdgv [5.404112922s]
Aug 14 16:27:18.019: INFO: Created: latency-svc-2m4xx
Aug 14 16:27:18.075: INFO: Created: latency-svc-zdk8r
Aug 14 16:27:18.077: INFO: Got endpoints: latency-svc-2m4xx [4.753395489s]
Aug 14 16:27:18.100: INFO: Got endpoints: latency-svc-zdk8r [3.787132264s]
Aug 14 16:27:18.366: INFO: Created: latency-svc-q8hvl
Aug 14 16:27:19.085: INFO: Got endpoints: latency-svc-q8hvl [4.771662446s]
Aug 14 16:27:19.090: INFO: Created: latency-svc-vkx8p
Aug 14 16:27:19.094: INFO: Created: latency-svc-cxb27
Aug 14 16:27:19.278: INFO: Created: latency-svc-9p44d
Aug 14 16:27:19.575: INFO: Got endpoints: latency-svc-vkx8p [4.925828969s]
Aug 14 16:27:19.576: INFO: Got endpoints: latency-svc-cxb27 [5.262602522s]
Aug 14 16:27:20.039: INFO: Created: latency-svc-vhtpb
Aug 14 16:27:20.039: INFO: Got endpoints: latency-svc-9p44d [5.389242849s]
Aug 14 16:27:20.049: INFO: Created: latency-svc-hnk4s
Aug 14 16:27:20.054: INFO: Got endpoints: latency-svc-vhtpb [4.742758915s]
Aug 14 16:27:20.063: INFO: Got endpoints: latency-svc-hnk4s [4.4313404s]
Aug 14 16:27:20.553: INFO: Created: latency-svc-2bf9f
Aug 14 16:27:20.792: INFO: Got endpoints: latency-svc-2bf9f [4.669001823s]
Aug 14 16:27:20.793: INFO: Created: latency-svc-wh6pj
Aug 14 16:27:21.106: INFO: Created: latency-svc-44v6z
Aug 14 16:27:21.107: INFO: Created: latency-svc-ljjqr
Aug 14 16:27:21.197: INFO: Created: latency-svc-ngjfn
Aug 14 16:27:21.312: INFO: Got endpoints: latency-svc-ljjqr [4.596176662s]
Aug 14 16:27:21.312: INFO: Got endpoints: latency-svc-wh6pj [5.187454537s]
Aug 14 16:27:21.312: INFO: Got endpoints: latency-svc-44v6z [5.187744159s]
Aug 14 16:27:21.325: INFO: Got endpoints: latency-svc-ngjfn [4.451061723s]
Aug 14 16:27:21.367: INFO: Created: latency-svc-lq562
Aug 14 16:27:21.714: INFO: Got endpoints: latency-svc-lq562 [4.279169748s]
Aug 14 16:27:21.716: INFO: Created: latency-svc-m7pmj
Aug 14 16:27:21.716: INFO: Created: latency-svc-qb4fk
Aug 14 16:27:21.816: INFO: Created: latency-svc-tkjd5
Aug 14 16:27:22.094: INFO: Got endpoints: latency-svc-qb4fk [782.230836ms]
Aug 14 16:27:22.095: INFO: Got endpoints: latency-svc-m7pmj [4.082956696s]
Aug 14 16:27:22.100: INFO: Got endpoints: latency-svc-tkjd5 [4.022615458s]
Aug 14 16:27:22.144: INFO: Created: latency-svc-9jkbp
Aug 14 16:27:22.521: INFO: Got endpoints: latency-svc-9jkbp [4.420752395s]
Aug 14 16:27:22.580: INFO: Created: latency-svc-c7zkn
Aug 14 16:27:23.151: INFO: Got endpoints: latency-svc-c7zkn [4.065932145s]
Aug 14 16:27:23.160: INFO: Created: latency-svc-rp9bf
Aug 14 16:27:23.160: INFO: Created: latency-svc-nnhnw
Aug 14 16:27:23.629: INFO: Got endpoints: latency-svc-nnhnw [4.052956717s]
Aug 14 16:27:23.629: INFO: Got endpoints: latency-svc-rp9bf [4.053484138s]
Aug 14 16:27:23.631: INFO: Created: latency-svc-xbvtb
Aug 14 16:27:23.631: INFO: Created: latency-svc-6jq5k
Aug 14 16:27:23.631: INFO: Created: latency-svc-t4db9
Aug 14 16:27:23.664: INFO: Created: latency-svc-8q7dn
Aug 14 16:27:23.664: INFO: Got endpoints: latency-svc-t4db9 [3.610066425s]
Aug 14 16:27:23.664: INFO: Got endpoints: latency-svc-6jq5k [3.625717345s]
Aug 14 16:27:23.665: INFO: Got endpoints: latency-svc-xbvtb [3.601742941s]
Aug 14 16:27:23.880: INFO: Got endpoints: latency-svc-8q7dn [3.087862171s]
Aug 14 16:27:23.881: INFO: Created: latency-svc-nkjx8
Aug 14 16:27:24.044: INFO: Got endpoints: latency-svc-nkjx8 [2.731986462s]
Aug 14 16:27:25.099: INFO: Created: latency-svc-m8xnp
Aug 14 16:27:25.214: INFO: Got endpoints: latency-svc-m8xnp [3.902370684s]
Aug 14 16:27:25.216: INFO: Created: latency-svc-pdbnc
Aug 14 16:27:25.216: INFO: Created: latency-svc-mft5x
Aug 14 16:27:25.216: INFO: Created: latency-svc-mvbvr
Aug 14 16:27:25.375: INFO: Created: latency-svc-cvnmm
Aug 14 16:27:25.880: INFO: Created: latency-svc-kqzzg
Aug 14 16:27:25.880: INFO: Got endpoints: latency-svc-cvnmm [3.785500379s]
Aug 14 16:27:25.880: INFO: Got endpoints: latency-svc-pdbnc [4.555543172s]
Aug 14 16:27:25.881: INFO: Got endpoints: latency-svc-mvbvr [4.167279354s]
Aug 14 16:27:25.881: INFO: Got endpoints: latency-svc-mft5x [3.786837535s]
Aug 14 16:27:26.294: INFO: Got endpoints: latency-svc-kqzzg [4.193886097s]
Aug 14 16:27:26.300: INFO: Created: latency-svc-wmmhd
Aug 14 16:27:26.537: INFO: Created: latency-svc-s6xj2
Aug 14 16:27:26.538: INFO: Got endpoints: latency-svc-wmmhd [4.017136874s]
Aug 14 16:27:26.952: INFO: Got endpoints: latency-svc-s6xj2 [3.801644614s]
Aug 14 16:27:26.953: INFO: Created: latency-svc-kj69h
Aug 14 16:27:27.231: INFO: Got endpoints: latency-svc-kj69h [3.602134511s]
Aug 14 16:27:27.251: INFO: Created: latency-svc-flq75
Aug 14 16:27:27.568: INFO: Got endpoints: latency-svc-flq75 [3.938879655s]
Aug 14 16:27:28.072: INFO: Created: latency-svc-bfnjr
Aug 14 16:27:28.818: INFO: Created: latency-svc-bdv25
Aug 14 16:27:28.818: INFO: Got endpoints: latency-svc-bfnjr [5.153370898s]
Aug 14 16:27:28.820: INFO: Created: latency-svc-jqtkq
Aug 14 16:27:28.820: INFO: Got endpoints: latency-svc-bdv25 [5.155987188s]
Aug 14 16:27:29.110: INFO: Got endpoints: latency-svc-jqtkq [5.443587107s]
Aug 14 16:27:29.123: INFO: Created: latency-svc-tzlml
Aug 14 16:27:29.391: INFO: Got endpoints: latency-svc-tzlml [5.511091794s]
Aug 14 16:27:29.391: INFO: Created: latency-svc-rds7b
Aug 14 16:27:30.105: INFO: Got endpoints: latency-svc-rds7b [6.061683521s]
Aug 14 16:27:30.448: INFO: Created: latency-svc-cvnd4
Aug 14 16:27:30.728: INFO: Created: latency-svc-btqrx
Aug 14 16:27:31.666: INFO: Got endpoints: latency-svc-cvnd4 [6.451357224s]
Aug 14 16:27:31.666: INFO: Got endpoints: latency-svc-btqrx [5.785746307s]
Aug 14 16:27:31.667: INFO: Created: latency-svc-pq84j
Aug 14 16:27:31.667: INFO: Created: latency-svc-p8rxc
Aug 14 16:27:31.738: INFO: Created: latency-svc-xv9x9
Aug 14 16:27:31.738: INFO: Got endpoints: latency-svc-p8rxc [5.857363048s]
Aug 14 16:27:31.738: INFO: Got endpoints: latency-svc-pq84j [5.857897909s]
Aug 14 16:27:31.753: INFO: Created: latency-svc-rt66g
Aug 14 16:27:31.760: INFO: Got endpoints: latency-svc-xv9x9 [5.878725469s]
Aug 14 16:27:32.512: INFO: Got endpoints: latency-svc-rt66g [6.21783104s]
Aug 14 16:27:33.074: INFO: Created: latency-svc-brpbf
Aug 14 16:27:33.074: INFO: Created: latency-svc-w7hkq
Aug 14 16:27:34.205: INFO: Created: latency-svc-6xm8c
Aug 14 16:27:34.205: INFO: Created: latency-svc-rqrtt
Aug 14 16:27:34.205: INFO: Got endpoints: latency-svc-w7hkq [7.252183744s]
Aug 14 16:27:34.205: INFO: Got endpoints: latency-svc-brpbf [7.666756232s]
Aug 14 16:27:34.739: INFO: Got endpoints: latency-svc-6xm8c [7.508031052s]
Aug 14 16:27:34.740: INFO: Created: latency-svc-vw75c
Aug 14 16:27:34.740: INFO: Created: latency-svc-hm5rh
Aug 14 16:27:34.741: INFO: Got endpoints: latency-svc-rqrtt [7.172836836s]
Aug 14 16:27:34.741: INFO: Created: latency-svc-dvtvh
Aug 14 16:27:34.741: INFO: Created: latency-svc-krzqp
Aug 14 16:27:34.778: INFO: Got endpoints: latency-svc-hm5rh [5.95752216s]
Aug 14 16:27:34.778: INFO: Got endpoints: latency-svc-vw75c [5.960535108s]
Aug 14 16:27:34.933: INFO: Got endpoints: latency-svc-dvtvh [5.541971227s]
Aug 14 16:27:34.934: INFO: Got endpoints: latency-svc-krzqp [5.823965222s]
Aug 14 16:27:34.936: INFO: Created: latency-svc-gjt7j
Aug 14 16:27:34.972: INFO: Created: latency-svc-gkmnp
Aug 14 16:27:35.216: INFO: Got endpoints: latency-svc-gjt7j [5.11024353s]
Aug 14 16:27:35.222: INFO: Got endpoints: latency-svc-gkmnp [3.556559306s]
Aug 14 16:27:35.224: INFO: Created: latency-svc-6w2zq
Aug 14 16:27:35.966: INFO: Got endpoints: latency-svc-6w2zq [4.300187415s]
Aug 14 16:27:36.335: INFO: Created: latency-svc-gfgd4
Aug 14 16:27:36.693: INFO: Got endpoints: latency-svc-gfgd4 [4.955051204s]
Aug 14 16:27:36.694: INFO: Created: latency-svc-hkjnk
Aug 14 16:27:36.694: INFO: Created: latency-svc-jjzjh
Aug 14 16:27:37.088: INFO: Got endpoints: latency-svc-jjzjh [5.349748091s]
Aug 14 16:27:37.089: INFO: Created: latency-svc-flvl7
Aug 14 16:27:37.089: INFO: Created: latency-svc-znn2c
Aug 14 16:27:37.089: INFO: Got endpoints: latency-svc-hkjnk [5.329532038s]
Aug 14 16:27:37.099: INFO: Created: latency-svc-hpj6p
Aug 14 16:27:37.099: INFO: Got endpoints: latency-svc-znn2c [2.894049375s]
Aug 14 16:27:37.099: INFO: Got endpoints: latency-svc-flvl7 [4.587464434s]
Aug 14 16:27:37.103: INFO: Created: latency-svc-tmkxh
Aug 14 16:27:37.108: INFO: Created: latency-svc-n27l9
Aug 14 16:27:37.776: INFO: Created: latency-svc-ts8gz
Aug 14 16:27:37.777: INFO: Got endpoints: latency-svc-tmkxh [3.038326252s]
Aug 14 16:27:37.779: INFO: Got endpoints: latency-svc-hpj6p [3.574206748s]
Aug 14 16:27:37.779: INFO: Got endpoints: latency-svc-n27l9 [3.038547438s]
Aug 14 16:27:37.784: INFO: Created: latency-svc-bntc5
Aug 14 16:27:37.784: INFO: Got endpoints: latency-svc-ts8gz [3.005240466s]
Aug 14 16:27:38.186: INFO: Got endpoints: latency-svc-bntc5 [3.408343189s]
Aug 14 16:27:38.187: INFO: Created: latency-svc-wl2zv
Aug 14 16:27:38.621: INFO: Got endpoints: latency-svc-wl2zv [3.687721064s]
Aug 14 16:27:38.723: INFO: Created: latency-svc-r2t9n
Aug 14 16:27:38.822: INFO: Created: latency-svc-x8vh5
Aug 14 16:27:38.823: INFO: Got endpoints: latency-svc-r2t9n [3.889348169s]
Aug 14 16:27:39.053: INFO: Created: latency-svc-t2px5
Aug 14 16:27:39.054: INFO: Got endpoints: latency-svc-x8vh5 [3.838159049s]
Aug 14 16:27:39.054: INFO: Created: latency-svc-mtb4q
Aug 14 16:27:39.294: INFO: Created: latency-svc-f94gt
Aug 14 16:27:39.315: INFO: Got endpoints: latency-svc-t2px5 [4.092194581s]
Aug 14 16:27:39.315: INFO: Got endpoints: latency-svc-mtb4q [3.34880789s]
Aug 14 16:27:40.279: INFO: Got endpoints: latency-svc-f94gt [3.585102992s]
Aug 14 16:27:40.281: INFO: Created: latency-svc-wmjf7
Aug 14 16:27:40.552: INFO: Created: latency-svc-lpnnr
Aug 14 16:27:40.559: INFO: Got endpoints: latency-svc-wmjf7 [3.470558449s]
Aug 14 16:27:40.937: INFO: Got endpoints: latency-svc-lpnnr [3.847362971s]
Aug 14 16:27:41.398: INFO: Created: latency-svc-wzncz
Aug 14 16:27:41.672: INFO: Got endpoints: latency-svc-wzncz [4.572958994s]
Aug 14 16:27:41.672: INFO: Created: latency-svc-zf4zk
Aug 14 16:27:41.814: INFO: Created: latency-svc-4m9qc
Aug 14 16:27:41.814: INFO: Created: latency-svc-8v548
Aug 14 16:27:42.119: INFO: Got endpoints: latency-svc-8v548 [4.341805789s]
Aug 14 16:27:42.119: INFO: Got endpoints: latency-svc-zf4zk [5.020169451s]
Aug 14 16:27:42.530: INFO: Created: latency-svc-w7qrx
Aug 14 16:27:42.531: INFO: Got endpoints: latency-svc-4m9qc [4.75181893s]
Aug 14 16:27:42.622: INFO: Created: latency-svc-j5b8m
Aug 14 16:27:42.855: INFO: Got endpoints: latency-svc-j5b8m [5.071752164s]
Aug 14 16:27:42.856: INFO: Got endpoints: latency-svc-w7qrx [5.076811986s]
Aug 14 16:27:43.212: INFO: Created: latency-svc-5hnh5
Aug 14 16:27:43.690: INFO: Got endpoints: latency-svc-5hnh5 [5.503455545s]
Aug 14 16:27:44.623: INFO: Created: latency-svc-2f4gd
Aug 14 16:27:45.554: INFO: Got endpoints: latency-svc-2f4gd [6.932801443s]
Aug 14 16:27:45.556: INFO: Created: latency-svc-krvdh
Aug 14 16:27:45.556: INFO: Created: latency-svc-cdk4x
Aug 14 16:27:45.581: INFO: Got endpoints: latency-svc-krvdh [6.757831403s]
Aug 14 16:27:45.581: INFO: Got endpoints: latency-svc-cdk4x [6.527486799s]
Aug 14 16:27:45.600: INFO: Created: latency-svc-fcwxv
Aug 14 16:27:45.616: INFO: Created: latency-svc-cffbq
Aug 14 16:27:45.616: INFO: Got endpoints: latency-svc-fcwxv [6.301449654s]
Aug 14 16:27:45.835: INFO: Created: latency-svc-hnvcm
Aug 14 16:27:46.148: INFO: Got endpoints: latency-svc-cffbq [6.832863276s]
Aug 14 16:27:46.465: INFO: Got endpoints: latency-svc-hnvcm [6.186915093s]
Aug 14 16:27:46.492: INFO: Created: latency-svc-dwmgb
Aug 14 16:27:46.492: INFO: Created: latency-svc-tbhh9
Aug 14 16:27:46.503: INFO: Got endpoints: latency-svc-dwmgb [5.944267825s]
Aug 14 16:27:46.503: INFO: Got endpoints: latency-svc-tbhh9 [5.566394578s]
Aug 14 16:27:46.761: INFO: Created: latency-svc-x42h9
Aug 14 16:27:47.057: INFO: Got endpoints: latency-svc-x42h9 [5.384682491s]
Aug 14 16:27:47.067: INFO: Created: latency-svc-2jh5s
Aug 14 16:27:47.427: INFO: Got endpoints: latency-svc-2jh5s [5.307177843s]
Aug 14 16:27:47.622: INFO: Created: latency-svc-2ptmq
Aug 14 16:27:47.622: INFO: Created: latency-svc-jfzqj
Aug 14 16:27:47.622: INFO: Created: latency-svc-l84gq
Aug 14 16:27:47.867: INFO: Created: latency-svc-6spwq
Aug 14 16:27:47.878: INFO: Got endpoints: latency-svc-2ptmq [5.022048351s]
Aug 14 16:27:47.878: INFO: Got endpoints: latency-svc-l84gq [5.758482543s]
Aug 14 16:27:47.879: INFO: Created: latency-svc-fhsfx
Aug 14 16:27:47.879: INFO: Got endpoints: latency-svc-6spwq [5.022989765s]
Aug 14 16:27:47.879: INFO: Got endpoints: latency-svc-jfzqj [5.348048779s]
Aug 14 16:27:48.149: INFO: Created: latency-svc-5p6rh
Aug 14 16:27:48.153: INFO: Got endpoints: latency-svc-fhsfx [4.462654367s]
Aug 14 16:27:48.173: INFO: Got endpoints: latency-svc-5p6rh [2.619028687s]
Aug 14 16:27:48.177: INFO: Created: latency-svc-9kns7
Aug 14 16:27:48.555: INFO: Got endpoints: latency-svc-9kns7 [2.97448069s]
Aug 14 16:27:48.559: INFO: Created: latency-svc-zvxmf
Aug 14 16:27:48.624: INFO: Created: latency-svc-rv45v
Aug 14 16:27:48.633: INFO: Got endpoints: latency-svc-zvxmf [3.051303323s]
Aug 14 16:27:49.044: INFO: Got endpoints: latency-svc-rv45v [3.427535107s]
Aug 14 16:27:49.057: INFO: Created: latency-svc-mxpqp
Aug 14 16:27:49.492: INFO: Got endpoints: latency-svc-mxpqp [3.344360036s]
Aug 14 16:27:49.493: INFO: Created: latency-svc-6sf4t
Aug 14 16:27:49.493: INFO: Created: latency-svc-q5n2s
Aug 14 16:27:49.494: INFO: Created: latency-svc-c2wg6
Aug 14 16:27:49.495: INFO: Created: latency-svc-cmdhn
Aug 14 16:27:49.988: INFO: Created: latency-svc-m825z
Aug 14 16:27:50.358: INFO: Got endpoints: latency-svc-6sf4t [3.854589948s]
Aug 14 16:27:50.363: INFO: Got endpoints: latency-svc-q5n2s [3.305825769s]
Aug 14 16:27:50.364: INFO: Got endpoints: latency-svc-c2wg6 [3.898190554s]
Aug 14 16:27:50.364: INFO: Got endpoints: latency-svc-cmdhn [3.860783742s]
Aug 14 16:27:50.839: INFO: Got endpoints: latency-svc-m825z [3.412376115s]
Aug 14 16:27:51.098: INFO: Created: latency-svc-ws749
Aug 14 16:27:51.438: INFO: Created: latency-svc-pfpg6
Aug 14 16:27:51.438: INFO: Got endpoints: latency-svc-ws749 [3.559722727s]
Aug 14 16:27:51.880: INFO: Got endpoints: latency-svc-pfpg6 [4.000340929s]
Aug 14 16:27:52.438: INFO: Created: latency-svc-45gvv
Aug 14 16:27:52.439: INFO: Created: latency-svc-9lqkx
Aug 14 16:27:52.439: INFO: Created: latency-svc-x8qm7
Aug 14 16:27:52.439: INFO: Created: latency-svc-qwt9m
Aug 14 16:27:52.750: INFO: Created: latency-svc-pfqnk
Aug 14 16:27:52.751: INFO: Got endpoints: latency-svc-45gvv [4.577717936s]
Aug 14 16:27:52.751: INFO: Got endpoints: latency-svc-qwt9m [4.872088255s]
Aug 14 16:27:52.752: INFO: Got endpoints: latency-svc-x8qm7 [4.599072197s]
Aug 14 16:27:52.756: INFO: Got endpoints: latency-svc-9lqkx [4.878627434s]
Aug 14 16:27:52.990: INFO: Got endpoints: latency-svc-pfqnk [4.434528569s]
Aug 14 16:27:53.846: INFO: Created: latency-svc-rnp6c
Aug 14 16:27:53.850: INFO: Created: latency-svc-xgdbl
Aug 14 16:27:53.851: INFO: Created: latency-svc-rgrs6
Aug 14 16:27:53.851: INFO: Created: latency-svc-v4rqz
Aug 14 16:27:54.075: INFO: Got endpoints: latency-svc-v4rqz [5.442009355s]
Aug 14 16:27:54.077: INFO: Got endpoints: latency-svc-rgrs6 [5.03265851s]
Aug 14 16:27:54.077: INFO: Got endpoints: latency-svc-xgdbl [4.5847527s]
Aug 14 16:27:54.077: INFO: Got endpoints: latency-svc-rnp6c [3.71876787s]
Aug 14 16:27:54.079: INFO: Created: latency-svc-46srv
Aug 14 16:27:54.801: INFO: Got endpoints: latency-svc-46srv [4.438507836s]
Aug 14 16:27:54.802: INFO: Created: latency-svc-px4jn
Aug 14 16:27:54.910: INFO: Created: latency-svc-7k4fk
Aug 14 16:27:55.206: INFO: Got endpoints: latency-svc-px4jn [4.84231888s]
Aug 14 16:27:55.208: INFO: Got endpoints: latency-svc-7k4fk [4.843632841s]
Aug 14 16:27:55.638: INFO: Created: latency-svc-7g99h
Aug 14 16:27:55.696: INFO: Got endpoints: latency-svc-7g99h [4.855603204s]
Aug 14 16:27:55.843: INFO: Created: latency-svc-587rr
Aug 14 16:27:56.512: INFO: Got endpoints: latency-svc-587rr [5.074009644s]
Aug 14 16:27:56.514: INFO: Created: latency-svc-g8gbk
Aug 14 16:27:56.925: INFO: Created: latency-svc-49wps
Aug 14 16:27:56.925: INFO: Got endpoints: latency-svc-g8gbk [5.045489689s]
Aug 14 16:27:56.982: INFO: Got endpoints: latency-svc-49wps [4.230758573s]
Aug 14 16:27:56.985: INFO: Created: latency-svc-rbhnh
Aug 14 16:27:56.985: INFO: Created: latency-svc-7stnl
Aug 14 16:27:56.985: INFO: Created: latency-svc-wkd86
Aug 14 16:27:56.986: INFO: Created: latency-svc-clx6g
Aug 14 16:27:56.986: INFO: Created: latency-svc-9ffd9
Aug 14 16:27:57.539: INFO: Created: latency-svc-6brr8
Aug 14 16:27:58.007: INFO: Got endpoints: latency-svc-wkd86 [5.255112559s]
Aug 14 16:27:58.007: INFO: Got endpoints: latency-svc-7stnl [5.250577188s]
Aug 14 16:27:58.007: INFO: Created: latency-svc-gs2sh
Aug 14 16:27:58.007: INFO: Got endpoints: latency-svc-9ffd9 [5.017079798s]
Aug 14 16:27:58.009: INFO: Got endpoints: latency-svc-clx6g [3.934180643s]
Aug 14 16:27:58.009: INFO: Got endpoints: latency-svc-rbhnh [5.25733203s]
Aug 14 16:27:58.798: INFO: Got endpoints: latency-svc-6brr8 [4.720745221s]
Aug 14 16:27:59.444: INFO: Got endpoints: latency-svc-gs2sh [5.367509386s]
Aug 14 16:27:59.449: INFO: Created: latency-svc-2rb92
Aug 14 16:27:59.904: INFO: Created: latency-svc-8p6jg
Aug 14 16:27:59.904: INFO: Created: latency-svc-sdxw4
Aug 14 16:27:59.904: INFO: Got endpoints: latency-svc-2rb92 [5.827562155s]
Aug 14 16:28:00.650: INFO: Got endpoints: latency-svc-8p6jg [5.848253408s]
Aug 14 16:28:00.651: INFO: Got endpoints: latency-svc-sdxw4 [5.444391107s]
Aug 14 16:28:00.846: INFO: Created: latency-svc-gk9pq
Aug 14 16:28:01.308: INFO: Created: latency-svc-thdpd
Aug 14 16:28:01.309: INFO: Got endpoints: latency-svc-gk9pq [6.101118411s]
Aug 14 16:28:01.707: INFO: Created: latency-svc-x8nmh
Aug 14 16:28:01.708: INFO: Got endpoints: latency-svc-thdpd [6.011477622s]
Aug 14 16:28:02.115: INFO: Created: latency-svc-d5nqj
Aug 14 16:28:02.115: INFO: Got endpoints: latency-svc-x8nmh [5.602508209s]
Aug 14 16:28:02.121: INFO: Created: latency-svc-md5dt
Aug 14 16:28:02.121: INFO: Got endpoints: latency-svc-d5nqj [5.195755927s]
Aug 14 16:28:02.123: INFO: Created: latency-svc-jqjkn
Aug 14 16:28:02.320: INFO: Got endpoints: latency-svc-md5dt [5.337960783s]
Aug 14 16:28:02.322: INFO: Got endpoints: latency-svc-jqjkn [4.315443488s]
Aug 14 16:28:02.577: INFO: Created: latency-svc-wbxfr
Aug 14 16:28:03.128: INFO: Got endpoints: latency-svc-wbxfr [5.118459488s]
Aug 14 16:28:03.603: INFO: Created: latency-svc-q795x
Aug 14 16:28:04.372: INFO: Got endpoints: latency-svc-q795x [6.364475467s]
Aug 14 16:28:04.386: INFO: Created: latency-svc-khg6r
Aug 14 16:28:04.789: INFO: Got endpoints: latency-svc-khg6r [6.782170413s]
Aug 14 16:28:05.361: INFO: Created: latency-svc-d8c6t
Aug 14 16:28:05.459: INFO: Got endpoints: latency-svc-d8c6t [7.449789689s]
Aug 14 16:28:05.702: INFO: Created: latency-svc-dsws8
Aug 14 16:28:06.461: INFO: Created: latency-svc-dspjv
Aug 14 16:28:06.461: INFO: Got endpoints: latency-svc-dsws8 [7.663529185s]
Aug 14 16:28:06.461: INFO: Created: latency-svc-sc7r7
Aug 14 16:28:06.879: INFO: Created: latency-svc-9h4fn
Aug 14 16:28:06.958: INFO: Got endpoints: latency-svc-dspjv [7.053621789s]
Aug 14 16:28:06.959: INFO: Got endpoints: latency-svc-sc7r7 [7.514261324s]
Aug 14 16:28:07.206: INFO: Got endpoints: latency-svc-9h4fn [6.556634092s]
Aug 14 16:28:07.207: INFO: Created: latency-svc-cwkck
Aug 14 16:28:07.469: INFO: Created: latency-svc-lprvf
Aug 14 16:28:07.480: INFO: Got endpoints: latency-svc-cwkck [6.829707721s]
Aug 14 16:28:07.655: INFO: Created: latency-svc-6k7ff
Aug 14 16:28:08.132: INFO: Got endpoints: latency-svc-lprvf [6.823422449s]
Aug 14 16:28:08.134: INFO: Got endpoints: latency-svc-6k7ff [6.426418392s]
Aug 14 16:28:08.449: INFO: Created: latency-svc-fzdjs
Aug 14 16:28:09.288: INFO: Created: latency-svc-wmbwh
Aug 14 16:28:09.288: INFO: Got endpoints: latency-svc-fzdjs [7.17363119s]
Aug 14 16:28:10.323: INFO: Got endpoints: latency-svc-wmbwh [8.202296906s]
Aug 14 16:28:10.326: INFO: Created: latency-svc-sgr2t
Aug 14 16:28:10.334: INFO: Created: latency-svc-vk62x
Aug 14 16:28:11.229: INFO: Got endpoints: latency-svc-sgr2t [8.9091093s]
Aug 14 16:28:11.229: INFO: Got endpoints: latency-svc-vk62x [8.907104897s]
Aug 14 16:28:11.232: INFO: Created: latency-svc-pn4cc
Aug 14 16:28:11.815: INFO: Created: latency-svc-qnrlj
Aug 14 16:28:11.815: INFO: Got endpoints: latency-svc-pn4cc [8.687520527s]
Aug 14 16:28:11.816: INFO: Created: latency-svc-r5r8d
Aug 14 16:28:11.816: INFO: Created: latency-svc-777cs
Aug 14 16:28:12.204: INFO: Got endpoints: latency-svc-qnrlj [6.744620682s]
Aug 14 16:28:12.204: INFO: Got endpoints: latency-svc-777cs [7.414941822s]
Aug 14 16:28:12.204: INFO: Got endpoints: latency-svc-r5r8d [7.832520765s]
Aug 14 16:28:12.209: INFO: Created: latency-svc-4z7r4
Aug 14 16:28:12.209: INFO: Created: latency-svc-mz4n5
Aug 14 16:28:12.209: INFO: Created: latency-svc-st58v
Aug 14 16:28:12.209: INFO: Created: latency-svc-rggnn
Aug 14 16:28:12.209: INFO: Created: latency-svc-nkhpq
Aug 14 16:28:12.264: INFO: Created: latency-svc-pdjhs
Aug 14 16:28:12.270: INFO: Got endpoints: latency-svc-4z7r4 [5.809206536s]
Aug 14 16:28:12.272: INFO: Got endpoints: latency-svc-st58v [5.31375174s]
Aug 14 16:28:12.272: INFO: Got endpoints: latency-svc-mz4n5 [5.31272223s]
Aug 14 16:28:12.279: INFO: Got endpoints: latency-svc-rggnn [4.798770859s]
Aug 14 16:28:12.280: INFO: Got endpoints: latency-svc-nkhpq [5.073176276s]
Aug 14 16:28:12.935: INFO: Got endpoints: latency-svc-pdjhs [4.802230833s]
Aug 14 16:28:12.936: INFO: Created: latency-svc-xwh9b
Aug 14 16:28:13.334: INFO: Got endpoints: latency-svc-xwh9b [5.200240662s]
Aug 14 16:28:14.226: INFO: Created: latency-svc-zblxw
Aug 14 16:28:14.226: INFO: Created: latency-svc-f4llt
Aug 14 16:28:14.769: INFO: Created: latency-svc-znqvk
Aug 14 16:28:14.780: INFO: Got endpoints: latency-svc-f4llt [4.456869918s]
Aug 14 16:28:14.783: INFO: Got endpoints: latency-svc-znqvk [3.553233177s]
Aug 14 16:28:14.783: INFO: Got endpoints: latency-svc-zblxw [5.494126415s]
Aug 14 16:28:15.560: INFO: Created: latency-svc-9m48b
Aug 14 16:28:15.560: INFO: Created: latency-svc-2gt7q
Aug 14 16:28:15.561: INFO: Created: latency-svc-jjvgk
Aug 14 16:28:16.006: INFO: Got endpoints: latency-svc-9m48b [4.19100826s]
Aug 14 16:28:16.006: INFO: Got endpoints: latency-svc-2gt7q [4.777193471s]
Aug 14 16:28:16.006: INFO: Got endpoints: latency-svc-jjvgk [3.80238239s]
Aug 14 16:28:17.289: INFO: Created: latency-svc-cr9mz
Aug 14 16:28:17.354: INFO: Created: latency-svc-b9cvs
Aug 14 16:28:17.676: INFO: Created: latency-svc-2mbh9
Aug 14 16:28:17.676: INFO: Created: latency-svc-hg24k
Aug 14 16:28:17.677: INFO: Created: latency-svc-rmclh
Aug 14 16:28:17.677: INFO: Created: latency-svc-rts8j
Aug 14 16:28:17.677: INFO: Got endpoints: latency-svc-b9cvs [5.473017716s]
Aug 14 16:28:17.677: INFO: Got endpoints: latency-svc-cr9mz [5.472896574s]
Aug 14 16:28:18.149: INFO: Created: latency-svc-jtmvx
Aug 14 16:28:18.151: INFO: Got endpoints: latency-svc-rts8j [5.879255236s]
Aug 14 16:28:18.151: INFO: Got endpoints: latency-svc-hg24k [5.880787727s]
Aug 14 16:28:18.151: INFO: Got endpoints: latency-svc-rmclh [5.879302905s]
Aug 14 16:28:18.152: INFO: Got endpoints: latency-svc-2mbh9 [5.872543062s]
Aug 14 16:28:18.646: INFO: Got endpoints: latency-svc-jtmvx [6.365977926s]
Aug 14 16:28:18.646: INFO: Latencies: [782.230836ms 970.383714ms 975.680017ms 1.485741801s 1.499331276s 2.215588562s 2.619028687s 2.731986462s 2.894049375s 2.97448069s 3.005240466s 3.038326252s 3.038547438s 3.051303323s 3.087862171s 3.204726718s 3.20495809s 3.20516854s 3.305825769s 3.344360036s 3.34880789s 3.408343189s 3.412376115s 3.427535107s 3.470558449s 3.541390577s 3.541488048s 3.553233177s 3.556559306s 3.559722727s 3.574206748s 3.585102992s 3.601742941s 3.602134511s 3.610066425s 3.625717345s 3.687721064s 3.71876787s 3.785500379s 3.786837535s 3.787132264s 3.801644614s 3.80238239s 3.838159049s 3.847362971s 3.854589948s 3.860783742s 3.889348169s 3.898190554s 3.902370684s 3.934180643s 3.938879655s 4.000340929s 4.017136874s 4.022615458s 4.052956717s 4.053484138s 4.065932145s 4.082956696s 4.092194581s 4.167279354s 4.19100826s 4.193886097s 4.203492104s 4.230758573s 4.279169748s 4.300187415s 4.315443488s 4.341805789s 4.420752395s 4.4313404s 4.434528569s 4.438507836s 4.451061723s 4.456869918s 4.462654367s 4.523623462s 4.555543172s 4.572958994s 4.577717936s 4.5847527s 4.587464434s 4.596176662s 4.599072197s 4.637098443s 4.669001823s 4.720745221s 4.742758915s 4.75181893s 4.753395489s 4.771662446s 4.777193471s 4.78984519s 4.798770859s 4.802230833s 4.840770939s 4.84231888s 4.843632841s 4.855603204s 4.872088255s 4.878627434s 4.925828969s 4.955051204s 5.015407471s 5.015954685s 5.016206656s 5.017079798s 5.020169451s 5.022048351s 5.022989765s 5.03265851s 5.045489689s 5.071752164s 5.073176276s 5.074009644s 5.076811986s 5.11024353s 5.118459488s 5.153370898s 5.155987188s 5.187454537s 5.187744159s 5.195755927s 5.200240662s 5.250577188s 5.255112559s 5.25733203s 5.262602522s 5.307177843s 5.31272223s 5.31375174s 5.329532038s 5.337960783s 5.348048779s 5.349748091s 5.367509386s 5.384682491s 5.389242849s 5.404112922s 5.442009355s 5.443587107s 5.444391107s 5.472896574s 5.473017716s 5.494126415s 5.503455545s 5.511091794s 5.541971227s 5.566394578s 5.602508209s 5.758482543s 5.785746307s 5.809206536s 5.823965222s 5.827562155s 5.848253408s 5.857363048s 5.857897909s 5.872543062s 5.878725469s 5.879255236s 5.879302905s 5.880787727s 5.944267825s 5.95752216s 5.960535108s 6.011477622s 6.061683521s 6.101118411s 6.186915093s 6.21783104s 6.301449654s 6.364475467s 6.365977926s 6.426418392s 6.451357224s 6.527486799s 6.556634092s 6.744620682s 6.757831403s 6.782170413s 6.823422449s 6.829707721s 6.832863276s 6.932801443s 7.053621789s 7.172836836s 7.17363119s 7.252183744s 7.414941822s 7.449789689s 7.508031052s 7.514261324s 7.663529185s 7.666756232s 7.832520765s 8.202296906s 8.687520527s 8.907104897s 8.9091093s]
Aug 14 16:28:18.646: INFO: 50 %ile: 4.878627434s
Aug 14 16:28:18.646: INFO: 90 %ile: 6.782170413s
Aug 14 16:28:18.646: INFO: 99 %ile: 8.907104897s
Aug 14 16:28:18.646: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:28:18.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2204" for this suite.
Aug 14 16:31:57.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:31:59.268: INFO: namespace svc-latency-2204 deletion completed in 3m40.615169486s

â€¢ [SLOW TEST:299.389 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:31:59.270: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-761
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Aug 14 16:32:00.649: INFO: Waiting up to 5m0s for pod "var-expansion-9bf241be-2f5d-4b85-ae07-e112353e57ce" in namespace "var-expansion-761" to be "success or failure"
Aug 14 16:32:01.085: INFO: Pod "var-expansion-9bf241be-2f5d-4b85-ae07-e112353e57ce": Phase="Pending", Reason="", readiness=false. Elapsed: 435.240641ms
Aug 14 16:32:03.089: INFO: Pod "var-expansion-9bf241be-2f5d-4b85-ae07-e112353e57ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.439910436s
Aug 14 16:32:05.128: INFO: Pod "var-expansion-9bf241be-2f5d-4b85-ae07-e112353e57ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.478427214s
Aug 14 16:32:07.517: INFO: Pod "var-expansion-9bf241be-2f5d-4b85-ae07-e112353e57ce": Phase="Pending", Reason="", readiness=false. Elapsed: 6.867406936s
Aug 14 16:32:09.653: INFO: Pod "var-expansion-9bf241be-2f5d-4b85-ae07-e112353e57ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.003891125s
STEP: Saw pod success
Aug 14 16:32:09.653: INFO: Pod "var-expansion-9bf241be-2f5d-4b85-ae07-e112353e57ce" satisfied condition "success or failure"
Aug 14 16:32:09.657: INFO: Trying to get logs from node slave2 pod var-expansion-9bf241be-2f5d-4b85-ae07-e112353e57ce container dapi-container: <nil>
STEP: delete the pod
Aug 14 16:32:10.332: INFO: Waiting for pod var-expansion-9bf241be-2f5d-4b85-ae07-e112353e57ce to disappear
Aug 14 16:32:10.342: INFO: Pod var-expansion-9bf241be-2f5d-4b85-ae07-e112353e57ce no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:32:10.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-761" for this suite.
Aug 14 16:32:20.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:32:22.585: INFO: namespace var-expansion-761 deletion completed in 12.227277209s

â€¢ [SLOW TEST:23.314 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:32:22.586: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8082
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 14 16:32:23.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-8082'
Aug 14 16:32:33.507: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 14 16:32:33.507: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Aug 14 16:32:33.513: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Aug 14 16:32:34.343: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Aug 14 16:32:35.616: INFO: scanned /root for discovery docs: <nil>
Aug 14 16:32:35.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-8082'
Aug 14 16:33:00.118: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 14 16:33:00.118: INFO: stdout: "Created e2e-test-nginx-rc-5fe053ac591ad8730bb93be5dfe0b26b\nScaling up e2e-test-nginx-rc-5fe053ac591ad8730bb93be5dfe0b26b from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-5fe053ac591ad8730bb93be5dfe0b26b up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-5fe053ac591ad8730bb93be5dfe0b26b to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Aug 14 16:33:00.118: INFO: stdout: "Created e2e-test-nginx-rc-5fe053ac591ad8730bb93be5dfe0b26b\nScaling up e2e-test-nginx-rc-5fe053ac591ad8730bb93be5dfe0b26b from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-5fe053ac591ad8730bb93be5dfe0b26b up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-5fe053ac591ad8730bb93be5dfe0b26b to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Aug 14 16:33:00.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-8082'
Aug 14 16:33:01.803: INFO: stderr: ""
Aug 14 16:33:01.803: INFO: stdout: "e2e-test-nginx-rc-5fe053ac591ad8730bb93be5dfe0b26b-znhmp "
Aug 14 16:33:01.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods e2e-test-nginx-rc-5fe053ac591ad8730bb93be5dfe0b26b-znhmp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8082'
Aug 14 16:33:03.122: INFO: stderr: ""
Aug 14 16:33:03.123: INFO: stdout: "true"
Aug 14 16:33:03.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pods e2e-test-nginx-rc-5fe053ac591ad8730bb93be5dfe0b26b-znhmp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8082'
Aug 14 16:33:04.733: INFO: stderr: ""
Aug 14 16:33:04.733: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Aug 14 16:33:04.733: INFO: e2e-test-nginx-rc-5fe053ac591ad8730bb93be5dfe0b26b-znhmp is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Aug 14 16:33:04.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete rc e2e-test-nginx-rc --namespace=kubectl-8082'
Aug 14 16:33:06.512: INFO: stderr: ""
Aug 14 16:33:06.512: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:33:06.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8082" for this suite.
Aug 14 16:33:39.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:33:43.248: INFO: namespace kubectl-8082 deletion completed in 36.501385338s

â€¢ [SLOW TEST:80.662 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:33:43.249: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7454
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 16:33:44.780: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b626af4-2133-4512-96e9-117795a6e55d" in namespace "projected-7454" to be "success or failure"
Aug 14 16:33:44.872: INFO: Pod "downwardapi-volume-2b626af4-2133-4512-96e9-117795a6e55d": Phase="Pending", Reason="", readiness=false. Elapsed: 92.02877ms
Aug 14 16:33:46.885: INFO: Pod "downwardapi-volume-2b626af4-2133-4512-96e9-117795a6e55d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.10534088s
Aug 14 16:33:48.895: INFO: Pod "downwardapi-volume-2b626af4-2133-4512-96e9-117795a6e55d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.114951552s
Aug 14 16:33:50.928: INFO: Pod "downwardapi-volume-2b626af4-2133-4512-96e9-117795a6e55d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.147770482s
Aug 14 16:33:53.055: INFO: Pod "downwardapi-volume-2b626af4-2133-4512-96e9-117795a6e55d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.27546656s
Aug 14 16:33:55.064: INFO: Pod "downwardapi-volume-2b626af4-2133-4512-96e9-117795a6e55d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.284488231s
Aug 14 16:33:57.069: INFO: Pod "downwardapi-volume-2b626af4-2133-4512-96e9-117795a6e55d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.28937736s
STEP: Saw pod success
Aug 14 16:33:57.069: INFO: Pod "downwardapi-volume-2b626af4-2133-4512-96e9-117795a6e55d" satisfied condition "success or failure"
Aug 14 16:33:57.074: INFO: Trying to get logs from node slave2 pod downwardapi-volume-2b626af4-2133-4512-96e9-117795a6e55d container client-container: <nil>
STEP: delete the pod
Aug 14 16:33:57.137: INFO: Waiting for pod downwardapi-volume-2b626af4-2133-4512-96e9-117795a6e55d to disappear
Aug 14 16:33:57.141: INFO: Pod downwardapi-volume-2b626af4-2133-4512-96e9-117795a6e55d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:33:57.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7454" for this suite.
Aug 14 16:34:07.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:34:09.976: INFO: namespace projected-7454 deletion completed in 12.826150849s

â€¢ [SLOW TEST:26.727 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:34:09.976: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-726
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 16:34:12.552: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"7fa1fc81-9c42-47dd-8709-5c83098a9bce", Controller:(*bool)(0xc0005d136a), BlockOwnerDeletion:(*bool)(0xc0005d136b)}}
Aug 14 16:34:12.891: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"837d5fb3-b0c9-4f75-a40e-8d9a49414955", Controller:(*bool)(0xc001f76186), BlockOwnerDeletion:(*bool)(0xc001f76187)}}
Aug 14 16:34:13.312: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c8a934fb-7b70-41be-a119-b8e2b0dfa299", Controller:(*bool)(0xc001a0b726), BlockOwnerDeletion:(*bool)(0xc001a0b727)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:34:19.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-726" for this suite.
Aug 14 16:34:30.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:34:32.190: INFO: namespace gc-726 deletion completed in 12.752058519s

â€¢ [SLOW TEST:22.214 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:34:32.192: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9220
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 16:34:33.936: INFO: Creating deployment "test-recreate-deployment"
Aug 14 16:34:34.004: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 14 16:34:34.825: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 14 16:34:36.978: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 14 16:34:37.030: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397275, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397275, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397275, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397274, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 16:34:39.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397275, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397275, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397275, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397274, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 16:34:41.035: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397275, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397275, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397275, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397274, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 16:34:43.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397275, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397275, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397275, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701397274, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 16:34:45.188: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 14 16:34:45.367: INFO: Updating deployment test-recreate-deployment
Aug 14 16:34:45.367: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 14 16:34:48.663: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-9220,SelfLink:/apis/apps/v1/namespaces/deployment-9220/deployments/test-recreate-deployment,UID:979ed998-e0a3-40ef-b17e-ec22b9920d89,ResourceVersion:389793,Generation:2,CreationTimestamp:2019-08-14 16:34:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-08-14 16:34:48 +0000 UTC 2019-08-14 16:34:48 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-14 16:34:48 +0000 UTC 2019-08-14 16:34:34 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Aug 14 16:34:48.940: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-9220,SelfLink:/apis/apps/v1/namespaces/deployment-9220/replicasets/test-recreate-deployment-5c8c9cc69d,UID:f987e8fa-cb5d-4856-9974-755d535e6034,ResourceVersion:389790,Generation:1,CreationTimestamp:2019-08-14 16:34:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 979ed998-e0a3-40ef-b17e-ec22b9920d89 0xc0005d0157 0xc0005d0158}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 14 16:34:48.940: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 14 16:34:48.941: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-9220,SelfLink:/apis/apps/v1/namespaces/deployment-9220/replicasets/test-recreate-deployment-6df85df6b9,UID:5b8a8b01-0625-42b2-8abc-ac0371ef6d4d,ResourceVersion:389774,Generation:2,CreationTimestamp:2019-08-14 16:34:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 979ed998-e0a3-40ef-b17e-ec22b9920d89 0xc0005d0237 0xc0005d0238}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 14 16:34:49.129: INFO: Pod "test-recreate-deployment-5c8c9cc69d-2x46h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-2x46h,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-9220,SelfLink:/api/v1/namespaces/deployment-9220/pods/test-recreate-deployment-5c8c9cc69d-2x46h,UID:fba83462-f19c-459a-ae7e-5a1f1dec2acd,ResourceVersion:389794,Generation:0,CreationTimestamp:2019-08-14 16:34:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d f987e8fa-cb5d-4856-9974-755d535e6034 0xc0005d1327 0xc0005d1328}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-pqrdn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pqrdn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-pqrdn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0005d1420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0005d1450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 16:34:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 16:34:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 16:34:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 16:34:47 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.59,PodIP:,StartTime:2019-08-14 16:34:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:34:49.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9220" for this suite.
Aug 14 16:35:05.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:35:08.430: INFO: namespace deployment-9220 deletion completed in 19.289142289s

â€¢ [SLOW TEST:36.238 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:35:08.430: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-77a91f6c-2cfd-4137-8881-2f80b41bb6d5 in namespace container-probe-826
Aug 14 16:35:15.847: INFO: Started pod busybox-77a91f6c-2cfd-4137-8881-2f80b41bb6d5 in namespace container-probe-826
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 16:35:15.851: INFO: Initial restart count of pod busybox-77a91f6c-2cfd-4137-8881-2f80b41bb6d5 is 0
Aug 14 16:36:12.505: INFO: Restart count of pod container-probe-826/busybox-77a91f6c-2cfd-4137-8881-2f80b41bb6d5 is now 1 (56.654540001s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:36:13.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-826" for this suite.
Aug 14 16:36:27.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:36:29.336: INFO: namespace container-probe-826 deletion completed in 16.086469575s

â€¢ [SLOW TEST:80.906 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:36:29.337: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4956
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 16:36:31.167: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bec24447-4351-4cb3-9cfb-7f9709ff1fa2" in namespace "projected-4956" to be "success or failure"
Aug 14 16:36:31.181: INFO: Pod "downwardapi-volume-bec24447-4351-4cb3-9cfb-7f9709ff1fa2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.210501ms
Aug 14 16:36:33.220: INFO: Pod "downwardapi-volume-bec24447-4351-4cb3-9cfb-7f9709ff1fa2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052197481s
Aug 14 16:36:35.313: INFO: Pod "downwardapi-volume-bec24447-4351-4cb3-9cfb-7f9709ff1fa2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.145407826s
Aug 14 16:36:37.397: INFO: Pod "downwardapi-volume-bec24447-4351-4cb3-9cfb-7f9709ff1fa2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.229633213s
Aug 14 16:36:39.531: INFO: Pod "downwardapi-volume-bec24447-4351-4cb3-9cfb-7f9709ff1fa2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.363046506s
Aug 14 16:36:41.621: INFO: Pod "downwardapi-volume-bec24447-4351-4cb3-9cfb-7f9709ff1fa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.453405365s
STEP: Saw pod success
Aug 14 16:36:41.621: INFO: Pod "downwardapi-volume-bec24447-4351-4cb3-9cfb-7f9709ff1fa2" satisfied condition "success or failure"
Aug 14 16:36:41.624: INFO: Trying to get logs from node slave3 pod downwardapi-volume-bec24447-4351-4cb3-9cfb-7f9709ff1fa2 container client-container: <nil>
STEP: delete the pod
Aug 14 16:36:42.091: INFO: Waiting for pod downwardapi-volume-bec24447-4351-4cb3-9cfb-7f9709ff1fa2 to disappear
Aug 14 16:36:42.341: INFO: Pod downwardapi-volume-bec24447-4351-4cb3-9cfb-7f9709ff1fa2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:36:42.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4956" for this suite.
Aug 14 16:36:50.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:36:53.121: INFO: namespace projected-4956 deletion completed in 10.766687243s

â€¢ [SLOW TEST:23.784 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:36:53.122: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8798
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 14 16:36:55.038: INFO: Waiting up to 5m0s for pod "pod-00f207f4-df25-4030-b54b-6580d67ede41" in namespace "emptydir-8798" to be "success or failure"
Aug 14 16:36:55.194: INFO: Pod "pod-00f207f4-df25-4030-b54b-6580d67ede41": Phase="Pending", Reason="", readiness=false. Elapsed: 155.300403ms
Aug 14 16:36:57.199: INFO: Pod "pod-00f207f4-df25-4030-b54b-6580d67ede41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.160214111s
Aug 14 16:36:59.273: INFO: Pod "pod-00f207f4-df25-4030-b54b-6580d67ede41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.234668184s
Aug 14 16:37:01.284: INFO: Pod "pod-00f207f4-df25-4030-b54b-6580d67ede41": Phase="Pending", Reason="", readiness=false. Elapsed: 6.245359162s
Aug 14 16:37:03.472: INFO: Pod "pod-00f207f4-df25-4030-b54b-6580d67ede41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.433875083s
STEP: Saw pod success
Aug 14 16:37:03.472: INFO: Pod "pod-00f207f4-df25-4030-b54b-6580d67ede41" satisfied condition "success or failure"
Aug 14 16:37:03.792: INFO: Trying to get logs from node slave1 pod pod-00f207f4-df25-4030-b54b-6580d67ede41 container test-container: <nil>
STEP: delete the pod
Aug 14 16:37:04.409: INFO: Waiting for pod pod-00f207f4-df25-4030-b54b-6580d67ede41 to disappear
Aug 14 16:37:04.414: INFO: Pod pod-00f207f4-df25-4030-b54b-6580d67ede41 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:37:04.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8798" for this suite.
Aug 14 16:37:13.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:37:14.988: INFO: namespace emptydir-8798 deletion completed in 10.567535089s

â€¢ [SLOW TEST:21.866 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:37:14.988: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 16:37:16.926: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e7bdfe7-0ad0-452f-9889-076d667de374" in namespace "downward-api-4721" to be "success or failure"
Aug 14 16:37:17.335: INFO: Pod "downwardapi-volume-6e7bdfe7-0ad0-452f-9889-076d667de374": Phase="Pending", Reason="", readiness=false. Elapsed: 408.215453ms
Aug 14 16:37:19.507: INFO: Pod "downwardapi-volume-6e7bdfe7-0ad0-452f-9889-076d667de374": Phase="Pending", Reason="", readiness=false. Elapsed: 2.580346389s
Aug 14 16:37:21.657: INFO: Pod "downwardapi-volume-6e7bdfe7-0ad0-452f-9889-076d667de374": Phase="Pending", Reason="", readiness=false. Elapsed: 4.730112321s
Aug 14 16:37:23.804: INFO: Pod "downwardapi-volume-6e7bdfe7-0ad0-452f-9889-076d667de374": Phase="Pending", Reason="", readiness=false. Elapsed: 6.877790318s
Aug 14 16:37:25.868: INFO: Pod "downwardapi-volume-6e7bdfe7-0ad0-452f-9889-076d667de374": Phase="Pending", Reason="", readiness=false. Elapsed: 8.941100322s
Aug 14 16:37:27.875: INFO: Pod "downwardapi-volume-6e7bdfe7-0ad0-452f-9889-076d667de374": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.948857446s
STEP: Saw pod success
Aug 14 16:37:27.875: INFO: Pod "downwardapi-volume-6e7bdfe7-0ad0-452f-9889-076d667de374" satisfied condition "success or failure"
Aug 14 16:37:27.879: INFO: Trying to get logs from node slave2 pod downwardapi-volume-6e7bdfe7-0ad0-452f-9889-076d667de374 container client-container: <nil>
STEP: delete the pod
Aug 14 16:37:28.743: INFO: Waiting for pod downwardapi-volume-6e7bdfe7-0ad0-452f-9889-076d667de374 to disappear
Aug 14 16:37:28.775: INFO: Pod downwardapi-volume-6e7bdfe7-0ad0-452f-9889-076d667de374 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:37:28.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4721" for this suite.
Aug 14 16:37:43.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:37:44.770: INFO: namespace downward-api-4721 deletion completed in 15.987652596s

â€¢ [SLOW TEST:29.782 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:37:44.771: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1731
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-j2hl6 in namespace proxy-1731
I0814 16:37:47.188150      20 runners.go:180] Created replication controller with name: proxy-service-j2hl6, namespace: proxy-1731, replica count: 1
I0814 16:37:48.238678      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:37:49.238889      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:37:50.239132      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:37:51.239349      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:37:52.239714      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:37:53.239864      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:37:54.240142      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:37:55.240331      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:37:56.240495      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:37:57.240690      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 16:37:58.240913      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 16:37:59.241096      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 16:38:00.241290      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 16:38:01.241455      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 16:38:02.241629      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 16:38:03.241799      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 16:38:04.241979      20 runners.go:180] proxy-service-j2hl6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 14 16:38:04.434: INFO: setup took 18.076642426s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 14 16:38:04.444: INFO: (0) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 9.18575ms)
Aug 14 16:38:04.444: INFO: (0) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 9.404003ms)
Aug 14 16:38:04.444: INFO: (0) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 9.320096ms)
Aug 14 16:38:04.449: INFO: (0) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 13.949861ms)
Aug 14 16:38:04.449: INFO: (0) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 14.676764ms)
Aug 14 16:38:04.449: INFO: (0) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 14.02271ms)
Aug 14 16:38:04.449: INFO: (0) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 14.623996ms)
Aug 14 16:38:04.449: INFO: (0) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 13.972664ms)
Aug 14 16:38:04.450: INFO: (0) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 14.82435ms)
Aug 14 16:38:04.450: INFO: (0) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 15.35157ms)
Aug 14 16:38:04.451: INFO: (0) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 16.514226ms)
Aug 14 16:38:04.452: INFO: (0) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 17.884185ms)
Aug 14 16:38:04.453: INFO: (0) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 17.831065ms)
Aug 14 16:38:04.454: INFO: (0) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 18.705626ms)
Aug 14 16:38:04.456: INFO: (0) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 21.220139ms)
Aug 14 16:38:04.457: INFO: (0) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 21.94841ms)
Aug 14 16:38:04.462: INFO: (1) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 4.928181ms)
Aug 14 16:38:04.465: INFO: (1) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 7.525892ms)
Aug 14 16:38:04.466: INFO: (1) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 8.373199ms)
Aug 14 16:38:04.466: INFO: (1) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 8.447881ms)
Aug 14 16:38:04.466: INFO: (1) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 8.796668ms)
Aug 14 16:38:04.466: INFO: (1) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 8.935519ms)
Aug 14 16:38:04.467: INFO: (1) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 9.088973ms)
Aug 14 16:38:04.467: INFO: (1) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 9.419276ms)
Aug 14 16:38:04.467: INFO: (1) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 9.511054ms)
Aug 14 16:38:04.468: INFO: (1) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 10.439957ms)
Aug 14 16:38:04.468: INFO: (1) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 10.955425ms)
Aug 14 16:38:04.469: INFO: (1) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 11.859314ms)
Aug 14 16:38:04.470: INFO: (1) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 12.127284ms)
Aug 14 16:38:04.471: INFO: (1) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 13.243854ms)
Aug 14 16:38:04.471: INFO: (1) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 13.240652ms)
Aug 14 16:38:04.471: INFO: (1) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 13.328836ms)
Aug 14 16:38:04.481: INFO: (2) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 9.626472ms)
Aug 14 16:38:04.481: INFO: (2) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 9.718476ms)
Aug 14 16:38:04.481: INFO: (2) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 10.334293ms)
Aug 14 16:38:04.481: INFO: (2) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 10.450934ms)
Aug 14 16:38:04.481: INFO: (2) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 10.383376ms)
Aug 14 16:38:04.482: INFO: (2) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 10.859913ms)
Aug 14 16:38:04.482: INFO: (2) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 11.062057ms)
Aug 14 16:38:04.482: INFO: (2) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 11.041831ms)
Aug 14 16:38:04.482: INFO: (2) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 11.380853ms)
Aug 14 16:38:04.483: INFO: (2) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 11.84412ms)
Aug 14 16:38:04.484: INFO: (2) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 12.595026ms)
Aug 14 16:38:04.486: INFO: (2) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 14.647285ms)
Aug 14 16:38:04.487: INFO: (2) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 16.160825ms)
Aug 14 16:38:04.488: INFO: (2) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 16.549503ms)
Aug 14 16:38:04.488: INFO: (2) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 16.664352ms)
Aug 14 16:38:04.488: INFO: (2) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 17.284012ms)
Aug 14 16:38:04.703: INFO: (3) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 215.095399ms)
Aug 14 16:38:04.703: INFO: (3) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 215.235355ms)
Aug 14 16:38:04.703: INFO: (3) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 215.10027ms)
Aug 14 16:38:04.703: INFO: (3) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 215.131014ms)
Aug 14 16:38:04.703: INFO: (3) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 215.13861ms)
Aug 14 16:38:04.703: INFO: (3) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 215.087583ms)
Aug 14 16:38:04.703: INFO: (3) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 215.079675ms)
Aug 14 16:38:04.704: INFO: (3) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 215.301077ms)
Aug 14 16:38:04.704: INFO: (3) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 215.510433ms)
Aug 14 16:38:04.704: INFO: (3) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 215.467299ms)
Aug 14 16:38:04.704: INFO: (3) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 215.855639ms)
Aug 14 16:38:04.708: INFO: (3) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 219.316688ms)
Aug 14 16:38:04.708: INFO: (3) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 219.379108ms)
Aug 14 16:38:04.708: INFO: (3) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 219.183112ms)
Aug 14 16:38:04.708: INFO: (3) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 219.41267ms)
Aug 14 16:38:04.708: INFO: (3) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 219.762038ms)
Aug 14 16:38:04.722: INFO: (4) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 13.180119ms)
Aug 14 16:38:04.722: INFO: (4) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 13.272212ms)
Aug 14 16:38:04.722: INFO: (4) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 12.927223ms)
Aug 14 16:38:04.722: INFO: (4) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 13.694552ms)
Aug 14 16:38:04.722: INFO: (4) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 13.250651ms)
Aug 14 16:38:04.722: INFO: (4) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 13.915715ms)
Aug 14 16:38:04.723: INFO: (4) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 14.648075ms)
Aug 14 16:38:04.724: INFO: (4) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 15.604974ms)
Aug 14 16:38:04.725: INFO: (4) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 16.10885ms)
Aug 14 16:38:04.731: INFO: (4) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 22.430087ms)
Aug 14 16:38:04.731: INFO: (4) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 22.703426ms)
Aug 14 16:38:04.731: INFO: (4) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 22.922108ms)
Aug 14 16:38:04.731: INFO: (4) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 22.271883ms)
Aug 14 16:38:04.731: INFO: (4) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 22.795192ms)
Aug 14 16:38:04.731: INFO: (4) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 22.364459ms)
Aug 14 16:38:04.731: INFO: (4) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 22.276335ms)
Aug 14 16:38:04.738: INFO: (5) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 6.950687ms)
Aug 14 16:38:04.738: INFO: (5) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 6.94989ms)
Aug 14 16:38:04.743: INFO: (5) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 11.267867ms)
Aug 14 16:38:04.743: INFO: (5) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 11.477429ms)
Aug 14 16:38:04.743: INFO: (5) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 11.799667ms)
Aug 14 16:38:04.743: INFO: (5) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 11.742931ms)
Aug 14 16:38:04.743: INFO: (5) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 11.870155ms)
Aug 14 16:38:04.743: INFO: (5) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 11.811868ms)
Aug 14 16:38:04.744: INFO: (5) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 12.448664ms)
Aug 14 16:38:04.744: INFO: (5) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 12.559145ms)
Aug 14 16:38:04.745: INFO: (5) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 13.268276ms)
Aug 14 16:38:04.745: INFO: (5) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 13.350303ms)
Aug 14 16:38:04.745: INFO: (5) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 13.30119ms)
Aug 14 16:38:04.746: INFO: (5) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 14.767201ms)
Aug 14 16:38:04.746: INFO: (5) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 15.116333ms)
Aug 14 16:38:04.747: INFO: (5) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 15.812303ms)
Aug 14 16:38:04.763: INFO: (6) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 15.238562ms)
Aug 14 16:38:04.763: INFO: (6) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 15.42387ms)
Aug 14 16:38:04.763: INFO: (6) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 15.522713ms)
Aug 14 16:38:04.764: INFO: (6) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 16.451472ms)
Aug 14 16:38:04.764: INFO: (6) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 16.747017ms)
Aug 14 16:38:04.764: INFO: (6) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 16.69572ms)
Aug 14 16:38:04.764: INFO: (6) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 16.885781ms)
Aug 14 16:38:04.764: INFO: (6) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 16.875901ms)
Aug 14 16:38:04.765: INFO: (6) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 17.438587ms)
Aug 14 16:38:04.765: INFO: (6) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 17.57821ms)
Aug 14 16:38:04.765: INFO: (6) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 17.760864ms)
Aug 14 16:38:04.768: INFO: (6) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 20.768009ms)
Aug 14 16:38:04.771: INFO: (6) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 23.072986ms)
Aug 14 16:38:04.771: INFO: (6) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 23.127723ms)
Aug 14 16:38:04.773: INFO: (6) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 25.526928ms)
Aug 14 16:38:04.774: INFO: (6) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 25.992254ms)
Aug 14 16:38:04.780: INFO: (7) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 5.751119ms)
Aug 14 16:38:04.780: INFO: (7) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 5.884922ms)
Aug 14 16:38:04.780: INFO: (7) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 6.275251ms)
Aug 14 16:38:04.780: INFO: (7) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 6.093865ms)
Aug 14 16:38:04.780: INFO: (7) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 6.210764ms)
Aug 14 16:38:04.784: INFO: (7) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 9.873479ms)
Aug 14 16:38:04.784: INFO: (7) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 10.381946ms)
Aug 14 16:38:04.785: INFO: (7) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 10.871081ms)
Aug 14 16:38:04.785: INFO: (7) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 11.144083ms)
Aug 14 16:38:04.785: INFO: (7) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 11.264387ms)
Aug 14 16:38:04.785: INFO: (7) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 11.194008ms)
Aug 14 16:38:04.785: INFO: (7) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 11.322416ms)
Aug 14 16:38:04.786: INFO: (7) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 11.901181ms)
Aug 14 16:38:04.786: INFO: (7) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 12.316561ms)
Aug 14 16:38:04.786: INFO: (7) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 12.294535ms)
Aug 14 16:38:04.786: INFO: (7) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 12.276163ms)
Aug 14 16:38:04.827: INFO: (8) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 40.607297ms)
Aug 14 16:38:04.827: INFO: (8) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 40.151665ms)
Aug 14 16:38:04.827: INFO: (8) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 40.998246ms)
Aug 14 16:38:04.828: INFO: (8) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 41.474967ms)
Aug 14 16:38:04.828: INFO: (8) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 41.577093ms)
Aug 14 16:38:04.829: INFO: (8) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 42.475693ms)
Aug 14 16:38:04.829: INFO: (8) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 42.223005ms)
Aug 14 16:38:04.829: INFO: (8) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 42.166163ms)
Aug 14 16:38:04.829: INFO: (8) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 42.061617ms)
Aug 14 16:38:04.829: INFO: (8) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 42.647587ms)
Aug 14 16:38:04.829: INFO: (8) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 41.833808ms)
Aug 14 16:38:04.834: INFO: (8) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 46.769357ms)
Aug 14 16:38:04.834: INFO: (8) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 47.378955ms)
Aug 14 16:38:04.834: INFO: (8) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 47.156391ms)
Aug 14 16:38:04.834: INFO: (8) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 47.360328ms)
Aug 14 16:38:04.834: INFO: (8) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 47.400538ms)
Aug 14 16:38:04.839: INFO: (9) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 4.319205ms)
Aug 14 16:38:04.840: INFO: (9) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 5.281593ms)
Aug 14 16:38:04.840: INFO: (9) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 5.514484ms)
Aug 14 16:38:04.840: INFO: (9) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 5.459606ms)
Aug 14 16:38:04.848: INFO: (9) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 13.349008ms)
Aug 14 16:38:04.848: INFO: (9) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 13.444917ms)
Aug 14 16:38:04.848: INFO: (9) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 12.926876ms)
Aug 14 16:38:04.848: INFO: (9) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 13.54957ms)
Aug 14 16:38:04.848: INFO: (9) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 13.379498ms)
Aug 14 16:38:04.848: INFO: (9) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 13.007945ms)
Aug 14 16:38:04.849: INFO: (9) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 14.117402ms)
Aug 14 16:38:04.849: INFO: (9) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 14.048433ms)
Aug 14 16:38:04.849: INFO: (9) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 13.77506ms)
Aug 14 16:38:04.849: INFO: (9) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 13.840877ms)
Aug 14 16:38:04.849: INFO: (9) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 13.83429ms)
Aug 14 16:38:04.849: INFO: (9) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 14.443962ms)
Aug 14 16:38:04.853: INFO: (10) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 3.89702ms)
Aug 14 16:38:04.855: INFO: (10) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 5.423333ms)
Aug 14 16:38:04.855: INFO: (10) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 5.710143ms)
Aug 14 16:38:04.855: INFO: (10) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 5.909226ms)
Aug 14 16:38:04.856: INFO: (10) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 5.755498ms)
Aug 14 16:38:04.856: INFO: (10) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 5.733241ms)
Aug 14 16:38:04.857: INFO: (10) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 6.956617ms)
Aug 14 16:38:04.857: INFO: (10) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 6.710277ms)
Aug 14 16:38:04.857: INFO: (10) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 6.933432ms)
Aug 14 16:38:04.857: INFO: (10) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 6.822572ms)
Aug 14 16:38:04.858: INFO: (10) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 8.046625ms)
Aug 14 16:38:04.861: INFO: (10) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 11.738718ms)
Aug 14 16:38:04.861: INFO: (10) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 11.220741ms)
Aug 14 16:38:04.862: INFO: (10) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 11.397044ms)
Aug 14 16:38:04.862: INFO: (10) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 11.750989ms)
Aug 14 16:38:04.862: INFO: (10) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 11.634865ms)
Aug 14 16:38:04.884: INFO: (11) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 21.841727ms)
Aug 14 16:38:04.885: INFO: (11) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 21.785512ms)
Aug 14 16:38:04.885: INFO: (11) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 21.725971ms)
Aug 14 16:38:04.885: INFO: (11) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 21.926704ms)
Aug 14 16:38:04.885: INFO: (11) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 22.864982ms)
Aug 14 16:38:04.887: INFO: (11) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 24.958832ms)
Aug 14 16:38:04.908: INFO: (11) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 45.921763ms)
Aug 14 16:38:04.908: INFO: (11) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 45.216462ms)
Aug 14 16:38:04.908: INFO: (11) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 45.817504ms)
Aug 14 16:38:04.909: INFO: (11) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 46.831417ms)
Aug 14 16:38:04.909: INFO: (11) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 46.607688ms)
Aug 14 16:38:04.909: INFO: (11) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 46.861562ms)
Aug 14 16:38:04.909: INFO: (11) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 47.090137ms)
Aug 14 16:38:04.909: INFO: (11) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 47.195717ms)
Aug 14 16:38:04.909: INFO: (11) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 46.889396ms)
Aug 14 16:38:04.910: INFO: (11) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 47.419585ms)
Aug 14 16:38:04.914: INFO: (12) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 4.317248ms)
Aug 14 16:38:04.918: INFO: (12) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 6.889626ms)
Aug 14 16:38:04.918: INFO: (12) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 8.119479ms)
Aug 14 16:38:04.918: INFO: (12) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 7.985594ms)
Aug 14 16:38:04.918: INFO: (12) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 7.747669ms)
Aug 14 16:38:04.918: INFO: (12) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 8.362232ms)
Aug 14 16:38:04.918: INFO: (12) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 7.601856ms)
Aug 14 16:38:04.919: INFO: (12) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 8.887229ms)
Aug 14 16:38:04.919: INFO: (12) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 7.809123ms)
Aug 14 16:38:04.919: INFO: (12) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 9.25613ms)
Aug 14 16:38:04.919: INFO: (12) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 8.824958ms)
Aug 14 16:38:04.919: INFO: (12) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 8.554667ms)
Aug 14 16:38:04.920: INFO: (12) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 9.700132ms)
Aug 14 16:38:04.920: INFO: (12) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 9.955709ms)
Aug 14 16:38:04.921: INFO: (12) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 10.181388ms)
Aug 14 16:38:04.921: INFO: (12) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 10.408781ms)
Aug 14 16:38:04.928: INFO: (13) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 6.379209ms)
Aug 14 16:38:04.928: INFO: (13) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 6.845033ms)
Aug 14 16:38:04.928: INFO: (13) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 6.716986ms)
Aug 14 16:38:04.928: INFO: (13) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 6.047754ms)
Aug 14 16:38:04.928: INFO: (13) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 6.357205ms)
Aug 14 16:38:04.928: INFO: (13) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 6.622165ms)
Aug 14 16:38:04.928: INFO: (13) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 7.068857ms)
Aug 14 16:38:04.929: INFO: (13) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 7.599548ms)
Aug 14 16:38:04.929: INFO: (13) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 8.444356ms)
Aug 14 16:38:04.930: INFO: (13) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 7.975045ms)
Aug 14 16:38:04.930: INFO: (13) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 8.512988ms)
Aug 14 16:38:04.930: INFO: (13) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 8.783666ms)
Aug 14 16:38:04.930: INFO: (13) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 8.553812ms)
Aug 14 16:38:04.931: INFO: (13) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 9.063576ms)
Aug 14 16:38:04.931: INFO: (13) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 9.501225ms)
Aug 14 16:38:04.931: INFO: (13) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 9.829381ms)
Aug 14 16:38:04.937: INFO: (14) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 6.22303ms)
Aug 14 16:38:04.938: INFO: (14) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 6.102647ms)
Aug 14 16:38:04.941: INFO: (14) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 9.293061ms)
Aug 14 16:38:04.941: INFO: (14) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 9.698334ms)
Aug 14 16:38:04.941: INFO: (14) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 9.02358ms)
Aug 14 16:38:04.941: INFO: (14) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 9.302049ms)
Aug 14 16:38:04.941: INFO: (14) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 9.205946ms)
Aug 14 16:38:04.941: INFO: (14) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 9.015686ms)
Aug 14 16:38:04.941: INFO: (14) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 9.601957ms)
Aug 14 16:38:04.941: INFO: (14) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 9.644862ms)
Aug 14 16:38:04.943: INFO: (14) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 11.827639ms)
Aug 14 16:38:04.943: INFO: (14) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 11.699513ms)
Aug 14 16:38:04.943: INFO: (14) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 11.487601ms)
Aug 14 16:38:04.944: INFO: (14) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 11.627682ms)
Aug 14 16:38:04.944: INFO: (14) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 11.592072ms)
Aug 14 16:38:04.944: INFO: (14) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 11.58474ms)
Aug 14 16:38:04.947: INFO: (15) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 3.713415ms)
Aug 14 16:38:05.070: INFO: (15) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 126.205208ms)
Aug 14 16:38:05.070: INFO: (15) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 126.127066ms)
Aug 14 16:38:05.071: INFO: (15) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 126.923819ms)
Aug 14 16:38:05.072: INFO: (15) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 127.562431ms)
Aug 14 16:38:05.072: INFO: (15) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 127.41117ms)
Aug 14 16:38:05.072: INFO: (15) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 127.101248ms)
Aug 14 16:38:05.072: INFO: (15) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 127.855ms)
Aug 14 16:38:05.072: INFO: (15) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 127.639819ms)
Aug 14 16:38:05.073: INFO: (15) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 127.970744ms)
Aug 14 16:38:05.074: INFO: (15) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 129.9338ms)
Aug 14 16:38:05.074: INFO: (15) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 130.121679ms)
Aug 14 16:38:05.076: INFO: (15) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 131.693759ms)
Aug 14 16:38:05.076: INFO: (15) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 131.20352ms)
Aug 14 16:38:05.457: INFO: (15) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 512.640071ms)
Aug 14 16:38:05.457: INFO: (15) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 512.723252ms)
Aug 14 16:38:05.540: INFO: (16) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 81.39016ms)
Aug 14 16:38:05.540: INFO: (16) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 81.676544ms)
Aug 14 16:38:05.540: INFO: (16) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 81.440831ms)
Aug 14 16:38:05.542: INFO: (16) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 83.938532ms)
Aug 14 16:38:05.543: INFO: (16) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 84.98704ms)
Aug 14 16:38:05.543: INFO: (16) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 84.976089ms)
Aug 14 16:38:05.543: INFO: (16) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 85.241305ms)
Aug 14 16:38:05.543: INFO: (16) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 85.018419ms)
Aug 14 16:38:05.544: INFO: (16) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 86.043804ms)
Aug 14 16:38:05.544: INFO: (16) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 85.571313ms)
Aug 14 16:38:05.545: INFO: (16) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 86.100012ms)
Aug 14 16:38:05.546: INFO: (16) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 87.140697ms)
Aug 14 16:38:05.546: INFO: (16) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 87.81373ms)
Aug 14 16:38:05.546: INFO: (16) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 87.605891ms)
Aug 14 16:38:05.546: INFO: (16) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 87.501399ms)
Aug 14 16:38:05.546: INFO: (16) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 87.64089ms)
Aug 14 16:38:05.849: INFO: (17) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 302.619698ms)
Aug 14 16:38:05.926: INFO: (17) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 378.865507ms)
Aug 14 16:38:05.927: INFO: (17) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 381.440922ms)
Aug 14 16:38:05.927: INFO: (17) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 381.17863ms)
Aug 14 16:38:05.927: INFO: (17) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 380.691155ms)
Aug 14 16:38:05.927: INFO: (17) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 381.08564ms)
Aug 14 16:38:05.927: INFO: (17) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 380.459102ms)
Aug 14 16:38:05.928: INFO: (17) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 381.397279ms)
Aug 14 16:38:05.928: INFO: (17) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 381.143306ms)
Aug 14 16:38:05.928: INFO: (17) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 381.242834ms)
Aug 14 16:38:05.929: INFO: (17) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 382.746406ms)
Aug 14 16:38:05.930: INFO: (17) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 384.006671ms)
Aug 14 16:38:05.930: INFO: (17) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 383.554171ms)
Aug 14 16:38:05.931: INFO: (17) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 384.096606ms)
Aug 14 16:38:05.931: INFO: (17) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 383.884018ms)
Aug 14 16:38:05.931: INFO: (17) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 384.516258ms)
Aug 14 16:38:06.095: INFO: (18) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 163.654122ms)
Aug 14 16:38:06.095: INFO: (18) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 163.6263ms)
Aug 14 16:38:06.095: INFO: (18) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 164.068547ms)
Aug 14 16:38:06.096: INFO: (18) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 164.362421ms)
Aug 14 16:38:06.096: INFO: (18) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 164.172414ms)
Aug 14 16:38:06.096: INFO: (18) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 164.210616ms)
Aug 14 16:38:06.096: INFO: (18) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 164.3084ms)
Aug 14 16:38:06.096: INFO: (18) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 164.962436ms)
Aug 14 16:38:06.097: INFO: (18) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 165.147795ms)
Aug 14 16:38:06.097: INFO: (18) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 165.41913ms)
Aug 14 16:38:06.099: INFO: (18) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 167.434216ms)
Aug 14 16:38:06.099: INFO: (18) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 168.220918ms)
Aug 14 16:38:06.100: INFO: (18) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 168.516072ms)
Aug 14 16:38:06.100: INFO: (18) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 168.757979ms)
Aug 14 16:38:06.100: INFO: (18) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 168.827794ms)
Aug 14 16:38:06.100: INFO: (18) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 169.108938ms)
Aug 14 16:38:06.105: INFO: (19) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx/proxy/rewriteme">test</a> (200; 5.071895ms)
Aug 14 16:38:06.128: INFO: (19) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 27.31944ms)
Aug 14 16:38:06.128: INFO: (19) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 27.172447ms)
Aug 14 16:38:06.128: INFO: (19) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:160/proxy/: foo (200; 26.78608ms)
Aug 14 16:38:06.135: INFO: (19) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:462/proxy/: tls qux (200; 34.341575ms)
Aug 14 16:38:06.136: INFO: (19) /api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">test<... (200; 34.882903ms)
Aug 14 16:38:06.136: INFO: (19) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:1080/proxy/rewriteme">... (200; 35.025884ms)
Aug 14 16:38:06.136: INFO: (19) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/: <a href="/api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:443/proxy/tlsrewritem... (200; 35.192559ms)
Aug 14 16:38:06.137: INFO: (19) /api/v1/namespaces/proxy-1731/pods/http:proxy-service-j2hl6-5fwnx:162/proxy/: bar (200; 35.615449ms)
Aug 14 16:38:06.137: INFO: (19) /api/v1/namespaces/proxy-1731/pods/https:proxy-service-j2hl6-5fwnx:460/proxy/: tls baz (200; 36.079027ms)
Aug 14 16:38:06.139: INFO: (19) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname1/proxy/: tls baz (200; 37.925645ms)
Aug 14 16:38:06.141: INFO: (19) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname2/proxy/: bar (200; 40.48774ms)
Aug 14 16:38:06.141: INFO: (19) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname1/proxy/: foo (200; 39.945264ms)
Aug 14 16:38:06.142: INFO: (19) /api/v1/namespaces/proxy-1731/services/proxy-service-j2hl6:portname1/proxy/: foo (200; 40.357022ms)
Aug 14 16:38:06.142: INFO: (19) /api/v1/namespaces/proxy-1731/services/http:proxy-service-j2hl6:portname2/proxy/: bar (200; 40.20631ms)
Aug 14 16:38:06.142: INFO: (19) /api/v1/namespaces/proxy-1731/services/https:proxy-service-j2hl6:tlsportname2/proxy/: tls qux (200; 40.818625ms)
STEP: deleting ReplicationController proxy-service-j2hl6 in namespace proxy-1731, will wait for the garbage collector to delete the pods
Aug 14 16:38:06.460: INFO: Deleting ReplicationController proxy-service-j2hl6 took: 126.88597ms
Aug 14 16:38:06.560: INFO: Terminating ReplicationController proxy-service-j2hl6 pods took: 100.232962ms
[AfterEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:38:21.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1731" for this suite.
Aug 14 16:38:32.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:38:34.032: INFO: namespace proxy-1731 deletion completed in 11.976661266s

â€¢ [SLOW TEST:49.262 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:38:34.038: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2359
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-36dd1a78-b88c-4f70-97b3-057cdd948d2d
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-36dd1a78-b88c-4f70-97b3-057cdd948d2d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:39:52.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2359" for this suite.
Aug 14 16:40:18.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:40:20.661: INFO: namespace projected-2359 deletion completed in 27.994477516s

â€¢ [SLOW TEST:106.624 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:40:20.662: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6770
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-ljsn
STEP: Creating a pod to test atomic-volume-subpath
Aug 14 16:40:21.822: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-ljsn" in namespace "subpath-6770" to be "success or failure"
Aug 14 16:40:21.856: INFO: Pod "pod-subpath-test-projected-ljsn": Phase="Pending", Reason="", readiness=false. Elapsed: 33.78911ms
Aug 14 16:40:23.861: INFO: Pod "pod-subpath-test-projected-ljsn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038459555s
Aug 14 16:40:25.865: INFO: Pod "pod-subpath-test-projected-ljsn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042549202s
Aug 14 16:40:27.888: INFO: Pod "pod-subpath-test-projected-ljsn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065214489s
Aug 14 16:40:30.008: INFO: Pod "pod-subpath-test-projected-ljsn": Phase="Running", Reason="", readiness=true. Elapsed: 8.186161337s
Aug 14 16:40:32.015: INFO: Pod "pod-subpath-test-projected-ljsn": Phase="Running", Reason="", readiness=true. Elapsed: 10.192319552s
Aug 14 16:40:34.832: INFO: Pod "pod-subpath-test-projected-ljsn": Phase="Running", Reason="", readiness=true. Elapsed: 13.009591523s
Aug 14 16:40:36.856: INFO: Pod "pod-subpath-test-projected-ljsn": Phase="Running", Reason="", readiness=true. Elapsed: 15.033726099s
Aug 14 16:40:38.862: INFO: Pod "pod-subpath-test-projected-ljsn": Phase="Running", Reason="", readiness=true. Elapsed: 17.039562284s
Aug 14 16:40:40.871: INFO: Pod "pod-subpath-test-projected-ljsn": Phase="Running", Reason="", readiness=true. Elapsed: 19.048424969s
Aug 14 16:40:42.879: INFO: Pod "pod-subpath-test-projected-ljsn": Phase="Running", Reason="", readiness=true. Elapsed: 21.056526092s
Aug 14 16:40:44.883: INFO: Pod "pod-subpath-test-projected-ljsn": Phase="Running", Reason="", readiness=true. Elapsed: 23.060739232s
Aug 14 16:40:46.888: INFO: Pod "pod-subpath-test-projected-ljsn": Phase="Running", Reason="", readiness=true. Elapsed: 25.065441177s
Aug 14 16:40:48.893: INFO: Pod "pod-subpath-test-projected-ljsn": Phase="Running", Reason="", readiness=true. Elapsed: 27.070278696s
Aug 14 16:40:51.419: INFO: Pod "pod-subpath-test-projected-ljsn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 29.59706588s
STEP: Saw pod success
Aug 14 16:40:51.420: INFO: Pod "pod-subpath-test-projected-ljsn" satisfied condition "success or failure"
Aug 14 16:40:51.424: INFO: Trying to get logs from node slave2 pod pod-subpath-test-projected-ljsn container test-container-subpath-projected-ljsn: <nil>
STEP: delete the pod
Aug 14 16:40:51.686: INFO: Waiting for pod pod-subpath-test-projected-ljsn to disappear
Aug 14 16:40:51.694: INFO: Pod pod-subpath-test-projected-ljsn no longer exists
STEP: Deleting pod pod-subpath-test-projected-ljsn
Aug 14 16:40:51.694: INFO: Deleting pod "pod-subpath-test-projected-ljsn" in namespace "subpath-6770"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:40:51.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6770" for this suite.
Aug 14 16:40:59.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:41:01.638: INFO: namespace subpath-6770 deletion completed in 9.931215466s

â€¢ [SLOW TEST:40.975 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:41:01.639: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5408
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5408
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 14 16:41:02.714: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 14 16:41:47.351: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.194.90:8080/dial?request=hostName&protocol=udp&host=10.151.32.63&port=8081&tries=1'] Namespace:pod-network-test-5408 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 16:41:47.351: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 16:41:47.670: INFO: Waiting for endpoints: map[]
Aug 14 16:41:47.857: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.194.90:8080/dial?request=hostName&protocol=udp&host=10.151.208.68&port=8081&tries=1'] Namespace:pod-network-test-5408 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 16:41:47.857: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 16:41:48.069: INFO: Waiting for endpoints: map[]
Aug 14 16:41:48.085: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.194.90:8080/dial?request=hostName&protocol=udp&host=10.151.51.112&port=8081&tries=1'] Namespace:pod-network-test-5408 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 16:41:48.085: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 16:41:48.303: INFO: Waiting for endpoints: map[]
Aug 14 16:41:48.310: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.194.90:8080/dial?request=hostName&protocol=udp&host=10.151.49.69&port=8081&tries=1'] Namespace:pod-network-test-5408 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 16:41:48.310: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 16:41:48.515: INFO: Waiting for endpoints: map[]
Aug 14 16:41:48.522: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.194.90:8080/dial?request=hostName&protocol=udp&host=10.151.161.109&port=8081&tries=1'] Namespace:pod-network-test-5408 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 16:41:48.522: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 16:41:48.716: INFO: Waiting for endpoints: map[]
Aug 14 16:41:48.796: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.194.90:8080/dial?request=hostName&protocol=udp&host=10.151.194.89&port=8081&tries=1'] Namespace:pod-network-test-5408 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 16:41:48.797: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 16:41:49.054: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:41:49.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5408" for this suite.
Aug 14 16:42:29.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:42:31.242: INFO: namespace pod-network-test-5408 deletion completed in 42.123761484s

â€¢ [SLOW TEST:89.603 seconds]
[sig-network] Networking
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:42:31.242: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6780
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 14 16:42:31.666: INFO: Waiting up to 5m0s for pod "downward-api-5ca13ab1-02cc-4260-a2b3-f4170a36e2dd" in namespace "downward-api-6780" to be "success or failure"
Aug 14 16:42:31.866: INFO: Pod "downward-api-5ca13ab1-02cc-4260-a2b3-f4170a36e2dd": Phase="Pending", Reason="", readiness=false. Elapsed: 200.665175ms
Aug 14 16:42:33.871: INFO: Pod "downward-api-5ca13ab1-02cc-4260-a2b3-f4170a36e2dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.205610106s
Aug 14 16:42:36.254: INFO: Pod "downward-api-5ca13ab1-02cc-4260-a2b3-f4170a36e2dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.588031024s
Aug 14 16:42:38.411: INFO: Pod "downward-api-5ca13ab1-02cc-4260-a2b3-f4170a36e2dd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.745633587s
Aug 14 16:42:40.415: INFO: Pod "downward-api-5ca13ab1-02cc-4260-a2b3-f4170a36e2dd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.749476988s
Aug 14 16:42:42.619: INFO: Pod "downward-api-5ca13ab1-02cc-4260-a2b3-f4170a36e2dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.953230804s
STEP: Saw pod success
Aug 14 16:42:42.621: INFO: Pod "downward-api-5ca13ab1-02cc-4260-a2b3-f4170a36e2dd" satisfied condition "success or failure"
Aug 14 16:42:42.628: INFO: Trying to get logs from node slave2 pod downward-api-5ca13ab1-02cc-4260-a2b3-f4170a36e2dd container dapi-container: <nil>
STEP: delete the pod
Aug 14 16:42:43.120: INFO: Waiting for pod downward-api-5ca13ab1-02cc-4260-a2b3-f4170a36e2dd to disappear
Aug 14 16:42:43.289: INFO: Pod downward-api-5ca13ab1-02cc-4260-a2b3-f4170a36e2dd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:42:43.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6780" for this suite.
Aug 14 16:42:53.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:42:56.000: INFO: namespace downward-api-6780 deletion completed in 12.701332164s

â€¢ [SLOW TEST:24.758 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:42:56.001: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9314
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9314
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9314
Aug 14 16:42:58.251: INFO: Found 0 stateful pods, waiting for 1
Aug 14 16:43:08.289: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 14 16:43:08.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 16:43:16.983: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 16:43:16.983: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 16:43:16.983: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 16:43:17.188: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 14 16:43:27.192: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 16:43:27.192: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 16:43:27.398: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999322s
Aug 14 16:43:28.571: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996012495s
Aug 14 16:43:30.153: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.823073805s
Aug 14 16:43:31.326: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.241336439s
Aug 14 16:43:32.377: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.067892731s
Aug 14 16:43:33.383: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.01705522s
Aug 14 16:43:34.851: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.011583181s
Aug 14 16:43:35.894: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.542591095s
Aug 14 16:43:37.235: INFO: Verifying statefulset ss doesn't scale past 1 for another 500.421326ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9314
Aug 14 16:43:38.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:43:40.725: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 14 16:43:40.725: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 16:43:40.725: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 16:43:40.733: INFO: Found 1 stateful pods, waiting for 3
Aug 14 16:43:50.810: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 16:43:50.810: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 16:43:50.810: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug 14 16:44:00.833: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 16:44:00.833: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 16:44:00.833: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 14 16:44:00.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 16:44:02.129: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 16:44:02.130: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 16:44:02.130: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 16:44:02.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 16:44:03.960: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 16:44:03.960: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 16:44:03.960: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 16:44:03.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 16:44:04.698: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 16:44:04.699: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 16:44:04.699: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 16:44:04.699: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 16:44:04.849: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 14 16:44:15.278: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 16:44:15.278: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 16:44:15.278: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 16:44:15.564: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999205s
Aug 14 16:44:16.643: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.979110025s
Aug 14 16:44:17.704: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.899865523s
Aug 14 16:44:18.773: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.838712306s
Aug 14 16:44:19.800: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.769639506s
Aug 14 16:44:20.809: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.742957048s
Aug 14 16:44:22.081: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.733901936s
Aug 14 16:44:23.147: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.462290303s
Aug 14 16:44:24.259: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.396154412s
Aug 14 16:44:25.264: INFO: Verifying statefulset ss doesn't scale past 3 for another 283.985995ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9314
Aug 14 16:44:26.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:44:27.632: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 14 16:44:27.632: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 16:44:27.632: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 16:44:27.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:44:28.145: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 14 16:44:28.145: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 16:44:28.145: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 16:44:28.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:44:29.306: INFO: rc: 1
Aug 14 16:44:29.306: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server: 
 [] <nil> 0xc00342ade0 exit status 1 <nil> <nil> true [0xc00035dec8 0xc00035def8 0xc00035df20] [0xc00035dec8 0xc00035def8 0xc00035df20] [0xc00035dee8 0xc00035df10] [0x9d17b0 0x9d17b0] 0xc0039aeba0 <nil>}:
Command stdout:

stderr:
Error from server: 

error:
exit status 1
Aug 14 16:44:39.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:44:40.235: INFO: rc: 1
Aug 14 16:44:40.235: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc00342b1d0 exit status 1 <nil> <nil> true [0xc00035df30 0xc00035df98 0xc00225a000] [0xc00035df30 0xc00035df98 0xc00225a000] [0xc00035df78 0xc00035dfe8] [0x9d17b0 0x9d17b0] 0xc0039af020 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Aug 14 16:44:50.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:44:51.000: INFO: rc: 1
Aug 14 16:44:51.000: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e09470 exit status 1 <nil> <nil> true [0xc0014c8ae8 0xc0014c8b00 0xc0014c8b20] [0xc0014c8ae8 0xc0014c8b00 0xc0014c8b20] [0xc0014c8af8 0xc0014c8b18] [0x9d17b0 0x9d17b0] 0xc001ca0c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:45:01.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:45:02.426: INFO: rc: 1
Aug 14 16:45:02.426: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0035c0330 exit status 1 <nil> <nil> true [0xc000011d10 0xc000182120 0xc000182238] [0xc000011d10 0xc000182120 0xc000182238] [0xc000182000 0xc0001821b8] [0x9d17b0 0x9d17b0] 0xc003388420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:45:12.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:45:13.168: INFO: rc: 1
Aug 14 16:45:13.168: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001c60f60 exit status 1 <nil> <nil> true [0xc00034c650 0xc00034c9d8 0xc00034cee0] [0xc00034c650 0xc00034c9d8 0xc00034cee0] [0xc00034c8c0 0xc00034ce28] [0x9d17b0 0x9d17b0] 0xc002e0a300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:45:23.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:45:24.018: INFO: rc: 1
Aug 14 16:45:24.018: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032b0360 exit status 1 <nil> <nil> true [0xc0008ba690 0xc0008bad38 0xc0008baf38] [0xc0008ba690 0xc0008bad38 0xc0008baf38] [0xc0008babf8 0xc0008baf10] [0x9d17b0 0x9d17b0] 0xc00292a600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:45:34.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:45:34.824: INFO: rc: 1
Aug 14 16:45:34.824: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001c612f0 exit status 1 <nil> <nil> true [0xc00034d1f8 0xc00034d630 0xc00034d970] [0xc00034d1f8 0xc00034d630 0xc00034d970] [0xc00034d598 0xc00034d8f8] [0x9d17b0 0x9d17b0] 0xc002e0a720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:45:44.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:45:45.439: INFO: rc: 1
Aug 14 16:45:45.439: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001c61650 exit status 1 <nil> <nil> true [0xc00034db20 0xc00034dd60 0xc00034df60] [0xc00034db20 0xc00034dd60 0xc00034df60] [0xc00034dce8 0xc00034de70] [0x9d17b0 0x9d17b0] 0xc002e0aae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:45:55.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:45:55.978: INFO: rc: 1
Aug 14 16:45:55.978: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032b06f0 exit status 1 <nil> <nil> true [0xc0008baf90 0xc0008bb1e0 0xc0008bb2e8] [0xc0008baf90 0xc0008bb1e0 0xc0008bb2e8] [0xc0008bb098 0xc0008bb2c0] [0x9d17b0 0x9d17b0] 0xc00292ac00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:46:05.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:46:06.995: INFO: rc: 1
Aug 14 16:46:06.995: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001c619e0 exit status 1 <nil> <nil> true [0xc00035c0b0 0xc00035c178 0xc00035c360] [0xc00035c0b0 0xc00035c178 0xc00035c360] [0xc00035c168 0xc00035c320] [0x9d17b0 0x9d17b0] 0xc002e0af60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:46:16.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:46:17.830: INFO: rc: 1
Aug 14 16:46:17.830: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003b883f0 exit status 1 <nil> <nil> true [0xc001b4c080 0xc001b4c2d8 0xc001b4c478] [0xc001b4c080 0xc001b4c2d8 0xc001b4c478] [0xc001b4c288 0xc001b4c3e8] [0x9d17b0 0x9d17b0] 0xc002bf4540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:46:27.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:46:28.254: INFO: rc: 1
Aug 14 16:46:28.254: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003b888d0 exit status 1 <nil> <nil> true [0xc001b4c4d8 0xc001b4c610 0xc001b4c6e8] [0xc001b4c4d8 0xc001b4c610 0xc001b4c6e8] [0xc001b4c5a0 0xc001b4c6a8] [0x9d17b0 0x9d17b0] 0xc002bf4cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:46:38.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:46:39.030: INFO: rc: 1
Aug 14 16:46:39.030: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003b88c60 exit status 1 <nil> <nil> true [0xc001b4c708 0xc001b4c798 0xc001b4c818] [0xc001b4c708 0xc001b4c798 0xc001b4c818] [0xc001b4c770 0xc001b4c7d8] [0x9d17b0 0x9d17b0] 0xc002bf50e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:46:49.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:46:49.416: INFO: rc: 1
Aug 14 16:46:49.416: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032b0ae0 exit status 1 <nil> <nil> true [0xc0008bb428 0xc0008bb638 0xc0008bb730] [0xc0008bb428 0xc0008bb638 0xc0008bb730] [0xc0008bb5c8 0xc0008bb6d0] [0x9d17b0 0x9d17b0] 0xc00292b260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:46:59.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:47:00.173: INFO: rc: 1
Aug 14 16:47:00.174: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0035c0300 exit status 1 <nil> <nil> true [0xc00034c710 0xc00034ca78 0xc00034d1f8] [0xc00034c710 0xc00034ca78 0xc00034d1f8] [0xc00034c9d8 0xc00034cee0] [0x9d17b0 0x9d17b0] 0xc003388420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:47:10.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:47:10.922: INFO: rc: 1
Aug 14 16:47:10.922: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001c60f90 exit status 1 <nil> <nil> true [0xc000011d10 0xc000182120 0xc000182238] [0xc000011d10 0xc000182120 0xc000182238] [0xc000182000 0xc0001821b8] [0x9d17b0 0x9d17b0] 0xc002e0a300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:47:20.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:47:22.312: INFO: rc: 1
Aug 14 16:47:22.312: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032b0390 exit status 1 <nil> <nil> true [0xc00035c0b0 0xc00035c178 0xc00035c360] [0xc00035c0b0 0xc00035c178 0xc00035c360] [0xc00035c168 0xc00035c320] [0x9d17b0 0x9d17b0] 0xc00292a600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:47:32.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:47:33.543: INFO: rc: 1
Aug 14 16:47:33.543: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001c61350 exit status 1 <nil> <nil> true [0xc000183f38 0xc0008babc0 0xc0008bae88] [0xc000183f38 0xc0008babc0 0xc0008bae88] [0xc0008ba690 0xc0008bad38] [0x9d17b0 0x9d17b0] 0xc002e0a720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:47:43.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:47:44.246: INFO: rc: 1
Aug 14 16:47:44.246: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003b88390 exit status 1 <nil> <nil> true [0xc001b4c030 0xc001b4c288 0xc001b4c3e8] [0xc001b4c030 0xc001b4c288 0xc001b4c3e8] [0xc001b4c120 0xc001b4c378] [0x9d17b0 0x9d17b0] 0xc002bf4540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:47:54.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:47:54.633: INFO: rc: 1
Aug 14 16:47:54.634: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0035c0720 exit status 1 <nil> <nil> true [0xc00034d3e8 0xc00034d720 0xc00034db20] [0xc00034d3e8 0xc00034d720 0xc00034db20] [0xc00034d630 0xc00034d970] [0x9d17b0 0x9d17b0] 0xc003388cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:48:04.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:48:05.470: INFO: rc: 1
Aug 14 16:48:05.470: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0035c0ab0 exit status 1 <nil> <nil> true [0xc00034dc20 0xc00034de00 0xc002a12010] [0xc00034dc20 0xc00034de00 0xc002a12010] [0xc00034dd60 0xc00034df60] [0x9d17b0 0x9d17b0] 0xc003389380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:48:15.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:48:15.902: INFO: rc: 1
Aug 14 16:48:15.902: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032b0720 exit status 1 <nil> <nil> true [0xc00035c3a8 0xc00035c428 0xc00035c590] [0xc00035c3a8 0xc00035c428 0xc00035c590] [0xc00035c3e0 0xc00035c548] [0x9d17b0 0x9d17b0] 0xc00292ac00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:48:25.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:48:26.814: INFO: rc: 1
Aug 14 16:48:26.814: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032b0ab0 exit status 1 <nil> <nil> true [0xc00035c5a8 0xc00035c698 0xc00035c730] [0xc00035c5a8 0xc00035c698 0xc00035c730] [0xc00035c638 0xc00035c700] [0x9d17b0 0x9d17b0] 0xc00292b260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:48:36.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:48:38.031: INFO: rc: 1
Aug 14 16:48:38.031: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032b0e70 exit status 1 <nil> <nil> true [0xc00035c788 0xc00035c7f8 0xc00035c928] [0xc00035c788 0xc00035c7f8 0xc00035c928] [0xc00035c7a8 0xc00035c8e8] [0x9d17b0 0x9d17b0] 0xc00292b860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:48:48.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:48:48.811: INFO: rc: 1
Aug 14 16:48:48.811: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003b88900 exit status 1 <nil> <nil> true [0xc001b4c478 0xc001b4c5a0 0xc001b4c6a8] [0xc001b4c478 0xc001b4c5a0 0xc001b4c6a8] [0xc001b4c4f8 0xc001b4c680] [0x9d17b0 0x9d17b0] 0xc002bf4cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:48:58.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:48:59.373: INFO: rc: 1
Aug 14 16:48:59.374: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003b883c0 exit status 1 <nil> <nil> true [0xc001b4c080 0xc001b4c2d8 0xc001b4c478] [0xc001b4c080 0xc001b4c2d8 0xc001b4c478] [0xc001b4c288 0xc001b4c3e8] [0x9d17b0 0x9d17b0] 0xc002bf4540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:49:09.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:49:10.483: INFO: rc: 1
Aug 14 16:49:10.483: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0035c0360 exit status 1 <nil> <nil> true [0xc000182000 0xc0001821b8 0xc000183fb8] [0xc000182000 0xc0001821b8 0xc000183fb8] [0xc000182128 0xc000183f38] [0x9d17b0 0x9d17b0] 0xc003388420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:49:20.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:49:21.674: INFO: rc: 1
Aug 14 16:49:21.674: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0035c06c0 exit status 1 <nil> <nil> true [0xc000011d10 0xc00034c710 0xc00034ca78] [0xc000011d10 0xc00034c710 0xc00034ca78] [0xc00034c650 0xc00034c9d8] [0x9d17b0 0x9d17b0] 0xc003388cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 16:49:31.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 exec --namespace=statefulset-9314 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 16:49:31.847: INFO: rc: 1
Aug 14 16:49:31.847: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Aug 14 16:49:31.847: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 14 16:49:32.042: INFO: Deleting all statefulset in ns statefulset-9314
Aug 14 16:49:32.046: INFO: Scaling statefulset ss to 0
Aug 14 16:49:32.056: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 16:49:32.059: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:49:32.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9314" for this suite.
Aug 14 16:49:52.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:49:54.701: INFO: namespace statefulset-9314 deletion completed in 22.575869286s

â€¢ [SLOW TEST:418.701 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:49:54.702: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4819
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 14 16:49:55.839: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:50:12.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4819" for this suite.
Aug 14 16:50:27.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:50:29.965: INFO: namespace init-container-4819 deletion completed in 17.629504651s

â€¢ [SLOW TEST:35.263 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:50:29.965: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6104
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:51:28.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6104" for this suite.
Aug 14 16:51:42.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:51:44.895: INFO: namespace container-runtime-6104 deletion completed in 16.414339417s

â€¢ [SLOW TEST:74.930 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:51:44.896: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9619
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 14 16:51:57.366: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9619 pod-service-account-efb59687-bce7-407d-ba2e-a92ddff020fe -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 14 16:51:58.801: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9619 pod-service-account-efb59687-bce7-407d-ba2e-a92ddff020fe -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 14 16:52:00.131: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9619 pod-service-account-efb59687-bce7-407d-ba2e-a92ddff020fe -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:52:01.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9619" for this suite.
Aug 14 16:52:12.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:52:14.573: INFO: namespace svcaccounts-9619 deletion completed in 12.542709766s

â€¢ [SLOW TEST:29.677 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:52:14.573: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 14 16:52:16.717: INFO: Waiting up to 5m0s for pod "downward-api-8d892a85-1a1a-422f-b4a3-aa1b192d3a57" in namespace "downward-api-4914" to be "success or failure"
Aug 14 16:52:16.728: INFO: Pod "downward-api-8d892a85-1a1a-422f-b4a3-aa1b192d3a57": Phase="Pending", Reason="", readiness=false. Elapsed: 10.976888ms
Aug 14 16:52:18.736: INFO: Pod "downward-api-8d892a85-1a1a-422f-b4a3-aa1b192d3a57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018492316s
Aug 14 16:52:21.100: INFO: Pod "downward-api-8d892a85-1a1a-422f-b4a3-aa1b192d3a57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.382992954s
Aug 14 16:52:23.105: INFO: Pod "downward-api-8d892a85-1a1a-422f-b4a3-aa1b192d3a57": Phase="Pending", Reason="", readiness=false. Elapsed: 6.387608298s
Aug 14 16:52:25.208: INFO: Pod "downward-api-8d892a85-1a1a-422f-b4a3-aa1b192d3a57": Phase="Pending", Reason="", readiness=false. Elapsed: 8.490278161s
Aug 14 16:52:27.212: INFO: Pod "downward-api-8d892a85-1a1a-422f-b4a3-aa1b192d3a57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.49466277s
STEP: Saw pod success
Aug 14 16:52:27.212: INFO: Pod "downward-api-8d892a85-1a1a-422f-b4a3-aa1b192d3a57" satisfied condition "success or failure"
Aug 14 16:52:27.216: INFO: Trying to get logs from node slave2 pod downward-api-8d892a85-1a1a-422f-b4a3-aa1b192d3a57 container dapi-container: <nil>
STEP: delete the pod
Aug 14 16:52:27.677: INFO: Waiting for pod downward-api-8d892a85-1a1a-422f-b4a3-aa1b192d3a57 to disappear
Aug 14 16:52:27.688: INFO: Pod downward-api-8d892a85-1a1a-422f-b4a3-aa1b192d3a57 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:52:27.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4914" for this suite.
Aug 14 16:52:40.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:52:41.765: INFO: namespace downward-api-4914 deletion completed in 14.070251531s

â€¢ [SLOW TEST:27.192 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:52:41.766: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5140
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 16:52:43.557: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 14 16:52:43.828: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 14 16:52:56.296: INFO: Creating deployment "test-rolling-update-deployment"
Aug 14 16:52:56.486: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 14 16:52:57.112: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 14 16:52:59.187: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 14 16:52:59.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398378, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398378, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398378, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398377, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 16:53:01.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398378, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398378, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398378, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398377, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 16:53:03.424: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398378, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398378, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398378, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398377, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 16:53:05.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398378, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398378, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398378, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398377, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 16:53:07.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398378, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398378, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398378, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701398377, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 16:53:09.347: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 14 16:53:09.552: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-5140,SelfLink:/apis/apps/v1/namespaces/deployment-5140/deployments/test-rolling-update-deployment,UID:b233e16a-fc59-4f5b-a482-118069675e0d,ResourceVersion:393439,Generation:1,CreationTimestamp:2019-08-14 16:52:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-14 16:52:58 +0000 UTC 2019-08-14 16:52:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-14 16:53:09 +0000 UTC 2019-08-14 16:52:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 14 16:53:09.556: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-5140,SelfLink:/apis/apps/v1/namespaces/deployment-5140/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:2c514187-5455-43aa-9023-f6d9b7caed4b,ResourceVersion:393425,Generation:1,CreationTimestamp:2019-08-14 16:52:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment b233e16a-fc59-4f5b-a482-118069675e0d 0xc003264ee7 0xc003264ee8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 14 16:53:09.556: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 14 16:53:09.556: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-5140,SelfLink:/apis/apps/v1/namespaces/deployment-5140/replicasets/test-rolling-update-controller,UID:0c1de61b-d8b4-4e4a-b194-7a42d2956965,ResourceVersion:393438,Generation:2,CreationTimestamp:2019-08-14 16:52:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment b233e16a-fc59-4f5b-a482-118069675e0d 0xc003264e17 0xc003264e18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 14 16:53:09.561: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-ssp9b" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-ssp9b,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-5140,SelfLink:/api/v1/namespaces/deployment-5140/pods/test-rolling-update-deployment-79f6b9d75c-ssp9b,UID:259731b2-54d1-474c-9f50-4dc931da9588,ResourceVersion:393424,Generation:0,CreationTimestamp:2019-08-14 16:52:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 2c514187-5455-43aa-9023-f6d9b7caed4b 0xc0032657d7 0xc0032657d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dsflj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dsflj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-dsflj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003265850} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003265870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 16:52:58 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 16:53:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 16:53:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 16:52:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.59,PodIP:10.151.51.116,StartTime:2019-08-14 16:52:58 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-14 16:53:05 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://02bfea0503d61a7098fbe9474d1a099bb71fc70a8a958b334764337bd65f4db4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:53:09.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5140" for this suite.
Aug 14 16:53:25.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:53:27.752: INFO: namespace deployment-5140 deletion completed in 18.1835565s

â€¢ [SLOW TEST:45.987 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:53:27.754: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7334
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7334, will wait for the garbage collector to delete the pods
Aug 14 16:53:41.236: INFO: Deleting Job.batch foo took: 22.802392ms
Aug 14 16:53:42.339: INFO: Terminating Job.batch foo pods took: 1.10341173s
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:54:22.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7334" for this suite.
Aug 14 16:54:35.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:54:37.650: INFO: namespace job-7334 deletion completed in 14.955119354s

â€¢ [SLOW TEST:69.896 seconds]
[sig-apps] Job
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:54:37.650: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5245
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 14 16:54:38.578: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:54:51.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5245" for this suite.
Aug 14 16:55:20.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:55:22.397: INFO: namespace init-container-5245 deletion completed in 30.627879138s

â€¢ [SLOW TEST:44.746 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:55:22.399: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6460
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 16:55:23.446: INFO: Waiting up to 5m0s for pod "downwardapi-volume-274bfc04-0562-4a77-8306-3dc12899fdd3" in namespace "projected-6460" to be "success or failure"
Aug 14 16:55:23.452: INFO: Pod "downwardapi-volume-274bfc04-0562-4a77-8306-3dc12899fdd3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.605032ms
Aug 14 16:55:25.611: INFO: Pod "downwardapi-volume-274bfc04-0562-4a77-8306-3dc12899fdd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.165201115s
Aug 14 16:55:27.628: INFO: Pod "downwardapi-volume-274bfc04-0562-4a77-8306-3dc12899fdd3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.182097272s
Aug 14 16:55:29.798: INFO: Pod "downwardapi-volume-274bfc04-0562-4a77-8306-3dc12899fdd3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.352274763s
Aug 14 16:55:31.804: INFO: Pod "downwardapi-volume-274bfc04-0562-4a77-8306-3dc12899fdd3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.358063323s
Aug 14 16:55:34.165: INFO: Pod "downwardapi-volume-274bfc04-0562-4a77-8306-3dc12899fdd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.718553175s
STEP: Saw pod success
Aug 14 16:55:34.165: INFO: Pod "downwardapi-volume-274bfc04-0562-4a77-8306-3dc12899fdd3" satisfied condition "success or failure"
Aug 14 16:55:34.170: INFO: Trying to get logs from node slave2 pod downwardapi-volume-274bfc04-0562-4a77-8306-3dc12899fdd3 container client-container: <nil>
STEP: delete the pod
Aug 14 16:55:34.633: INFO: Waiting for pod downwardapi-volume-274bfc04-0562-4a77-8306-3dc12899fdd3 to disappear
Aug 14 16:55:34.639: INFO: Pod downwardapi-volume-274bfc04-0562-4a77-8306-3dc12899fdd3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:55:34.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6460" for this suite.
Aug 14 16:55:45.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:55:47.045: INFO: namespace projected-6460 deletion completed in 12.217241116s

â€¢ [SLOW TEST:24.646 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:55:47.046: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4946
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-28f49ca4-bee0-45c2-bed4-65691de2c275
STEP: Creating a pod to test consume configMaps
Aug 14 16:55:48.383: INFO: Waiting up to 5m0s for pod "pod-configmaps-0a15a124-3412-47ff-ab6a-0bd33aec2900" in namespace "configmap-4946" to be "success or failure"
Aug 14 16:55:48.391: INFO: Pod "pod-configmaps-0a15a124-3412-47ff-ab6a-0bd33aec2900": Phase="Pending", Reason="", readiness=false. Elapsed: 8.155739ms
Aug 14 16:55:50.404: INFO: Pod "pod-configmaps-0a15a124-3412-47ff-ab6a-0bd33aec2900": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020980687s
Aug 14 16:55:52.574: INFO: Pod "pod-configmaps-0a15a124-3412-47ff-ab6a-0bd33aec2900": Phase="Pending", Reason="", readiness=false. Elapsed: 4.190952967s
Aug 14 16:55:54.636: INFO: Pod "pod-configmaps-0a15a124-3412-47ff-ab6a-0bd33aec2900": Phase="Pending", Reason="", readiness=false. Elapsed: 6.253333996s
Aug 14 16:55:56.644: INFO: Pod "pod-configmaps-0a15a124-3412-47ff-ab6a-0bd33aec2900": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.260694212s
STEP: Saw pod success
Aug 14 16:55:56.644: INFO: Pod "pod-configmaps-0a15a124-3412-47ff-ab6a-0bd33aec2900" satisfied condition "success or failure"
Aug 14 16:55:56.648: INFO: Trying to get logs from node slave3 pod pod-configmaps-0a15a124-3412-47ff-ab6a-0bd33aec2900 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 16:55:56.883: INFO: Waiting for pod pod-configmaps-0a15a124-3412-47ff-ab6a-0bd33aec2900 to disappear
Aug 14 16:55:56.894: INFO: Pod pod-configmaps-0a15a124-3412-47ff-ab6a-0bd33aec2900 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:55:56.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4946" for this suite.
Aug 14 16:56:05.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:56:07.179: INFO: namespace configmap-4946 deletion completed in 10.278122922s

â€¢ [SLOW TEST:20.133 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:56:07.180: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 14 16:56:17.560: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:56:17.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5694" for this suite.
Aug 14 16:56:30.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:56:32.072: INFO: namespace container-runtime-5694 deletion completed in 14.080827306s

â€¢ [SLOW TEST:24.893 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:56:32.073: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3078
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 14 16:56:55.172: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 16:56:55.242: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 16:56:57.242: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 16:56:57.331: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 16:56:59.242: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 16:56:59.259: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 16:57:01.242: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 16:57:01.417: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 16:57:03.242: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 16:57:03.247: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 16:57:05.242: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 16:57:05.252: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 16:57:07.242: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 16:57:07.249: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 16:57:09.242: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 16:57:09.363: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 16:57:11.242: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 16:57:11.251: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 16:57:13.242: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 16:57:13.371: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 16:57:15.242: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 16:57:15.471: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 16:57:17.242: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 16:57:17.287: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 16:57:19.242: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 16:57:19.254: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 16:57:21.242: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 16:57:21.895: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 16:57:23.242: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 16:57:24.200: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:57:24.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3078" for this suite.
Aug 14 16:58:13.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:58:15.331: INFO: namespace container-lifecycle-hook-3078 deletion completed in 50.286711936s

â€¢ [SLOW TEST:103.258 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:58:15.332: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2646
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-19879b89-ab22-4e6f-b4b1-bc6bd6460aad
STEP: Creating a pod to test consume configMaps
Aug 14 16:58:17.659: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7cdedb86-0de2-4c12-8c1d-a35e787f0509" in namespace "projected-2646" to be "success or failure"
Aug 14 16:58:17.734: INFO: Pod "pod-projected-configmaps-7cdedb86-0de2-4c12-8c1d-a35e787f0509": Phase="Pending", Reason="", readiness=false. Elapsed: 74.668736ms
Aug 14 16:58:19.738: INFO: Pod "pod-projected-configmaps-7cdedb86-0de2-4c12-8c1d-a35e787f0509": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079341007s
Aug 14 16:58:21.962: INFO: Pod "pod-projected-configmaps-7cdedb86-0de2-4c12-8c1d-a35e787f0509": Phase="Pending", Reason="", readiness=false. Elapsed: 4.303010515s
Aug 14 16:58:24.247: INFO: Pod "pod-projected-configmaps-7cdedb86-0de2-4c12-8c1d-a35e787f0509": Phase="Pending", Reason="", readiness=false. Elapsed: 6.587599091s
Aug 14 16:58:26.292: INFO: Pod "pod-projected-configmaps-7cdedb86-0de2-4c12-8c1d-a35e787f0509": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.632578826s
STEP: Saw pod success
Aug 14 16:58:26.292: INFO: Pod "pod-projected-configmaps-7cdedb86-0de2-4c12-8c1d-a35e787f0509" satisfied condition "success or failure"
Aug 14 16:58:26.295: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-7cdedb86-0de2-4c12-8c1d-a35e787f0509 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 16:58:26.903: INFO: Waiting for pod pod-projected-configmaps-7cdedb86-0de2-4c12-8c1d-a35e787f0509 to disappear
Aug 14 16:58:27.223: INFO: Pod pod-projected-configmaps-7cdedb86-0de2-4c12-8c1d-a35e787f0509 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:58:27.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2646" for this suite.
Aug 14 16:58:40.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:58:41.819: INFO: namespace projected-2646 deletion completed in 14.510836272s

â€¢ [SLOW TEST:26.487 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:58:41.819: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4647
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Aug 14 16:58:46.431: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0814 16:58:46.431515      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:58:46.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4647" for this suite.
Aug 14 16:58:54.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:58:56.552: INFO: namespace gc-4647 deletion completed in 10.113623999s

â€¢ [SLOW TEST:14.734 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:58:56.553: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4449
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 16:58:59.130: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d3e3c26-13f9-4ab8-bbb4-3549eef320a1" in namespace "downward-api-4449" to be "success or failure"
Aug 14 16:58:59.843: INFO: Pod "downwardapi-volume-7d3e3c26-13f9-4ab8-bbb4-3549eef320a1": Phase="Pending", Reason="", readiness=false. Elapsed: 713.31704ms
Aug 14 16:59:01.877: INFO: Pod "downwardapi-volume-7d3e3c26-13f9-4ab8-bbb4-3549eef320a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.746401928s
Aug 14 16:59:03.881: INFO: Pod "downwardapi-volume-7d3e3c26-13f9-4ab8-bbb4-3549eef320a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.751273978s
Aug 14 16:59:05.992: INFO: Pod "downwardapi-volume-7d3e3c26-13f9-4ab8-bbb4-3549eef320a1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.86163321s
Aug 14 16:59:08.053: INFO: Pod "downwardapi-volume-7d3e3c26-13f9-4ab8-bbb4-3549eef320a1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.922944189s
Aug 14 16:59:10.273: INFO: Pod "downwardapi-volume-7d3e3c26-13f9-4ab8-bbb4-3549eef320a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.142908422s
STEP: Saw pod success
Aug 14 16:59:10.273: INFO: Pod "downwardapi-volume-7d3e3c26-13f9-4ab8-bbb4-3549eef320a1" satisfied condition "success or failure"
Aug 14 16:59:10.303: INFO: Trying to get logs from node slave1 pod downwardapi-volume-7d3e3c26-13f9-4ab8-bbb4-3549eef320a1 container client-container: <nil>
STEP: delete the pod
Aug 14 16:59:11.028: INFO: Waiting for pod downwardapi-volume-7d3e3c26-13f9-4ab8-bbb4-3549eef320a1 to disappear
Aug 14 16:59:11.033: INFO: Pod downwardapi-volume-7d3e3c26-13f9-4ab8-bbb4-3549eef320a1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 16:59:11.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4449" for this suite.
Aug 14 16:59:25.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 16:59:27.217: INFO: namespace downward-api-4449 deletion completed in 16.177067745s

â€¢ [SLOW TEST:30.665 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 16:59:27.218: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4708
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4708
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 14 16:59:28.605: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 14 17:00:20.207: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.194.98:8080/dial?request=hostName&protocol=http&host=10.151.49.77&port=8080&tries=1'] Namespace:pod-network-test-4708 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 17:00:20.207: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 17:00:20.407: INFO: Waiting for endpoints: map[]
Aug 14 17:00:20.411: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.194.98:8080/dial?request=hostName&protocol=http&host=10.151.208.69&port=8080&tries=1'] Namespace:pod-network-test-4708 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 17:00:20.411: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 17:00:20.651: INFO: Waiting for endpoints: map[]
Aug 14 17:00:20.656: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.194.98:8080/dial?request=hostName&protocol=http&host=10.151.51.122&port=8080&tries=1'] Namespace:pod-network-test-4708 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 17:00:20.656: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 17:00:20.858: INFO: Waiting for endpoints: map[]
Aug 14 17:00:20.881: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.194.98:8080/dial?request=hostName&protocol=http&host=10.151.161.110&port=8080&tries=1'] Namespace:pod-network-test-4708 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 17:00:20.881: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 17:00:21.133: INFO: Waiting for endpoints: map[]
Aug 14 17:00:21.233: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.194.98:8080/dial?request=hostName&protocol=http&host=10.151.194.99&port=8080&tries=1'] Namespace:pod-network-test-4708 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 17:00:21.233: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 17:00:21.497: INFO: Waiting for endpoints: map[]
Aug 14 17:00:21.600: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.194.98:8080/dial?request=hostName&protocol=http&host=10.151.32.64&port=8080&tries=1'] Namespace:pod-network-test-4708 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 17:00:21.600: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
Aug 14 17:00:21.796: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:00:21.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4708" for this suite.
Aug 14 17:01:10.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:01:12.265: INFO: namespace pod-network-test-4708 deletion completed in 50.461507813s

â€¢ [SLOW TEST:105.046 seconds]
[sig-network] Networking
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 17:01:12.265: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2857
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Aug 14 17:01:48.111: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:01:48.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0814 17:01:48.111286      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2857" for this suite.
Aug 14 17:02:06.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:02:08.169: INFO: namespace gc-2857 deletion completed in 20.052124092s

â€¢ [SLOW TEST:55.905 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 17:02:08.170: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7518
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-4753
STEP: Creating secret with name secret-test-9ba19546-d0a9-4ce4-be44-f4075dc3f8c0
STEP: Creating a pod to test consume secrets
Aug 14 17:02:14.092: INFO: Waiting up to 5m0s for pod "pod-secrets-d675ac35-ab5c-4b59-b4d9-d202f6a451ea" in namespace "secrets-7518" to be "success or failure"
Aug 14 17:02:15.033: INFO: Pod "pod-secrets-d675ac35-ab5c-4b59-b4d9-d202f6a451ea": Phase="Pending", Reason="", readiness=false. Elapsed: 941.270278ms
Aug 14 17:02:17.040: INFO: Pod "pod-secrets-d675ac35-ab5c-4b59-b4d9-d202f6a451ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.94841665s
Aug 14 17:02:19.075: INFO: Pod "pod-secrets-d675ac35-ab5c-4b59-b4d9-d202f6a451ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.983482665s
Aug 14 17:02:21.092: INFO: Pod "pod-secrets-d675ac35-ab5c-4b59-b4d9-d202f6a451ea": Phase="Pending", Reason="", readiness=false. Elapsed: 7.000409743s
Aug 14 17:02:23.106: INFO: Pod "pod-secrets-d675ac35-ab5c-4b59-b4d9-d202f6a451ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.013988238s
STEP: Saw pod success
Aug 14 17:02:23.106: INFO: Pod "pod-secrets-d675ac35-ab5c-4b59-b4d9-d202f6a451ea" satisfied condition "success or failure"
Aug 14 17:02:23.405: INFO: Trying to get logs from node slave3 pod pod-secrets-d675ac35-ab5c-4b59-b4d9-d202f6a451ea container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 17:02:23.931: INFO: Waiting for pod pod-secrets-d675ac35-ab5c-4b59-b4d9-d202f6a451ea to disappear
Aug 14 17:02:23.935: INFO: Pod pod-secrets-d675ac35-ab5c-4b59-b4d9-d202f6a451ea no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:02:23.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7518" for this suite.
Aug 14 17:02:36.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:02:38.059: INFO: namespace secrets-7518 deletion completed in 14.115090212s
STEP: Destroying namespace "secret-namespace-4753" for this suite.
Aug 14 17:02:48.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:02:50.045: INFO: namespace secret-namespace-4753 deletion completed in 11.986374297s

â€¢ [SLOW TEST:41.876 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 17:02:50.045: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-133e25c5-349c-40e1-b198-266a6eeb69e7
STEP: Creating a pod to test consume secrets
Aug 14 17:02:53.046: INFO: Waiting up to 5m0s for pod "pod-secrets-00501e29-a688-44cd-a07c-df61e0f602c1" in namespace "secrets-2650" to be "success or failure"
Aug 14 17:02:53.073: INFO: Pod "pod-secrets-00501e29-a688-44cd-a07c-df61e0f602c1": Phase="Pending", Reason="", readiness=false. Elapsed: 27.379009ms
Aug 14 17:02:55.079: INFO: Pod "pod-secrets-00501e29-a688-44cd-a07c-df61e0f602c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033116602s
Aug 14 17:02:57.084: INFO: Pod "pod-secrets-00501e29-a688-44cd-a07c-df61e0f602c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037702237s
Aug 14 17:02:59.166: INFO: Pod "pod-secrets-00501e29-a688-44cd-a07c-df61e0f602c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.120048669s
Aug 14 17:03:01.177: INFO: Pod "pod-secrets-00501e29-a688-44cd-a07c-df61e0f602c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.131491324s
STEP: Saw pod success
Aug 14 17:03:01.177: INFO: Pod "pod-secrets-00501e29-a688-44cd-a07c-df61e0f602c1" satisfied condition "success or failure"
Aug 14 17:03:01.181: INFO: Trying to get logs from node slave1 pod pod-secrets-00501e29-a688-44cd-a07c-df61e0f602c1 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 17:03:01.568: INFO: Waiting for pod pod-secrets-00501e29-a688-44cd-a07c-df61e0f602c1 to disappear
Aug 14 17:03:01.574: INFO: Pod pod-secrets-00501e29-a688-44cd-a07c-df61e0f602c1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:03:01.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2650" for this suite.
Aug 14 17:03:11.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:03:13.754: INFO: namespace secrets-2650 deletion completed in 12.174172544s

â€¢ [SLOW TEST:23.709 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 17:03:13.755: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9535
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 17:03:15.981: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f18f116b-bf8c-4c61-acac-5d7021fd4a26" in namespace "downward-api-9535" to be "success or failure"
Aug 14 17:03:15.988: INFO: Pod "downwardapi-volume-f18f116b-bf8c-4c61-acac-5d7021fd4a26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.418316ms
Aug 14 17:03:17.992: INFO: Pod "downwardapi-volume-f18f116b-bf8c-4c61-acac-5d7021fd4a26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010967445s
Aug 14 17:03:20.023: INFO: Pod "downwardapi-volume-f18f116b-bf8c-4c61-acac-5d7021fd4a26": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041592712s
Aug 14 17:03:22.175: INFO: Pod "downwardapi-volume-f18f116b-bf8c-4c61-acac-5d7021fd4a26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.193860794s
Aug 14 17:03:24.277: INFO: Pod "downwardapi-volume-f18f116b-bf8c-4c61-acac-5d7021fd4a26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.296273693s
STEP: Saw pod success
Aug 14 17:03:24.277: INFO: Pod "downwardapi-volume-f18f116b-bf8c-4c61-acac-5d7021fd4a26" satisfied condition "success or failure"
Aug 14 17:03:24.282: INFO: Trying to get logs from node slave2 pod downwardapi-volume-f18f116b-bf8c-4c61-acac-5d7021fd4a26 container client-container: <nil>
STEP: delete the pod
Aug 14 17:03:25.292: INFO: Waiting for pod downwardapi-volume-f18f116b-bf8c-4c61-acac-5d7021fd4a26 to disappear
Aug 14 17:03:25.299: INFO: Pod downwardapi-volume-f18f116b-bf8c-4c61-acac-5d7021fd4a26 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:03:25.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9535" for this suite.
Aug 14 17:03:36.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:03:38.045: INFO: namespace downward-api-9535 deletion completed in 12.73833538s

â€¢ [SLOW TEST:24.290 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 17:03:38.047: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2866
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-5d2e5846-5a01-45c1-86c9-160d01c96bbb
STEP: Creating configMap with name cm-test-opt-upd-758dec40-f4cd-4906-b45b-702b1fe669b6
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-5d2e5846-5a01-45c1-86c9-160d01c96bbb
STEP: Updating configmap cm-test-opt-upd-758dec40-f4cd-4906-b45b-702b1fe669b6
STEP: Creating configMap with name cm-test-opt-create-4a39b9ef-88bf-4e33-82bb-ff08ae1c0878
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:03:54.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2866" for this suite.
Aug 14 17:04:38.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:04:40.675: INFO: namespace configmap-2866 deletion completed in 46.386559938s

â€¢ [SLOW TEST:62.629 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 17:04:40.676: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9901
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9901.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9901.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9901.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9901.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9901.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9901.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 17:04:56.123: INFO: DNS probes using dns-9901/dns-test-5c755689-4f5d-4a5e-9729-cb21b037db15 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:04:56.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9901" for this suite.
Aug 14 17:05:11.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:05:13.037: INFO: namespace dns-9901 deletion completed in 16.333459903s

â€¢ [SLOW TEST:32.362 seconds]
[sig-network] DNS
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 17:05:13.038: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5693
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-9f49d01e-ba19-4bf9-8020-759f94a7231a
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-9f49d01e-ba19-4bf9-8020-759f94a7231a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:06:44.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5693" for this suite.
Aug 14 17:07:12.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:07:14.749: INFO: namespace configmap-5693 deletion completed in 29.991579867s

â€¢ [SLOW TEST:121.711 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 17:07:14.750: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8588
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 14 17:07:16.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-8588'
Aug 14 17:07:24.802: INFO: stderr: ""
Aug 14 17:07:24.802: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Aug 14 17:07:34.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 get pod e2e-test-nginx-pod --namespace=kubectl-8588 -o json'
Aug 14 17:07:35.256: INFO: stderr: ""
Aug 14 17:07:35.256: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-08-14T17:07:24Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-8588\",\n        \"resourceVersion\": \"396584\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8588/pods/e2e-test-nginx-pod\",\n        \"uid\": \"a894eb9d-c3af-4bbb-ba7a-a260a954a3b5\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-bkfms\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"slave3\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-bkfms\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-bkfms\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-14T17:07:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-14T17:07:31Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-14T17:07:31Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-14T17:07:24Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://66c415f37e04969aeca1c0c525b8305ef0de318ac38029c5cd458c832d4c49d0\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-08-14T17:07:31Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.202.67\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.151.194.103\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-08-14T17:07:25Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 14 17:07:35.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 replace -f - --namespace=kubectl-8588'
Aug 14 17:07:36.859: INFO: stderr: ""
Aug 14 17:07:36.859: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Aug 14 17:07:37.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-938832172 delete pods e2e-test-nginx-pod --namespace=kubectl-8588'
Aug 14 17:07:45.227: INFO: stderr: ""
Aug 14 17:07:45.227: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:07:45.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8588" for this suite.
Aug 14 17:07:55.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:07:57.004: INFO: namespace kubectl-8588 deletion completed in 11.762582768s

â€¢ [SLOW TEST:42.254 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 17:07:57.005: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1086
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 14 17:07:57.661: INFO: Waiting up to 5m0s for pod "pod-9843693b-d6c5-40d0-a64d-66e9915cd1cb" in namespace "emptydir-1086" to be "success or failure"
Aug 14 17:07:57.669: INFO: Pod "pod-9843693b-d6c5-40d0-a64d-66e9915cd1cb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.432049ms
Aug 14 17:07:59.693: INFO: Pod "pod-9843693b-d6c5-40d0-a64d-66e9915cd1cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032469857s
Aug 14 17:08:01.771: INFO: Pod "pod-9843693b-d6c5-40d0-a64d-66e9915cd1cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.110670089s
Aug 14 17:08:03.781: INFO: Pod "pod-9843693b-d6c5-40d0-a64d-66e9915cd1cb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.120437611s
Aug 14 17:08:05.934: INFO: Pod "pod-9843693b-d6c5-40d0-a64d-66e9915cd1cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.273150249s
STEP: Saw pod success
Aug 14 17:08:05.934: INFO: Pod "pod-9843693b-d6c5-40d0-a64d-66e9915cd1cb" satisfied condition "success or failure"
Aug 14 17:08:05.942: INFO: Trying to get logs from node slave1 pod pod-9843693b-d6c5-40d0-a64d-66e9915cd1cb container test-container: <nil>
STEP: delete the pod
Aug 14 17:08:06.350: INFO: Waiting for pod pod-9843693b-d6c5-40d0-a64d-66e9915cd1cb to disappear
Aug 14 17:08:06.354: INFO: Pod pod-9843693b-d6c5-40d0-a64d-66e9915cd1cb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:08:06.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1086" for this suite.
Aug 14 17:08:15.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:08:16.793: INFO: namespace emptydir-1086 deletion completed in 10.422498555s

â€¢ [SLOW TEST:19.788 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 17:08:16.794: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7787
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 17:08:17.845: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 14 17:08:22.876: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 14 17:08:24.994: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 14 17:08:37.901: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-7787,SelfLink:/apis/apps/v1/namespaces/deployment-7787/deployments/test-cleanup-deployment,UID:d2cd8eba-d3f4-42e3-a03b-3ef4d7db30dc,ResourceVersion:396854,Generation:1,CreationTimestamp:2019-08-14 17:08:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-14 17:08:26 +0000 UTC 2019-08-14 17:08:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-14 17:08:37 +0000 UTC 2019-08-14 17:08:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 14 17:08:38.160: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-7787,SelfLink:/apis/apps/v1/namespaces/deployment-7787/replicasets/test-cleanup-deployment-55bbcbc84c,UID:e0fd6e97-45c3-448d-8fd4-3172738949c9,ResourceVersion:396836,Generation:1,CreationTimestamp:2019-08-14 17:08:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment d2cd8eba-d3f4-42e3-a03b-3ef4d7db30dc 0xc0022ed017 0xc0022ed018}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 14 17:08:38.407: INFO: Pod "test-cleanup-deployment-55bbcbc84c-wfhtr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-wfhtr,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-7787,SelfLink:/api/v1/namespaces/deployment-7787/pods/test-cleanup-deployment-55bbcbc84c-wfhtr,UID:92529619-e13c-4687-a6e6-dfd202168d6c,ResourceVersion:396835,Generation:0,CreationTimestamp:2019-08-14 17:08:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c e0fd6e97-45c3-448d-8fd4-3172738949c9 0xc0022ed617 0xc0022ed618}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zwqlf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zwqlf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-zwqlf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022ed690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022ed6b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 17:08:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 17:08:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 17:08:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 17:08:26 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.67,PodIP:10.151.194.104,StartTime:2019-08-14 17:08:26 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-14 17:08:33 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://9006f4affc4aff037e81af02b664f9d19076ff88d903e864314daefbf7248783}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:08:38.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7787" for this suite.
Aug 14 17:08:52.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:08:54.180: INFO: namespace deployment-7787 deletion completed in 15.761717696s

â€¢ [SLOW TEST:37.386 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 17:08:54.181: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-812
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 14 17:08:55.963: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 14 17:08:55.995: INFO: Waiting for terminating namespaces to be deleted...
Aug 14 17:08:55.998: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Aug 14 17:08:56.010: INFO: kube-scheduler-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 17:08:56.010: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-zx5nv from heptio-sonobuoy started at 2019-08-14 14:03:36 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.010: INFO: 	Container sonobuoy-worker ready: true, restart count 3
Aug 14 17:08:56.010: INFO: 	Container systemd-logs ready: true, restart count 3
Aug 14 17:08:56.010: INFO: coredns-9c98d6cbb-nz84n from kube-system started at 2019-08-13 01:20:04 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.010: INFO: 	Container coredns ready: true, restart count 0
Aug 14 17:08:56.010: INFO: node-exporter-ht9c6 from monitoring started at 2019-08-13 01:20:52 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.010: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 17:08:56.010: INFO: vpa-updater-557c96bf46-lhlk7 from kube-system started at 2019-08-13 07:04:16 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.010: INFO: 	Container updater ready: true, restart count 0
Aug 14 17:08:56.010: INFO: istio-policy-5c69497f5-m2kph from istio-system started at 2019-08-13 07:22:06 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.010: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 17:08:56.010: INFO: 	Container mixer ready: true, restart count 0
Aug 14 17:08:56.010: INFO: resource-reserver-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 17:08:56.010: INFO: nginx-proxy-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 17:08:56.010: INFO: calico-node-qjwhs from kube-system started at 2019-08-13 01:17:48 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.010: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 17:08:56.010: INFO: metrics-server-5cb9b96667-8wn6n from kube-system started at 2019-08-13 01:21:42 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.010: INFO: 	Container metrics-server ready: true, restart count 0
Aug 14 17:08:56.010: INFO: prometheus-adapter-6798664dcf-2jb4b from monitoring started at 2019-08-13 01:25:35 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.010: INFO: 	Container prometheus-adapter ready: true, restart count 10
Aug 14 17:08:56.010: INFO: istio-ingressgateway-798c45bf9c-qqqcw from istio-system started at 2019-08-13 07:04:19 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.010: INFO: 	Container ingressgateway ready: true, restart count 0
Aug 14 17:08:56.010: INFO: fluentd-zqgvd from monitoring started at 2019-08-13 01:21:56 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.010: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 17:08:56.010: INFO: kube-proxy-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 17:08:56.010: INFO: dns-autoscaler-6b9bbb69f4-v77pg from kube-system started at 2019-08-13 07:04:20 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.010: INFO: 	Container autoscaler ready: true, restart count 0
Aug 14 17:08:56.010: INFO: kube-apiserver-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 17:08:56.010: INFO: kube-controller-manager-master1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 17:08:56.010: INFO: istio-egressgateway-68856cf966-k7975 from istio-system started at 2019-08-14 11:49:09 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.010: INFO: 	Container egressgateway ready: true, restart count 0
Aug 14 17:08:56.010: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Aug 14 17:08:56.018: INFO: kube-scheduler-master2 from kube-system started at 2019-08-14 01:15:06 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.018: INFO: 	Container kube-scheduler ready: true, restart count 1
Aug 14 17:08:56.018: INFO: nginx-proxy-master2 from kube-system started at 2019-08-13 07:53:47 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.018: INFO: 	Container nginx-proxy ready: true, restart count 0
Aug 14 17:08:56.018: INFO: prometheus-5c8cd4644d-jvrsq from monitoring started at 2019-08-13 01:27:28 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.018: INFO: 	Container configmap-reload ready: false, restart count 0
Aug 14 17:08:56.018: INFO: 	Container prometheus ready: false, restart count 0
Aug 14 17:08:56.018: INFO: fluentd-c2sf9 from monitoring started at 2019-08-13 01:21:56 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.019: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 17:08:56.019: INFO: resource-reserver-master2 from kube-system started at 2019-08-13 09:05:46 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.019: INFO: 	Container sleep-forever ready: true, restart count 1
Aug 14 17:08:56.019: INFO: kube-apiserver-master2 from kube-system started at 2019-08-13 09:05:26 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.019: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 14 17:08:56.019: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-2t2pw from heptio-sonobuoy started at 2019-08-14 14:03:37 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.019: INFO: 	Container sonobuoy-worker ready: true, restart count 3
Aug 14 17:08:56.019: INFO: 	Container systemd-logs ready: true, restart count 3
Aug 14 17:08:56.019: INFO: kube-proxy-master2 from kube-system started at 2019-08-14 01:13:46 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.019: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 14 17:08:56.019: INFO: kube-controller-manager-master2 from kube-system started at 2019-08-14 01:13:46 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.019: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 14 17:08:56.019: INFO: node-exporter-mqllj from monitoring started at 2019-08-13 01:20:53 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.019: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 17:08:56.019: INFO: calico-node-ltbzp from kube-system started at 2019-08-13 01:17:46 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.019: INFO: 	Container calico-node ready: true, restart count 1
Aug 14 17:08:56.019: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Aug 14 17:08:56.030: INFO: istio-egressgateway-68856cf966-f2pbv from istio-system started at 2019-08-13 07:40:44 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.030: INFO: 	Container egressgateway ready: true, restart count 0
Aug 14 17:08:56.030: INFO: kube-controller-manager-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 17:08:56.030: INFO: kube-scheduler-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 17:08:56.030: INFO: calico-node-dzqhc from kube-system started at 2019-08-13 01:17:48 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.030: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 17:08:56.030: INFO: coredns-9c98d6cbb-ncrr2 from kube-system started at 2019-08-13 01:20:32 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.030: INFO: 	Container coredns ready: true, restart count 0
Aug 14 17:08:56.030: INFO: node-exporter-xjj7z from monitoring started at 2019-08-13 01:20:53 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.030: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 17:08:56.030: INFO: istio-policy-5c69497f5-ztbzd from istio-system started at 2019-08-13 06:02:25 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.030: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 17:08:56.030: INFO: 	Container mixer ready: true, restart count 0
Aug 14 17:08:56.030: INFO: nginx-proxy-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 17:08:56.030: INFO: kube-proxy-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 17:08:56.030: INFO: alertmanager-67c75747cd-5f5g9 from monitoring started at 2019-08-13 01:21:11 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.030: INFO: 	Container alertmanager ready: true, restart count 0
Aug 14 17:08:56.030: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-t75nt from heptio-sonobuoy started at 2019-08-14 14:03:37 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.030: INFO: 	Container sonobuoy-worker ready: true, restart count 3
Aug 14 17:08:56.030: INFO: 	Container systemd-logs ready: true, restart count 3
Aug 14 17:08:56.030: INFO: istio-ingressgateway-798c45bf9c-8gqfd from istio-system started at 2019-08-13 01:45:35 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.030: INFO: 	Container ingressgateway ready: true, restart count 0
Aug 14 17:08:56.030: INFO: blackbox-exporter-679b8bc8c-wktmk from monitoring started at 2019-08-13 07:04:16 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.030: INFO: 	Container blackbox-exporter ready: true, restart count 0
Aug 14 17:08:56.030: INFO: kube-apiserver-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 17:08:56.030: INFO: fluentd-7nzvz from monitoring started at 2019-08-13 01:21:56 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.030: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 17:08:56.030: INFO: vpa-recommender-658cb5b88b-8fsxl from kube-system started at 2019-08-13 01:26:06 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.030: INFO: 	Container recommender ready: true, restart count 0
Aug 14 17:08:56.030: INFO: vpa-admission-controller-77f89cb6d9-fhnf6 from kube-system started at 2019-08-13 01:26:09 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.030: INFO: 	Container admission-controller ready: true, restart count 0
Aug 14 17:08:56.030: INFO: kube-state-metrics-7f59b96454-g872c from monitoring started at 2019-08-13 07:04:16 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.030: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 14 17:08:56.030: INFO: resource-reserver-master3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 17:08:56.030: INFO: prometheus-5c8cd4644d-97xk7 from monitoring started at 2019-08-13 07:04:15 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.030: INFO: 	Container configmap-reload ready: false, restart count 0
Aug 14 17:08:56.030: INFO: 	Container prometheus ready: false, restart count 0
Aug 14 17:08:56.030: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Aug 14 17:08:56.039: INFO: node-exporter-crjnb from monitoring started at 2019-08-13 01:20:54 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.039: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 17:08:56.039: INFO: fluentd-kmfgj from monitoring started at 2019-08-13 01:21:55 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.039: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 17:08:56.039: INFO: trigger-d85ff74f-r87hh from kube-system started at 2019-08-13 01:22:17 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.039: INFO: 	Container trigger ready: true, restart count 0
Aug 14 17:08:56.039: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-f5ljw from heptio-sonobuoy started at 2019-08-14 14:03:36 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.039: INFO: 	Container sonobuoy-worker ready: true, restart count 3
Aug 14 17:08:56.039: INFO: 	Container systemd-logs ready: true, restart count 3
Aug 14 17:08:56.039: INFO: istio-telemetry-7fb5cd9cb4-k8bs9 from istio-system started at 2019-08-13 04:00:41 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.040: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 17:08:56.040: INFO: 	Container mixer ready: true, restart count 0
Aug 14 17:08:56.040: INFO: kube-proxy-slave1 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 17:08:56.040: INFO: istio-egressgateway-68856cf966-5ksfj from istio-system started at 2019-08-13 01:23:51 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.040: INFO: 	Container egressgateway ready: true, restart count 0
Aug 14 17:08:56.040: INFO: db-daemon-server-7859f8794b-n69wx from kube-system started at 2019-08-13 01:24:34 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.040: INFO: 	Container db-daemon-server ready: true, restart count 0
Aug 14 17:08:56.040: INFO: calico-node-jgcdn from kube-system started at 2019-08-13 01:17:49 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.040: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 17:08:56.040: INFO: istio-telemetry-7fb5cd9cb4-rvc4k from istio-system started at 2019-08-13 01:41:01 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.040: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 17:08:56.040: INFO: 	Container mixer ready: true, restart count 0
Aug 14 17:08:56.040: INFO: istio-ingressgateway-798c45bf9c-cr97n from istio-system started at 2019-08-13 01:40:49 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.040: INFO: 	Container ingressgateway ready: true, restart count 0
Aug 14 17:08:56.040: INFO: istio-policy-5c69497f5-l7vkr from istio-system started at 2019-08-13 01:41:01 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.040: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 17:08:56.040: INFO: 	Container mixer ready: true, restart count 0
Aug 14 17:08:56.040: INFO: istio-statsd-prom-bridge-c98c48fd5-m9l8x from istio-system started at 2019-08-13 01:23:53 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.040: INFO: 	Container statsd-prom-bridge ready: true, restart count 0
Aug 14 17:08:56.040: INFO: istio-citadel-6b64f55c84-vkx5j from istio-system started at 2019-08-13 01:23:54 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.040: INFO: 	Container citadel ready: true, restart count 0
Aug 14 17:08:56.040: INFO: 
Logging pods the kubelet thinks is on node slave2 before test
Aug 14 17:08:56.049: INFO: tiller-deploy-cdd5d96ff-qw744 from kube-system started at 2019-08-13 01:19:35 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.049: INFO: 	Container tiller ready: true, restart count 0
Aug 14 17:08:56.049: INFO: istio-telemetry-7fb5cd9cb4-9wrct from istio-system started at 2019-08-13 01:42:03 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.049: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 17:08:56.049: INFO: 	Container mixer ready: true, restart count 0
Aug 14 17:08:56.049: INFO: node-exporter-zj7rc from monitoring started at 2019-08-13 01:20:54 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.049: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 17:08:56.049: INFO: fluentd-v22vb from monitoring started at 2019-08-13 01:21:56 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.049: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 17:08:56.049: INFO: istio-ingressgateway-798c45bf9c-cg9np from istio-system started at 2019-08-13 01:50:41 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.049: INFO: 	Container ingressgateway ready: true, restart count 0
Aug 14 17:08:56.049: INFO: kube-proxy-slave2 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 17:08:56.049: INFO: calico-node-csv54 from kube-system started at 2019-08-13 01:17:48 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.049: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 17:08:56.050: INFO: istio-policy-5c69497f5-rfhzl from istio-system started at 2019-08-13 01:23:52 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.050: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 17:08:56.050: INFO: 	Container mixer ready: true, restart count 0
Aug 14 17:08:56.050: INFO: istio-pilot-57bf44f4f7-5hs9t from istio-system started at 2019-08-13 01:23:53 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.050: INFO: 	Container discovery ready: true, restart count 0
Aug 14 17:08:56.050: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 17:08:56.050: INFO: istio-egressgateway-68856cf966-vqqz4 from istio-system started at 2019-08-13 01:26:00 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.050: INFO: 	Container egressgateway ready: true, restart count 0
Aug 14 17:08:56.050: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-mng6c from heptio-sonobuoy started at 2019-08-14 14:03:38 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.050: INFO: 	Container sonobuoy-worker ready: true, restart count 3
Aug 14 17:08:56.050: INFO: 	Container systemd-logs ready: true, restart count 3
Aug 14 17:08:56.050: INFO: 
Logging pods the kubelet thinks is on node slave3 before test
Aug 14 17:08:56.061: INFO: kube-proxy-slave3 from kube-system started at <nil> (0 container statuses recorded)
Aug 14 17:08:56.061: INFO: node-exporter-tbv9g from monitoring started at 2019-08-13 01:20:54 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.061: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 17:08:56.061: INFO: istio-telemetry-7fb5cd9cb4-t2qmf from istio-system started at 2019-08-13 06:11:07 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.061: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 17:08:56.061: INFO: 	Container mixer ready: true, restart count 0
Aug 14 17:08:56.061: INFO: fluentd-jdlm5 from monitoring started at 2019-08-13 01:21:55 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.061: INFO: 	Container fluentd ready: true, restart count 0
Aug 14 17:08:56.061: INFO: istio-ingressgateway-798c45bf9c-rt89k from istio-system started at 2019-08-13 01:23:52 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.061: INFO: 	Container ingressgateway ready: true, restart count 0
Aug 14 17:08:56.061: INFO: istio-telemetry-7fb5cd9cb4-5ttsx from istio-system started at 2019-08-13 01:23:52 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.061: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 17:08:56.061: INFO: 	Container mixer ready: true, restart count 0
Aug 14 17:08:56.061: INFO: istio-tracing-64f74595f-s2g9w from istio-system started at 2019-08-13 01:23:54 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.061: INFO: 	Container jaeger ready: true, restart count 0
Aug 14 17:08:56.061: INFO: sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-sttpd from heptio-sonobuoy started at 2019-08-14 14:03:35 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.061: INFO: 	Container sonobuoy-worker ready: true, restart count 3
Aug 14 17:08:56.062: INFO: 	Container systemd-logs ready: true, restart count 3
Aug 14 17:08:56.062: INFO: istio-sidecar-injector-8886f58d8-64v9m from istio-system started at 2019-08-13 01:23:53 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.062: INFO: 	Container sidecar-injector-webhook ready: true, restart count 0
Aug 14 17:08:56.062: INFO: istio-policy-5c69497f5-nblx2 from istio-system started at 2019-08-13 04:31:07 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.062: INFO: 	Container istio-proxy ready: true, restart count 0
Aug 14 17:08:56.062: INFO: 	Container mixer ready: true, restart count 0
Aug 14 17:08:56.062: INFO: sonobuoy-e2e-job-a99adffb6ed94bd6 from heptio-sonobuoy started at 2019-08-14 14:03:35 +0000 UTC (2 container statuses recorded)
Aug 14 17:08:56.062: INFO: 	Container e2e ready: true, restart count 0
Aug 14 17:08:56.062: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 14 17:08:56.062: INFO: calico-node-zwdcq from kube-system started at 2019-08-13 01:17:47 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.062: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 17:08:56.062: INFO: calico-kube-controllers-56f8894747-j4gdw from kube-system started at 2019-08-13 01:18:13 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.062: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 14 17:08:56.062: INFO: istio-egressgateway-68856cf966-nczfg from istio-system started at 2019-08-13 01:26:00 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.062: INFO: 	Container egressgateway ready: true, restart count 0
Aug 14 17:08:56.062: INFO: istio-galley-5f97c5c497-cfvs2 from istio-system started at 2019-08-13 07:47:20 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.062: INFO: 	Container validator ready: true, restart count 1
Aug 14 17:08:56.062: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-14 14:03:22 +0000 UTC (1 container statuses recorded)
Aug 14 17:08:56.062: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node master1
STEP: verifying the node has the label node master2
STEP: verifying the node has the label node master3
STEP: verifying the node has the label node slave1
STEP: verifying the node has the label node slave2
STEP: verifying the node has the label node slave3
Aug 14 17:08:57.909: INFO: Pod sonobuoy requesting resource cpu=0m on Node slave3
Aug 14 17:08:57.909: INFO: Pod sonobuoy-e2e-job-a99adffb6ed94bd6 requesting resource cpu=0m on Node slave3
Aug 14 17:08:57.909: INFO: Pod sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-2t2pw requesting resource cpu=0m on Node master2
Aug 14 17:08:57.909: INFO: Pod sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-f5ljw requesting resource cpu=0m on Node slave1
Aug 14 17:08:57.909: INFO: Pod sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-mng6c requesting resource cpu=0m on Node slave2
Aug 14 17:08:57.909: INFO: Pod sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-sttpd requesting resource cpu=0m on Node slave3
Aug 14 17:08:57.909: INFO: Pod sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-t75nt requesting resource cpu=0m on Node master3
Aug 14 17:08:57.909: INFO: Pod sonobuoy-systemd-logs-daemon-set-78a94dc7a5d84a95-zx5nv requesting resource cpu=0m on Node master1
Aug 14 17:08:57.909: INFO: Pod istio-citadel-6b64f55c84-vkx5j requesting resource cpu=10m on Node slave1
Aug 14 17:08:57.909: INFO: Pod istio-egressgateway-68856cf966-5ksfj requesting resource cpu=10m on Node slave1
Aug 14 17:08:57.909: INFO: Pod istio-egressgateway-68856cf966-f2pbv requesting resource cpu=10m on Node master3
Aug 14 17:08:57.909: INFO: Pod istio-egressgateway-68856cf966-k7975 requesting resource cpu=10m on Node master1
Aug 14 17:08:57.909: INFO: Pod istio-egressgateway-68856cf966-nczfg requesting resource cpu=10m on Node slave3
Aug 14 17:08:57.909: INFO: Pod istio-egressgateway-68856cf966-vqqz4 requesting resource cpu=10m on Node slave2
Aug 14 17:08:57.909: INFO: Pod istio-galley-5f97c5c497-cfvs2 requesting resource cpu=10m on Node slave3
Aug 14 17:08:57.909: INFO: Pod istio-ingressgateway-798c45bf9c-8gqfd requesting resource cpu=10m on Node master3
Aug 14 17:08:57.909: INFO: Pod istio-ingressgateway-798c45bf9c-cg9np requesting resource cpu=10m on Node slave2
Aug 14 17:08:57.909: INFO: Pod istio-ingressgateway-798c45bf9c-cr97n requesting resource cpu=10m on Node slave1
Aug 14 17:08:57.909: INFO: Pod istio-ingressgateway-798c45bf9c-qqqcw requesting resource cpu=10m on Node master1
Aug 14 17:08:57.909: INFO: Pod istio-ingressgateway-798c45bf9c-rt89k requesting resource cpu=10m on Node slave3
Aug 14 17:08:57.909: INFO: Pod istio-pilot-57bf44f4f7-5hs9t requesting resource cpu=510m on Node slave2
Aug 14 17:08:57.909: INFO: Pod istio-policy-5c69497f5-l7vkr requesting resource cpu=20m on Node slave1
Aug 14 17:08:57.909: INFO: Pod istio-policy-5c69497f5-m2kph requesting resource cpu=20m on Node master1
Aug 14 17:08:57.909: INFO: Pod istio-policy-5c69497f5-nblx2 requesting resource cpu=20m on Node slave3
Aug 14 17:08:57.909: INFO: Pod istio-policy-5c69497f5-rfhzl requesting resource cpu=20m on Node slave2
Aug 14 17:08:57.909: INFO: Pod istio-policy-5c69497f5-ztbzd requesting resource cpu=20m on Node master3
Aug 14 17:08:57.909: INFO: Pod istio-sidecar-injector-8886f58d8-64v9m requesting resource cpu=10m on Node slave3
Aug 14 17:08:57.909: INFO: Pod istio-statsd-prom-bridge-c98c48fd5-m9l8x requesting resource cpu=10m on Node slave1
Aug 14 17:08:57.909: INFO: Pod istio-telemetry-7fb5cd9cb4-5ttsx requesting resource cpu=20m on Node slave3
Aug 14 17:08:57.909: INFO: Pod istio-telemetry-7fb5cd9cb4-9wrct requesting resource cpu=20m on Node slave2
Aug 14 17:08:57.909: INFO: Pod istio-telemetry-7fb5cd9cb4-k8bs9 requesting resource cpu=20m on Node slave1
Aug 14 17:08:57.909: INFO: Pod istio-telemetry-7fb5cd9cb4-rvc4k requesting resource cpu=20m on Node slave1
Aug 14 17:08:57.909: INFO: Pod istio-telemetry-7fb5cd9cb4-t2qmf requesting resource cpu=20m on Node slave3
Aug 14 17:08:57.909: INFO: Pod istio-tracing-64f74595f-s2g9w requesting resource cpu=10m on Node slave3
Aug 14 17:08:57.909: INFO: Pod calico-kube-controllers-56f8894747-j4gdw requesting resource cpu=30m on Node slave3
Aug 14 17:08:57.909: INFO: Pod calico-node-csv54 requesting resource cpu=150m on Node slave2
Aug 14 17:08:57.909: INFO: Pod calico-node-dzqhc requesting resource cpu=150m on Node master3
Aug 14 17:08:57.909: INFO: Pod calico-node-jgcdn requesting resource cpu=150m on Node slave1
Aug 14 17:08:57.909: INFO: Pod calico-node-ltbzp requesting resource cpu=150m on Node master2
Aug 14 17:08:57.909: INFO: Pod calico-node-qjwhs requesting resource cpu=150m on Node master1
Aug 14 17:08:57.909: INFO: Pod calico-node-zwdcq requesting resource cpu=150m on Node slave3
Aug 14 17:08:57.909: INFO: Pod coredns-9c98d6cbb-ncrr2 requesting resource cpu=100m on Node master3
Aug 14 17:08:57.909: INFO: Pod coredns-9c98d6cbb-nz84n requesting resource cpu=100m on Node master1
Aug 14 17:08:57.909: INFO: Pod db-daemon-server-7859f8794b-n69wx requesting resource cpu=0m on Node slave1
Aug 14 17:08:57.909: INFO: Pod dns-autoscaler-6b9bbb69f4-v77pg requesting resource cpu=20m on Node master1
Aug 14 17:08:57.909: INFO: Pod kube-apiserver-master1 requesting resource cpu=100m on Node master1
Aug 14 17:08:57.909: INFO: Pod kube-apiserver-master2 requesting resource cpu=100m on Node master2
Aug 14 17:08:57.909: INFO: Pod kube-apiserver-master3 requesting resource cpu=100m on Node master3
Aug 14 17:08:57.909: INFO: Pod kube-controller-manager-master1 requesting resource cpu=100m on Node master1
Aug 14 17:08:57.909: INFO: Pod kube-controller-manager-master2 requesting resource cpu=100m on Node master2
Aug 14 17:08:57.909: INFO: Pod kube-controller-manager-master3 requesting resource cpu=100m on Node master3
Aug 14 17:08:57.909: INFO: Pod kube-proxy-master1 requesting resource cpu=150m on Node master1
Aug 14 17:08:57.909: INFO: Pod kube-proxy-master2 requesting resource cpu=150m on Node master2
Aug 14 17:08:57.909: INFO: Pod kube-proxy-master3 requesting resource cpu=150m on Node master3
Aug 14 17:08:57.909: INFO: Pod kube-proxy-slave1 requesting resource cpu=150m on Node slave1
Aug 14 17:08:57.909: INFO: Pod kube-proxy-slave2 requesting resource cpu=150m on Node slave2
Aug 14 17:08:57.909: INFO: Pod kube-proxy-slave3 requesting resource cpu=150m on Node slave3
Aug 14 17:08:57.909: INFO: Pod kube-scheduler-master1 requesting resource cpu=80m on Node master1
Aug 14 17:08:57.909: INFO: Pod kube-scheduler-master2 requesting resource cpu=80m on Node master2
Aug 14 17:08:57.909: INFO: Pod kube-scheduler-master3 requesting resource cpu=80m on Node master3
Aug 14 17:08:57.909: INFO: Pod metrics-server-5cb9b96667-8wn6n requesting resource cpu=0m on Node master1
Aug 14 17:08:57.909: INFO: Pod nginx-proxy-master1 requesting resource cpu=25m on Node master1
Aug 14 17:08:57.909: INFO: Pod nginx-proxy-master2 requesting resource cpu=25m on Node master2
Aug 14 17:08:57.909: INFO: Pod nginx-proxy-master3 requesting resource cpu=25m on Node master3
Aug 14 17:08:57.909: INFO: Pod resource-reserver-master1 requesting resource cpu=800m on Node master1
Aug 14 17:08:57.909: INFO: Pod resource-reserver-master2 requesting resource cpu=800m on Node master2
Aug 14 17:08:57.909: INFO: Pod resource-reserver-master3 requesting resource cpu=800m on Node master3
Aug 14 17:08:57.909: INFO: Pod tiller-deploy-cdd5d96ff-qw744 requesting resource cpu=0m on Node slave2
Aug 14 17:08:57.909: INFO: Pod trigger-d85ff74f-r87hh requesting resource cpu=100m on Node slave1
Aug 14 17:08:57.909: INFO: Pod vpa-admission-controller-77f89cb6d9-fhnf6 requesting resource cpu=50m on Node master3
Aug 14 17:08:57.909: INFO: Pod vpa-recommender-658cb5b88b-8fsxl requesting resource cpu=50m on Node master3
Aug 14 17:08:57.909: INFO: Pod vpa-updater-557c96bf46-lhlk7 requesting resource cpu=50m on Node master1
Aug 14 17:08:57.909: INFO: Pod alertmanager-67c75747cd-5f5g9 requesting resource cpu=0m on Node master3
Aug 14 17:08:57.909: INFO: Pod blackbox-exporter-679b8bc8c-wktmk requesting resource cpu=0m on Node master3
Aug 14 17:08:57.909: INFO: Pod fluentd-7nzvz requesting resource cpu=0m on Node master3
Aug 14 17:08:57.909: INFO: Pod fluentd-c2sf9 requesting resource cpu=0m on Node master2
Aug 14 17:08:57.909: INFO: Pod fluentd-jdlm5 requesting resource cpu=0m on Node slave3
Aug 14 17:08:57.909: INFO: Pod fluentd-kmfgj requesting resource cpu=0m on Node slave1
Aug 14 17:08:57.909: INFO: Pod fluentd-v22vb requesting resource cpu=0m on Node slave2
Aug 14 17:08:57.909: INFO: Pod fluentd-zqgvd requesting resource cpu=0m on Node master1
Aug 14 17:08:57.909: INFO: Pod kube-state-metrics-7f59b96454-g872c requesting resource cpu=0m on Node master3
Aug 14 17:08:57.909: INFO: Pod node-exporter-crjnb requesting resource cpu=100m on Node slave1
Aug 14 17:08:57.909: INFO: Pod node-exporter-ht9c6 requesting resource cpu=100m on Node master1
Aug 14 17:08:57.909: INFO: Pod node-exporter-mqllj requesting resource cpu=100m on Node master2
Aug 14 17:08:57.909: INFO: Pod node-exporter-tbv9g requesting resource cpu=100m on Node slave3
Aug 14 17:08:57.909: INFO: Pod node-exporter-xjj7z requesting resource cpu=100m on Node master3
Aug 14 17:08:57.909: INFO: Pod node-exporter-zj7rc requesting resource cpu=100m on Node slave2
Aug 14 17:08:57.909: INFO: Pod prometheus-5c8cd4644d-97xk7 requesting resource cpu=0m on Node master3
Aug 14 17:08:57.909: INFO: Pod prometheus-5c8cd4644d-jvrsq requesting resource cpu=0m on Node master2
Aug 14 17:08:57.909: INFO: Pod prometheus-adapter-6798664dcf-2jb4b requesting resource cpu=0m on Node master1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-469e91b3-2323-4d93-8797-25a6fa8e161a.15bad91d0c8754f9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-812/filler-pod-469e91b3-2323-4d93-8797-25a6fa8e161a to master3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-469e91b3-2323-4d93-8797-25a6fa8e161a.15bad91e853ddd9b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-469e91b3-2323-4d93-8797-25a6fa8e161a.15bad91f14bd7a1c], Reason = [Created], Message = [Created container filler-pod-469e91b3-2323-4d93-8797-25a6fa8e161a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-469e91b3-2323-4d93-8797-25a6fa8e161a.15bad91f5926866c], Reason = [Started], Message = [Started container filler-pod-469e91b3-2323-4d93-8797-25a6fa8e161a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d3773c0-ecef-42db-9a2c-23d14a45bcdb.15bad91dbe4bb534], Reason = [Scheduled], Message = [Successfully assigned sched-pred-812/filler-pod-4d3773c0-ecef-42db-9a2c-23d14a45bcdb to slave3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d3773c0-ecef-42db-9a2c-23d14a45bcdb.15bad91fc6ce457b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d3773c0-ecef-42db-9a2c-23d14a45bcdb.15bad9201c74f535], Reason = [Created], Message = [Created container filler-pod-4d3773c0-ecef-42db-9a2c-23d14a45bcdb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4d3773c0-ecef-42db-9a2c-23d14a45bcdb.15bad9204da95f46], Reason = [Started], Message = [Started container filler-pod-4d3773c0-ecef-42db-9a2c-23d14a45bcdb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e7e61f9-b572-4ae2-b69f-2fb7a195fd14.15bad91ce2cd8ec3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-812/filler-pod-5e7e61f9-b572-4ae2-b69f-2fb7a195fd14 to master1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e7e61f9-b572-4ae2-b69f-2fb7a195fd14.15bad91f1bed3a46], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e7e61f9-b572-4ae2-b69f-2fb7a195fd14.15bad91fb2ef0dfc], Reason = [Created], Message = [Created container filler-pod-5e7e61f9-b572-4ae2-b69f-2fb7a195fd14]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e7e61f9-b572-4ae2-b69f-2fb7a195fd14.15bad91ffbba1c82], Reason = [Started], Message = [Started container filler-pod-5e7e61f9-b572-4ae2-b69f-2fb7a195fd14]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6c31f89-8459-465f-8758-eccc04161e48.15bad91d78604df9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-812/filler-pod-a6c31f89-8459-465f-8758-eccc04161e48 to slave2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6c31f89-8459-465f-8758-eccc04161e48.15bad91fbd511c8e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6c31f89-8459-465f-8758-eccc04161e48.15bad9200feddc42], Reason = [Created], Message = [Created container filler-pod-a6c31f89-8459-465f-8758-eccc04161e48]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6c31f89-8459-465f-8758-eccc04161e48.15bad9202a2ace7f], Reason = [Started], Message = [Started container filler-pod-a6c31f89-8459-465f-8758-eccc04161e48]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df82c0e9-49cd-4e24-88dc-49d3333df77d.15bad91d0c89ce16], Reason = [Scheduled], Message = [Successfully assigned sched-pred-812/filler-pod-df82c0e9-49cd-4e24-88dc-49d3333df77d to master2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df82c0e9-49cd-4e24-88dc-49d3333df77d.15bad91f10914c6a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df82c0e9-49cd-4e24-88dc-49d3333df77d.15bad91fb5443a90], Reason = [Created], Message = [Created container filler-pod-df82c0e9-49cd-4e24-88dc-49d3333df77d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-df82c0e9-49cd-4e24-88dc-49d3333df77d.15bad91fee87dffd], Reason = [Started], Message = [Started container filler-pod-df82c0e9-49cd-4e24-88dc-49d3333df77d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcfc8433-0e4c-4772-82b4-169692cbca19.15bad91d353c8b4b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-812/filler-pod-fcfc8433-0e4c-4772-82b4-169692cbca19 to slave1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcfc8433-0e4c-4772-82b4-169692cbca19.15bad91e5b0aa02d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcfc8433-0e4c-4772-82b4-169692cbca19.15bad91e9fa8f5b9], Reason = [Created], Message = [Created container filler-pod-fcfc8433-0e4c-4772-82b4-169692cbca19]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcfc8433-0e4c-4772-82b4-169692cbca19.15bad91eb44ef331], Reason = [Started], Message = [Started container filler-pod-fcfc8433-0e4c-4772-82b4-169692cbca19]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15bad920a98655ce], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 Insufficient cpu.]
STEP: removing the label node off the node slave2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node slave3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node master1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node master2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node master3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node slave1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:09:19.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-812" for this suite.
Aug 14 17:09:38.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:09:40.250: INFO: namespace sched-pred-812 deletion completed in 20.814010822s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

â€¢ [SLOW TEST:46.070 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 17:09:40.254: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6085
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-5b0166aa-214c-489d-b64d-a63db6ef8fd3
STEP: Creating a pod to test consume configMaps
Aug 14 17:09:42.919: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ca681b4b-13e2-4925-b46d-34e3f778f076" in namespace "projected-6085" to be "success or failure"
Aug 14 17:09:43.746: INFO: Pod "pod-projected-configmaps-ca681b4b-13e2-4925-b46d-34e3f778f076": Phase="Pending", Reason="", readiness=false. Elapsed: 826.805779ms
Aug 14 17:09:45.755: INFO: Pod "pod-projected-configmaps-ca681b4b-13e2-4925-b46d-34e3f778f076": Phase="Pending", Reason="", readiness=false. Elapsed: 2.83585921s
Aug 14 17:09:47.768: INFO: Pod "pod-projected-configmaps-ca681b4b-13e2-4925-b46d-34e3f778f076": Phase="Pending", Reason="", readiness=false. Elapsed: 4.848520691s
Aug 14 17:09:49.774: INFO: Pod "pod-projected-configmaps-ca681b4b-13e2-4925-b46d-34e3f778f076": Phase="Pending", Reason="", readiness=false. Elapsed: 6.854287959s
Aug 14 17:09:52.189: INFO: Pod "pod-projected-configmaps-ca681b4b-13e2-4925-b46d-34e3f778f076": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.269706754s
STEP: Saw pod success
Aug 14 17:09:52.189: INFO: Pod "pod-projected-configmaps-ca681b4b-13e2-4925-b46d-34e3f778f076" satisfied condition "success or failure"
Aug 14 17:09:52.195: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-ca681b4b-13e2-4925-b46d-34e3f778f076 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 17:09:52.708: INFO: Waiting for pod pod-projected-configmaps-ca681b4b-13e2-4925-b46d-34e3f778f076 to disappear
Aug 14 17:09:52.722: INFO: Pod pod-projected-configmaps-ca681b4b-13e2-4925-b46d-34e3f778f076 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:09:52.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6085" for this suite.
Aug 14 17:10:03.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:10:04.963: INFO: namespace projected-6085 deletion completed in 12.234988978s

â€¢ [SLOW TEST:24.709 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 17:10:04.963: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7570
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-cdd3eb3e-f303-4d93-bde2-ce24ea0e682e
STEP: Creating a pod to test consume configMaps
Aug 14 17:10:05.967: INFO: Waiting up to 5m0s for pod "pod-configmaps-7638ce9c-fbd7-4035-afb7-5127951f28e5" in namespace "configmap-7570" to be "success or failure"
Aug 14 17:10:05.975: INFO: Pod "pod-configmaps-7638ce9c-fbd7-4035-afb7-5127951f28e5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.312691ms
Aug 14 17:10:08.284: INFO: Pod "pod-configmaps-7638ce9c-fbd7-4035-afb7-5127951f28e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.316608664s
Aug 14 17:10:10.427: INFO: Pod "pod-configmaps-7638ce9c-fbd7-4035-afb7-5127951f28e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.459626183s
Aug 14 17:10:12.652: INFO: Pod "pod-configmaps-7638ce9c-fbd7-4035-afb7-5127951f28e5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.684595944s
Aug 14 17:10:14.664: INFO: Pod "pod-configmaps-7638ce9c-fbd7-4035-afb7-5127951f28e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.696436663s
STEP: Saw pod success
Aug 14 17:10:14.664: INFO: Pod "pod-configmaps-7638ce9c-fbd7-4035-afb7-5127951f28e5" satisfied condition "success or failure"
Aug 14 17:10:14.683: INFO: Trying to get logs from node slave2 pod pod-configmaps-7638ce9c-fbd7-4035-afb7-5127951f28e5 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 17:10:15.228: INFO: Waiting for pod pod-configmaps-7638ce9c-fbd7-4035-afb7-5127951f28e5 to disappear
Aug 14 17:10:15.232: INFO: Pod pod-configmaps-7638ce9c-fbd7-4035-afb7-5127951f28e5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:10:15.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7570" for this suite.
Aug 14 17:10:25.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:10:27.382: INFO: namespace configmap-7570 deletion completed in 12.143839773s

â€¢ [SLOW TEST:22.419 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 17:10:27.386: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3314
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-3e574131-3e04-4b9b-b761-e440e8e1b2dc
STEP: Creating a pod to test consume secrets
Aug 14 17:10:28.963: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bdc3d048-4ba8-4bda-a500-20219c2fec0a" in namespace "projected-3314" to be "success or failure"
Aug 14 17:10:29.152: INFO: Pod "pod-projected-secrets-bdc3d048-4ba8-4bda-a500-20219c2fec0a": Phase="Pending", Reason="", readiness=false. Elapsed: 189.033498ms
Aug 14 17:10:31.235: INFO: Pod "pod-projected-secrets-bdc3d048-4ba8-4bda-a500-20219c2fec0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.272582607s
Aug 14 17:10:33.244: INFO: Pod "pod-projected-secrets-bdc3d048-4ba8-4bda-a500-20219c2fec0a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280646239s
Aug 14 17:10:35.289: INFO: Pod "pod-projected-secrets-bdc3d048-4ba8-4bda-a500-20219c2fec0a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.326009986s
Aug 14 17:10:37.294: INFO: Pod "pod-projected-secrets-bdc3d048-4ba8-4bda-a500-20219c2fec0a": Phase="Running", Reason="", readiness=true. Elapsed: 8.330922305s
Aug 14 17:10:39.728: INFO: Pod "pod-projected-secrets-bdc3d048-4ba8-4bda-a500-20219c2fec0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.764707849s
STEP: Saw pod success
Aug 14 17:10:39.728: INFO: Pod "pod-projected-secrets-bdc3d048-4ba8-4bda-a500-20219c2fec0a" satisfied condition "success or failure"
Aug 14 17:10:39.934: INFO: Trying to get logs from node slave3 pod pod-projected-secrets-bdc3d048-4ba8-4bda-a500-20219c2fec0a container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 14 17:10:40.313: INFO: Waiting for pod pod-projected-secrets-bdc3d048-4ba8-4bda-a500-20219c2fec0a to disappear
Aug 14 17:10:40.322: INFO: Pod pod-projected-secrets-bdc3d048-4ba8-4bda-a500-20219c2fec0a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:10:40.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3314" for this suite.
Aug 14 17:10:50.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:10:52.555: INFO: namespace projected-3314 deletion completed in 12.223285163s

â€¢ [SLOW TEST:25.169 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 17:10:52.556: INFO: >>> kubeConfig: /tmp/kubeconfig-938832172
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3668
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Aug 14 17:10:53.838: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-938832172 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 17:10:53.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3668" for this suite.
Aug 14 17:11:04.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 17:11:05.846: INFO: namespace kubectl-3668 deletion completed in 11.897268905s

â€¢ [SLOW TEST:13.290 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSAug 14 17:11:05.846: INFO: Running AfterSuite actions on all nodes
Aug 14 17:11:05.846: INFO: Running AfterSuite actions on node 1
Aug 14 17:11:05.846: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 11229.368 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 3h7m11.673714986s
Test Suite Passed
