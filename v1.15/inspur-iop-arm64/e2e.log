I0905 07:16:16.680500      16 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-815508381
I0905 07:16:16.680749      16 e2e.go:241] Starting e2e run "cd157687-fd2b-4739-8214-fd3a982a4770" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1567667773 - Will randomize all specs
Will run 215 of 4413 specs

Sep  5 07:16:17.215: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 07:16:17.220: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep  5 07:16:17.249: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  5 07:16:17.305: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  5 07:16:17.305: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Sep  5 07:16:17.305: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  5 07:16:17.321: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Sep  5 07:16:17.321: INFO: e2e test version: v1.15.3
Sep  5 07:16:17.322: INFO: kube-apiserver version: v1.15.3
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:16:17.324: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename secrets
Sep  5 07:16:17.440: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Sep  5 07:16:17.452: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8023
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-ca0c8485-ff86-4309-a9e7-1fa6dd8cde41
STEP: Creating a pod to test consume secrets
Sep  5 07:16:17.576: INFO: Waiting up to 5m0s for pod "pod-secrets-63fd237f-11c8-4c43-82bf-a316a2a4283a" in namespace "secrets-8023" to be "success or failure"
Sep  5 07:16:17.582: INFO: Pod "pod-secrets-63fd237f-11c8-4c43-82bf-a316a2a4283a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.874795ms
Sep  5 07:16:19.618: INFO: Pod "pod-secrets-63fd237f-11c8-4c43-82bf-a316a2a4283a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041779832s
Sep  5 07:16:21.622: INFO: Pod "pod-secrets-63fd237f-11c8-4c43-82bf-a316a2a4283a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046498401s
STEP: Saw pod success
Sep  5 07:16:21.622: INFO: Pod "pod-secrets-63fd237f-11c8-4c43-82bf-a316a2a4283a" satisfied condition "success or failure"
Sep  5 07:16:21.631: INFO: Trying to get logs from node slave1 pod pod-secrets-63fd237f-11c8-4c43-82bf-a316a2a4283a container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 07:16:21.783: INFO: Waiting for pod pod-secrets-63fd237f-11c8-4c43-82bf-a316a2a4283a to disappear
Sep  5 07:16:21.789: INFO: Pod pod-secrets-63fd237f-11c8-4c43-82bf-a316a2a4283a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:16:21.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8023" for this suite.
Sep  5 07:16:27.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:16:27.938: INFO: namespace secrets-8023 deletion completed in 6.143454778s

• [SLOW TEST:10.614 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:16:27.939: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3677
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-54c2a865-99f2-48d8-af82-41172a1efea9 in namespace container-probe-3677
Sep  5 07:16:34.193: INFO: Started pod liveness-54c2a865-99f2-48d8-af82-41172a1efea9 in namespace container-probe-3677
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 07:16:34.195: INFO: Initial restart count of pod liveness-54c2a865-99f2-48d8-af82-41172a1efea9 is 0
Sep  5 07:16:50.234: INFO: Restart count of pod container-probe-3677/liveness-54c2a865-99f2-48d8-af82-41172a1efea9 is now 1 (16.03873376s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:16:50.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3677" for this suite.
Sep  5 07:16:56.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:16:56.451: INFO: namespace container-probe-3677 deletion completed in 6.138934565s

• [SLOW TEST:28.512 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:16:56.451: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep  5 07:16:56.739: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5322,SelfLink:/api/v1/namespaces/watch-5322/configmaps/e2e-watch-test-resource-version,UID:ad22a3cc-fc93-467a-8338-43b079b1dd37,ResourceVersion:221427,Generation:0,CreationTimestamp:2019-09-05 07:16:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  5 07:16:56.739: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5322,SelfLink:/api/v1/namespaces/watch-5322/configmaps/e2e-watch-test-resource-version,UID:ad22a3cc-fc93-467a-8338-43b079b1dd37,ResourceVersion:221428,Generation:0,CreationTimestamp:2019-09-05 07:16:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:16:56.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5322" for this suite.
Sep  5 07:17:02.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:17:02.897: INFO: namespace watch-5322 deletion completed in 6.153176242s

• [SLOW TEST:6.446 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:17:02.898: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9936
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9936
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Sep  5 07:17:03.142: INFO: Found 0 stateful pods, waiting for 3
Sep  5 07:17:13.147: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 07:17:13.147: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 07:17:13.147: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 07:17:23.147: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 07:17:23.147: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 07:17:23.147: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 07:17:23.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9936 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 07:17:26.071: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 07:17:26.071: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 07:17:26.071: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep  5 07:17:36.106: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep  5 07:17:46.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9936 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:17:46.604: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  5 07:17:46.605: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 07:17:46.605: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 07:18:16.628: INFO: Waiting for StatefulSet statefulset-9936/ss2 to complete update
Sep  5 07:18:16.628: INFO: Waiting for Pod statefulset-9936/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  5 07:18:26.640: INFO: Waiting for StatefulSet statefulset-9936/ss2 to complete update
STEP: Rolling back to a previous revision
Sep  5 07:18:36.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9936 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 07:18:37.187: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 07:18:37.187: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 07:18:37.187: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 07:18:47.221: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep  5 07:18:57.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9936 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:18:57.736: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  5 07:18:57.736: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 07:18:57.736: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 07:19:07.770: INFO: Waiting for StatefulSet statefulset-9936/ss2 to complete update
Sep  5 07:19:07.770: INFO: Waiting for Pod statefulset-9936/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep  5 07:19:07.770: INFO: Waiting for Pod statefulset-9936/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep  5 07:19:17.800: INFO: Waiting for StatefulSet statefulset-9936/ss2 to complete update
Sep  5 07:19:17.800: INFO: Waiting for Pod statefulset-9936/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep  5 07:19:17.800: INFO: Waiting for Pod statefulset-9936/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep  5 07:19:27.778: INFO: Waiting for StatefulSet statefulset-9936/ss2 to complete update
Sep  5 07:19:27.778: INFO: Waiting for Pod statefulset-9936/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep  5 07:19:37.781: INFO: Waiting for StatefulSet statefulset-9936/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep  5 07:19:47.818: INFO: Deleting all statefulset in ns statefulset-9936
Sep  5 07:19:47.821: INFO: Scaling statefulset ss2 to 0
Sep  5 07:20:07.841: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 07:20:07.844: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:20:07.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9936" for this suite.
Sep  5 07:20:15.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:20:15.996: INFO: namespace statefulset-9936 deletion completed in 8.113018294s

• [SLOW TEST:193.098 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:20:15.996: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-353
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep  5 07:20:16.256: INFO: Waiting up to 5m0s for pod "downward-api-6e639b71-434a-4c32-b7e2-853f0a87ced4" in namespace "downward-api-353" to be "success or failure"
Sep  5 07:20:16.287: INFO: Pod "downward-api-6e639b71-434a-4c32-b7e2-853f0a87ced4": Phase="Pending", Reason="", readiness=false. Elapsed: 30.788089ms
Sep  5 07:20:18.304: INFO: Pod "downward-api-6e639b71-434a-4c32-b7e2-853f0a87ced4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048073653s
Sep  5 07:20:20.308: INFO: Pod "downward-api-6e639b71-434a-4c32-b7e2-853f0a87ced4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052265053s
STEP: Saw pod success
Sep  5 07:20:20.308: INFO: Pod "downward-api-6e639b71-434a-4c32-b7e2-853f0a87ced4" satisfied condition "success or failure"
Sep  5 07:20:20.311: INFO: Trying to get logs from node slave1 pod downward-api-6e639b71-434a-4c32-b7e2-853f0a87ced4 container dapi-container: <nil>
STEP: delete the pod
Sep  5 07:20:20.361: INFO: Waiting for pod downward-api-6e639b71-434a-4c32-b7e2-853f0a87ced4 to disappear
Sep  5 07:20:20.366: INFO: Pod downward-api-6e639b71-434a-4c32-b7e2-853f0a87ced4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:20:20.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-353" for this suite.
Sep  5 07:20:26.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:20:26.546: INFO: namespace downward-api-353 deletion completed in 6.151406688s

• [SLOW TEST:10.550 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:20:26.546: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7543
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 07:20:26.791: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  5 07:20:32.824: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  5 07:20:34.828: INFO: Creating deployment "test-rollover-deployment"
Sep  5 07:20:34.915: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  5 07:20:36.976: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  5 07:20:37.005: INFO: Ensure that both replica sets have 1 created replica
Sep  5 07:20:37.018: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  5 07:20:37.027: INFO: Updating deployment test-rollover-deployment
Sep  5 07:20:37.027: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  5 07:20:39.058: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  5 07:20:39.116: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  5 07:20:39.123: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 07:20:39.123: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264829, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 07:20:41.131: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 07:20:41.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264829, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 07:20:43.131: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 07:20:43.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264833, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 07:20:45.130: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 07:20:45.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264833, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 07:20:47.131: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 07:20:47.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264833, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 07:20:49.131: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 07:20:49.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264833, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 07:20:51.131: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 07:20:51.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264833, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 07:20:53.131: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 07:20:53.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264833, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 07:20:55.131: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 07:20:55.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264833, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 07:20:57.133: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 07:20:57.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264833, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 07:20:59.131: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 07:20:59.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264833, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 07:21:01.215: INFO: all replica sets need to contain the pod-template-hash label
Sep  5 07:21:01.215: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264833, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703264826, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 07:21:03.131: INFO: 
Sep  5 07:21:03.131: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep  5 07:21:03.239: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-7543,SelfLink:/apis/apps/v1/namespaces/deployment-7543/deployments/test-rollover-deployment,UID:edd8f136-a59f-4fce-a65b-ad6c93a2ea0a,ResourceVersion:222354,Generation:2,CreationTimestamp:2019-09-05 07:20:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-05 07:20:26 +0000 UTC 2019-09-05 07:20:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-05 07:20:53 +0000 UTC 2019-09-05 07:20:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep  5 07:21:03.244: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-7543,SelfLink:/apis/apps/v1/namespaces/deployment-7543/replicasets/test-rollover-deployment-854595fc44,UID:d1b20d94-a714-4324-b24d-e79db15bbc9a,ResourceVersion:222342,Generation:2,CreationTimestamp:2019-09-05 07:20:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment edd8f136-a59f-4fce-a65b-ad6c93a2ea0a 0x40002eda77 0x40002eda78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  5 07:21:03.244: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  5 07:21:03.244: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-7543,SelfLink:/apis/apps/v1/namespaces/deployment-7543/replicasets/test-rollover-controller,UID:bbe2548d-3ab7-4fe9-a835-ad34912601be,ResourceVersion:222353,Generation:2,CreationTimestamp:2019-09-05 07:20:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment edd8f136-a59f-4fce-a65b-ad6c93a2ea0a 0x40002ed997 0x40002ed998}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  5 07:21:03.245: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-7543,SelfLink:/apis/apps/v1/namespaces/deployment-7543/replicasets/test-rollover-deployment-9b8b997cf,UID:998852d5-3e25-46e6-b551-14f6f6ad406c,ResourceVersion:222289,Generation:2,CreationTimestamp:2019-09-05 07:20:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment edd8f136-a59f-4fce-a65b-ad6c93a2ea0a 0x40002edb90 0x40002edb91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  5 07:21:03.249: INFO: Pod "test-rollover-deployment-854595fc44-jtzrm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-jtzrm,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-7543,SelfLink:/api/v1/namespaces/deployment-7543/pods/test-rollover-deployment-854595fc44-jtzrm,UID:f721436a-9639-458d-a7b0-eb45d2f67332,ResourceVersion:222307,Generation:0,CreationTimestamp:2019-09-05 07:20:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 d1b20d94-a714-4324-b24d-e79db15bbc9a 0x40021097d7 0x40021097d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kdz2q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kdz2q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-kdz2q true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002109850} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002109870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:20:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:20:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:20:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:20:29 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.30,PodIP:172.31.51.150,StartTime:2019-09-05 07:20:37 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-05 07:20:40 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://444fc2934a09db3f1d959a2df7ee0855c43b7e70f1f44ece15e1e71aec2ec60d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:21:03.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7543" for this suite.
Sep  5 07:21:09.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:21:09.414: INFO: namespace deployment-7543 deletion completed in 6.16133207s

• [SLOW TEST:42.868 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:21:09.415: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8326
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 07:21:09.649: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ece0dfdb-6a2c-4f94-84d8-4b70f40ecd3a" in namespace "projected-8326" to be "success or failure"
Sep  5 07:21:09.660: INFO: Pod "downwardapi-volume-ece0dfdb-6a2c-4f94-84d8-4b70f40ecd3a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.755754ms
Sep  5 07:21:11.663: INFO: Pod "downwardapi-volume-ece0dfdb-6a2c-4f94-84d8-4b70f40ecd3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014091171s
Sep  5 07:21:13.667: INFO: Pod "downwardapi-volume-ece0dfdb-6a2c-4f94-84d8-4b70f40ecd3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017722386s
STEP: Saw pod success
Sep  5 07:21:13.667: INFO: Pod "downwardapi-volume-ece0dfdb-6a2c-4f94-84d8-4b70f40ecd3a" satisfied condition "success or failure"
Sep  5 07:21:13.670: INFO: Trying to get logs from node slave1 pod downwardapi-volume-ece0dfdb-6a2c-4f94-84d8-4b70f40ecd3a container client-container: <nil>
STEP: delete the pod
Sep  5 07:21:13.756: INFO: Waiting for pod downwardapi-volume-ece0dfdb-6a2c-4f94-84d8-4b70f40ecd3a to disappear
Sep  5 07:21:13.767: INFO: Pod downwardapi-volume-ece0dfdb-6a2c-4f94-84d8-4b70f40ecd3a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:21:13.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8326" for this suite.
Sep  5 07:21:19.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:21:19.893: INFO: namespace projected-8326 deletion completed in 6.121255356s

• [SLOW TEST:10.479 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:21:19.894: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-953
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-1bab9f83-706a-4cfc-ac0a-2b0a5c0380a3
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:21:20.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-953" for this suite.
Sep  5 07:21:26.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:21:26.331: INFO: namespace secrets-953 deletion completed in 6.210984652s

• [SLOW TEST:6.438 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:21:26.332: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Sep  5 07:21:26.546: INFO: namespace kubectl-2109
Sep  5 07:21:26.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 create -f - --namespace=kubectl-2109'
Sep  5 07:21:27.191: INFO: stderr: ""
Sep  5 07:21:27.191: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  5 07:21:28.215: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 07:21:28.215: INFO: Found 0 / 1
Sep  5 07:21:29.199: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 07:21:29.199: INFO: Found 0 / 1
Sep  5 07:21:30.221: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 07:21:30.221: INFO: Found 0 / 1
Sep  5 07:21:31.195: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 07:21:31.195: INFO: Found 1 / 1
Sep  5 07:21:31.195: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  5 07:21:31.198: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 07:21:31.198: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  5 07:21:31.199: INFO: wait on redis-master startup in kubectl-2109 
Sep  5 07:21:31.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 logs redis-master-9glgb redis-master --namespace=kubectl-2109'
Sep  5 07:21:31.394: INFO: stderr: ""
Sep  5 07:21:31.394: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Sep 07:21:29.930 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Sep 07:21:29.930 # Server started, Redis version 3.2.12\n1:M 05 Sep 07:21:29.930 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Sep 07:21:29.930 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Sep  5 07:21:31.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-2109'
Sep  5 07:21:31.639: INFO: stderr: ""
Sep  5 07:21:31.639: INFO: stdout: "service/rm2 exposed\n"
Sep  5 07:21:31.649: INFO: Service rm2 in namespace kubectl-2109 found.
STEP: exposing service
Sep  5 07:21:33.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-2109'
Sep  5 07:21:33.976: INFO: stderr: ""
Sep  5 07:21:33.977: INFO: stdout: "service/rm3 exposed\n"
Sep  5 07:21:33.982: INFO: Service rm3 in namespace kubectl-2109 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:21:35.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2109" for this suite.
Sep  5 07:21:58.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:21:58.095: INFO: namespace kubectl-2109 deletion completed in 22.102858774s

• [SLOW TEST:31.764 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:21:58.096: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-292
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 07:21:58.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 version'
Sep  5 07:21:58.515: INFO: stderr: ""
Sep  5 07:21:58.515: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:13:54Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/arm64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:05:50Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/arm64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:21:58.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-292" for this suite.
Sep  5 07:22:04.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:22:04.664: INFO: namespace kubectl-292 deletion completed in 6.142333609s

• [SLOW TEST:6.568 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:22:04.664: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3829
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 07:22:04.919: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep  5 07:22:04.927: INFO: Number of nodes with available pods: 0
Sep  5 07:22:04.927: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep  5 07:22:05.029: INFO: Number of nodes with available pods: 0
Sep  5 07:22:05.029: INFO: Node master1 is running more than one daemon pod
Sep  5 07:22:06.034: INFO: Number of nodes with available pods: 0
Sep  5 07:22:06.034: INFO: Node master1 is running more than one daemon pod
Sep  5 07:22:07.034: INFO: Number of nodes with available pods: 0
Sep  5 07:22:07.034: INFO: Node master1 is running more than one daemon pod
Sep  5 07:22:08.033: INFO: Number of nodes with available pods: 1
Sep  5 07:22:08.033: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep  5 07:22:08.075: INFO: Number of nodes with available pods: 1
Sep  5 07:22:08.075: INFO: Number of running nodes: 0, number of available pods: 1
Sep  5 07:22:09.079: INFO: Number of nodes with available pods: 0
Sep  5 07:22:09.079: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep  5 07:22:09.093: INFO: Number of nodes with available pods: 0
Sep  5 07:22:09.093: INFO: Node master1 is running more than one daemon pod
Sep  5 07:22:10.118: INFO: Number of nodes with available pods: 0
Sep  5 07:22:10.118: INFO: Node master1 is running more than one daemon pod
Sep  5 07:22:11.097: INFO: Number of nodes with available pods: 0
Sep  5 07:22:11.097: INFO: Node master1 is running more than one daemon pod
Sep  5 07:22:12.097: INFO: Number of nodes with available pods: 0
Sep  5 07:22:12.097: INFO: Node master1 is running more than one daemon pod
Sep  5 07:22:13.097: INFO: Number of nodes with available pods: 0
Sep  5 07:22:13.097: INFO: Node master1 is running more than one daemon pod
Sep  5 07:22:14.228: INFO: Number of nodes with available pods: 0
Sep  5 07:22:14.228: INFO: Node master1 is running more than one daemon pod
Sep  5 07:22:15.097: INFO: Number of nodes with available pods: 0
Sep  5 07:22:15.097: INFO: Node master1 is running more than one daemon pod
Sep  5 07:22:16.097: INFO: Number of nodes with available pods: 0
Sep  5 07:22:16.097: INFO: Node master1 is running more than one daemon pod
Sep  5 07:22:17.097: INFO: Number of nodes with available pods: 0
Sep  5 07:22:17.097: INFO: Node master1 is running more than one daemon pod
Sep  5 07:22:18.097: INFO: Number of nodes with available pods: 1
Sep  5 07:22:18.097: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3829, will wait for the garbage collector to delete the pods
Sep  5 07:22:18.161: INFO: Deleting DaemonSet.extensions daemon-set took: 6.189554ms
Sep  5 07:22:18.461: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.146165ms
Sep  5 07:22:22.865: INFO: Number of nodes with available pods: 0
Sep  5 07:22:22.865: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 07:22:22.886: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3829/daemonsets","resourceVersion":"222695"},"items":null}

Sep  5 07:22:22.889: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3829/pods","resourceVersion":"222695"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:22:22.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3829" for this suite.
Sep  5 07:22:28.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:22:29.065: INFO: namespace daemonsets-3829 deletion completed in 6.105432997s

• [SLOW TEST:24.401 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:22:29.065: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6594
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-99b35e8c-6b5d-456f-8462-41cbd31e1645
STEP: Creating configMap with name cm-test-opt-upd-a259581a-3bbb-4d17-aff8-38048131536d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-99b35e8c-6b5d-456f-8462-41cbd31e1645
STEP: Updating configmap cm-test-opt-upd-a259581a-3bbb-4d17-aff8-38048131536d
STEP: Creating configMap with name cm-test-opt-create-31f0f5a4-8a23-46f1-95fb-c993f82cffb4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:22:37.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6594" for this suite.
Sep  5 07:22:59.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:22:59.702: INFO: namespace projected-6594 deletion completed in 22.14742928s

• [SLOW TEST:30.637 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:22:59.702: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6367
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep  5 07:22:59.910: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6367,SelfLink:/api/v1/namespaces/watch-6367/configmaps/e2e-watch-test-configmap-a,UID:d078622a-7393-4023-9d84-628f4452455c,ResourceVersion:222832,Generation:0,CreationTimestamp:2019-09-05 07:22:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  5 07:22:59.910: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6367,SelfLink:/api/v1/namespaces/watch-6367/configmaps/e2e-watch-test-configmap-a,UID:d078622a-7393-4023-9d84-628f4452455c,ResourceVersion:222832,Generation:0,CreationTimestamp:2019-09-05 07:22:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep  5 07:23:09.918: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6367,SelfLink:/api/v1/namespaces/watch-6367/configmaps/e2e-watch-test-configmap-a,UID:d078622a-7393-4023-9d84-628f4452455c,ResourceVersion:222846,Generation:0,CreationTimestamp:2019-09-05 07:22:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep  5 07:23:09.918: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6367,SelfLink:/api/v1/namespaces/watch-6367/configmaps/e2e-watch-test-configmap-a,UID:d078622a-7393-4023-9d84-628f4452455c,ResourceVersion:222846,Generation:0,CreationTimestamp:2019-09-05 07:22:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep  5 07:23:19.925: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6367,SelfLink:/api/v1/namespaces/watch-6367/configmaps/e2e-watch-test-configmap-a,UID:d078622a-7393-4023-9d84-628f4452455c,ResourceVersion:222861,Generation:0,CreationTimestamp:2019-09-05 07:22:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  5 07:23:19.925: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6367,SelfLink:/api/v1/namespaces/watch-6367/configmaps/e2e-watch-test-configmap-a,UID:d078622a-7393-4023-9d84-628f4452455c,ResourceVersion:222861,Generation:0,CreationTimestamp:2019-09-05 07:22:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep  5 07:23:30.017: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6367,SelfLink:/api/v1/namespaces/watch-6367/configmaps/e2e-watch-test-configmap-a,UID:d078622a-7393-4023-9d84-628f4452455c,ResourceVersion:222877,Generation:0,CreationTimestamp:2019-09-05 07:22:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  5 07:23:30.017: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6367,SelfLink:/api/v1/namespaces/watch-6367/configmaps/e2e-watch-test-configmap-a,UID:d078622a-7393-4023-9d84-628f4452455c,ResourceVersion:222877,Generation:0,CreationTimestamp:2019-09-05 07:22:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep  5 07:23:40.024: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6367,SelfLink:/api/v1/namespaces/watch-6367/configmaps/e2e-watch-test-configmap-b,UID:08b0f8a2-ecd6-409c-9f82-c3e37d50117d,ResourceVersion:222891,Generation:0,CreationTimestamp:2019-09-05 07:23:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  5 07:23:40.025: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6367,SelfLink:/api/v1/namespaces/watch-6367/configmaps/e2e-watch-test-configmap-b,UID:08b0f8a2-ecd6-409c-9f82-c3e37d50117d,ResourceVersion:222891,Generation:0,CreationTimestamp:2019-09-05 07:23:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep  5 07:23:50.032: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6367,SelfLink:/api/v1/namespaces/watch-6367/configmaps/e2e-watch-test-configmap-b,UID:08b0f8a2-ecd6-409c-9f82-c3e37d50117d,ResourceVersion:222906,Generation:0,CreationTimestamp:2019-09-05 07:23:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  5 07:23:50.032: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6367,SelfLink:/api/v1/namespaces/watch-6367/configmaps/e2e-watch-test-configmap-b,UID:08b0f8a2-ecd6-409c-9f82-c3e37d50117d,ResourceVersion:222906,Generation:0,CreationTimestamp:2019-09-05 07:23:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:24:00.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6367" for this suite.
Sep  5 07:24:06.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:24:06.198: INFO: namespace watch-6367 deletion completed in 6.161246983s

• [SLOW TEST:66.496 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:24:06.199: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1237
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 07:24:06.448: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  5 07:24:11.452: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  5 07:24:11.452: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep  5 07:24:11.536: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-1237,SelfLink:/apis/apps/v1/namespaces/deployment-1237/deployments/test-cleanup-deployment,UID:ff47d59d-08a5-4095-97ec-163c9dcaee1b,ResourceVersion:222966,Generation:1,CreationTimestamp:2019-09-05 07:24:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Sep  5 07:24:11.546: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-1237,SelfLink:/apis/apps/v1/namespaces/deployment-1237/replicasets/test-cleanup-deployment-55bbcbc84c,UID:f1a59709-61bb-4675-90d4-3b4ef88d4b3f,ResourceVersion:222968,Generation:1,CreationTimestamp:2019-09-05 07:24:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment ff47d59d-08a5-4095-97ec-163c9dcaee1b 0x4000ef76a7 0x4000ef76a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  5 07:24:11.546: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep  5 07:24:11.546: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-1237,SelfLink:/apis/apps/v1/namespaces/deployment-1237/replicasets/test-cleanup-controller,UID:e12655ea-1ba7-48df-a8dd-51853070f1b7,ResourceVersion:222967,Generation:1,CreationTimestamp:2019-09-05 07:23:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment ff47d59d-08a5-4095-97ec-163c9dcaee1b 0x4000ef75d7 0x4000ef75d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  5 07:24:11.587: INFO: Pod "test-cleanup-controller-v6dpc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-v6dpc,GenerateName:test-cleanup-controller-,Namespace:deployment-1237,SelfLink:/api/v1/namespaces/deployment-1237/pods/test-cleanup-controller-v6dpc,UID:8412a71f-59eb-4fa5-975b-a44f0b621c90,ResourceVersion:222963,Generation:0,CreationTimestamp:2019-09-05 07:23:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller e12655ea-1ba7-48df-a8dd-51853070f1b7 0x4000ef7fa7 0x4000ef7fa8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-6zhrn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6zhrn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6zhrn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400274a020} {node.kubernetes.io/unreachable Exists  NoExecute 0x400274a040}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:24:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:24:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:24:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:23:58 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.30,PodIP:172.31.51.152,StartTime:2019-09-05 07:24:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 07:24:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://5a1c25475a6667cb638c00a0cf320386c2d53215de7645101d832c1dfcc7f6c8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:24:11.587: INFO: Pod "test-cleanup-deployment-55bbcbc84c-wnfpw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-wnfpw,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-1237,SelfLink:/api/v1/namespaces/deployment-1237/pods/test-cleanup-deployment-55bbcbc84c-wnfpw,UID:6ff8fbbd-8654-42d4-a18e-d032dddc56b1,ResourceVersion:222972,Generation:0,CreationTimestamp:2019-09-05 07:24:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c f1a59709-61bb-4675-90d4-3b4ef88d4b3f 0x400274a137 0x400274a138}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-6zhrn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6zhrn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-6zhrn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400274a1b0} {node.kubernetes.io/unreachable Exists  NoExecute 0x400274a1d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:24:03 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:24:11.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1237" for this suite.
Sep  5 07:24:17.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:24:17.843: INFO: namespace deployment-1237 deletion completed in 6.227156839s

• [SLOW TEST:11.644 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:24:17.844: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Sep  5 07:24:18.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 create -f - --namespace=kubectl-7826'
Sep  5 07:24:18.549: INFO: stderr: ""
Sep  5 07:24:18.549: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 07:24:18.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7826'
Sep  5 07:24:18.744: INFO: stderr: ""
Sep  5 07:24:18.744: INFO: stdout: "update-demo-nautilus-6lrp5 update-demo-nautilus-zmv2g "
Sep  5 07:24:18.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-6lrp5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7826'
Sep  5 07:24:18.921: INFO: stderr: ""
Sep  5 07:24:18.922: INFO: stdout: ""
Sep  5 07:24:18.922: INFO: update-demo-nautilus-6lrp5 is created but not running
Sep  5 07:24:23.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7826'
Sep  5 07:24:24.099: INFO: stderr: ""
Sep  5 07:24:24.099: INFO: stdout: "update-demo-nautilus-6lrp5 update-demo-nautilus-zmv2g "
Sep  5 07:24:24.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-6lrp5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7826'
Sep  5 07:24:24.279: INFO: stderr: ""
Sep  5 07:24:24.280: INFO: stdout: "true"
Sep  5 07:24:24.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-6lrp5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7826'
Sep  5 07:24:24.445: INFO: stderr: ""
Sep  5 07:24:24.445: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 07:24:24.445: INFO: validating pod update-demo-nautilus-6lrp5
Sep  5 07:24:24.486: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 07:24:24.486: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 07:24:24.486: INFO: update-demo-nautilus-6lrp5 is verified up and running
Sep  5 07:24:24.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-zmv2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7826'
Sep  5 07:24:24.661: INFO: stderr: ""
Sep  5 07:24:24.661: INFO: stdout: "true"
Sep  5 07:24:24.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-zmv2g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7826'
Sep  5 07:24:24.824: INFO: stderr: ""
Sep  5 07:24:24.824: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 07:24:24.824: INFO: validating pod update-demo-nautilus-zmv2g
Sep  5 07:24:24.851: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 07:24:24.851: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 07:24:24.851: INFO: update-demo-nautilus-zmv2g is verified up and running
STEP: rolling-update to new replication controller
Sep  5 07:24:24.854: INFO: scanned /root for discovery docs: <nil>
Sep  5 07:24:24.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7826'
Sep  5 07:24:48.284: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep  5 07:24:48.284: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 07:24:48.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7826'
Sep  5 07:24:48.493: INFO: stderr: ""
Sep  5 07:24:48.493: INFO: stdout: "update-demo-kitten-b96xs update-demo-kitten-h96qw update-demo-nautilus-zmv2g "
STEP: Replicas for name=update-demo: expected=2 actual=3
Sep  5 07:24:53.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7826'
Sep  5 07:24:53.666: INFO: stderr: ""
Sep  5 07:24:53.666: INFO: stdout: "update-demo-kitten-b96xs update-demo-kitten-h96qw "
Sep  5 07:24:53.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-kitten-b96xs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7826'
Sep  5 07:24:53.830: INFO: stderr: ""
Sep  5 07:24:53.830: INFO: stdout: "true"
Sep  5 07:24:53.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-kitten-b96xs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7826'
Sep  5 07:24:54.009: INFO: stderr: ""
Sep  5 07:24:54.009: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep  5 07:24:54.009: INFO: validating pod update-demo-kitten-b96xs
Sep  5 07:24:54.049: INFO: got data: {
  "image": "kitten.jpg"
}

Sep  5 07:24:54.049: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep  5 07:24:54.049: INFO: update-demo-kitten-b96xs is verified up and running
Sep  5 07:24:54.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-kitten-h96qw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7826'
Sep  5 07:24:54.219: INFO: stderr: ""
Sep  5 07:24:54.219: INFO: stdout: "true"
Sep  5 07:24:54.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-kitten-h96qw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7826'
Sep  5 07:24:54.386: INFO: stderr: ""
Sep  5 07:24:54.386: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep  5 07:24:54.386: INFO: validating pod update-demo-kitten-h96qw
Sep  5 07:24:54.432: INFO: got data: {
  "image": "kitten.jpg"
}

Sep  5 07:24:54.432: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep  5 07:24:54.432: INFO: update-demo-kitten-h96qw is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:24:54.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7826" for this suite.
Sep  5 07:25:18.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:25:18.561: INFO: namespace kubectl-7826 deletion completed in 24.124433898s

• [SLOW TEST:60.718 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:25:18.562: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5793
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-76bafcc8-64f3-4fd3-a768-ea539072ae99 in namespace container-probe-5793
Sep  5 07:25:22.840: INFO: Started pod liveness-76bafcc8-64f3-4fd3-a768-ea539072ae99 in namespace container-probe-5793
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 07:25:22.843: INFO: Initial restart count of pod liveness-76bafcc8-64f3-4fd3-a768-ea539072ae99 is 0
Sep  5 07:25:34.869: INFO: Restart count of pod container-probe-5793/liveness-76bafcc8-64f3-4fd3-a768-ea539072ae99 is now 1 (12.026492465s elapsed)
Sep  5 07:25:54.907: INFO: Restart count of pod container-probe-5793/liveness-76bafcc8-64f3-4fd3-a768-ea539072ae99 is now 2 (32.064307402s elapsed)
Sep  5 07:26:14.945: INFO: Restart count of pod container-probe-5793/liveness-76bafcc8-64f3-4fd3-a768-ea539072ae99 is now 3 (52.102048896s elapsed)
Sep  5 07:26:34.983: INFO: Restart count of pod container-probe-5793/liveness-76bafcc8-64f3-4fd3-a768-ea539072ae99 is now 4 (1m12.139812946s elapsed)
Sep  5 07:27:49.157: INFO: Restart count of pod container-probe-5793/liveness-76bafcc8-64f3-4fd3-a768-ea539072ae99 is now 5 (2m26.314401747s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:27:49.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5793" for this suite.
Sep  5 07:27:55.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:27:55.317: INFO: namespace container-probe-5793 deletion completed in 6.136236908s

• [SLOW TEST:156.755 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:27:55.317: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7735
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-509e57c4-cf42-4f5a-94a8-21f1b6e71bf1
STEP: Creating a pod to test consume configMaps
Sep  5 07:27:55.612: INFO: Waiting up to 5m0s for pod "pod-configmaps-d1febd55-97bd-41b8-85a2-f86674c257a7" in namespace "configmap-7735" to be "success or failure"
Sep  5 07:27:55.620: INFO: Pod "pod-configmaps-d1febd55-97bd-41b8-85a2-f86674c257a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.677467ms
Sep  5 07:27:57.625: INFO: Pod "pod-configmaps-d1febd55-97bd-41b8-85a2-f86674c257a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01228539s
Sep  5 07:27:59.628: INFO: Pod "pod-configmaps-d1febd55-97bd-41b8-85a2-f86674c257a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015687258s
STEP: Saw pod success
Sep  5 07:27:59.628: INFO: Pod "pod-configmaps-d1febd55-97bd-41b8-85a2-f86674c257a7" satisfied condition "success or failure"
Sep  5 07:27:59.631: INFO: Trying to get logs from node slave1 pod pod-configmaps-d1febd55-97bd-41b8-85a2-f86674c257a7 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 07:27:59.656: INFO: Waiting for pod pod-configmaps-d1febd55-97bd-41b8-85a2-f86674c257a7 to disappear
Sep  5 07:27:59.661: INFO: Pod pod-configmaps-d1febd55-97bd-41b8-85a2-f86674c257a7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:27:59.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7735" for this suite.
Sep  5 07:28:05.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:28:05.807: INFO: namespace configmap-7735 deletion completed in 6.141143523s

• [SLOW TEST:10.490 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:28:05.807: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1540
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-d6n9
STEP: Creating a pod to test atomic-volume-subpath
Sep  5 07:28:06.054: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-d6n9" in namespace "subpath-1540" to be "success or failure"
Sep  5 07:28:06.082: INFO: Pod "pod-subpath-test-downwardapi-d6n9": Phase="Pending", Reason="", readiness=false. Elapsed: 27.852081ms
Sep  5 07:28:08.086: INFO: Pod "pod-subpath-test-downwardapi-d6n9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031821145s
Sep  5 07:28:10.090: INFO: Pod "pod-subpath-test-downwardapi-d6n9": Phase="Running", Reason="", readiness=true. Elapsed: 4.03574783s
Sep  5 07:28:12.093: INFO: Pod "pod-subpath-test-downwardapi-d6n9": Phase="Running", Reason="", readiness=true. Elapsed: 6.039040097s
Sep  5 07:28:14.097: INFO: Pod "pod-subpath-test-downwardapi-d6n9": Phase="Running", Reason="", readiness=true. Elapsed: 8.042893021s
Sep  5 07:28:16.101: INFO: Pod "pod-subpath-test-downwardapi-d6n9": Phase="Running", Reason="", readiness=true. Elapsed: 10.046572166s
Sep  5 07:28:18.105: INFO: Pod "pod-subpath-test-downwardapi-d6n9": Phase="Running", Reason="", readiness=true. Elapsed: 12.050597669s
Sep  5 07:28:20.109: INFO: Pod "pod-subpath-test-downwardapi-d6n9": Phase="Running", Reason="", readiness=true. Elapsed: 14.054341913s
Sep  5 07:28:22.112: INFO: Pod "pod-subpath-test-downwardapi-d6n9": Phase="Running", Reason="", readiness=true. Elapsed: 16.057890738s
Sep  5 07:28:24.116: INFO: Pod "pod-subpath-test-downwardapi-d6n9": Phase="Running", Reason="", readiness=true. Elapsed: 18.06211532s
Sep  5 07:28:26.120: INFO: Pod "pod-subpath-test-downwardapi-d6n9": Phase="Running", Reason="", readiness=true. Elapsed: 20.065628804s
Sep  5 07:28:28.124: INFO: Pod "pod-subpath-test-downwardapi-d6n9": Phase="Running", Reason="", readiness=true. Elapsed: 22.069368027s
Sep  5 07:28:30.127: INFO: Pod "pod-subpath-test-downwardapi-d6n9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.07312283s
STEP: Saw pod success
Sep  5 07:28:30.127: INFO: Pod "pod-subpath-test-downwardapi-d6n9" satisfied condition "success or failure"
Sep  5 07:28:30.130: INFO: Trying to get logs from node slave1 pod pod-subpath-test-downwardapi-d6n9 container test-container-subpath-downwardapi-d6n9: <nil>
STEP: delete the pod
Sep  5 07:28:30.223: INFO: Waiting for pod pod-subpath-test-downwardapi-d6n9 to disappear
Sep  5 07:28:30.232: INFO: Pod pod-subpath-test-downwardapi-d6n9 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-d6n9
Sep  5 07:28:30.232: INFO: Deleting pod "pod-subpath-test-downwardapi-d6n9" in namespace "subpath-1540"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:28:30.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1540" for this suite.
Sep  5 07:28:36.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:28:36.398: INFO: namespace subpath-1540 deletion completed in 6.159293375s

• [SLOW TEST:30.591 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:28:36.398: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-hskf
STEP: Creating a pod to test atomic-volume-subpath
Sep  5 07:28:36.717: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hskf" in namespace "subpath-7735" to be "success or failure"
Sep  5 07:28:36.722: INFO: Pod "pod-subpath-test-projected-hskf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.617836ms
Sep  5 07:28:38.726: INFO: Pod "pod-subpath-test-projected-hskf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009422458s
Sep  5 07:28:40.730: INFO: Pod "pod-subpath-test-projected-hskf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01318694s
Sep  5 07:28:42.734: INFO: Pod "pod-subpath-test-projected-hskf": Phase="Running", Reason="", readiness=true. Elapsed: 6.017470299s
Sep  5 07:28:44.738: INFO: Pod "pod-subpath-test-projected-hskf": Phase="Running", Reason="", readiness=true. Elapsed: 8.021031101s
Sep  5 07:28:46.742: INFO: Pod "pod-subpath-test-projected-hskf": Phase="Running", Reason="", readiness=true. Elapsed: 10.024827282s
Sep  5 07:28:48.746: INFO: Pod "pod-subpath-test-projected-hskf": Phase="Running", Reason="", readiness=true. Elapsed: 12.029034741s
Sep  5 07:28:50.750: INFO: Pod "pod-subpath-test-projected-hskf": Phase="Running", Reason="", readiness=true. Elapsed: 14.032671362s
Sep  5 07:28:52.754: INFO: Pod "pod-subpath-test-projected-hskf": Phase="Running", Reason="", readiness=true. Elapsed: 16.0370997s
Sep  5 07:28:54.757: INFO: Pod "pod-subpath-test-projected-hskf": Phase="Running", Reason="", readiness=true. Elapsed: 18.040623601s
Sep  5 07:28:56.762: INFO: Pod "pod-subpath-test-projected-hskf": Phase="Running", Reason="", readiness=true. Elapsed: 20.044787959s
Sep  5 07:28:58.766: INFO: Pod "pod-subpath-test-projected-hskf": Phase="Running", Reason="", readiness=true. Elapsed: 22.048844138s
Sep  5 07:29:00.775: INFO: Pod "pod-subpath-test-projected-hskf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.058056734s
STEP: Saw pod success
Sep  5 07:29:00.775: INFO: Pod "pod-subpath-test-projected-hskf" satisfied condition "success or failure"
Sep  5 07:29:00.779: INFO: Trying to get logs from node slave1 pod pod-subpath-test-projected-hskf container test-container-subpath-projected-hskf: <nil>
STEP: delete the pod
Sep  5 07:29:00.867: INFO: Waiting for pod pod-subpath-test-projected-hskf to disappear
Sep  5 07:29:00.891: INFO: Pod pod-subpath-test-projected-hskf no longer exists
STEP: Deleting pod pod-subpath-test-projected-hskf
Sep  5 07:29:00.891: INFO: Deleting pod "pod-subpath-test-projected-hskf" in namespace "subpath-7735"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:29:00.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7735" for this suite.
Sep  5 07:29:06.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:29:07.000: INFO: namespace subpath-7735 deletion completed in 6.10188363s

• [SLOW TEST:30.601 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:29:07.000: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-876
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Sep  5 07:29:07.215: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Sep  5 07:29:07.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 create -f - --namespace=kubectl-876'
Sep  5 07:29:10.085: INFO: stderr: ""
Sep  5 07:29:10.085: INFO: stdout: "service/redis-slave created\n"
Sep  5 07:29:10.086: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Sep  5 07:29:10.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 create -f - --namespace=kubectl-876'
Sep  5 07:29:10.561: INFO: stderr: ""
Sep  5 07:29:10.561: INFO: stdout: "service/redis-master created\n"
Sep  5 07:29:10.561: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  5 07:29:10.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 create -f - --namespace=kubectl-876'
Sep  5 07:29:11.014: INFO: stderr: ""
Sep  5 07:29:11.014: INFO: stdout: "service/frontend created\n"
Sep  5 07:29:11.015: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Sep  5 07:29:11.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 create -f - --namespace=kubectl-876'
Sep  5 07:29:11.519: INFO: stderr: ""
Sep  5 07:29:11.519: INFO: stdout: "deployment.apps/frontend created\n"
Sep  5 07:29:11.533: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  5 07:29:11.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 create -f - --namespace=kubectl-876'
Sep  5 07:29:11.967: INFO: stderr: ""
Sep  5 07:29:11.968: INFO: stdout: "deployment.apps/redis-master created\n"
Sep  5 07:29:11.968: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Sep  5 07:29:11.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 create -f - --namespace=kubectl-876'
Sep  5 07:29:12.435: INFO: stderr: ""
Sep  5 07:29:12.435: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Sep  5 07:29:12.435: INFO: Waiting for all frontend pods to be Running.
Sep  5 07:29:22.536: INFO: Waiting for frontend to serve content.
Sep  5 07:29:22.601: INFO: Trying to add a new entry to the guestbook.
Sep  5 07:29:22.623: INFO: Verifying that added entry can be retrieved.
Sep  5 07:29:22.736: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep  5 07:29:27.756: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep  5 07:29:32.778: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep  5 07:29:37.799: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep  5 07:29:42.822: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep  5 07:29:47.930: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep  5 07:29:52.952: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep  5 07:29:57.973: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep  5 07:30:02.993: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep  5 07:30:08.014: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep  5 07:30:13.034: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep  5 07:30:18.135: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Sep  5 07:30:23.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete --grace-period=0 --force -f - --namespace=kubectl-876'
Sep  5 07:30:23.605: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 07:30:23.605: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 07:30:23.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete --grace-period=0 --force -f - --namespace=kubectl-876'
Sep  5 07:30:23.855: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 07:30:23.855: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 07:30:23.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete --grace-period=0 --force -f - --namespace=kubectl-876'
Sep  5 07:30:24.081: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 07:30:24.081: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 07:30:24.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete --grace-period=0 --force -f - --namespace=kubectl-876'
Sep  5 07:30:24.289: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 07:30:24.289: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 07:30:24.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete --grace-period=0 --force -f - --namespace=kubectl-876'
Sep  5 07:30:24.463: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 07:30:24.463: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 07:30:24.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete --grace-period=0 --force -f - --namespace=kubectl-876'
Sep  5 07:30:24.728: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 07:30:24.729: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:30:24.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-876" for this suite.
Sep  5 07:31:10.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:31:10.851: INFO: namespace kubectl-876 deletion completed in 46.107573765s

• [SLOW TEST:123.851 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:31:10.851: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6213
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-9ffe6e53-d472-4538-9285-c7a0b9828314
STEP: Creating a pod to test consume secrets
Sep  5 07:31:11.099: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-98848be7-b600-4b20-bdb8-2f82238f5a6b" in namespace "projected-6213" to be "success or failure"
Sep  5 07:31:11.110: INFO: Pod "pod-projected-secrets-98848be7-b600-4b20-bdb8-2f82238f5a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.432991ms
Sep  5 07:31:13.117: INFO: Pod "pod-projected-secrets-98848be7-b600-4b20-bdb8-2f82238f5a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018477002s
Sep  5 07:31:15.121: INFO: Pod "pod-projected-secrets-98848be7-b600-4b20-bdb8-2f82238f5a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022221367s
STEP: Saw pod success
Sep  5 07:31:15.121: INFO: Pod "pod-projected-secrets-98848be7-b600-4b20-bdb8-2f82238f5a6b" satisfied condition "success or failure"
Sep  5 07:31:15.126: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-98848be7-b600-4b20-bdb8-2f82238f5a6b container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 07:31:15.181: INFO: Waiting for pod pod-projected-secrets-98848be7-b600-4b20-bdb8-2f82238f5a6b to disappear
Sep  5 07:31:15.193: INFO: Pod pod-projected-secrets-98848be7-b600-4b20-bdb8-2f82238f5a6b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:31:15.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6213" for this suite.
Sep  5 07:31:21.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:31:21.327: INFO: namespace projected-6213 deletion completed in 6.128938969s

• [SLOW TEST:10.476 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:31:21.327: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3003
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep  5 07:31:21.563: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:31:30.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3003" for this suite.
Sep  5 07:31:36.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:31:36.935: INFO: namespace pods-3003 deletion completed in 6.247665276s

• [SLOW TEST:15.608 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:31:36.935: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1680
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 07:31:37.153: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94ecd008-6a91-4d52-8fcb-0ede7403e89a" in namespace "downward-api-1680" to be "success or failure"
Sep  5 07:31:37.165: INFO: Pod "downwardapi-volume-94ecd008-6a91-4d52-8fcb-0ede7403e89a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.175508ms
Sep  5 07:31:39.174: INFO: Pod "downwardapi-volume-94ecd008-6a91-4d52-8fcb-0ede7403e89a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020462711s
Sep  5 07:31:41.177: INFO: Pod "downwardapi-volume-94ecd008-6a91-4d52-8fcb-0ede7403e89a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024181553s
STEP: Saw pod success
Sep  5 07:31:41.177: INFO: Pod "downwardapi-volume-94ecd008-6a91-4d52-8fcb-0ede7403e89a" satisfied condition "success or failure"
Sep  5 07:31:41.180: INFO: Trying to get logs from node slave1 pod downwardapi-volume-94ecd008-6a91-4d52-8fcb-0ede7403e89a container client-container: <nil>
STEP: delete the pod
Sep  5 07:31:41.209: INFO: Waiting for pod downwardapi-volume-94ecd008-6a91-4d52-8fcb-0ede7403e89a to disappear
Sep  5 07:31:41.214: INFO: Pod downwardapi-volume-94ecd008-6a91-4d52-8fcb-0ede7403e89a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:31:41.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1680" for this suite.
Sep  5 07:31:47.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:31:47.391: INFO: namespace downward-api-1680 deletion completed in 6.122601168s

• [SLOW TEST:10.456 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:31:47.391: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3766
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-fdbb005a-32e5-4783-8819-030c68867a3b
STEP: Creating a pod to test consume configMaps
Sep  5 07:31:47.643: INFO: Waiting up to 5m0s for pod "pod-configmaps-0b290bcc-c7a7-46f5-a0eb-e7be412c90d0" in namespace "configmap-3766" to be "success or failure"
Sep  5 07:31:47.647: INFO: Pod "pod-configmaps-0b290bcc-c7a7-46f5-a0eb-e7be412c90d0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.953963ms
Sep  5 07:31:49.651: INFO: Pod "pod-configmaps-0b290bcc-c7a7-46f5-a0eb-e7be412c90d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007643885s
Sep  5 07:31:51.655: INFO: Pod "pod-configmaps-0b290bcc-c7a7-46f5-a0eb-e7be412c90d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011343766s
Sep  5 07:31:53.658: INFO: Pod "pod-configmaps-0b290bcc-c7a7-46f5-a0eb-e7be412c90d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015176286s
STEP: Saw pod success
Sep  5 07:31:53.658: INFO: Pod "pod-configmaps-0b290bcc-c7a7-46f5-a0eb-e7be412c90d0" satisfied condition "success or failure"
Sep  5 07:31:53.661: INFO: Trying to get logs from node slave1 pod pod-configmaps-0b290bcc-c7a7-46f5-a0eb-e7be412c90d0 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 07:31:53.721: INFO: Waiting for pod pod-configmaps-0b290bcc-c7a7-46f5-a0eb-e7be412c90d0 to disappear
Sep  5 07:31:53.730: INFO: Pod pod-configmaps-0b290bcc-c7a7-46f5-a0eb-e7be412c90d0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:31:53.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3766" for this suite.
Sep  5 07:31:59.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:31:59.876: INFO: namespace configmap-3766 deletion completed in 6.14222608s

• [SLOW TEST:12.485 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:31:59.877: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6843
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 07:32:00.075: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep  5 07:32:02.115: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:32:02.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6843" for this suite.
Sep  5 07:32:08.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:32:08.381: INFO: namespace replication-controller-6843 deletion completed in 6.207270898s

• [SLOW TEST:8.504 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:32:08.381: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3101
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-20bc85f0-e6ff-452c-9cdd-684a68842251
STEP: Creating a pod to test consume secrets
Sep  5 07:32:08.656: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1d2fd965-a467-4ca3-a38d-ad8493cce4a6" in namespace "projected-3101" to be "success or failure"
Sep  5 07:32:08.678: INFO: Pod "pod-projected-secrets-1d2fd965-a467-4ca3-a38d-ad8493cce4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.382365ms
Sep  5 07:32:10.721: INFO: Pod "pod-projected-secrets-1d2fd965-a467-4ca3-a38d-ad8493cce4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064673818s
Sep  5 07:32:12.729: INFO: Pod "pod-projected-secrets-1d2fd965-a467-4ca3-a38d-ad8493cce4a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073113897s
STEP: Saw pod success
Sep  5 07:32:12.729: INFO: Pod "pod-projected-secrets-1d2fd965-a467-4ca3-a38d-ad8493cce4a6" satisfied condition "success or failure"
Sep  5 07:32:12.732: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-1d2fd965-a467-4ca3-a38d-ad8493cce4a6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 07:32:12.756: INFO: Waiting for pod pod-projected-secrets-1d2fd965-a467-4ca3-a38d-ad8493cce4a6 to disappear
Sep  5 07:32:12.761: INFO: Pod pod-projected-secrets-1d2fd965-a467-4ca3-a38d-ad8493cce4a6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:32:12.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3101" for this suite.
Sep  5 07:32:18.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:32:18.869: INFO: namespace projected-3101 deletion completed in 6.103580919s

• [SLOW TEST:10.488 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:32:18.869: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1998
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 07:32:19.081: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:32:20.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1998" for this suite.
Sep  5 07:32:26.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:32:26.507: INFO: namespace custom-resource-definition-1998 deletion completed in 6.186325523s

• [SLOW TEST:7.638 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:32:26.507: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-6509
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep  5 07:32:36.815: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6509 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 07:32:36.815: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 07:32:37.090: INFO: Exec stderr: ""
Sep  5 07:32:37.090: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6509 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 07:32:37.090: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 07:32:37.389: INFO: Exec stderr: ""
Sep  5 07:32:37.389: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6509 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 07:32:37.389: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 07:32:37.669: INFO: Exec stderr: ""
Sep  5 07:32:37.669: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6509 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 07:32:37.669: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 07:32:37.954: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep  5 07:32:37.954: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6509 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 07:32:37.954: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 07:32:38.233: INFO: Exec stderr: ""
Sep  5 07:32:38.233: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6509 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 07:32:38.233: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 07:32:38.519: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep  5 07:32:38.519: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6509 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 07:32:38.520: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 07:32:38.809: INFO: Exec stderr: ""
Sep  5 07:32:38.809: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6509 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 07:32:38.810: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 07:32:39.097: INFO: Exec stderr: ""
Sep  5 07:32:39.097: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6509 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 07:32:39.097: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 07:32:39.393: INFO: Exec stderr: ""
Sep  5 07:32:39.393: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6509 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 07:32:39.393: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 07:32:39.715: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:32:39.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6509" for this suite.
Sep  5 07:33:19.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:33:19.830: INFO: namespace e2e-kubelet-etc-hosts-6509 deletion completed in 40.109957821s

• [SLOW TEST:53.322 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:33:19.830: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7913
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 07:33:20.046: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8f68eb92-69ef-4401-a034-9f732ef72657" in namespace "projected-7913" to be "success or failure"
Sep  5 07:33:20.052: INFO: Pod "downwardapi-volume-8f68eb92-69ef-4401-a034-9f732ef72657": Phase="Pending", Reason="", readiness=false. Elapsed: 5.217837ms
Sep  5 07:33:22.085: INFO: Pod "downwardapi-volume-8f68eb92-69ef-4401-a034-9f732ef72657": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038290823s
Sep  5 07:33:24.088: INFO: Pod "downwardapi-volume-8f68eb92-69ef-4401-a034-9f732ef72657": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041964454s
STEP: Saw pod success
Sep  5 07:33:24.089: INFO: Pod "downwardapi-volume-8f68eb92-69ef-4401-a034-9f732ef72657" satisfied condition "success or failure"
Sep  5 07:33:24.091: INFO: Trying to get logs from node slave1 pod downwardapi-volume-8f68eb92-69ef-4401-a034-9f732ef72657 container client-container: <nil>
STEP: delete the pod
Sep  5 07:33:24.120: INFO: Waiting for pod downwardapi-volume-8f68eb92-69ef-4401-a034-9f732ef72657 to disappear
Sep  5 07:33:24.126: INFO: Pod downwardapi-volume-8f68eb92-69ef-4401-a034-9f732ef72657 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:33:24.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7913" for this suite.
Sep  5 07:33:30.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:33:30.308: INFO: namespace projected-7913 deletion completed in 6.178485795s

• [SLOW TEST:10.479 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:33:30.309: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5682
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep  5 07:33:35.146: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5682 pod-service-account-3e7f2452-71fb-416c-9adf-7b138f54b0ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep  5 07:33:35.604: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5682 pod-service-account-3e7f2452-71fb-416c-9adf-7b138f54b0ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep  5 07:33:36.072: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5682 pod-service-account-3e7f2452-71fb-416c-9adf-7b138f54b0ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:33:36.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5682" for this suite.
Sep  5 07:33:42.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:33:42.673: INFO: namespace svcaccounts-5682 deletion completed in 6.118265349s

• [SLOW TEST:12.364 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:33:42.673: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-5051
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5051, will wait for the garbage collector to delete the pods
Sep  5 07:33:48.954: INFO: Deleting Job.batch foo took: 5.978575ms
Sep  5 07:33:49.354: INFO: Terminating Job.batch foo pods took: 400.196124ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:34:34.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5051" for this suite.
Sep  5 07:34:40.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:34:40.568: INFO: namespace job-5051 deletion completed in 6.305937346s

• [SLOW TEST:57.895 seconds]
[sig-apps] Job
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:34:40.569: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8757
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  5 07:34:40.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8757'
Sep  5 07:34:40.966: INFO: stderr: ""
Sep  5 07:34:40.966: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Sep  5 07:34:41.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete pods e2e-test-nginx-pod --namespace=kubectl-8757'
Sep  5 07:34:45.922: INFO: stderr: ""
Sep  5 07:34:45.922: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:34:45.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8757" for this suite.
Sep  5 07:34:51.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:34:52.034: INFO: namespace kubectl-8757 deletion completed in 6.106708317s

• [SLOW TEST:11.465 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:34:52.035: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  5 07:34:52.277: INFO: Waiting up to 5m0s for pod "pod-89834406-b24a-44d8-8692-ba573a18030e" in namespace "emptydir-8553" to be "success or failure"
Sep  5 07:34:52.280: INFO: Pod "pod-89834406-b24a-44d8-8692-ba573a18030e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.374566ms
Sep  5 07:34:54.284: INFO: Pod "pod-89834406-b24a-44d8-8692-ba573a18030e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006978228s
Sep  5 07:34:56.287: INFO: Pod "pod-89834406-b24a-44d8-8692-ba573a18030e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010398331s
Sep  5 07:34:58.329: INFO: Pod "pod-89834406-b24a-44d8-8692-ba573a18030e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051648632s
STEP: Saw pod success
Sep  5 07:34:58.329: INFO: Pod "pod-89834406-b24a-44d8-8692-ba573a18030e" satisfied condition "success or failure"
Sep  5 07:34:58.332: INFO: Trying to get logs from node slave1 pod pod-89834406-b24a-44d8-8692-ba573a18030e container test-container: <nil>
STEP: delete the pod
Sep  5 07:34:58.356: INFO: Waiting for pod pod-89834406-b24a-44d8-8692-ba573a18030e to disappear
Sep  5 07:34:58.363: INFO: Pod pod-89834406-b24a-44d8-8692-ba573a18030e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:34:58.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8553" for this suite.
Sep  5 07:35:04.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:35:04.474: INFO: namespace emptydir-8553 deletion completed in 6.106362575s

• [SLOW TEST:12.439 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:35:04.474: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9142
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9142.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9142.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 07:35:10.733: INFO: DNS probes using dns-9142/dns-test-7748ca8d-2f92-4bdd-90be-f842a07d49af succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:35:10.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9142" for this suite.
Sep  5 07:35:16.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:35:16.938: INFO: namespace dns-9142 deletion completed in 6.156738875s

• [SLOW TEST:12.464 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:35:16.939: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3585
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-3585/secret-test-6eebc74e-2cc3-4f87-93a8-4e9e7a1fddb2
STEP: Creating a pod to test consume secrets
Sep  5 07:35:17.205: INFO: Waiting up to 5m0s for pod "pod-configmaps-de98a0c1-3ac1-4c8e-bf5c-eeca5509fc21" in namespace "secrets-3585" to be "success or failure"
Sep  5 07:35:17.210: INFO: Pod "pod-configmaps-de98a0c1-3ac1-4c8e-bf5c-eeca5509fc21": Phase="Pending", Reason="", readiness=false. Elapsed: 5.493036ms
Sep  5 07:35:19.214: INFO: Pod "pod-configmaps-de98a0c1-3ac1-4c8e-bf5c-eeca5509fc21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009362095s
Sep  5 07:35:21.218: INFO: Pod "pod-configmaps-de98a0c1-3ac1-4c8e-bf5c-eeca5509fc21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013187734s
STEP: Saw pod success
Sep  5 07:35:21.218: INFO: Pod "pod-configmaps-de98a0c1-3ac1-4c8e-bf5c-eeca5509fc21" satisfied condition "success or failure"
Sep  5 07:35:21.221: INFO: Trying to get logs from node slave1 pod pod-configmaps-de98a0c1-3ac1-4c8e-bf5c-eeca5509fc21 container env-test: <nil>
STEP: delete the pod
Sep  5 07:35:21.245: INFO: Waiting for pod pod-configmaps-de98a0c1-3ac1-4c8e-bf5c-eeca5509fc21 to disappear
Sep  5 07:35:21.252: INFO: Pod pod-configmaps-de98a0c1-3ac1-4c8e-bf5c-eeca5509fc21 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:35:21.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3585" for this suite.
Sep  5 07:35:27.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:35:27.361: INFO: namespace secrets-3585 deletion completed in 6.105309852s

• [SLOW TEST:10.423 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:35:27.362: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-656
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  5 07:35:27.578: INFO: Waiting up to 5m0s for pod "pod-44807d0d-c5a1-4460-84a2-12b98f8f5b2f" in namespace "emptydir-656" to be "success or failure"
Sep  5 07:35:27.584: INFO: Pod "pod-44807d0d-c5a1-4460-84a2-12b98f8f5b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.316692ms
Sep  5 07:35:29.588: INFO: Pod "pod-44807d0d-c5a1-4460-84a2-12b98f8f5b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01024917s
Sep  5 07:35:31.607: INFO: Pod "pod-44807d0d-c5a1-4460-84a2-12b98f8f5b2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029239382s
STEP: Saw pod success
Sep  5 07:35:31.607: INFO: Pod "pod-44807d0d-c5a1-4460-84a2-12b98f8f5b2f" satisfied condition "success or failure"
Sep  5 07:35:31.610: INFO: Trying to get logs from node slave1 pod pod-44807d0d-c5a1-4460-84a2-12b98f8f5b2f container test-container: <nil>
STEP: delete the pod
Sep  5 07:35:31.654: INFO: Waiting for pod pod-44807d0d-c5a1-4460-84a2-12b98f8f5b2f to disappear
Sep  5 07:35:31.659: INFO: Pod pod-44807d0d-c5a1-4460-84a2-12b98f8f5b2f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:35:31.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-656" for this suite.
Sep  5 07:35:37.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:35:37.777: INFO: namespace emptydir-656 deletion completed in 6.113115195s

• [SLOW TEST:10.415 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:35:37.777: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2621
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  5 07:35:37.978: INFO: Waiting up to 5m0s for pod "pod-194b2f49-262c-4dfb-99e5-b176f0e87b86" in namespace "emptydir-2621" to be "success or failure"
Sep  5 07:35:37.984: INFO: Pod "pod-194b2f49-262c-4dfb-99e5-b176f0e87b86": Phase="Pending", Reason="", readiness=false. Elapsed: 6.263673ms
Sep  5 07:35:39.988: INFO: Pod "pod-194b2f49-262c-4dfb-99e5-b176f0e87b86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010180749s
Sep  5 07:35:41.992: INFO: Pod "pod-194b2f49-262c-4dfb-99e5-b176f0e87b86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014160085s
STEP: Saw pod success
Sep  5 07:35:41.992: INFO: Pod "pod-194b2f49-262c-4dfb-99e5-b176f0e87b86" satisfied condition "success or failure"
Sep  5 07:35:41.995: INFO: Trying to get logs from node slave1 pod pod-194b2f49-262c-4dfb-99e5-b176f0e87b86 container test-container: <nil>
STEP: delete the pod
Sep  5 07:35:42.049: INFO: Waiting for pod pod-194b2f49-262c-4dfb-99e5-b176f0e87b86 to disappear
Sep  5 07:35:42.057: INFO: Pod pod-194b2f49-262c-4dfb-99e5-b176f0e87b86 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:35:42.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2621" for this suite.
Sep  5 07:35:48.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:35:48.162: INFO: namespace emptydir-2621 deletion completed in 6.09967721s

• [SLOW TEST:10.385 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:35:48.163: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9152
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 07:35:48.482: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c4ae89c4-1550-4603-ac31-632cdfe09a81", Controller:(*bool)(0x4002ec3b72), BlockOwnerDeletion:(*bool)(0x4002ec3b73)}}
Sep  5 07:35:48.508: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9a9fb45f-d9ab-4fd8-8593-e3d451427b7d", Controller:(*bool)(0x400329ef72), BlockOwnerDeletion:(*bool)(0x400329ef73)}}
Sep  5 07:35:48.515: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4227ca8a-b5bb-44fb-8717-4c550d274af0", Controller:(*bool)(0x4002ec3d1a), BlockOwnerDeletion:(*bool)(0x4002ec3d1b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:35:53.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9152" for this suite.
Sep  5 07:35:59.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:35:59.736: INFO: namespace gc-9152 deletion completed in 6.120900675s

• [SLOW TEST:11.573 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:35:59.737: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5446
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-19e2778f-eb95-42cc-bcb9-5598d8ff1dce
STEP: Creating a pod to test consume secrets
Sep  5 07:36:00.011: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3251aa8c-6724-45eb-92d9-70a43cac2b79" in namespace "projected-5446" to be "success or failure"
Sep  5 07:36:00.022: INFO: Pod "pod-projected-secrets-3251aa8c-6724-45eb-92d9-70a43cac2b79": Phase="Pending", Reason="", readiness=false. Elapsed: 10.942953ms
Sep  5 07:36:02.026: INFO: Pod "pod-projected-secrets-3251aa8c-6724-45eb-92d9-70a43cac2b79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014561548s
Sep  5 07:36:04.029: INFO: Pod "pod-projected-secrets-3251aa8c-6724-45eb-92d9-70a43cac2b79": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018131564s
Sep  5 07:36:06.033: INFO: Pod "pod-projected-secrets-3251aa8c-6724-45eb-92d9-70a43cac2b79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021885658s
STEP: Saw pod success
Sep  5 07:36:06.033: INFO: Pod "pod-projected-secrets-3251aa8c-6724-45eb-92d9-70a43cac2b79" satisfied condition "success or failure"
Sep  5 07:36:06.036: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-3251aa8c-6724-45eb-92d9-70a43cac2b79 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 07:36:06.075: INFO: Waiting for pod pod-projected-secrets-3251aa8c-6724-45eb-92d9-70a43cac2b79 to disappear
Sep  5 07:36:06.108: INFO: Pod pod-projected-secrets-3251aa8c-6724-45eb-92d9-70a43cac2b79 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:36:06.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5446" for this suite.
Sep  5 07:36:12.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:36:12.231: INFO: namespace projected-5446 deletion completed in 6.119174498s

• [SLOW TEST:12.495 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:36:12.232: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-40d4a204-1ded-4b0c-bab5-0232a2b5e338
STEP: Creating a pod to test consume secrets
Sep  5 07:36:12.478: INFO: Waiting up to 5m0s for pod "pod-secrets-604564b5-c081-4b9e-8825-1b4c901abd71" in namespace "secrets-8140" to be "success or failure"
Sep  5 07:36:12.508: INFO: Pod "pod-secrets-604564b5-c081-4b9e-8825-1b4c901abd71": Phase="Pending", Reason="", readiness=false. Elapsed: 30.670689ms
Sep  5 07:36:14.516: INFO: Pod "pod-secrets-604564b5-c081-4b9e-8825-1b4c901abd71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038225646s
Sep  5 07:36:16.520: INFO: Pod "pod-secrets-604564b5-c081-4b9e-8825-1b4c901abd71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042008299s
STEP: Saw pod success
Sep  5 07:36:16.520: INFO: Pod "pod-secrets-604564b5-c081-4b9e-8825-1b4c901abd71" satisfied condition "success or failure"
Sep  5 07:36:16.522: INFO: Trying to get logs from node slave1 pod pod-secrets-604564b5-c081-4b9e-8825-1b4c901abd71 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 07:36:16.546: INFO: Waiting for pod pod-secrets-604564b5-c081-4b9e-8825-1b4c901abd71 to disappear
Sep  5 07:36:16.553: INFO: Pod pod-secrets-604564b5-c081-4b9e-8825-1b4c901abd71 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:36:16.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8140" for this suite.
Sep  5 07:36:22.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:36:22.676: INFO: namespace secrets-8140 deletion completed in 6.119041476s

• [SLOW TEST:10.444 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:36:22.676: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6999
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  5 07:36:22.898: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  5 07:36:51.127: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.31.161.11:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6999 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 07:36:51.127: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 07:36:51.431: INFO: Found all expected endpoints: [netserver-0]
Sep  5 07:36:51.435: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.31.51.190:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6999 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 07:36:51.435: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 07:36:51.729: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:36:51.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6999" for this suite.
Sep  5 07:37:13.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:37:13.866: INFO: namespace pod-network-test-6999 deletion completed in 22.132030306s

• [SLOW TEST:51.189 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:37:13.866: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4369
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-n8m2
STEP: Creating a pod to test atomic-volume-subpath
Sep  5 07:37:14.081: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-n8m2" in namespace "subpath-4369" to be "success or failure"
Sep  5 07:37:14.129: INFO: Pod "pod-subpath-test-secret-n8m2": Phase="Pending", Reason="", readiness=false. Elapsed: 47.719595ms
Sep  5 07:37:16.158: INFO: Pod "pod-subpath-test-secret-n8m2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076869834s
Sep  5 07:37:18.162: INFO: Pod "pod-subpath-test-secret-n8m2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.08089868s
Sep  5 07:37:20.166: INFO: Pod "pod-subpath-test-secret-n8m2": Phase="Running", Reason="", readiness=true. Elapsed: 6.085174705s
Sep  5 07:37:22.171: INFO: Pod "pod-subpath-test-secret-n8m2": Phase="Running", Reason="", readiness=true. Elapsed: 8.089765628s
Sep  5 07:37:24.175: INFO: Pod "pod-subpath-test-secret-n8m2": Phase="Running", Reason="", readiness=true. Elapsed: 10.093746694s
Sep  5 07:37:26.178: INFO: Pod "pod-subpath-test-secret-n8m2": Phase="Running", Reason="", readiness=true. Elapsed: 12.097163401s
Sep  5 07:37:28.182: INFO: Pod "pod-subpath-test-secret-n8m2": Phase="Running", Reason="", readiness=true. Elapsed: 14.100928128s
Sep  5 07:37:30.186: INFO: Pod "pod-subpath-test-secret-n8m2": Phase="Running", Reason="", readiness=true. Elapsed: 16.104812113s
Sep  5 07:37:32.190: INFO: Pod "pod-subpath-test-secret-n8m2": Phase="Running", Reason="", readiness=true. Elapsed: 18.108568659s
Sep  5 07:37:34.194: INFO: Pod "pod-subpath-test-secret-n8m2": Phase="Running", Reason="", readiness=true. Elapsed: 20.112451344s
Sep  5 07:37:36.197: INFO: Pod "pod-subpath-test-secret-n8m2": Phase="Running", Reason="", readiness=true. Elapsed: 22.11606153s
Sep  5 07:37:38.201: INFO: Pod "pod-subpath-test-secret-n8m2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.119857415s
STEP: Saw pod success
Sep  5 07:37:38.201: INFO: Pod "pod-subpath-test-secret-n8m2" satisfied condition "success or failure"
Sep  5 07:37:38.221: INFO: Trying to get logs from node slave1 pod pod-subpath-test-secret-n8m2 container test-container-subpath-secret-n8m2: <nil>
STEP: delete the pod
Sep  5 07:37:38.273: INFO: Waiting for pod pod-subpath-test-secret-n8m2 to disappear
Sep  5 07:37:38.284: INFO: Pod pod-subpath-test-secret-n8m2 no longer exists
STEP: Deleting pod pod-subpath-test-secret-n8m2
Sep  5 07:37:38.284: INFO: Deleting pod "pod-subpath-test-secret-n8m2" in namespace "subpath-4369"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:37:38.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4369" for this suite.
Sep  5 07:37:44.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:37:44.388: INFO: namespace subpath-4369 deletion completed in 6.097065966s

• [SLOW TEST:30.522 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:37:44.388: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1930
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  5 07:37:44.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1930'
Sep  5 07:37:44.816: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  5 07:37:44.816: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Sep  5 07:37:44.832: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Sep  5 07:37:44.841: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Sep  5 07:37:44.906: INFO: scanned /root for discovery docs: <nil>
Sep  5 07:37:44.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1930'
Sep  5 07:38:02.166: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep  5 07:38:02.166: INFO: stdout: "Created e2e-test-nginx-rc-61e7299eb071f707782794ab9f44acc7\nScaling up e2e-test-nginx-rc-61e7299eb071f707782794ab9f44acc7 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-61e7299eb071f707782794ab9f44acc7 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-61e7299eb071f707782794ab9f44acc7 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Sep  5 07:38:02.166: INFO: stdout: "Created e2e-test-nginx-rc-61e7299eb071f707782794ab9f44acc7\nScaling up e2e-test-nginx-rc-61e7299eb071f707782794ab9f44acc7 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-61e7299eb071f707782794ab9f44acc7 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-61e7299eb071f707782794ab9f44acc7 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Sep  5 07:38:02.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1930'
Sep  5 07:38:02.345: INFO: stderr: ""
Sep  5 07:38:02.345: INFO: stdout: "e2e-test-nginx-rc-61e7299eb071f707782794ab9f44acc7-cjfw9 "
Sep  5 07:38:02.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods e2e-test-nginx-rc-61e7299eb071f707782794ab9f44acc7-cjfw9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1930'
Sep  5 07:38:02.531: INFO: stderr: ""
Sep  5 07:38:02.531: INFO: stdout: "true"
Sep  5 07:38:02.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods e2e-test-nginx-rc-61e7299eb071f707782794ab9f44acc7-cjfw9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1930'
Sep  5 07:38:02.704: INFO: stderr: ""
Sep  5 07:38:02.704: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Sep  5 07:38:02.704: INFO: e2e-test-nginx-rc-61e7299eb071f707782794ab9f44acc7-cjfw9 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Sep  5 07:38:02.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete rc e2e-test-nginx-rc --namespace=kubectl-1930'
Sep  5 07:38:02.933: INFO: stderr: ""
Sep  5 07:38:02.933: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:38:02.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1930" for this suite.
Sep  5 07:38:25.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:38:25.102: INFO: namespace kubectl-1930 deletion completed in 22.107669413s

• [SLOW TEST:40.714 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:38:25.103: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Sep  5 07:38:25.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 create -f - --namespace=kubectl-2527'
Sep  5 07:38:25.960: INFO: stderr: ""
Sep  5 07:38:25.960: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 07:38:25.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2527'
Sep  5 07:38:26.189: INFO: stderr: ""
Sep  5 07:38:26.189: INFO: stdout: "update-demo-nautilus-rhmt6 update-demo-nautilus-w4x6p "
Sep  5 07:38:26.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-rhmt6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2527'
Sep  5 07:38:26.359: INFO: stderr: ""
Sep  5 07:38:26.359: INFO: stdout: ""
Sep  5 07:38:26.359: INFO: update-demo-nautilus-rhmt6 is created but not running
Sep  5 07:38:31.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2527'
Sep  5 07:38:31.533: INFO: stderr: ""
Sep  5 07:38:31.533: INFO: stdout: "update-demo-nautilus-rhmt6 update-demo-nautilus-w4x6p "
Sep  5 07:38:31.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-rhmt6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2527'
Sep  5 07:38:31.706: INFO: stderr: ""
Sep  5 07:38:31.706: INFO: stdout: "true"
Sep  5 07:38:31.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-rhmt6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2527'
Sep  5 07:38:31.898: INFO: stderr: ""
Sep  5 07:38:31.898: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 07:38:31.898: INFO: validating pod update-demo-nautilus-rhmt6
Sep  5 07:38:31.912: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 07:38:31.912: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 07:38:31.912: INFO: update-demo-nautilus-rhmt6 is verified up and running
Sep  5 07:38:31.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-w4x6p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2527'
Sep  5 07:38:32.081: INFO: stderr: ""
Sep  5 07:38:32.081: INFO: stdout: "true"
Sep  5 07:38:32.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-w4x6p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2527'
Sep  5 07:38:32.267: INFO: stderr: ""
Sep  5 07:38:32.267: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 07:38:32.267: INFO: validating pod update-demo-nautilus-w4x6p
Sep  5 07:38:32.271: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 07:38:32.272: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 07:38:32.272: INFO: update-demo-nautilus-w4x6p is verified up and running
STEP: using delete to clean up resources
Sep  5 07:38:32.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete --grace-period=0 --force -f - --namespace=kubectl-2527'
Sep  5 07:38:32.519: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 07:38:32.520: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  5 07:38:32.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2527'
Sep  5 07:38:32.702: INFO: stderr: "No resources found.\n"
Sep  5 07:38:32.702: INFO: stdout: ""
Sep  5 07:38:32.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -l name=update-demo --namespace=kubectl-2527 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 07:38:32.887: INFO: stderr: ""
Sep  5 07:38:32.887: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:38:32.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2527" for this suite.
Sep  5 07:38:54.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:38:54.999: INFO: namespace kubectl-2527 deletion completed in 22.106731786s

• [SLOW TEST:29.896 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:38:55.000: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5983
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep  5 07:38:55.186: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:39:01.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5983" for this suite.
Sep  5 07:39:07.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:39:07.397: INFO: namespace init-container-5983 deletion completed in 6.148127702s

• [SLOW TEST:12.397 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:39:07.397: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3114
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-2058cab2-b374-4bba-9924-825f365b4a84
STEP: Creating secret with name s-test-opt-upd-621c5714-0d4f-4d15-ba71-6c7dd0345214
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-2058cab2-b374-4bba-9924-825f365b4a84
STEP: Updating secret s-test-opt-upd-621c5714-0d4f-4d15-ba71-6c7dd0345214
STEP: Creating secret with name s-test-opt-create-a7dfc6ed-99a7-4c9f-8e9e-c72803dcee23
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:39:17.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3114" for this suite.
Sep  5 07:39:39.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:39:39.954: INFO: namespace projected-3114 deletion completed in 22.116052017s

• [SLOW TEST:32.557 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:39:39.955: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4002
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep  5 07:39:40.180: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:39:46.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4002" for this suite.
Sep  5 07:39:52.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:39:53.081: INFO: namespace init-container-4002 deletion completed in 6.146255516s

• [SLOW TEST:13.126 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:39:53.082: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6040
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 07:39:53.300: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  5 07:39:53.329: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  5 07:39:58.333: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  5 07:39:58.333: INFO: Creating deployment "test-rolling-update-deployment"
Sep  5 07:39:58.338: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  5 07:39:58.360: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep  5 07:40:00.368: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  5 07:40:00.381: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703265990, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703265990, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703265990, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703265990, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 07:40:02.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703265990, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703265990, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703265990, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703265990, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 07:40:04.385: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep  5 07:40:04.396: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-6040,SelfLink:/apis/apps/v1/namespaces/deployment-6040/deployments/test-rolling-update-deployment,UID:ec886399-15ce-4797-ab77-b25b860d5cf8,ResourceVersion:226427,Generation:1,CreationTimestamp:2019-09-05 07:39:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-05 07:39:50 +0000 UTC 2019-09-05 07:39:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-05 07:39:54 +0000 UTC 2019-09-05 07:39:50 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep  5 07:40:04.400: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-6040,SelfLink:/apis/apps/v1/namespaces/deployment-6040/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:df4ee375-ce09-4fae-bbee-35ba2687c8f6,ResourceVersion:226416,Generation:1,CreationTimestamp:2019-09-05 07:39:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment ec886399-15ce-4797-ab77-b25b860d5cf8 0x40024f3f67 0x40024f3f68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  5 07:40:04.400: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  5 07:40:04.400: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-6040,SelfLink:/apis/apps/v1/namespaces/deployment-6040/replicasets/test-rolling-update-controller,UID:8f366101-6287-4292-b964-60156e71268a,ResourceVersion:226425,Generation:2,CreationTimestamp:2019-09-05 07:39:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment ec886399-15ce-4797-ab77-b25b860d5cf8 0x40024f3e87 0x40024f3e88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  5 07:40:04.404: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-wn2wm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-wn2wm,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-6040,SelfLink:/api/v1/namespaces/deployment-6040/pods/test-rolling-update-deployment-79f6b9d75c-wn2wm,UID:3b92a588-39fa-4a1e-97d4-09bde0e4f473,ResourceVersion:226415,Generation:0,CreationTimestamp:2019-09-05 07:39:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c df4ee375-ce09-4fae-bbee-35ba2687c8f6 0x40022a4857 0x40022a4858}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fskft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fskft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-fskft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40022a48d0} {node.kubernetes.io/unreachable Exists  NoExecute 0x40022a48f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:39:58 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:40:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:40:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:39:50 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.30,PodIP:172.31.51.197,StartTime:2019-09-05 07:39:58 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-05 07:40:01 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://7f806b8d084253e11e795b077c3b971d7963afb09a3384b98884602765910029}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:40:04.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6040" for this suite.
Sep  5 07:40:10.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:40:10.601: INFO: namespace deployment-6040 deletion completed in 6.19288543s

• [SLOW TEST:17.519 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:40:10.602: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4084
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Sep  5 07:40:10.915: INFO: Found 0 stateful pods, waiting for 3
Sep  5 07:40:20.920: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 07:40:20.920: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 07:40:20.920: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 07:40:30.920: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 07:40:30.920: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 07:40:30.920: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep  5 07:40:30.948: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep  5 07:40:40.992: INFO: Updating stateful set ss2
Sep  5 07:40:41.040: INFO: Waiting for Pod statefulset-4084/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  5 07:40:51.049: INFO: Waiting for Pod statefulset-4084/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Sep  5 07:41:01.191: INFO: Found 2 stateful pods, waiting for 3
Sep  5 07:41:11.196: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 07:41:11.196: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 07:41:11.196: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Sep  5 07:41:21.196: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 07:41:21.196: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 07:41:21.196: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep  5 07:41:21.222: INFO: Updating stateful set ss2
Sep  5 07:41:21.277: INFO: Waiting for Pod statefulset-4084/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  5 07:41:31.303: INFO: Updating stateful set ss2
Sep  5 07:41:31.572: INFO: Waiting for StatefulSet statefulset-4084/ss2 to complete update
Sep  5 07:41:31.572: INFO: Waiting for Pod statefulset-4084/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep  5 07:41:41.594: INFO: Deleting all statefulset in ns statefulset-4084
Sep  5 07:41:41.597: INFO: Scaling statefulset ss2 to 0
Sep  5 07:42:11.614: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 07:42:11.617: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:42:11.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4084" for this suite.
Sep  5 07:42:17.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:42:17.791: INFO: namespace statefulset-4084 deletion completed in 6.153413104s

• [SLOW TEST:127.189 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:42:17.791: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7235
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-rrmln in namespace proxy-7235
I0905 07:42:18.081346      16 runners.go:180] Created replication controller with name: proxy-service-rrmln, namespace: proxy-7235, replica count: 1
I0905 07:42:19.131792      16 runners.go:180] proxy-service-rrmln Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0905 07:42:20.131992      16 runners.go:180] proxy-service-rrmln Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0905 07:42:21.132185      16 runners.go:180] proxy-service-rrmln Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0905 07:42:22.132344      16 runners.go:180] proxy-service-rrmln Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0905 07:42:23.132530      16 runners.go:180] proxy-service-rrmln Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0905 07:42:24.132736      16 runners.go:180] proxy-service-rrmln Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0905 07:42:25.132926      16 runners.go:180] proxy-service-rrmln Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0905 07:42:26.133121      16 runners.go:180] proxy-service-rrmln Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0905 07:42:27.133355      16 runners.go:180] proxy-service-rrmln Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0905 07:42:28.133551      16 runners.go:180] proxy-service-rrmln Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0905 07:42:29.133748      16 runners.go:180] proxy-service-rrmln Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0905 07:42:30.133950      16 runners.go:180] proxy-service-rrmln Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0905 07:42:31.134144      16 runners.go:180] proxy-service-rrmln Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0905 07:42:32.134381      16 runners.go:180] proxy-service-rrmln Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  5 07:42:32.167: INFO: setup took 14.147714442s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep  5 07:42:32.178: INFO: (0) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 10.196756ms)
Sep  5 07:42:32.180: INFO: (0) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 11.56607ms)
Sep  5 07:42:32.180: INFO: (0) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 12.040468ms)
Sep  5 07:42:32.180: INFO: (0) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 12.657706ms)
Sep  5 07:42:32.180: INFO: (0) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 12.449466ms)
Sep  5 07:42:32.180: INFO: (0) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 12.675746ms)
Sep  5 07:42:32.180: INFO: (0) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 12.655425ms)
Sep  5 07:42:32.182: INFO: (0) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 13.97356ms)
Sep  5 07:42:32.182: INFO: (0) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 14.462918ms)
Sep  5 07:42:32.182: INFO: (0) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 14.274419ms)
Sep  5 07:42:32.182: INFO: (0) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 14.622977ms)
Sep  5 07:42:32.243: INFO: (0) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 75.123856ms)
Sep  5 07:42:32.243: INFO: (0) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 75.484595ms)
Sep  5 07:42:32.245: INFO: (0) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 76.721649ms)
Sep  5 07:42:32.245: INFO: (0) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 76.727869ms)
Sep  5 07:42:32.246: INFO: (0) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 77.526685ms)
Sep  5 07:42:32.252: INFO: (1) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 6.451452ms)
Sep  5 07:42:32.255: INFO: (1) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 8.711702ms)
Sep  5 07:42:32.255: INFO: (1) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 8.593983ms)
Sep  5 07:42:32.255: INFO: (1) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 8.940202ms)
Sep  5 07:42:32.255: INFO: (1) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 8.745342ms)
Sep  5 07:42:32.255: INFO: (1) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 8.885561ms)
Sep  5 07:42:32.255: INFO: (1) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 8.983181ms)
Sep  5 07:42:32.255: INFO: (1) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 8.773902ms)
Sep  5 07:42:32.255: INFO: (1) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 9.17286ms)
Sep  5 07:42:32.255: INFO: (1) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 9.438719ms)
Sep  5 07:42:32.256: INFO: (1) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 9.470439ms)
Sep  5 07:42:32.256: INFO: (1) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 9.848138ms)
Sep  5 07:42:32.256: INFO: (1) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 9.793558ms)
Sep  5 07:42:32.256: INFO: (1) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 10.068036ms)
Sep  5 07:42:32.256: INFO: (1) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 10.420715ms)
Sep  5 07:42:32.256: INFO: (1) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 10.235956ms)
Sep  5 07:42:32.261: INFO: (2) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 4.60128ms)
Sep  5 07:42:32.261: INFO: (2) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 4.797399ms)
Sep  5 07:42:32.264: INFO: (2) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 6.350072ms)
Sep  5 07:42:32.264: INFO: (2) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 6.859371ms)
Sep  5 07:42:32.264: INFO: (2) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 6.94425ms)
Sep  5 07:42:32.264: INFO: (2) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 6.828691ms)
Sep  5 07:42:32.265: INFO: (2) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 7.279249ms)
Sep  5 07:42:32.265: INFO: (2) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 8.799142ms)
Sep  5 07:42:32.265: INFO: (2) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 7.757787ms)
Sep  5 07:42:32.266: INFO: (2) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 7.773146ms)
Sep  5 07:42:32.266: INFO: (2) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 9.398199ms)
Sep  5 07:42:32.266: INFO: (2) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 8.518363ms)
Sep  5 07:42:32.266: INFO: (2) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 8.313944ms)
Sep  5 07:42:32.275: INFO: (2) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 17.718504ms)
Sep  5 07:42:32.275: INFO: (2) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 16.636309ms)
Sep  5 07:42:32.275: INFO: (2) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 17.087326ms)
Sep  5 07:42:32.284: INFO: (3) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 8.893642ms)
Sep  5 07:42:32.284: INFO: (3) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 9.26042ms)
Sep  5 07:42:32.284: INFO: (3) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 8.523163ms)
Sep  5 07:42:32.284: INFO: (3) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 8.851842ms)
Sep  5 07:42:32.284: INFO: (3) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 9.153901ms)
Sep  5 07:42:32.284: INFO: (3) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 8.641742ms)
Sep  5 07:42:32.285: INFO: (3) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 9.523599ms)
Sep  5 07:42:32.285: INFO: (3) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 10.109456ms)
Sep  5 07:42:32.285: INFO: (3) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 10.284516ms)
Sep  5 07:42:32.285: INFO: (3) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 9.640138ms)
Sep  5 07:42:32.286: INFO: (3) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 10.728754ms)
Sep  5 07:42:32.286: INFO: (3) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 11.377451ms)
Sep  5 07:42:32.286: INFO: (3) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 10.935673ms)
Sep  5 07:42:32.286: INFO: (3) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 11.49035ms)
Sep  5 07:42:32.286: INFO: (3) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 10.990232ms)
Sep  5 07:42:32.287: INFO: (3) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 11.49439ms)
Sep  5 07:42:32.293: INFO: (4) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 6.449073ms)
Sep  5 07:42:32.293: INFO: (4) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 6.443913ms)
Sep  5 07:42:32.293: INFO: (4) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 6.683192ms)
Sep  5 07:42:32.293: INFO: (4) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 6.621471ms)
Sep  5 07:42:32.295: INFO: (4) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 8.216044ms)
Sep  5 07:42:32.295: INFO: (4) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 8.918621ms)
Sep  5 07:42:32.296: INFO: (4) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 8.881362ms)
Sep  5 07:42:32.296: INFO: (4) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 8.746182ms)
Sep  5 07:42:32.296: INFO: (4) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 8.687863ms)
Sep  5 07:42:32.296: INFO: (4) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 9.081881ms)
Sep  5 07:42:32.296: INFO: (4) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 9.410399ms)
Sep  5 07:42:32.296: INFO: (4) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 9.649598ms)
Sep  5 07:42:32.296: INFO: (4) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 9.593799ms)
Sep  5 07:42:32.296: INFO: (4) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 9.555779ms)
Sep  5 07:42:32.296: INFO: (4) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 9.632339ms)
Sep  5 07:42:32.296: INFO: (4) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 9.634498ms)
Sep  5 07:42:32.304: INFO: (5) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 7.712647ms)
Sep  5 07:42:32.305: INFO: (5) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 8.492383ms)
Sep  5 07:42:32.305: INFO: (5) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 8.600863ms)
Sep  5 07:42:32.305: INFO: (5) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 8.519984ms)
Sep  5 07:42:32.306: INFO: (5) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 8.745122ms)
Sep  5 07:42:32.306: INFO: (5) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 9.031861ms)
Sep  5 07:42:32.306: INFO: (5) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 8.700983ms)
Sep  5 07:42:32.306: INFO: (5) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 9.019361ms)
Sep  5 07:42:32.306: INFO: (5) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 9.401299ms)
Sep  5 07:42:32.306: INFO: (5) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 9.656158ms)
Sep  5 07:42:32.306: INFO: (5) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 9.41556ms)
Sep  5 07:42:32.307: INFO: (5) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 10.184696ms)
Sep  5 07:42:32.307: INFO: (5) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 10.341675ms)
Sep  5 07:42:32.307: INFO: (5) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 10.340675ms)
Sep  5 07:42:32.307: INFO: (5) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 10.452195ms)
Sep  5 07:42:32.308: INFO: (5) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 10.712293ms)
Sep  5 07:42:32.313: INFO: (6) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 5.722615ms)
Sep  5 07:42:32.313: INFO: (6) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 5.808575ms)
Sep  5 07:42:32.314: INFO: (6) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 5.690215ms)
Sep  5 07:42:32.314: INFO: (6) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 6.128034ms)
Sep  5 07:42:32.314: INFO: (6) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 6.019754ms)
Sep  5 07:42:32.315: INFO: (6) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 7.191249ms)
Sep  5 07:42:32.315: INFO: (6) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 7.399088ms)
Sep  5 07:42:32.315: INFO: (6) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 7.589607ms)
Sep  5 07:42:32.316: INFO: (6) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 8.177825ms)
Sep  5 07:42:32.316: INFO: (6) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 8.344484ms)
Sep  5 07:42:32.316: INFO: (6) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 8.086965ms)
Sep  5 07:42:32.316: INFO: (6) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 8.476744ms)
Sep  5 07:42:32.316: INFO: (6) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 8.483024ms)
Sep  5 07:42:32.316: INFO: (6) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 8.529623ms)
Sep  5 07:42:32.316: INFO: (6) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 8.651383ms)
Sep  5 07:42:32.316: INFO: (6) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 8.700063ms)
Sep  5 07:42:32.324: INFO: (7) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 6.178134ms)
Sep  5 07:42:32.324: INFO: (7) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 6.831511ms)
Sep  5 07:42:32.324: INFO: (7) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 6.365253ms)
Sep  5 07:42:32.324: INFO: (7) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 6.95419ms)
Sep  5 07:42:32.324: INFO: (7) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 6.562331ms)
Sep  5 07:42:32.325: INFO: (7) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 8.661842ms)
Sep  5 07:42:32.325: INFO: (7) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 8.456283ms)
Sep  5 07:42:32.326: INFO: (7) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 8.054165ms)
Sep  5 07:42:32.327: INFO: (7) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 9.042881ms)
Sep  5 07:42:32.327: INFO: (7) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 9.028581ms)
Sep  5 07:42:32.327: INFO: (7) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 9.460719ms)
Sep  5 07:42:32.327: INFO: (7) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 9.325699ms)
Sep  5 07:42:32.327: INFO: (7) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 9.476019ms)
Sep  5 07:42:32.327: INFO: (7) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 9.485459ms)
Sep  5 07:42:32.327: INFO: (7) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 9.3286ms)
Sep  5 07:42:32.327: INFO: (7) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 9.591299ms)
Sep  5 07:42:32.334: INFO: (8) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 6.683571ms)
Sep  5 07:42:32.334: INFO: (8) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 6.93735ms)
Sep  5 07:42:32.335: INFO: (8) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 7.600247ms)
Sep  5 07:42:32.335: INFO: (8) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 7.454908ms)
Sep  5 07:42:32.336: INFO: (8) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 8.863602ms)
Sep  5 07:42:32.336: INFO: (8) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 8.584683ms)
Sep  5 07:42:32.336: INFO: (8) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 8.635683ms)
Sep  5 07:42:32.336: INFO: (8) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 8.798522ms)
Sep  5 07:42:32.336: INFO: (8) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 8.657343ms)
Sep  5 07:42:32.336: INFO: (8) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 8.841262ms)
Sep  5 07:42:32.336: INFO: (8) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 8.990521ms)
Sep  5 07:42:32.336: INFO: (8) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 8.905141ms)
Sep  5 07:42:32.336: INFO: (8) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 8.867042ms)
Sep  5 07:42:32.336: INFO: (8) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 8.823502ms)
Sep  5 07:42:32.336: INFO: (8) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 8.826782ms)
Sep  5 07:42:32.336: INFO: (8) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 8.963982ms)
Sep  5 07:42:32.343: INFO: (9) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 6.557171ms)
Sep  5 07:42:32.343: INFO: (9) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 6.744671ms)
Sep  5 07:42:32.344: INFO: (9) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 7.523988ms)
Sep  5 07:42:32.344: INFO: (9) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 7.940686ms)
Sep  5 07:42:32.344: INFO: (9) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 7.960726ms)
Sep  5 07:42:32.344: INFO: (9) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 7.951486ms)
Sep  5 07:42:32.344: INFO: (9) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 7.878686ms)
Sep  5 07:42:32.344: INFO: (9) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 7.974326ms)
Sep  5 07:42:32.344: INFO: (9) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 7.919246ms)
Sep  5 07:42:32.345: INFO: (9) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 8.335384ms)
Sep  5 07:42:32.347: INFO: (9) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 10.696154ms)
Sep  5 07:42:32.347: INFO: (9) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 10.833733ms)
Sep  5 07:42:32.347: INFO: (9) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 10.947593ms)
Sep  5 07:42:32.347: INFO: (9) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 11.075852ms)
Sep  5 07:42:32.347: INFO: (9) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 11.096032ms)
Sep  5 07:42:32.347: INFO: (9) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 10.823954ms)
Sep  5 07:42:32.357: INFO: (10) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 10.023277ms)
Sep  5 07:42:32.359: INFO: (10) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 10.718114ms)
Sep  5 07:42:32.359: INFO: (10) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 11.313592ms)
Sep  5 07:42:32.359: INFO: (10) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 11.235651ms)
Sep  5 07:42:32.359: INFO: (10) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 11.122552ms)
Sep  5 07:42:32.359: INFO: (10) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 11.653869ms)
Sep  5 07:42:32.359: INFO: (10) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 11.342311ms)
Sep  5 07:42:32.360: INFO: (10) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 11.175172ms)
Sep  5 07:42:32.360: INFO: (10) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 11.283371ms)
Sep  5 07:42:32.360: INFO: (10) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 11.52713ms)
Sep  5 07:42:32.360: INFO: (10) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 12.202227ms)
Sep  5 07:42:32.360: INFO: (10) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 12.338747ms)
Sep  5 07:42:32.360: INFO: (10) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 12.376947ms)
Sep  5 07:42:32.361: INFO: (10) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 12.397647ms)
Sep  5 07:42:32.361: INFO: (10) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 12.367067ms)
Sep  5 07:42:32.361: INFO: (10) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 12.366266ms)
Sep  5 07:42:32.368: INFO: (11) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 6.611311ms)
Sep  5 07:42:32.368: INFO: (11) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 7.00763ms)
Sep  5 07:42:32.368: INFO: (11) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 7.03265ms)
Sep  5 07:42:32.368: INFO: (11) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 6.89111ms)
Sep  5 07:42:32.369: INFO: (11) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 8.344604ms)
Sep  5 07:42:32.369: INFO: (11) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 8.215105ms)
Sep  5 07:42:32.369: INFO: (11) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 8.368284ms)
Sep  5 07:42:32.369: INFO: (11) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 8.467364ms)
Sep  5 07:42:32.370: INFO: (11) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 8.839062ms)
Sep  5 07:42:32.370: INFO: (11) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 8.784762ms)
Sep  5 07:42:32.370: INFO: (11) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 8.892022ms)
Sep  5 07:42:32.370: INFO: (11) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 9.19024ms)
Sep  5 07:42:32.370: INFO: (11) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 9.336419ms)
Sep  5 07:42:32.371: INFO: (11) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 9.804897ms)
Sep  5 07:42:32.371: INFO: (11) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 10.268876ms)
Sep  5 07:42:32.371: INFO: (11) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 10.390716ms)
Sep  5 07:42:32.411: INFO: (12) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 39.932568ms)
Sep  5 07:42:32.411: INFO: (12) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 39.737669ms)
Sep  5 07:42:32.411: INFO: (12) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 39.43987ms)
Sep  5 07:42:32.411: INFO: (12) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 39.939848ms)
Sep  5 07:42:32.415: INFO: (12) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 43.012494ms)
Sep  5 07:42:32.415: INFO: (12) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 43.410313ms)
Sep  5 07:42:32.415: INFO: (12) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 43.857791ms)
Sep  5 07:42:32.415: INFO: (12) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 43.772151ms)
Sep  5 07:42:32.416: INFO: (12) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 44.262029ms)
Sep  5 07:42:32.416: INFO: (12) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 43.893611ms)
Sep  5 07:42:32.417: INFO: (12) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 44.968206ms)
Sep  5 07:42:32.417: INFO: (12) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 45.163925ms)
Sep  5 07:42:32.417: INFO: (12) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 45.741483ms)
Sep  5 07:42:32.417: INFO: (12) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 45.739543ms)
Sep  5 07:42:32.418: INFO: (12) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 45.959222ms)
Sep  5 07:42:32.418: INFO: (12) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 46.062902ms)
Sep  5 07:42:32.424: INFO: (13) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 6.097774ms)
Sep  5 07:42:32.425: INFO: (13) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 7.016509ms)
Sep  5 07:42:32.426: INFO: (13) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 7.998466ms)
Sep  5 07:42:32.427: INFO: (13) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 9.2759ms)
Sep  5 07:42:32.427: INFO: (13) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 9.25474ms)
Sep  5 07:42:32.427: INFO: (13) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 9.404399ms)
Sep  5 07:42:32.427: INFO: (13) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 9.20844ms)
Sep  5 07:42:32.427: INFO: (13) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 9.22908ms)
Sep  5 07:42:32.427: INFO: (13) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 9.34248ms)
Sep  5 07:42:32.428: INFO: (13) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 9.2313ms)
Sep  5 07:42:32.428: INFO: (13) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 9.34862ms)
Sep  5 07:42:32.428: INFO: (13) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 9.3716ms)
Sep  5 07:42:32.428: INFO: (13) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 9.686519ms)
Sep  5 07:42:32.428: INFO: (13) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 9.259681ms)
Sep  5 07:42:32.428: INFO: (13) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 9.864537ms)
Sep  5 07:42:32.428: INFO: (13) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 10.014217ms)
Sep  5 07:42:32.436: INFO: (14) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 8.164184ms)
Sep  5 07:42:32.436: INFO: (14) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 7.907225ms)
Sep  5 07:42:32.436: INFO: (14) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 7.931265ms)
Sep  5 07:42:32.436: INFO: (14) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 8.167244ms)
Sep  5 07:42:32.439: INFO: (14) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 10.285636ms)
Sep  5 07:42:32.439: INFO: (14) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 10.611514ms)
Sep  5 07:42:32.439: INFO: (14) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 10.532995ms)
Sep  5 07:42:32.439: INFO: (14) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 10.757973ms)
Sep  5 07:42:32.439: INFO: (14) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 10.627994ms)
Sep  5 07:42:32.439: INFO: (14) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 10.428335ms)
Sep  5 07:42:32.441: INFO: (14) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 13.227403ms)
Sep  5 07:42:32.441: INFO: (14) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 13.245723ms)
Sep  5 07:42:32.442: INFO: (14) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 13.693001ms)
Sep  5 07:42:32.442: INFO: (14) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 13.644841ms)
Sep  5 07:42:32.442: INFO: (14) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 13.597042ms)
Sep  5 07:42:32.442: INFO: (14) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 13.539602ms)
Sep  5 07:42:32.451: INFO: (15) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 9.039001ms)
Sep  5 07:42:32.451: INFO: (15) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 9.45514ms)
Sep  5 07:42:32.452: INFO: (15) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 9.3024ms)
Sep  5 07:42:32.452: INFO: (15) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 9.473199ms)
Sep  5 07:42:32.452: INFO: (15) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 10.090596ms)
Sep  5 07:42:32.452: INFO: (15) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 10.130276ms)
Sep  5 07:42:32.453: INFO: (15) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 10.753053ms)
Sep  5 07:42:32.453: INFO: (15) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 10.754054ms)
Sep  5 07:42:32.453: INFO: (15) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 10.695154ms)
Sep  5 07:42:32.453: INFO: (15) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 11.135112ms)
Sep  5 07:42:32.453: INFO: (15) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 11.312871ms)
Sep  5 07:42:32.454: INFO: (15) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 12.122688ms)
Sep  5 07:42:32.454: INFO: (15) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 12.079308ms)
Sep  5 07:42:32.454: INFO: (15) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 11.882789ms)
Sep  5 07:42:32.454: INFO: (15) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 11.989868ms)
Sep  5 07:42:32.454: INFO: (15) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 11.926288ms)
Sep  5 07:42:32.461: INFO: (16) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 6.690952ms)
Sep  5 07:42:32.461: INFO: (16) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 6.92455ms)
Sep  5 07:42:32.461: INFO: (16) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 6.88453ms)
Sep  5 07:42:32.461: INFO: (16) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 6.800731ms)
Sep  5 07:42:32.461: INFO: (16) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 6.91071ms)
Sep  5 07:42:32.461: INFO: (16) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 7.02787ms)
Sep  5 07:42:32.463: INFO: (16) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 8.414584ms)
Sep  5 07:42:32.463: INFO: (16) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 8.821882ms)
Sep  5 07:42:32.463: INFO: (16) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 8.928921ms)
Sep  5 07:42:32.463: INFO: (16) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 9.016061ms)
Sep  5 07:42:32.463: INFO: (16) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 8.940121ms)
Sep  5 07:42:32.464: INFO: (16) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 9.29592ms)
Sep  5 07:42:32.464: INFO: (16) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 9.474979ms)
Sep  5 07:42:32.464: INFO: (16) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 9.38554ms)
Sep  5 07:42:32.464: INFO: (16) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 9.474699ms)
Sep  5 07:42:32.464: INFO: (16) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 9.644638ms)
Sep  5 07:42:32.469: INFO: (17) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 4.861279ms)
Sep  5 07:42:32.469: INFO: (17) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 4.904499ms)
Sep  5 07:42:32.472: INFO: (17) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 7.920266ms)
Sep  5 07:42:32.473: INFO: (17) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 8.300324ms)
Sep  5 07:42:32.473: INFO: (17) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 8.427484ms)
Sep  5 07:42:32.473: INFO: (17) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 8.459644ms)
Sep  5 07:42:32.473: INFO: (17) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 8.409244ms)
Sep  5 07:42:32.473: INFO: (17) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 9.1302ms)
Sep  5 07:42:32.474: INFO: (17) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 9.553059ms)
Sep  5 07:42:32.474: INFO: (17) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 9.585558ms)
Sep  5 07:42:32.474: INFO: (17) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 9.659218ms)
Sep  5 07:42:32.474: INFO: (17) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 10.154236ms)
Sep  5 07:42:32.475: INFO: (17) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 10.335876ms)
Sep  5 07:42:32.475: INFO: (17) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 10.707774ms)
Sep  5 07:42:32.475: INFO: (17) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 10.758234ms)
Sep  5 07:42:32.509: INFO: (17) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 45.223065ms)
Sep  5 07:42:32.517: INFO: (18) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 7.458208ms)
Sep  5 07:42:32.517: INFO: (18) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 7.315888ms)
Sep  5 07:42:32.517: INFO: (18) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 7.457768ms)
Sep  5 07:42:32.517: INFO: (18) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 7.422788ms)
Sep  5 07:42:32.517: INFO: (18) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 7.817186ms)
Sep  5 07:42:32.518: INFO: (18) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 7.991545ms)
Sep  5 07:42:32.518: INFO: (18) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 8.017605ms)
Sep  5 07:42:32.518: INFO: (18) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 8.762103ms)
Sep  5 07:42:32.521: INFO: (18) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 11.438991ms)
Sep  5 07:42:32.521: INFO: (18) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 11.367551ms)
Sep  5 07:42:32.521: INFO: (18) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 11.671549ms)
Sep  5 07:42:32.521: INFO: (18) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 11.64381ms)
Sep  5 07:42:32.521: INFO: (18) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 11.693229ms)
Sep  5 07:42:32.521: INFO: (18) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 11.901908ms)
Sep  5 07:42:32.521: INFO: (18) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 11.927508ms)
Sep  5 07:42:32.523: INFO: (18) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 13.143083ms)
Sep  5 07:42:32.529: INFO: (19) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 6.296893ms)
Sep  5 07:42:32.530: INFO: (19) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">test<... (200; 7.358529ms)
Sep  5 07:42:32.530: INFO: (19) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:162/proxy/: bar (200; 7.424568ms)
Sep  5 07:42:32.530: INFO: (19) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:462/proxy/: tls qux (200; 7.265449ms)
Sep  5 07:42:32.530: INFO: (19) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 7.334289ms)
Sep  5 07:42:32.531: INFO: (19) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:460/proxy/: tls baz (200; 7.691607ms)
Sep  5 07:42:32.531: INFO: (19) /api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/https:proxy-service-rrmln-8hv2f:443/proxy/tlsrewritem... (200; 7.905326ms)
Sep  5 07:42:32.531: INFO: (19) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f:160/proxy/: foo (200; 8.328124ms)
Sep  5 07:42:32.531: INFO: (19) /api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/proxy-service-rrmln-8hv2f/proxy/rewriteme">test</a> (200; 8.375724ms)
Sep  5 07:42:32.531: INFO: (19) /api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-7235/pods/http:proxy-service-rrmln-8hv2f:1080/proxy/rewriteme">... (200; 8.466404ms)
Sep  5 07:42:32.532: INFO: (19) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname1/proxy/: foo (200; 9.27814ms)
Sep  5 07:42:32.532: INFO: (19) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname1/proxy/: foo (200; 9.369179ms)
Sep  5 07:42:32.533: INFO: (19) /api/v1/namespaces/proxy-7235/services/http:proxy-service-rrmln:portname2/proxy/: bar (200; 9.539119ms)
Sep  5 07:42:32.533: INFO: (19) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname2/proxy/: tls qux (200; 9.724338ms)
Sep  5 07:42:32.533: INFO: (19) /api/v1/namespaces/proxy-7235/services/proxy-service-rrmln:portname2/proxy/: bar (200; 9.829998ms)
Sep  5 07:42:32.533: INFO: (19) /api/v1/namespaces/proxy-7235/services/https:proxy-service-rrmln:tlsportname1/proxy/: tls baz (200; 9.859858ms)
STEP: deleting ReplicationController proxy-service-rrmln in namespace proxy-7235, will wait for the garbage collector to delete the pods
Sep  5 07:42:32.593: INFO: Deleting ReplicationController proxy-service-rrmln took: 7.147729ms
Sep  5 07:42:32.693: INFO: Terminating ReplicationController proxy-service-rrmln pods took: 100.212588ms
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:42:44.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7235" for this suite.
Sep  5 07:42:50.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:42:50.271: INFO: namespace proxy-7235 deletion completed in 6.172753731s

• [SLOW TEST:32.480 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:42:50.271: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6448
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:42:54.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6448" for this suite.
Sep  5 07:43:00.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:43:00.807: INFO: namespace emptydir-wrapper-6448 deletion completed in 6.156114s

• [SLOW TEST:10.536 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:43:00.808: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3780
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-1c9e2a20-f927-4cc9-b39e-a11e04a27ecb
Sep  5 07:43:01.111: INFO: Pod name my-hostname-basic-1c9e2a20-f927-4cc9-b39e-a11e04a27ecb: Found 0 pods out of 1
Sep  5 07:43:06.115: INFO: Pod name my-hostname-basic-1c9e2a20-f927-4cc9-b39e-a11e04a27ecb: Found 1 pods out of 1
Sep  5 07:43:06.115: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1c9e2a20-f927-4cc9-b39e-a11e04a27ecb" are running
Sep  5 07:43:06.118: INFO: Pod "my-hostname-basic-1c9e2a20-f927-4cc9-b39e-a11e04a27ecb-jmm4k" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 07:43:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 07:43:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 07:43:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 07:42:53 +0000 UTC Reason: Message:}])
Sep  5 07:43:06.118: INFO: Trying to dial the pod
Sep  5 07:43:11.133: INFO: Controller my-hostname-basic-1c9e2a20-f927-4cc9-b39e-a11e04a27ecb: Got expected result from replica 1 [my-hostname-basic-1c9e2a20-f927-4cc9-b39e-a11e04a27ecb-jmm4k]: "my-hostname-basic-1c9e2a20-f927-4cc9-b39e-a11e04a27ecb-jmm4k", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:43:11.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3780" for this suite.
Sep  5 07:43:17.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:43:17.294: INFO: namespace replication-controller-3780 deletion completed in 6.156794132s

• [SLOW TEST:16.486 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:43:17.295: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-56272894-9683-4165-bb23-f9da3e06f997 in namespace container-probe-9577
Sep  5 07:43:23.584: INFO: Started pod busybox-56272894-9683-4165-bb23-f9da3e06f997 in namespace container-probe-9577
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 07:43:23.587: INFO: Initial restart count of pod busybox-56272894-9683-4165-bb23-f9da3e06f997 is 0
Sep  5 07:44:13.747: INFO: Restart count of pod container-probe-9577/busybox-56272894-9683-4165-bb23-f9da3e06f997 is now 1 (50.160078454s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:44:13.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9577" for this suite.
Sep  5 07:44:19.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:44:19.946: INFO: namespace container-probe-9577 deletion completed in 6.100521098s

• [SLOW TEST:62.651 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:44:19.947: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-4898
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Sep  5 07:44:20.190: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4898" to be "success or failure"
Sep  5 07:44:20.196: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 5.574336ms
Sep  5 07:44:22.222: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032013366s
Sep  5 07:44:24.226: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035629973s
STEP: Saw pod success
Sep  5 07:44:24.226: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Sep  5 07:44:24.229: INFO: Trying to get logs from node slave1 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep  5 07:44:24.265: INFO: Waiting for pod pod-host-path-test to disappear
Sep  5 07:44:24.271: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:44:24.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4898" for this suite.
Sep  5 07:44:30.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:44:30.389: INFO: namespace hostpath-4898 deletion completed in 6.109121738s

• [SLOW TEST:10.443 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:44:30.390: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3573
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-9be74767-6c39-4c93-b24e-1ecc3b07f838
STEP: Creating a pod to test consume configMaps
Sep  5 07:44:30.655: INFO: Waiting up to 5m0s for pod "pod-configmaps-686016da-67a1-4363-a447-bad1e21d561d" in namespace "configmap-3573" to be "success or failure"
Sep  5 07:44:30.712: INFO: Pod "pod-configmaps-686016da-67a1-4363-a447-bad1e21d561d": Phase="Pending", Reason="", readiness=false. Elapsed: 56.271037ms
Sep  5 07:44:32.716: INFO: Pod "pod-configmaps-686016da-67a1-4363-a447-bad1e21d561d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060596962s
Sep  5 07:44:34.753: INFO: Pod "pod-configmaps-686016da-67a1-4363-a447-bad1e21d561d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097486265s
Sep  5 07:44:36.757: INFO: Pod "pod-configmaps-686016da-67a1-4363-a447-bad1e21d561d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.101250071s
STEP: Saw pod success
Sep  5 07:44:36.757: INFO: Pod "pod-configmaps-686016da-67a1-4363-a447-bad1e21d561d" satisfied condition "success or failure"
Sep  5 07:44:36.809: INFO: Trying to get logs from node slave1 pod pod-configmaps-686016da-67a1-4363-a447-bad1e21d561d container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 07:44:36.846: INFO: Waiting for pod pod-configmaps-686016da-67a1-4363-a447-bad1e21d561d to disappear
Sep  5 07:44:36.852: INFO: Pod pod-configmaps-686016da-67a1-4363-a447-bad1e21d561d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:44:36.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3573" for this suite.
Sep  5 07:44:42.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:44:42.978: INFO: namespace configmap-3573 deletion completed in 6.122103819s

• [SLOW TEST:12.589 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:44:42.979: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1275
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-9fcf8359-4972-4b82-bd9d-03b4b697278f
STEP: Creating a pod to test consume configMaps
Sep  5 07:44:43.195: INFO: Waiting up to 5m0s for pod "pod-configmaps-3d41dc52-3881-4689-abcc-cf87161d96e9" in namespace "configmap-1275" to be "success or failure"
Sep  5 07:44:43.227: INFO: Pod "pod-configmaps-3d41dc52-3881-4689-abcc-cf87161d96e9": Phase="Pending", Reason="", readiness=false. Elapsed: 31.359645ms
Sep  5 07:44:45.230: INFO: Pod "pod-configmaps-3d41dc52-3881-4689-abcc-cf87161d96e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034854751s
Sep  5 07:44:47.303: INFO: Pod "pod-configmaps-3d41dc52-3881-4689-abcc-cf87161d96e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.107081041s
Sep  5 07:44:49.307: INFO: Pod "pod-configmaps-3d41dc52-3881-4689-abcc-cf87161d96e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.111281944s
STEP: Saw pod success
Sep  5 07:44:49.307: INFO: Pod "pod-configmaps-3d41dc52-3881-4689-abcc-cf87161d96e9" satisfied condition "success or failure"
Sep  5 07:44:49.310: INFO: Trying to get logs from node slave1 pod pod-configmaps-3d41dc52-3881-4689-abcc-cf87161d96e9 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 07:44:49.367: INFO: Waiting for pod pod-configmaps-3d41dc52-3881-4689-abcc-cf87161d96e9 to disappear
Sep  5 07:44:49.384: INFO: Pod pod-configmaps-3d41dc52-3881-4689-abcc-cf87161d96e9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:44:49.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1275" for this suite.
Sep  5 07:44:55.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:44:55.533: INFO: namespace configmap-1275 deletion completed in 6.144340279s

• [SLOW TEST:12.554 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:44:55.533: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6198
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-cc7f28c7-f6dc-4ab9-b9b2-be5b068ccd0a
STEP: Creating a pod to test consume secrets
Sep  5 07:44:55.777: INFO: Waiting up to 5m0s for pod "pod-secrets-4780156e-0761-4055-ac12-7f68ba14e1ac" in namespace "secrets-6198" to be "success or failure"
Sep  5 07:44:55.783: INFO: Pod "pod-secrets-4780156e-0761-4055-ac12-7f68ba14e1ac": Phase="Pending", Reason="", readiness=false. Elapsed: 5.756075ms
Sep  5 07:44:57.792: INFO: Pod "pod-secrets-4780156e-0761-4055-ac12-7f68ba14e1ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014355518s
Sep  5 07:44:59.797: INFO: Pod "pod-secrets-4780156e-0761-4055-ac12-7f68ba14e1ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019099838s
STEP: Saw pod success
Sep  5 07:44:59.797: INFO: Pod "pod-secrets-4780156e-0761-4055-ac12-7f68ba14e1ac" satisfied condition "success or failure"
Sep  5 07:44:59.802: INFO: Trying to get logs from node slave1 pod pod-secrets-4780156e-0761-4055-ac12-7f68ba14e1ac container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 07:44:59.852: INFO: Waiting for pod pod-secrets-4780156e-0761-4055-ac12-7f68ba14e1ac to disappear
Sep  5 07:44:59.858: INFO: Pod pod-secrets-4780156e-0761-4055-ac12-7f68ba14e1ac no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:44:59.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6198" for this suite.
Sep  5 07:45:05.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:45:05.966: INFO: namespace secrets-6198 deletion completed in 6.103322234s

• [SLOW TEST:10.433 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:45:05.967: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1732
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  5 07:45:10.712: INFO: Successfully updated pod "pod-update-activedeadlineseconds-fe4f1b04-994b-4ab0-8989-752af3698aed"
Sep  5 07:45:10.712: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-fe4f1b04-994b-4ab0-8989-752af3698aed" in namespace "pods-1732" to be "terminated due to deadline exceeded"
Sep  5 07:45:10.750: INFO: Pod "pod-update-activedeadlineseconds-fe4f1b04-994b-4ab0-8989-752af3698aed": Phase="Running", Reason="", readiness=true. Elapsed: 37.885136ms
Sep  5 07:45:12.754: INFO: Pod "pod-update-activedeadlineseconds-fe4f1b04-994b-4ab0-8989-752af3698aed": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.042397196s
Sep  5 07:45:12.754: INFO: Pod "pod-update-activedeadlineseconds-fe4f1b04-994b-4ab0-8989-752af3698aed" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:45:12.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1732" for this suite.
Sep  5 07:45:18.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:45:18.993: INFO: namespace pods-1732 deletion completed in 6.233930646s

• [SLOW TEST:13.027 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:45:18.994: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5355
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-c79e3a66-0718-4bab-810b-8c5822c1f793
STEP: Creating a pod to test consume configMaps
Sep  5 07:45:19.246: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7168a880-aa11-4aec-9ca7-8bcd0d73218e" in namespace "projected-5355" to be "success or failure"
Sep  5 07:45:19.255: INFO: Pod "pod-projected-configmaps-7168a880-aa11-4aec-9ca7-8bcd0d73218e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.618698ms
Sep  5 07:45:21.259: INFO: Pod "pod-projected-configmaps-7168a880-aa11-4aec-9ca7-8bcd0d73218e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013323601s
Sep  5 07:45:23.263: INFO: Pod "pod-projected-configmaps-7168a880-aa11-4aec-9ca7-8bcd0d73218e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017044283s
STEP: Saw pod success
Sep  5 07:45:23.263: INFO: Pod "pod-projected-configmaps-7168a880-aa11-4aec-9ca7-8bcd0d73218e" satisfied condition "success or failure"
Sep  5 07:45:23.265: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-7168a880-aa11-4aec-9ca7-8bcd0d73218e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 07:45:23.335: INFO: Waiting for pod pod-projected-configmaps-7168a880-aa11-4aec-9ca7-8bcd0d73218e to disappear
Sep  5 07:45:23.355: INFO: Pod pod-projected-configmaps-7168a880-aa11-4aec-9ca7-8bcd0d73218e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:45:23.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5355" for this suite.
Sep  5 07:45:29.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:45:29.464: INFO: namespace projected-5355 deletion completed in 6.099109167s

• [SLOW TEST:10.470 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:45:29.464: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8178
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Sep  5 07:45:29.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 create -f - --namespace=kubectl-8178'
Sep  5 07:45:32.437: INFO: stderr: ""
Sep  5 07:45:32.437: INFO: stdout: "pod/pause created\n"
Sep  5 07:45:32.437: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  5 07:45:32.438: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8178" to be "running and ready"
Sep  5 07:45:32.508: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 70.765999ms
Sep  5 07:45:34.512: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074462378s
Sep  5 07:45:36.516: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.078386515s
Sep  5 07:45:36.516: INFO: Pod "pause" satisfied condition "running and ready"
Sep  5 07:45:36.516: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Sep  5 07:45:36.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 label pods pause testing-label=testing-label-value --namespace=kubectl-8178'
Sep  5 07:45:36.693: INFO: stderr: ""
Sep  5 07:45:36.694: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep  5 07:45:36.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pod pause -L testing-label --namespace=kubectl-8178'
Sep  5 07:45:36.877: INFO: stderr: ""
Sep  5 07:45:36.877: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep  5 07:45:36.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 label pods pause testing-label- --namespace=kubectl-8178'
Sep  5 07:45:37.051: INFO: stderr: ""
Sep  5 07:45:37.051: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep  5 07:45:37.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pod pause -L testing-label --namespace=kubectl-8178'
Sep  5 07:45:37.227: INFO: stderr: ""
Sep  5 07:45:37.228: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Sep  5 07:45:37.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete --grace-period=0 --force -f - --namespace=kubectl-8178'
Sep  5 07:45:37.414: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 07:45:37.415: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  5 07:45:37.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get rc,svc -l name=pause --no-headers --namespace=kubectl-8178'
Sep  5 07:45:37.602: INFO: stderr: "No resources found.\n"
Sep  5 07:45:37.602: INFO: stdout: ""
Sep  5 07:45:37.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -l name=pause --namespace=kubectl-8178 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 07:45:37.771: INFO: stderr: ""
Sep  5 07:45:37.771: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:45:37.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8178" for this suite.
Sep  5 07:45:43.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:45:43.923: INFO: namespace kubectl-8178 deletion completed in 6.123092048s

• [SLOW TEST:14.459 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:45:43.923: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-501
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 07:45:44.149: INFO: Creating deployment "nginx-deployment"
Sep  5 07:45:44.153: INFO: Waiting for observed generation 1
Sep  5 07:45:46.166: INFO: Waiting for all required pods to come up
Sep  5 07:45:46.173: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep  5 07:45:52.207: INFO: Waiting for deployment "nginx-deployment" to complete
Sep  5 07:45:52.214: INFO: Updating deployment "nginx-deployment" with a non-existent image
Sep  5 07:45:52.221: INFO: Updating deployment nginx-deployment
Sep  5 07:45:52.221: INFO: Waiting for observed generation 2
Sep  5 07:45:54.284: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  5 07:45:54.287: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  5 07:45:54.291: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep  5 07:45:54.301: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  5 07:45:54.301: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  5 07:45:54.305: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep  5 07:45:54.313: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Sep  5 07:45:54.313: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Sep  5 07:45:54.322: INFO: Updating deployment nginx-deployment
Sep  5 07:45:54.322: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Sep  5 07:45:54.336: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  5 07:45:54.346: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep  5 07:45:54.495: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-501,SelfLink:/apis/apps/v1/namespaces/deployment-501/deployments/nginx-deployment,UID:cd3727ec-ea48-41f1-99df-210d6aa79957,ResourceVersion:228037,Generation:3,CreationTimestamp:2019-09-05 07:45:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-09-05 07:45:44 +0000 UTC 2019-09-05 07:45:36 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2019-09-05 07:45:46 +0000 UTC 2019-09-05 07:45:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Sep  5 07:45:54.584: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-501,SelfLink:/apis/apps/v1/namespaces/deployment-501/replicasets/nginx-deployment-55fb7cb77f,UID:38dedf91-d161-4795-ae70-0f09838ba173,ResourceVersion:228031,Generation:3,CreationTimestamp:2019-09-05 07:45:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment cd3727ec-ea48-41f1-99df-210d6aa79957 0x40039f1b27 0x40039f1b28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  5 07:45:54.584: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Sep  5 07:45:54.585: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-501,SelfLink:/apis/apps/v1/namespaces/deployment-501/replicasets/nginx-deployment-7b8c6f4498,UID:33d2c906-5761-4eab-8f31-f28a7d0c4233,ResourceVersion:228066,Generation:3,CreationTimestamp:2019-09-05 07:45:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment cd3727ec-ea48-41f1-99df-210d6aa79957 0x40039f1bf7 0x40039f1bf8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Sep  5 07:45:54.659: INFO: Pod "nginx-deployment-55fb7cb77f-5rg65" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-5rg65,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-55fb7cb77f-5rg65,UID:e3dadf54-2e06-4364-9255-48222552bb4e,ResourceVersion:228038,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 38dedf91-d161-4795-ae70-0f09838ba173 0x400329e837 0x400329e838}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329e8b0} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329e8d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.660: INFO: Pod "nginx-deployment-55fb7cb77f-6b4td" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6b4td,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-55fb7cb77f-6b4td,UID:9b81ac3d-158b-4e08-a451-4b07c381a5cc,ResourceVersion:228078,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 38dedf91-d161-4795-ae70-0f09838ba173 0x400329e950 0x400329e951}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329e9d0} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329e9f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.660: INFO: Pod "nginx-deployment-55fb7cb77f-6wjnl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6wjnl,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-55fb7cb77f-6wjnl,UID:efc6244e-db48-443d-9503-40857e18450e,ResourceVersion:228004,Generation:0,CreationTimestamp:2019-09-05 07:45:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 38dedf91-d161-4795-ae70-0f09838ba173 0x400329ea90 0x400329ea91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329eb10} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329eb30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:44 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.30,PodIP:,StartTime:2019-09-05 07:45:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.660: INFO: Pod "nginx-deployment-55fb7cb77f-9ft69" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9ft69,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-55fb7cb77f-9ft69,UID:88700030-d0f6-4494-9108-be72353a0830,ResourceVersion:228069,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 38dedf91-d161-4795-ae70-0f09838ba173 0x400329ec00 0x400329ec01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329ec80} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329eca0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.661: INFO: Pod "nginx-deployment-55fb7cb77f-bps7j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-bps7j,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-55fb7cb77f-bps7j,UID:8bd935dd-1ae7-47cc-9577-eacc7e5c853d,ResourceVersion:228016,Generation:0,CreationTimestamp:2019-09-05 07:45:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 38dedf91-d161-4795-ae70-0f09838ba173 0x400329ed20 0x400329ed21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329eda0} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329edc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:44 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.2,PodIP:,StartTime:2019-09-05 07:45:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.661: INFO: Pod "nginx-deployment-55fb7cb77f-d4vl2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-d4vl2,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-55fb7cb77f-d4vl2,UID:f5e80fd2-0354-4fa6-b40a-d05a4868a946,ResourceVersion:227990,Generation:0,CreationTimestamp:2019-09-05 07:45:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 38dedf91-d161-4795-ae70-0f09838ba173 0x400329ee90 0x400329ee91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329ef10} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329ef30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:44 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.30,PodIP:,StartTime:2019-09-05 07:45:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.662: INFO: Pod "nginx-deployment-55fb7cb77f-d8xsg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-d8xsg,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-55fb7cb77f-d8xsg,UID:ba61dcfb-1c15-4493-bff6-422ac7d49ae5,ResourceVersion:228076,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 38dedf91-d161-4795-ae70-0f09838ba173 0x400329f000 0x400329f001}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329f080} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329f0a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.662: INFO: Pod "nginx-deployment-55fb7cb77f-f78vs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-f78vs,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-55fb7cb77f-f78vs,UID:0f59f022-d165-4e03-b633-69abf18b0c72,ResourceVersion:227991,Generation:0,CreationTimestamp:2019-09-05 07:45:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 38dedf91-d161-4795-ae70-0f09838ba173 0x400329f120 0x400329f121}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329f1a0} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329f1c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:44 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.2,PodIP:,StartTime:2019-09-05 07:45:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.662: INFO: Pod "nginx-deployment-55fb7cb77f-gtrdx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-gtrdx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-55fb7cb77f-gtrdx,UID:6c409d06-e0c4-429f-b62e-d83cc89d83c5,ResourceVersion:228079,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 38dedf91-d161-4795-ae70-0f09838ba173 0x400329f290 0x400329f291}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329f310} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329f330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.663: INFO: Pod "nginx-deployment-55fb7cb77f-hzp2s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hzp2s,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-55fb7cb77f-hzp2s,UID:2aa07c6f-94bc-4a8b-a6c6-c9feb5b5c421,ResourceVersion:228084,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 38dedf91-d161-4795-ae70-0f09838ba173 0x400329f3b0 0x400329f3b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329f430} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329f450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.2,PodIP:,StartTime:2019-09-05 07:45:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.674: INFO: Pod "nginx-deployment-55fb7cb77f-lt7k5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-lt7k5,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-55fb7cb77f-lt7k5,UID:b8b9587d-2709-463e-9399-a28ad2b04c34,ResourceVersion:228061,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 38dedf91-d161-4795-ae70-0f09838ba173 0x400329f520 0x400329f521}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329f5a0} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329f5c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.675: INFO: Pod "nginx-deployment-55fb7cb77f-mjgcj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mjgcj,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-55fb7cb77f-mjgcj,UID:0ada1404-cf27-45e2-b93f-4a6f251aa186,ResourceVersion:228018,Generation:0,CreationTimestamp:2019-09-05 07:45:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 38dedf91-d161-4795-ae70-0f09838ba173 0x400329f640 0x400329f641}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329f6c0} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329f6e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:44 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.30,PodIP:,StartTime:2019-09-05 07:45:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.675: INFO: Pod "nginx-deployment-55fb7cb77f-zx495" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-zx495,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-55fb7cb77f-zx495,UID:5f58c39b-0407-4d7b-a6b2-e3a088582a06,ResourceVersion:228071,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 38dedf91-d161-4795-ae70-0f09838ba173 0x400329f7b0 0x400329f7b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329f830} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329f850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.675: INFO: Pod "nginx-deployment-7b8c6f4498-2cwmp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-2cwmp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-2cwmp,UID:2cb929ac-28c0-4fd3-a5dd-5dc2fb998936,ResourceVersion:227917,Generation:0,CreationTimestamp:2019-09-05 07:45:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x400329f8d0 0x400329f8d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329f940} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329f960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:36 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:36 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.2,PodIP:172.31.161.58,StartTime:2019-09-05 07:45:36 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 07:45:40 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://95de7587eb006567fbf6f8b5b2cdca8cc24958c749a34496b20b51ee04e4ed57}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.676: INFO: Pod "nginx-deployment-7b8c6f4498-462bn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-462bn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-462bn,UID:79a87ee3-e71f-407f-94db-9d22a62909f3,ResourceVersion:227930,Generation:0,CreationTimestamp:2019-09-05 07:45:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x400329fa30 0x400329fa31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329faa0} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329fac0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:36 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:36 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.2,PodIP:172.31.161.56,StartTime:2019-09-05 07:45:36 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 07:45:40 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://3ff2f63bdb67c158273eed341b25d88513d1bfcde2808f03eb7b2763ae166a92}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.676: INFO: Pod "nginx-deployment-7b8c6f4498-4xhvb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4xhvb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-4xhvb,UID:aee41383-7c60-4e7b-9aa3-b6ed3f9a1a1f,ResourceVersion:228085,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x400329fb90 0x400329fb91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329fc00} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329fc20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.30,PodIP:,StartTime:2019-09-05 07:45:54 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.677: INFO: Pod "nginx-deployment-7b8c6f4498-5rmbp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5rmbp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-5rmbp,UID:29cbd357-867d-41c9-b495-af0c4f51978d,ResourceVersion:228039,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x400329fce7 0x400329fce8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329fd60} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329fd80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.677: INFO: Pod "nginx-deployment-7b8c6f4498-5s7hl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5s7hl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-5s7hl,UID:7bda6cdd-a376-4b65-b02c-17252ccb3c9c,ResourceVersion:227956,Generation:0,CreationTimestamp:2019-09-05 07:45:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x400329fe00 0x400329fe01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329fe70} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329fe90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:36 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:36 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.2,PodIP:172.31.161.60,StartTime:2019-09-05 07:45:36 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 07:45:40 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://50b29857b2e18ec08d128d23b637fea5853b90bee14ca4362117f6cebc613711}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.677: INFO: Pod "nginx-deployment-7b8c6f4498-697lv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-697lv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-697lv,UID:4d789817-99ae-4208-8e34-616451de0c79,ResourceVersion:228073,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x400329ff60 0x400329ff61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400329ffd0} {node.kubernetes.io/unreachable Exists  NoExecute 0x400329fff0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.678: INFO: Pod "nginx-deployment-7b8c6f4498-d4lrt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-d4lrt,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-d4lrt,UID:a24df9eb-4534-4aeb-9560-1b3170f21168,ResourceVersion:228042,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x4003184070 0x4003184071}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40031840e0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003184100}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.682: INFO: Pod "nginx-deployment-7b8c6f4498-dpkrv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dpkrv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-dpkrv,UID:af257c84-37fb-4774-a1f3-b16eaa04f633,ResourceVersion:228077,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x4003184180 0x4003184181}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40031841f0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003184210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.683: INFO: Pod "nginx-deployment-7b8c6f4498-dzz6b" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dzz6b,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-dzz6b,UID:e0f550e3-2fa7-4184-99dd-c78aa85d84fc,ResourceVersion:227923,Generation:0,CreationTimestamp:2019-09-05 07:45:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x4003184290 0x4003184291}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003184300} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003184320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:36 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.30,PodIP:172.31.51.73,StartTime:2019-09-05 07:45:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 07:45:48 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c888dbdddcf6d4704d30c853780aa016d5809f6572641cfe20bfb09c07d2f683}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.683: INFO: Pod "nginx-deployment-7b8c6f4498-g7p74" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-g7p74,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-g7p74,UID:022897b9-ba07-4d79-9592-be97b2ff8f95,ResourceVersion:228075,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x40031843f7 0x40031843f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003184470} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003184490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.684: INFO: Pod "nginx-deployment-7b8c6f4498-gp2x6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gp2x6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-gp2x6,UID:4d325dc3-a319-4966-9bcc-31c5515f5974,ResourceVersion:228074,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x4003184510 0x4003184511}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003184580} {node.kubernetes.io/unreachable Exists  NoExecute 0x40031845a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.684: INFO: Pod "nginx-deployment-7b8c6f4498-jcd98" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jcd98,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-jcd98,UID:1fe5e053-ed9e-4b45-94bb-2b487a81c28f,ResourceVersion:228070,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x4003184620 0x4003184621}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003184690} {node.kubernetes.io/unreachable Exists  NoExecute 0x40031846b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.684: INFO: Pod "nginx-deployment-7b8c6f4498-m8ctb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-m8ctb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-m8ctb,UID:585c92c3-cdca-4c7e-91dc-1687a1a7901e,ResourceVersion:227944,Generation:0,CreationTimestamp:2019-09-05 07:45:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x4003184730 0x4003184731}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40031847a0} {node.kubernetes.io/unreachable Exists  NoExecute 0x40031847c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:36 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.30,PodIP:172.31.51.210,StartTime:2019-09-05 07:45:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 07:45:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://42069bdf80ea45b59307dd4b52b667bc167d97541795bd835f9244943dcc3a04}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.685: INFO: Pod "nginx-deployment-7b8c6f4498-pxmxp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-pxmxp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-pxmxp,UID:a51b74b1-a8eb-49e5-87c2-2c3391a3b2a4,ResourceVersion:227953,Generation:0,CreationTimestamp:2019-09-05 07:45:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x4003184897 0x4003184898}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003184910} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003184930}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:36 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:36 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.2,PodIP:172.31.161.61,StartTime:2019-09-05 07:45:36 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 07:45:41 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8e4f26b2e6067aa9d86b7c02240930f1e2607c9b0913ae1eae653b2b1ce1dbd0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.685: INFO: Pod "nginx-deployment-7b8c6f4498-sp5dn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-sp5dn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-sp5dn,UID:db3ba7c1-9457-424a-9c2d-84c1f4b57373,ResourceVersion:228063,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x4003184a00 0x4003184a01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003184a70} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003184a90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.686: INFO: Pod "nginx-deployment-7b8c6f4498-tstdr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tstdr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-tstdr,UID:2a08ebfa-615a-445b-866b-cbae10fcbb1d,ResourceVersion:227941,Generation:0,CreationTimestamp:2019-09-05 07:45:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x4003184b10 0x4003184b11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003184b80} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003184ba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:36 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.30,PodIP:172.31.51.216,StartTime:2019-09-05 07:45:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 07:45:49 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://38cdef0c48d8dca12bf73bec49d894b71e7885a82ab6373e55159f1a018e1410}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.686: INFO: Pod "nginx-deployment-7b8c6f4498-vsfrv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vsfrv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-vsfrv,UID:2f8606bf-7a79-4a0c-a6cb-6ce0b23743c7,ResourceVersion:228050,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x4003184c77 0x4003184c78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003184cf0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003184d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.687: INFO: Pod "nginx-deployment-7b8c6f4498-wx7cl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wx7cl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-wx7cl,UID:86c92394-ff6f-4734-8b9d-86046f6416a2,ResourceVersion:228053,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x4003184d90 0x4003184d91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003184e00} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003184e20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.687: INFO: Pod "nginx-deployment-7b8c6f4498-x5ttn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-x5ttn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-x5ttn,UID:b2d674f5-c8d9-4edc-9869-9c58bc3cfb95,ResourceVersion:228062,Generation:0,CreationTimestamp:2019-09-05 07:45:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x4003184ea0 0x4003184ea1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003184f10} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003184f30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  5 07:45:54.688: INFO: Pod "nginx-deployment-7b8c6f4498-xjmbp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xjmbp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-501,SelfLink:/api/v1/namespaces/deployment-501/pods/nginx-deployment-7b8c6f4498-xjmbp,UID:6516e147-f8dd-447e-8724-d02e15559f8d,ResourceVersion:227950,Generation:0,CreationTimestamp:2019-09-05 07:45:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 33d2c906-5761-4eab-8f31-f28a7d0c4233 0x4003184fb0 0x4003184fb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tpxft {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tpxft,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tpxft true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003185020} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003185040}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:36 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 07:45:36 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.2,PodIP:172.31.161.57,StartTime:2019-09-05 07:45:36 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-05 07:45:41 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ac2c8fa8a9f8e062941c163039ea9faf63b83a5601d8c0334f25158c7967fffc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:45:54.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-501" for this suite.
Sep  5 07:46:10.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:46:11.091: INFO: namespace deployment-501 deletion completed in 16.294349257s

• [SLOW TEST:27.167 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:46:11.093: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7598
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:46:15.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7598" for this suite.
Sep  5 07:46:21.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:46:21.511: INFO: namespace kubelet-test-7598 deletion completed in 6.15773161s

• [SLOW TEST:10.418 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:46:21.512: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Sep  5 07:46:21.739: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  5 07:46:21.747: INFO: Waiting for terminating namespaces to be deleted...
Sep  5 07:46:21.751: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Sep  5 07:46:21.763: INFO: kube-scheduler-master1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 07:46:21.763: INFO: metrics-server-687c949fd7-k6ctz from kube-system started at 2019-09-02 10:10:34 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.763: INFO: 	Container metrics-server ready: true, restart count 7
Sep  5 07:46:21.763: INFO: update-demo-nautilus-bwggq from kubectl-3647 started at 2019-09-04 03:18:12 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.763: INFO: 	Container update-demo ready: true, restart count 4
Sep  5 07:46:21.763: INFO: resource-reserver-master1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 07:46:21.764: INFO: coredns-84957c5548-fzgvg from kube-system started at 2019-09-03 09:41:40 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.764: INFO: 	Container coredns ready: true, restart count 6
Sep  5 07:46:21.764: INFO: ss-1 from statefulset-8350 started at 2019-09-04 07:06:28 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.764: INFO: 	Container nginx ready: true, restart count 2
Sep  5 07:46:21.764: INFO: calico-node-vb6dx from kube-system started at 2019-09-04 08:01:05 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.764: INFO: 	Container calico-node ready: true, restart count 2
Sep  5 07:46:21.764: INFO: simpletest.rc-hc5gf from gc-4645 started at 2019-09-05 07:11:19 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.764: INFO: 	Container nginx ready: true, restart count 0
Sep  5 07:46:21.764: INFO: kube-apiserver-master1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 07:46:21.764: INFO: calico-kube-controllers-769b56f588-r7cwg from kube-system started at 2019-09-02 10:08:06 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.764: INFO: 	Container calico-kube-controllers ready: true, restart count 7
Sep  5 07:46:21.764: INFO: tiller-deploy-6cfdc48f4f-7pm5m from kube-system started at 2019-09-02 10:09:09 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.764: INFO: 	Container tiller ready: true, restart count 7
Sep  5 07:46:21.764: INFO: simpletest.rc-zvjjm from gc-4645 started at 2019-09-05 07:11:20 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.764: INFO: 	Container nginx ready: true, restart count 0
Sep  5 07:46:21.764: INFO: kube-controller-manager-master1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 07:46:21.764: INFO: simpletest.rc-htxhl from gc-4645 started at 2019-09-05 07:11:19 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.764: INFO: 	Container nginx ready: true, restart count 0
Sep  5 07:46:21.764: INFO: simpletest.rc-xdhdv from gc-4645 started at 2019-09-05 07:11:19 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.764: INFO: 	Container nginx ready: true, restart count 0
Sep  5 07:46:21.764: INFO: netserver-0 from pod-network-test-8884 started at 2019-09-03 11:09:14 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.764: INFO: 	Container webserver ready: true, restart count 6
Sep  5 07:46:21.764: INFO: kube-proxy-master1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 07:46:21.764: INFO: simpletest.rc-wfnx5 from gc-4645 started at 2019-09-05 07:11:20 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.764: INFO: 	Container nginx ready: true, restart count 0
Sep  5 07:46:21.764: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Sep  5 07:46:21.777: INFO: test-webserver-edbf445f-a7f4-42d8-aa6f-ff39e531c400 from container-probe-1206 started at 2019-09-04 08:35:03 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container test-webserver ready: false, restart count 1
Sep  5 07:46:21.777: INFO: downwardapi-volume-e50296f7-5d62-48e2-99cc-cd1cbab6f497 from downward-api-6195 started at 2019-09-03 11:57:56 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container client-container ready: false, restart count 0
Sep  5 07:46:21.777: INFO: calico-node-tkcp8 from kube-system started at 2019-09-04 08:00:57 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container calico-node ready: true, restart count 1
Sep  5 07:46:21.777: INFO: kube-proxy-slave1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 07:46:21.777: INFO: update-demo-nautilus-tgtmr from kubectl-3647 started at 2019-09-04 03:18:12 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container update-demo ready: true, restart count 2
Sep  5 07:46:21.777: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-05 07:15:58 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  5 07:46:21.777: INFO: nginx-proxy-slave1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 07:46:21.777: INFO: ss-0 from statefulset-8350 started at 2019-09-04 07:05:55 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container nginx ready: true, restart count 1
Sep  5 07:46:21.777: INFO: ss-2 from statefulset-8350 started at 2019-09-04 07:06:18 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container nginx ready: true, restart count 1
Sep  5 07:46:21.777: INFO: simpletest.rc-t6282 from gc-4645 started at 2019-09-05 07:11:27 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container nginx ready: true, restart count 0
Sep  5 07:46:21.777: INFO: netserver-1 from pod-network-test-8884 started at 2019-09-03 11:09:14 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container webserver ready: true, restart count 3
Sep  5 07:46:21.777: INFO: simpletest.rc-khvx2 from gc-4645 started at 2019-09-05 07:11:28 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container nginx ready: true, restart count 0
Sep  5 07:46:21.777: INFO: simpletest.rc-rmlpm from gc-4645 started at 2019-09-05 07:11:28 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container nginx ready: true, restart count 0
Sep  5 07:46:21.777: INFO: busybox-d60e7c8e-4409-459a-b131-36debda93eda from container-probe-6051 started at 2019-09-04 02:41:13 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container busybox ready: true, restart count 105
Sep  5 07:46:21.777: INFO: sonobuoy-e2e-job-88e7e0d06e6141c3 from heptio-sonobuoy started at 2019-09-05 07:16:01 +0000 UTC (2 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container e2e ready: true, restart count 0
Sep  5 07:46:21.777: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  5 07:46:21.777: INFO: simpletest.rc-q64hx from gc-4645 started at 2019-09-05 07:11:27 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container nginx ready: true, restart count 0
Sep  5 07:46:21.777: INFO: update-demo-kitten-gz5rs from kubectl-3647 started at 2019-09-04 03:18:19 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container update-demo ready: true, restart count 2
Sep  5 07:46:21.777: INFO: simpletest.rc-bbp2f from gc-4645 started at 2019-09-05 07:11:27 +0000 UTC (1 container statuses recorded)
Sep  5 07:46:21.777: INFO: 	Container nginx ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node master1
STEP: verifying the node has the label node slave1
Sep  5 07:46:21.906: INFO: Pod test-webserver-edbf445f-a7f4-42d8-aa6f-ff39e531c400 requesting resource cpu=0m on Node slave1
Sep  5 07:46:21.906: INFO: Pod busybox-d60e7c8e-4409-459a-b131-36debda93eda requesting resource cpu=0m on Node slave1
Sep  5 07:46:21.906: INFO: Pod simpletest.rc-bbp2f requesting resource cpu=0m on Node slave1
Sep  5 07:46:21.906: INFO: Pod simpletest.rc-hc5gf requesting resource cpu=0m on Node master1
Sep  5 07:46:21.906: INFO: Pod simpletest.rc-htxhl requesting resource cpu=0m on Node master1
Sep  5 07:46:21.906: INFO: Pod simpletest.rc-khvx2 requesting resource cpu=0m on Node slave1
Sep  5 07:46:21.906: INFO: Pod simpletest.rc-q64hx requesting resource cpu=0m on Node slave1
Sep  5 07:46:21.907: INFO: Pod simpletest.rc-rmlpm requesting resource cpu=0m on Node slave1
Sep  5 07:46:21.907: INFO: Pod simpletest.rc-t6282 requesting resource cpu=0m on Node slave1
Sep  5 07:46:21.907: INFO: Pod simpletest.rc-wfnx5 requesting resource cpu=0m on Node master1
Sep  5 07:46:21.907: INFO: Pod simpletest.rc-xdhdv requesting resource cpu=0m on Node master1
Sep  5 07:46:21.907: INFO: Pod simpletest.rc-zvjjm requesting resource cpu=0m on Node master1
Sep  5 07:46:21.907: INFO: Pod sonobuoy requesting resource cpu=0m on Node slave1
Sep  5 07:46:21.907: INFO: Pod sonobuoy-e2e-job-88e7e0d06e6141c3 requesting resource cpu=0m on Node slave1
Sep  5 07:46:21.907: INFO: Pod calico-kube-controllers-769b56f588-r7cwg requesting resource cpu=30m on Node master1
Sep  5 07:46:21.907: INFO: Pod calico-node-tkcp8 requesting resource cpu=150m on Node slave1
Sep  5 07:46:21.907: INFO: Pod calico-node-vb6dx requesting resource cpu=150m on Node master1
Sep  5 07:46:21.907: INFO: Pod coredns-84957c5548-fzgvg requesting resource cpu=100m on Node master1
Sep  5 07:46:21.907: INFO: Pod kube-apiserver-master1 requesting resource cpu=100m on Node master1
Sep  5 07:46:21.907: INFO: Pod kube-controller-manager-master1 requesting resource cpu=100m on Node master1
Sep  5 07:46:21.907: INFO: Pod kube-proxy-master1 requesting resource cpu=150m on Node master1
Sep  5 07:46:21.907: INFO: Pod kube-proxy-slave1 requesting resource cpu=150m on Node slave1
Sep  5 07:46:21.907: INFO: Pod kube-scheduler-master1 requesting resource cpu=80m on Node master1
Sep  5 07:46:21.907: INFO: Pod metrics-server-687c949fd7-k6ctz requesting resource cpu=0m on Node master1
Sep  5 07:46:21.907: INFO: Pod nginx-proxy-slave1 requesting resource cpu=25m on Node slave1
Sep  5 07:46:21.907: INFO: Pod resource-reserver-master1 requesting resource cpu=800m on Node master1
Sep  5 07:46:21.907: INFO: Pod tiller-deploy-6cfdc48f4f-7pm5m requesting resource cpu=0m on Node master1
Sep  5 07:46:21.907: INFO: Pod update-demo-kitten-gz5rs requesting resource cpu=0m on Node slave1
Sep  5 07:46:21.907: INFO: Pod update-demo-nautilus-bwggq requesting resource cpu=0m on Node master1
Sep  5 07:46:21.907: INFO: Pod update-demo-nautilus-tgtmr requesting resource cpu=0m on Node slave1
Sep  5 07:46:21.907: INFO: Pod netserver-0 requesting resource cpu=0m on Node master1
Sep  5 07:46:21.907: INFO: Pod netserver-1 requesting resource cpu=0m on Node slave1
Sep  5 07:46:21.907: INFO: Pod ss-0 requesting resource cpu=0m on Node slave1
Sep  5 07:46:21.907: INFO: Pod ss-1 requesting resource cpu=0m on Node master1
Sep  5 07:46:21.907: INFO: Pod ss-2 requesting resource cpu=0m on Node slave1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3a8e03e6-b8dd-4844-bbdf-cf9d70e302b1.15c17b2beee2f79e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8972/filler-pod-3a8e03e6-b8dd-4844-bbdf-cf9d70e302b1 to slave1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3a8e03e6-b8dd-4844-bbdf-cf9d70e302b1.15c17b2e50d4ed7a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3a8e03e6-b8dd-4844-bbdf-cf9d70e302b1.15c17b2e5e361683], Reason = [Created], Message = [Created container filler-pod-3a8e03e6-b8dd-4844-bbdf-cf9d70e302b1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3a8e03e6-b8dd-4844-bbdf-cf9d70e302b1.15c17b2e7d36c065], Reason = [Started], Message = [Started container filler-pod-3a8e03e6-b8dd-4844-bbdf-cf9d70e302b1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d9ffcab-7b4f-494d-b8b7-d34f7f972be9.15c17b2bea0c15ff], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8972/filler-pod-6d9ffcab-7b4f-494d-b8b7-d34f7f972be9 to master1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d9ffcab-7b4f-494d-b8b7-d34f7f972be9.15c17b2c6b6382a7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d9ffcab-7b4f-494d-b8b7-d34f7f972be9.15c17b2c7bcc9d34], Reason = [Created], Message = [Created container filler-pod-6d9ffcab-7b4f-494d-b8b7-d34f7f972be9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d9ffcab-7b4f-494d-b8b7-d34f7f972be9.15c17b2c9875ea61], Reason = [Started], Message = [Started container filler-pod-6d9ffcab-7b4f-494d-b8b7-d34f7f972be9]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15c17b2cdf3c547e], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node master1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node slave1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:46:27.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8972" for this suite.
Sep  5 07:46:35.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:46:35.263: INFO: namespace sched-pred-8972 deletion completed in 8.109208081s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:13.751 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:46:35.263: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1781
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 07:46:35.551: INFO: Creating ReplicaSet my-hostname-basic-5b29c318-26af-4e42-a359-22577a3baaf7
Sep  5 07:46:35.566: INFO: Pod name my-hostname-basic-5b29c318-26af-4e42-a359-22577a3baaf7: Found 0 pods out of 1
Sep  5 07:46:40.607: INFO: Pod name my-hostname-basic-5b29c318-26af-4e42-a359-22577a3baaf7: Found 1 pods out of 1
Sep  5 07:46:40.607: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-5b29c318-26af-4e42-a359-22577a3baaf7" is running
Sep  5 07:46:40.610: INFO: Pod "my-hostname-basic-5b29c318-26af-4e42-a359-22577a3baaf7-qjmw7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 07:46:35 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 07:46:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 07:46:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-05 07:46:27 +0000 UTC Reason: Message:}])
Sep  5 07:46:40.610: INFO: Trying to dial the pod
Sep  5 07:46:45.708: INFO: Controller my-hostname-basic-5b29c318-26af-4e42-a359-22577a3baaf7: Got expected result from replica 1 [my-hostname-basic-5b29c318-26af-4e42-a359-22577a3baaf7-qjmw7]: "my-hostname-basic-5b29c318-26af-4e42-a359-22577a3baaf7-qjmw7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:46:45.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1781" for this suite.
Sep  5 07:46:51.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:46:51.825: INFO: namespace replicaset-1781 deletion completed in 6.113072633s

• [SLOW TEST:16.562 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:46:51.826: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5310
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5310
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  5 07:46:52.077: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  5 07:47:16.278: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.31.51.239:8080/dial?request=hostName&protocol=http&host=172.31.161.72&port=8080&tries=1'] Namespace:pod-network-test-5310 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 07:47:16.278: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 07:47:16.572: INFO: Waiting for endpoints: map[]
Sep  5 07:47:16.576: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.31.51.239:8080/dial?request=hostName&protocol=http&host=172.31.51.240&port=8080&tries=1'] Namespace:pod-network-test-5310 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 07:47:16.576: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 07:47:16.851: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:47:16.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5310" for this suite.
Sep  5 07:47:38.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:47:38.990: INFO: namespace pod-network-test-5310 deletion completed in 22.127638675s

• [SLOW TEST:47.164 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:47:38.990: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-7f1afa49-3bef-4070-a255-8e2961c6dd2c
STEP: Creating a pod to test consume secrets
Sep  5 07:47:39.210: INFO: Waiting up to 5m0s for pod "pod-secrets-0f1fc368-59b5-4aea-ae87-9caafab1e1e5" in namespace "secrets-7730" to be "success or failure"
Sep  5 07:47:39.266: INFO: Pod "pod-secrets-0f1fc368-59b5-4aea-ae87-9caafab1e1e5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.130616ms
Sep  5 07:47:41.279: INFO: Pod "pod-secrets-0f1fc368-59b5-4aea-ae87-9caafab1e1e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068705719s
Sep  5 07:47:43.283: INFO: Pod "pod-secrets-0f1fc368-59b5-4aea-ae87-9caafab1e1e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072382897s
STEP: Saw pod success
Sep  5 07:47:43.283: INFO: Pod "pod-secrets-0f1fc368-59b5-4aea-ae87-9caafab1e1e5" satisfied condition "success or failure"
Sep  5 07:47:43.285: INFO: Trying to get logs from node slave1 pod pod-secrets-0f1fc368-59b5-4aea-ae87-9caafab1e1e5 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 07:47:43.309: INFO: Waiting for pod pod-secrets-0f1fc368-59b5-4aea-ae87-9caafab1e1e5 to disappear
Sep  5 07:47:43.315: INFO: Pod pod-secrets-0f1fc368-59b5-4aea-ae87-9caafab1e1e5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:47:43.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7730" for this suite.
Sep  5 07:47:49.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:47:49.457: INFO: namespace secrets-7730 deletion completed in 6.137677446s

• [SLOW TEST:10.467 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:47:49.458: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3374
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 07:47:49.694: INFO: Create a RollingUpdate DaemonSet
Sep  5 07:47:49.700: INFO: Check that daemon pods launch on every node of the cluster
Sep  5 07:47:49.761: INFO: Number of nodes with available pods: 0
Sep  5 07:47:49.761: INFO: Node master1 is running more than one daemon pod
Sep  5 07:47:50.770: INFO: Number of nodes with available pods: 0
Sep  5 07:47:50.770: INFO: Node master1 is running more than one daemon pod
Sep  5 07:47:51.836: INFO: Number of nodes with available pods: 0
Sep  5 07:47:51.836: INFO: Node master1 is running more than one daemon pod
Sep  5 07:47:52.772: INFO: Number of nodes with available pods: 0
Sep  5 07:47:52.772: INFO: Node master1 is running more than one daemon pod
Sep  5 07:47:53.770: INFO: Number of nodes with available pods: 1
Sep  5 07:47:53.770: INFO: Node slave1 is running more than one daemon pod
Sep  5 07:47:54.769: INFO: Number of nodes with available pods: 2
Sep  5 07:47:54.769: INFO: Number of running nodes: 2, number of available pods: 2
Sep  5 07:47:54.769: INFO: Update the DaemonSet to trigger a rollout
Sep  5 07:47:54.777: INFO: Updating DaemonSet daemon-set
Sep  5 07:48:04.822: INFO: Roll back the DaemonSet before rollout is complete
Sep  5 07:48:04.829: INFO: Updating DaemonSet daemon-set
Sep  5 07:48:04.829: INFO: Make sure DaemonSet rollback is complete
Sep  5 07:48:04.839: INFO: Wrong image for pod: daemon-set-zzws4. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  5 07:48:04.839: INFO: Pod daemon-set-zzws4 is not available
Sep  5 07:48:05.878: INFO: Wrong image for pod: daemon-set-zzws4. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  5 07:48:05.879: INFO: Pod daemon-set-zzws4 is not available
Sep  5 07:48:06.879: INFO: Wrong image for pod: daemon-set-zzws4. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  5 07:48:06.879: INFO: Pod daemon-set-zzws4 is not available
Sep  5 07:48:07.898: INFO: Pod daemon-set-t5gxf is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3374, will wait for the garbage collector to delete the pods
Sep  5 07:48:07.994: INFO: Deleting DaemonSet.extensions daemon-set took: 6.571334ms
Sep  5 07:48:08.395: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.230241ms
Sep  5 07:48:11.398: INFO: Number of nodes with available pods: 0
Sep  5 07:48:11.398: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 07:48:11.400: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3374/daemonsets","resourceVersion":"229022"},"items":null}

Sep  5 07:48:11.403: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3374/pods","resourceVersion":"229022"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:48:11.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3374" for this suite.
Sep  5 07:48:17.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:48:17.544: INFO: namespace daemonsets-3374 deletion completed in 6.126153319s

• [SLOW TEST:28.086 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:48:17.545: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1041
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:48:17.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1041" for this suite.
Sep  5 07:48:39.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:48:39.924: INFO: namespace pods-1041 deletion completed in 22.128788801s

• [SLOW TEST:22.380 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:48:39.925: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4496
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 07:48:40.140: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c61f8d09-ee37-4ad2-9337-1843e2c8c115" in namespace "downward-api-4496" to be "success or failure"
Sep  5 07:48:40.173: INFO: Pod "downwardapi-volume-c61f8d09-ee37-4ad2-9337-1843e2c8c115": Phase="Pending", Reason="", readiness=false. Elapsed: 33.222707ms
Sep  5 07:48:42.177: INFO: Pod "downwardapi-volume-c61f8d09-ee37-4ad2-9337-1843e2c8c115": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037348514s
Sep  5 07:48:44.181: INFO: Pod "downwardapi-volume-c61f8d09-ee37-4ad2-9337-1843e2c8c115": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041034782s
Sep  5 07:48:46.185: INFO: Pod "downwardapi-volume-c61f8d09-ee37-4ad2-9337-1843e2c8c115": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04469353s
STEP: Saw pod success
Sep  5 07:48:46.185: INFO: Pod "downwardapi-volume-c61f8d09-ee37-4ad2-9337-1843e2c8c115" satisfied condition "success or failure"
Sep  5 07:48:46.188: INFO: Trying to get logs from node slave1 pod downwardapi-volume-c61f8d09-ee37-4ad2-9337-1843e2c8c115 container client-container: <nil>
STEP: delete the pod
Sep  5 07:48:46.234: INFO: Waiting for pod downwardapi-volume-c61f8d09-ee37-4ad2-9337-1843e2c8c115 to disappear
Sep  5 07:48:46.241: INFO: Pod downwardapi-volume-c61f8d09-ee37-4ad2-9337-1843e2c8c115 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:48:46.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4496" for this suite.
Sep  5 07:48:52.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:48:52.351: INFO: namespace downward-api-4496 deletion completed in 6.105869762s

• [SLOW TEST:12.426 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:48:52.352: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6571
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 07:48:52.536: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:48:56.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6571" for this suite.
Sep  5 07:49:48.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:49:48.930: INFO: namespace pods-6571 deletion completed in 52.110695769s

• [SLOW TEST:56.578 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:49:48.930: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Sep  5 07:49:49.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 --namespace=kubectl-3108 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Sep  5 07:49:53.908: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Sep  5 07:49:53.908: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:49:55.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3108" for this suite.
Sep  5 07:50:02.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:50:02.163: INFO: namespace kubectl-3108 deletion completed in 6.24553235s

• [SLOW TEST:13.233 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:50:02.164: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4102
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  5 07:50:02.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4102'
Sep  5 07:50:02.576: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  5 07:50:02.576: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Sep  5 07:50:02.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete jobs e2e-test-nginx-job --namespace=kubectl-4102'
Sep  5 07:50:02.775: INFO: stderr: ""
Sep  5 07:50:02.775: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:50:02.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4102" for this suite.
Sep  5 07:50:24.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:50:24.915: INFO: namespace kubectl-4102 deletion completed in 22.132458324s

• [SLOW TEST:22.751 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:50:24.916: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6563
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep  5 07:50:32.192: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:50:33.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6563" for this suite.
Sep  5 07:51:09.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:51:09.349: INFO: namespace replicaset-6563 deletion completed in 36.122767418s

• [SLOW TEST:44.434 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:51:09.350: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Sep  5 07:51:13.660: INFO: Pod pod-hostip-a5077f4b-c2be-4f1e-a469-feaff6be80a6 has hostIP: 10.200.72.30
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:51:13.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1159" for this suite.
Sep  5 07:51:35.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:51:35.783: INFO: namespace pods-1159 deletion completed in 22.118498339s

• [SLOW TEST:26.433 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:51:35.783: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1030
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Sep  5 07:51:40.083: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-815508381 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Sep  5 07:51:55.250: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:51:55.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1030" for this suite.
Sep  5 07:52:01.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:52:01.362: INFO: namespace pods-1030 deletion completed in 6.104524238s

• [SLOW TEST:25.579 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:52:01.362: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3046
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:52:05.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3046" for this suite.
Sep  5 07:52:57.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:52:57.786: INFO: namespace kubelet-test-3046 deletion completed in 52.159052877s

• [SLOW TEST:56.423 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:52:57.786: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2
Sep  5 07:52:58.031: INFO: Found 0 stateful pods, waiting for 1
Sep  5 07:53:08.036: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep  5 07:53:08.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 07:53:08.670: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 07:53:08.670: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 07:53:08.670: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 07:53:08.675: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  5 07:53:18.680: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 07:53:18.680: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 07:53:18.712: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999942s
Sep  5 07:53:19.716: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994575482s
Sep  5 07:53:20.720: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99010334s
Sep  5 07:53:21.728: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985703237s
Sep  5 07:53:22.732: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.978419786s
Sep  5 07:53:23.748: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.973851725s
Sep  5 07:53:24.752: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.958101428s
Sep  5 07:53:25.805: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.953995945s
Sep  5 07:53:26.809: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.900821039s
Sep  5 07:53:27.814: INFO: Verifying statefulset ss doesn't scale past 1 for another 896.677176ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2
Sep  5 07:53:28.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:53:29.303: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  5 07:53:29.303: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 07:53:29.303: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 07:53:29.308: INFO: Found 1 stateful pods, waiting for 3
Sep  5 07:53:39.313: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 07:53:39.313: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 07:53:39.313: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep  5 07:53:39.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 07:53:39.766: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 07:53:39.766: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 07:53:39.766: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 07:53:39.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 07:53:40.294: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 07:53:40.294: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 07:53:40.294: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 07:53:40.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 07:53:40.813: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 07:53:40.813: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 07:53:40.813: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 07:53:40.813: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 07:53:40.816: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep  5 07:53:50.825: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 07:53:50.825: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 07:53:50.825: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 07:53:50.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999942s
Sep  5 07:53:51.856: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981746715s
Sep  5 07:53:52.861: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.976690858s
Sep  5 07:53:53.866: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.971402941s
Sep  5 07:53:54.871: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.966693143s
Sep  5 07:53:55.876: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.961120448s
Sep  5 07:53:56.881: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95623623s
Sep  5 07:53:57.886: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.951243113s
Sep  5 07:53:58.892: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.945932757s
Sep  5 07:53:59.897: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.621601ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2
Sep  5 07:54:00.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:54:01.376: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  5 07:54:01.376: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 07:54:01.376: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 07:54:01.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:54:01.850: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  5 07:54:01.851: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 07:54:01.851: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 07:54:01.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:54:02.341: INFO: rc: 126
Sep  5 07:54:02.342: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil> OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused "process_linux.go:91: executing setns process caused \"exit status 21\"": unknown
 command terminated with exit code 126
 [] <nil> 0x400301da10 exit status 126 <nil> <nil> true [0x40034460c0 0x40034460d8 0x40034460f0] [0x40034460c0 0x40034460d8 0x40034460f0] [0x40034460d0 0x40034460e8] [0x933a78 0x933a78] 0x4002716ea0 <nil>}:
Command stdout:
OCI runtime exec failed: exec failed: container_linux.go:345: starting container process caused "process_linux.go:91: executing setns process caused \"exit status 21\"": unknown

stderr:
command terminated with exit code 126

error:
exit status 126
Sep  5 07:54:12.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:54:12.526: INFO: rc: 1
Sep  5 07:54:12.526: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x400301dd40 exit status 1 <nil> <nil> true [0x40034460f8 0x4003446110 0x4003446128] [0x40034460f8 0x4003446110 0x4003446128] [0x4003446108 0x4003446120] [0x933a78 0x933a78] 0x4002717320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:54:22.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:54:22.716: INFO: rc: 1
Sep  5 07:54:22.717: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x40013349f0 exit status 1 <nil> <nil> true [0x4002ca49f0 0x4002ca4a08 0x4002ca4a20] [0x4002ca49f0 0x4002ca4a08 0x4002ca4a20] [0x4002ca4a00 0x4002ca4a18] [0x933a78 0x933a78] 0x4002817bc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:54:32.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:54:32.905: INFO: rc: 1
Sep  5 07:54:32.905: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002a7a360 exit status 1 <nil> <nil> true [0x4000010010 0x4000011268 0x40000113c0] [0x4000010010 0x4000011268 0x40000113c0] [0x4000011240 0x40000112f8] [0x933a78 0x933a78] 0x4002625e60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:54:42.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:54:43.076: INFO: rc: 1
Sep  5 07:54:43.077: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002a7a6c0 exit status 1 <nil> <nil> true [0x4000011530 0x4000011608 0x4000011688] [0x4000011530 0x4000011608 0x4000011688] [0x4000011600 0x4000011658] [0x933a78 0x933a78] 0x4002a42c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:54:53.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:54:53.317: INFO: rc: 1
Sep  5 07:54:53.317: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4003538300 exit status 1 <nil> <nil> true [0x400050c028 0x400050c0f8 0x400050c4d8] [0x400050c028 0x400050c0f8 0x400050c4d8] [0x400050c0c8 0x400050c2b0] [0x933a78 0x933a78] 0x400287d680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:55:03.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:55:03.505: INFO: rc: 1
Sep  5 07:55:03.505: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x40039344b0 exit status 1 <nil> <nil> true [0x40020f8010 0x40020f8048 0x40020f8088] [0x40020f8010 0x40020f8048 0x40020f8088] [0x40020f8030 0x40020f8078] [0x933a78 0x933a78] 0x40024d1380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:55:13.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:55:13.680: INFO: rc: 1
Sep  5 07:55:13.680: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4003934810 exit status 1 <nil> <nil> true [0x40020f8098 0x40020f80e8 0x40020f8100] [0x40020f8098 0x40020f80e8 0x40020f8100] [0x40020f80d8 0x40020f80f8] [0x933a78 0x933a78] 0x40022543c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:55:23.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:55:23.851: INFO: rc: 1
Sep  5 07:55:23.852: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4003538690 exit status 1 <nil> <nil> true [0x400050c500 0x400050c5c8 0x400050c708] [0x400050c500 0x400050c5c8 0x400050c708] [0x400050c5a0 0x400050c6f0] [0x933a78 0x933a78] 0x4001b9e0c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:55:33.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:55:36.440: INFO: rc: 1
Sep  5 07:55:36.441: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x40035389f0 exit status 1 <nil> <nil> true [0x400050c740 0x400050c868 0x400050c8f0] [0x400050c740 0x400050c868 0x400050c8f0] [0x400050c7a8 0x400050c8e0] [0x933a78 0x933a78] 0x40020f43c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:55:46.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:55:46.609: INFO: rc: 1
Sep  5 07:55:46.610: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002a7aab0 exit status 1 <nil> <nil> true [0x4000011730 0x40000117f8 0x40000119c0] [0x4000011730 0x40000117f8 0x40000119c0] [0x40000117a8 0x4000011990] [0x933a78 0x933a78] 0x4002a43980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:55:56.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:55:56.784: INFO: rc: 1
Sep  5 07:55:56.784: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001f0e360 exit status 1 <nil> <nil> true [0x40000ea330 0x40000ea538 0x40000ea770] [0x40000ea330 0x40000ea538 0x40000ea770] [0x40000ea4c0 0x40000ea728] [0x933a78 0x933a78] 0x4001f057a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:56:06.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:56:06.956: INFO: rc: 1
Sep  5 07:56:06.956: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001f0e6f0 exit status 1 <nil> <nil> true [0x40000ea830 0x40000ea948 0x40000eaa10] [0x40000ea830 0x40000ea948 0x40000eaa10] [0x40000ea910 0x40000ea9f0] [0x933a78 0x933a78] 0x40022cf320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:56:16.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:56:17.125: INFO: rc: 1
Sep  5 07:56:17.125: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4003538db0 exit status 1 <nil> <nil> true [0x400050c900 0x400050c9e0 0x400050cb50] [0x400050c900 0x400050c9e0 0x400050cb50] [0x400050c968 0x400050cac8] [0x933a78 0x933a78] 0x4002531860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:56:27.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:56:27.408: INFO: rc: 1
Sep  5 07:56:27.408: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4003934000 exit status 1 <nil> <nil> true [0x400050cbe0 0x400050ce00 0x400050cf48] [0x400050cbe0 0x400050ce00 0x400050cf48] [0x400050cd58 0x400050ce78] [0x933a78 0x933a78] 0x4002530420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:56:37.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:56:37.577: INFO: rc: 1
Sep  5 07:56:37.577: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001f0e300 exit status 1 <nil> <nil> true [0x40020f8010 0x40020f8048 0x40020f8088] [0x40020f8010 0x40020f8048 0x40020f8088] [0x40020f8030 0x40020f8078] [0x933a78 0x933a78] 0x4001f057a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:56:47.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:56:47.757: INFO: rc: 1
Sep  5 07:56:47.758: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001f0e6c0 exit status 1 <nil> <nil> true [0x40020f8098 0x40020f80e8 0x40020f8100] [0x40020f8098 0x40020f80e8 0x40020f8100] [0x40020f80d8 0x40020f80f8] [0x933a78 0x933a78] 0x4001f7a120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:56:57.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:56:57.944: INFO: rc: 1
Sep  5 07:56:57.944: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001f0ea20 exit status 1 <nil> <nil> true [0x40020f8138 0x40020f8170 0x40020f81b0] [0x40020f8138 0x40020f8170 0x40020f81b0] [0x40020f8168 0x40020f8180] [0x933a78 0x933a78] 0x4001b9e0c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:57:07.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:57:08.114: INFO: rc: 1
Sep  5 07:57:08.115: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001f0ed50 exit status 1 <nil> <nil> true [0x40020f81e8 0x40020f8210 0x40020f8258] [0x40020f81e8 0x40020f8210 0x40020f8258] [0x40020f8208 0x40020f8240] [0x933a78 0x933a78] 0x4001c19200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:57:18.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:57:18.285: INFO: rc: 1
Sep  5 07:57:18.286: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001f0f0b0 exit status 1 <nil> <nil> true [0x40020f8268 0x40020f8280 0x40020f82f8] [0x40020f8268 0x40020f8280 0x40020f82f8] [0x40020f8278 0x40020f82d8] [0x933a78 0x933a78] 0x4002018360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:57:28.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:57:28.458: INFO: rc: 1
Sep  5 07:57:28.458: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002a7a3f0 exit status 1 <nil> <nil> true [0x40000ea330 0x40000ea538 0x40000ea770] [0x40000ea330 0x40000ea538 0x40000ea770] [0x40000ea4c0 0x40000ea728] [0x933a78 0x933a78] 0x400287d680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:57:38.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:57:38.647: INFO: rc: 1
Sep  5 07:57:38.647: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002a7a7b0 exit status 1 <nil> <nil> true [0x40000ea830 0x40000ea948 0x40000eaa10] [0x40000ea830 0x40000ea948 0x40000eaa10] [0x40000ea910 0x40000ea9f0] [0x933a78 0x933a78] 0x4002625920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:57:48.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:57:48.903: INFO: rc: 1
Sep  5 07:57:48.903: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4003934570 exit status 1 <nil> <nil> true [0x4000010010 0x4000011268 0x40000113c0] [0x4000010010 0x4000011268 0x40000113c0] [0x4000011240 0x40000112f8] [0x933a78 0x933a78] 0x4002254960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:57:58.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:57:59.073: INFO: rc: 1
Sep  5 07:57:59.073: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002a7abd0 exit status 1 <nil> <nil> true [0x40000eaa90 0x40000ead40 0x40000eaf18] [0x40000eaa90 0x40000ead40 0x40000eaf18] [0x40000eacf0 0x40000eae60] [0x933a78 0x933a78] 0x4002a42180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:58:09.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:58:09.244: INFO: rc: 1
Sep  5 07:58:09.244: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002a7af30 exit status 1 <nil> <nil> true [0x40000eaf38 0x40000eb0c8 0x40000eb1a8] [0x40000eaf38 0x40000eb0c8 0x40000eb1a8] [0x40000eb038 0x40000eb138] [0x933a78 0x933a78] 0x4002a42f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:58:19.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:58:19.416: INFO: rc: 1
Sep  5 07:58:19.417: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4003538330 exit status 1 <nil> <nil> true [0x400050c028 0x400050c0f8 0x400050c4d8] [0x400050c028 0x400050c0f8 0x400050c4d8] [0x400050c0c8 0x400050c2b0] [0x933a78 0x933a78] 0x400209a900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:58:29.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:58:29.586: INFO: rc: 1
Sep  5 07:58:29.586: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002a7a360 exit status 1 <nil> <nil> true [0x40000ea330 0x40000ea538 0x40000ea770] [0x40000ea330 0x40000ea538 0x40000ea770] [0x40000ea4c0 0x40000ea728] [0x933a78 0x933a78] 0x4002433e60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:58:39.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:58:39.754: INFO: rc: 1
Sep  5 07:58:39.754: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001f0e360 exit status 1 <nil> <nil> true [0x40020f8010 0x40020f8048 0x40020f8088] [0x40020f8010 0x40020f8048 0x40020f8088] [0x40020f8030 0x40020f8078] [0x933a78 0x933a78] 0x40025319e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:58:49.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:58:49.930: INFO: rc: 1
Sep  5 07:58:49.930: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001f0e6f0 exit status 1 <nil> <nil> true [0x40020f8098 0x40020f80e8 0x40020f8100] [0x40020f8098 0x40020f80e8 0x40020f8100] [0x40020f80d8 0x40020f80f8] [0x933a78 0x933a78] 0x4002625ce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:58:59.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:59:00.101: INFO: rc: 1
Sep  5 07:59:00.101: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001f0ea50 exit status 1 <nil> <nil> true [0x40020f8138 0x40020f8170 0x40020f81b0] [0x40020f8138 0x40020f8170 0x40020f81b0] [0x40020f8168 0x40020f8180] [0x933a78 0x933a78] 0x400287d680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Sep  5 07:59:10.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-2 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 07:59:10.287: INFO: rc: 1
Sep  5 07:59:10.287: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Sep  5 07:59:10.287: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep  5 07:59:10.300: INFO: Deleting all statefulset in ns statefulset-2
Sep  5 07:59:10.303: INFO: Scaling statefulset ss to 0
Sep  5 07:59:10.312: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 07:59:10.314: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:59:10.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2" for this suite.
Sep  5 07:59:16.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:59:16.477: INFO: namespace statefulset-2 deletion completed in 6.14332623s

• [SLOW TEST:378.691 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:59:16.478: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4186
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-be6b20b1-56ef-430f-9627-f3ae80384762
STEP: Creating a pod to test consume secrets
Sep  5 07:59:16.713: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5d423ebb-9eab-4a22-b140-10e4be923c21" in namespace "projected-4186" to be "success or failure"
Sep  5 07:59:16.719: INFO: Pod "pod-projected-secrets-5d423ebb-9eab-4a22-b140-10e4be923c21": Phase="Pending", Reason="", readiness=false. Elapsed: 6.503694ms
Sep  5 07:59:18.723: INFO: Pod "pod-projected-secrets-5d423ebb-9eab-4a22-b140-10e4be923c21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010086009s
Sep  5 07:59:20.727: INFO: Pod "pod-projected-secrets-5d423ebb-9eab-4a22-b140-10e4be923c21": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013957162s
Sep  5 07:59:22.731: INFO: Pod "pod-projected-secrets-5d423ebb-9eab-4a22-b140-10e4be923c21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018264474s
STEP: Saw pod success
Sep  5 07:59:22.731: INFO: Pod "pod-projected-secrets-5d423ebb-9eab-4a22-b140-10e4be923c21" satisfied condition "success or failure"
Sep  5 07:59:22.734: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-5d423ebb-9eab-4a22-b140-10e4be923c21 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 07:59:22.787: INFO: Waiting for pod pod-projected-secrets-5d423ebb-9eab-4a22-b140-10e4be923c21 to disappear
Sep  5 07:59:22.801: INFO: Pod pod-projected-secrets-5d423ebb-9eab-4a22-b140-10e4be923c21 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:59:22.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4186" for this suite.
Sep  5 07:59:28.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:59:28.942: INFO: namespace projected-4186 deletion completed in 6.135670116s

• [SLOW TEST:12.464 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:59:28.942: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4236
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-98e19f45-6f8c-4b0e-ab55-9a73549052fb
STEP: Creating secret with name secret-projected-all-test-volume-b5b79239-6f0f-48da-8927-940ca7577afe
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep  5 07:59:29.153: INFO: Waiting up to 5m0s for pod "projected-volume-719368c2-ee06-4a76-9deb-8f0033fded63" in namespace "projected-4236" to be "success or failure"
Sep  5 07:59:29.161: INFO: Pod "projected-volume-719368c2-ee06-4a76-9deb-8f0033fded63": Phase="Pending", Reason="", readiness=false. Elapsed: 8.204067ms
Sep  5 07:59:31.164: INFO: Pod "projected-volume-719368c2-ee06-4a76-9deb-8f0033fded63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011736041s
Sep  5 07:59:33.168: INFO: Pod "projected-volume-719368c2-ee06-4a76-9deb-8f0033fded63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015143914s
Sep  5 07:59:35.171: INFO: Pod "projected-volume-719368c2-ee06-4a76-9deb-8f0033fded63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018656927s
STEP: Saw pod success
Sep  5 07:59:35.171: INFO: Pod "projected-volume-719368c2-ee06-4a76-9deb-8f0033fded63" satisfied condition "success or failure"
Sep  5 07:59:35.174: INFO: Trying to get logs from node slave1 pod projected-volume-719368c2-ee06-4a76-9deb-8f0033fded63 container projected-all-volume-test: <nil>
STEP: delete the pod
Sep  5 07:59:35.222: INFO: Waiting for pod projected-volume-719368c2-ee06-4a76-9deb-8f0033fded63 to disappear
Sep  5 07:59:35.233: INFO: Pod projected-volume-719368c2-ee06-4a76-9deb-8f0033fded63 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:59:35.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4236" for this suite.
Sep  5 07:59:41.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 07:59:41.341: INFO: namespace projected-4236 deletion completed in 6.104362298s

• [SLOW TEST:12.399 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 07:59:41.343: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 07:59:45.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6604" for this suite.
Sep  5 08:00:26.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:00:26.847: INFO: namespace kubelet-test-6604 deletion completed in 41.242090648s

• [SLOW TEST:45.505 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:00:26.848: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6150
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-79b03bc0-07ce-48d7-80b9-1459ca2cea20
STEP: Creating a pod to test consume configMaps
Sep  5 08:00:28.764: INFO: Waiting up to 5m0s for pod "pod-configmaps-06995adc-8910-494f-a669-d787036fc5a0" in namespace "configmap-6150" to be "success or failure"
Sep  5 08:00:29.122: INFO: Pod "pod-configmaps-06995adc-8910-494f-a669-d787036fc5a0": Phase="Pending", Reason="", readiness=false. Elapsed: 357.823951ms
Sep  5 08:00:31.171: INFO: Pod "pod-configmaps-06995adc-8910-494f-a669-d787036fc5a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407212971s
Sep  5 08:00:33.185: INFO: Pod "pod-configmaps-06995adc-8910-494f-a669-d787036fc5a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.421564492s
STEP: Saw pod success
Sep  5 08:00:33.185: INFO: Pod "pod-configmaps-06995adc-8910-494f-a669-d787036fc5a0" satisfied condition "success or failure"
Sep  5 08:00:33.209: INFO: Trying to get logs from node slave1 pod pod-configmaps-06995adc-8910-494f-a669-d787036fc5a0 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 08:00:33.500: INFO: Waiting for pod pod-configmaps-06995adc-8910-494f-a669-d787036fc5a0 to disappear
Sep  5 08:00:33.676: INFO: Pod pod-configmaps-06995adc-8910-494f-a669-d787036fc5a0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:00:33.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6150" for this suite.
Sep  5 08:00:42.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:00:42.370: INFO: namespace configmap-6150 deletion completed in 8.652310132s

• [SLOW TEST:15.523 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:00:42.371: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-642
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep  5 08:00:47.444: INFO: Successfully updated pod "labelsupdate1f8e44a6-370f-4f4f-8ef9-e845a0d40d95"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:00:49.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-642" for this suite.
Sep  5 08:01:13.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:01:13.852: INFO: namespace projected-642 deletion completed in 24.138535684s

• [SLOW TEST:31.481 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:01:13.853: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6287
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  5 08:01:14.100: INFO: Waiting up to 5m0s for pod "pod-923e4665-821f-4522-b639-5b67353334ae" in namespace "emptydir-6287" to be "success or failure"
Sep  5 08:01:14.105: INFO: Pod "pod-923e4665-821f-4522-b639-5b67353334ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.427302ms
Sep  5 08:01:16.108: INFO: Pod "pod-923e4665-821f-4522-b639-5b67353334ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00822154s
Sep  5 08:01:18.113: INFO: Pod "pod-923e4665-821f-4522-b639-5b67353334ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012338617s
Sep  5 08:01:20.116: INFO: Pod "pod-923e4665-821f-4522-b639-5b67353334ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015938755s
STEP: Saw pod success
Sep  5 08:01:20.116: INFO: Pod "pod-923e4665-821f-4522-b639-5b67353334ae" satisfied condition "success or failure"
Sep  5 08:01:20.119: INFO: Trying to get logs from node slave1 pod pod-923e4665-821f-4522-b639-5b67353334ae container test-container: <nil>
STEP: delete the pod
Sep  5 08:01:20.139: INFO: Waiting for pod pod-923e4665-821f-4522-b639-5b67353334ae to disappear
Sep  5 08:01:20.145: INFO: Pod pod-923e4665-821f-4522-b639-5b67353334ae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:01:20.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6287" for this suite.
Sep  5 08:01:26.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:01:26.252: INFO: namespace emptydir-6287 deletion completed in 6.102175784s

• [SLOW TEST:12.399 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:01:26.252: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5024
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep  5 08:01:30.603: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-7525177c-e100-4045-aa29-ef069e6a40cd,GenerateName:,Namespace:events-5024,SelfLink:/api/v1/namespaces/events-5024/pods/send-events-7525177c-e100-4045-aa29-ef069e6a40cd,UID:b1182d4f-80c5-46ab-bcd3-18b99968e313,ResourceVersion:231096,Generation:0,CreationTimestamp:2019-09-05 08:01:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 499852885,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hrd7l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hrd7l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-hrd7l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40005b99c0} {node.kubernetes.io/unreachable Exists  NoExecute 0x40005b99e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:01:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:01:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:01:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:01:18 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.30,PodIP:172.31.51.9,StartTime:2019-09-05 08:01:26 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-09-05 08:01:29 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://648d80a8b429b88dd1e4f150bededf6fcfae40e01dc38355b6685c9d1dbcefdc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Sep  5 08:01:32.608: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep  5 08:01:34.613: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:01:34.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5024" for this suite.
Sep  5 08:02:14.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:02:14.787: INFO: namespace events-5024 deletion completed in 40.107171794s

• [SLOW TEST:48.535 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:02:14.787: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6881
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  5 08:02:25.032: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:25.037: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 08:02:27.037: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:27.041: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 08:02:29.037: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:29.043: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 08:02:31.038: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:31.042: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 08:02:33.037: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:33.041: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 08:02:35.037: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:35.042: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 08:02:37.037: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:37.041: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 08:02:39.037: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:39.041: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 08:02:41.037: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:41.043: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 08:02:43.037: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:43.051: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 08:02:45.037: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:45.041: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 08:02:47.037: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:47.041: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 08:02:49.037: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:49.041: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 08:02:51.037: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:51.041: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 08:02:53.037: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:53.041: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 08:02:55.038: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 08:02:55.042: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:02:55.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6881" for this suite.
Sep  5 08:03:17.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:03:17.156: INFO: namespace container-lifecycle-hook-6881 deletion completed in 22.099401077s

• [SLOW TEST:62.369 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:03:17.156: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8971
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-a24e439c-c8c2-4c03-91d4-7f326fc73b3d
STEP: Creating a pod to test consume configMaps
Sep  5 08:03:17.398: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3b17c610-3baa-47cd-bc46-6d10e161ba1e" in namespace "projected-8971" to be "success or failure"
Sep  5 08:03:17.404: INFO: Pod "pod-projected-configmaps-3b17c610-3baa-47cd-bc46-6d10e161ba1e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.104495ms
Sep  5 08:03:19.408: INFO: Pod "pod-projected-configmaps-3b17c610-3baa-47cd-bc46-6d10e161ba1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009813927s
Sep  5 08:03:21.412: INFO: Pod "pod-projected-configmaps-3b17c610-3baa-47cd-bc46-6d10e161ba1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013870638s
STEP: Saw pod success
Sep  5 08:03:21.412: INFO: Pod "pod-projected-configmaps-3b17c610-3baa-47cd-bc46-6d10e161ba1e" satisfied condition "success or failure"
Sep  5 08:03:21.415: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-3b17c610-3baa-47cd-bc46-6d10e161ba1e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 08:03:21.439: INFO: Waiting for pod pod-projected-configmaps-3b17c610-3baa-47cd-bc46-6d10e161ba1e to disappear
Sep  5 08:03:21.445: INFO: Pod pod-projected-configmaps-3b17c610-3baa-47cd-bc46-6d10e161ba1e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:03:21.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8971" for this suite.
Sep  5 08:03:27.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:03:27.589: INFO: namespace projected-8971 deletion completed in 6.139483074s

• [SLOW TEST:10.432 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:03:27.589: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5448
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Sep  5 08:03:27.782: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-815508381 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:03:28.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5448" for this suite.
Sep  5 08:03:34.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:03:34.401: INFO: namespace kubectl-5448 deletion completed in 6.384322218s

• [SLOW TEST:6.812 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:03:34.402: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2581
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2581.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2581.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2581.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2581.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2581.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2581.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2581.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2581.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2581.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2581.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2581.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2581.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2581.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 29.240.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.240.29_udp@PTR;check="$$(dig +tcp +noall +answer +search 29.240.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.240.29_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2581.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2581.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2581.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2581.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2581.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2581.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2581.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2581.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2581.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2581.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2581.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2581.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2581.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 29.240.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.240.29_udp@PTR;check="$$(dig +tcp +noall +answer +search 29.240.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.240.29_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 08:03:40.810: INFO: Unable to read wheezy_udp@dns-test-service.dns-2581.svc.cluster.local from pod dns-2581/dns-test-465959a4-c0eb-4b23-8063-1594d4991931: the server could not find the requested resource (get pods dns-test-465959a4-c0eb-4b23-8063-1594d4991931)
Sep  5 08:03:40.814: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2581.svc.cluster.local from pod dns-2581/dns-test-465959a4-c0eb-4b23-8063-1594d4991931: the server could not find the requested resource (get pods dns-test-465959a4-c0eb-4b23-8063-1594d4991931)
Sep  5 08:03:40.818: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2581.svc.cluster.local from pod dns-2581/dns-test-465959a4-c0eb-4b23-8063-1594d4991931: the server could not find the requested resource (get pods dns-test-465959a4-c0eb-4b23-8063-1594d4991931)
Sep  5 08:03:40.822: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2581.svc.cluster.local from pod dns-2581/dns-test-465959a4-c0eb-4b23-8063-1594d4991931: the server could not find the requested resource (get pods dns-test-465959a4-c0eb-4b23-8063-1594d4991931)
Sep  5 08:03:40.899: INFO: Unable to read jessie_udp@dns-test-service.dns-2581.svc.cluster.local from pod dns-2581/dns-test-465959a4-c0eb-4b23-8063-1594d4991931: the server could not find the requested resource (get pods dns-test-465959a4-c0eb-4b23-8063-1594d4991931)
Sep  5 08:03:40.903: INFO: Unable to read jessie_tcp@dns-test-service.dns-2581.svc.cluster.local from pod dns-2581/dns-test-465959a4-c0eb-4b23-8063-1594d4991931: the server could not find the requested resource (get pods dns-test-465959a4-c0eb-4b23-8063-1594d4991931)
Sep  5 08:03:40.907: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2581.svc.cluster.local from pod dns-2581/dns-test-465959a4-c0eb-4b23-8063-1594d4991931: the server could not find the requested resource (get pods dns-test-465959a4-c0eb-4b23-8063-1594d4991931)
Sep  5 08:03:40.910: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2581.svc.cluster.local from pod dns-2581/dns-test-465959a4-c0eb-4b23-8063-1594d4991931: the server could not find the requested resource (get pods dns-test-465959a4-c0eb-4b23-8063-1594d4991931)
Sep  5 08:03:40.932: INFO: Lookups using dns-2581/dns-test-465959a4-c0eb-4b23-8063-1594d4991931 failed for: [wheezy_udp@dns-test-service.dns-2581.svc.cluster.local wheezy_tcp@dns-test-service.dns-2581.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2581.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2581.svc.cluster.local jessie_udp@dns-test-service.dns-2581.svc.cluster.local jessie_tcp@dns-test-service.dns-2581.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2581.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2581.svc.cluster.local]

Sep  5 08:03:46.062: INFO: DNS probes using dns-2581/dns-test-465959a4-c0eb-4b23-8063-1594d4991931 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:03:46.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2581" for this suite.
Sep  5 08:03:52.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:03:52.434: INFO: namespace dns-2581 deletion completed in 6.107005257s

• [SLOW TEST:18.033 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:03:52.435: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:03:52.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8972" for this suite.
Sep  5 08:03:58.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:03:58.849: INFO: namespace kubelet-test-8972 deletion completed in 6.106270657s

• [SLOW TEST:6.414 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:03:58.849: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 08:03:59.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 create -f - --namespace=kubectl-6527'
Sep  5 08:03:59.491: INFO: stderr: ""
Sep  5 08:03:59.491: INFO: stdout: "replicationcontroller/redis-master created\n"
Sep  5 08:03:59.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 create -f - --namespace=kubectl-6527'
Sep  5 08:03:59.950: INFO: stderr: ""
Sep  5 08:03:59.950: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  5 08:04:00.956: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:04:00.956: INFO: Found 0 / 1
Sep  5 08:04:01.956: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:04:01.956: INFO: Found 0 / 1
Sep  5 08:04:02.955: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:04:02.955: INFO: Found 0 / 1
Sep  5 08:04:03.955: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:04:03.955: INFO: Found 1 / 1
Sep  5 08:04:03.955: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  5 08:04:03.959: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:04:03.959: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  5 08:04:03.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 describe pod redis-master-sn66m --namespace=kubectl-6527'
Sep  5 08:04:04.158: INFO: stderr: ""
Sep  5 08:04:04.158: INFO: stdout: "Name:           redis-master-sn66m\nNamespace:      kubectl-6527\nPriority:       0\nNode:           slave1/10.200.72.30\nStart Time:     Thu, 05 Sep 2019 08:03:59 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             172.31.51.13\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://4ae90e73213d6d7b55e017bd6b2b7c0efa38118b90f8384463ac01b35cc95405\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 05 Sep 2019 08:04:02 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n2d6v (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-n2d6v:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-n2d6v\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  13s   default-scheduler  Successfully assigned kubectl-6527/redis-master-sn66m to slave1\n  Normal  Pulled     3s    kubelet, slave1    Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    3s    kubelet, slave1    Created container redis-master\n  Normal  Started    2s    kubelet, slave1    Started container redis-master\n"
Sep  5 08:04:04.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 describe rc redis-master --namespace=kubectl-6527'
Sep  5 08:04:04.403: INFO: stderr: ""
Sep  5 08:04:04.403: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-6527\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  13s   replication-controller  Created pod: redis-master-sn66m\n"
Sep  5 08:04:04.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 describe service redis-master --namespace=kubectl-6527'
Sep  5 08:04:04.591: INFO: stderr: ""
Sep  5 08:04:04.591: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-6527\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.30.191.232\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.31.51.13:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep  5 08:04:04.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 describe node master1'
Sep  5 08:04:04.835: INFO: stderr: ""
Sep  5 08:04:04.835: INFO: stdout: "Name:               master1\nRoles:              master,monitor,node\nLabels:             beta.kubernetes.io/arch=arm64\n                    beta.kubernetes.io/os=linux\n                    exportnode=iop_true\n                    kubernetes.io/arch=arm64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=true\n                    node-role.kubernetes.io/monitor=true\n                    node-role.kubernetes.io/node=true\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 02 Sep 2019 08:06:19 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 05 Sep 2019 06:55:23 +0000   Thu, 05 Sep 2019 06:55:23 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 05 Sep 2019 08:03:23 +0000   Mon, 02 Sep 2019 08:06:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 05 Sep 2019 08:03:23 +0000   Mon, 02 Sep 2019 08:06:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 05 Sep 2019 08:03:23 +0000   Mon, 02 Sep 2019 08:06:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 05 Sep 2019 08:03:23 +0000   Tue, 03 Sep 2019 07:28:58 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.200.72.2\n  Hostname:    master1\nCapacity:\n cpu:                16\n ephemeral-storage:  5805784176Ki\n hugepages-2Mi:      0\n memory:             16413272Ki\n pods:               60\nAllocatable:\n cpu:                15\n ephemeral-storage:  5795298416Ki\n hugepages-2Mi:      0\n memory:             14888984Ki\n pods:               60\nSystem Info:\n Machine ID:                 95c0306f6fad4687aec0e9da859a81b5\n System UUID:                95c0306f6fad4687aec0e9da859a81b5\n Boot ID:                    8d1c26c5-c42c-43c3-bdba-65e4aac5f8ab\n Kernel Version:             4.4.58-20180615.kylin.server.YUN+-generic\n OS Image:                   Debian GNU/Linux 9 (stretch)\n Operating System:           linux\n Architecture:               arm64\n Container Runtime Version:  docker://18.9.8\n Kubelet Version:            v1.14.3\n Kube-Proxy Version:         v1.14.3\nNon-terminated Pods:         (18 in total)\n  Namespace                  Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                        ------------  ----------  ---------------  -------------  ---\n  gc-4645                    simpletest.rc-hc5gf                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         52m\n  gc-4645                    simpletest.rc-htxhl                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         52m\n  gc-4645                    simpletest.rc-wfnx5                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         52m\n  gc-4645                    simpletest.rc-xdhdv                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         52m\n  gc-4645                    simpletest.rc-zvjjm                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         52m\n  kube-system                calico-kube-controllers-769b56f588-r7cwg    30m (0%)      1 (6%)      64Mi (0%)        4Gi (28%)      2d21h\n  kube-system                calico-node-vb6dx                           150m (1%)     1 (6%)      64Mi (0%)        4Gi (28%)      24h\n  kube-system                coredns-84957c5548-fzgvg                    100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     46h\n  kube-system                kube-apiserver-master1                      100m (0%)     2 (13%)     256Mi (1%)       6Gi (42%)      2d\n  kube-system                kube-controller-manager-master1             100m (0%)     2 (13%)     100Mi (0%)       6Gi (42%)      2d\n  kube-system                kube-proxy-master1                          150m (1%)     500m (3%)   64M (0%)         2G (13%)       2d\n  kube-system                kube-scheduler-master1                      80m (0%)      2 (13%)     170Mi (1%)       6Gi (42%)      2d\n  kube-system                metrics-server-687c949fd7-k6ctz             0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d21h\n  kube-system                resource-reserver-master1                   800m (5%)     800m (5%)   512Mi (3%)       512Mi (3%)     2d\n  kube-system                tiller-deploy-6cfdc48f4f-7pm5m              0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d21h\n  kubectl-3647               update-demo-nautilus-bwggq                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         28h\n  pod-network-test-8884      netserver-0                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         44h\n  statefulset-8350           ss-1                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                1510m (10%)     9300m (62%)\n  memory             1328164Ki (8%)  29914469Ki (200%)\n  ephemeral-storage  0 (0%)          0 (0%)\nEvents:              <none>\n"
Sep  5 08:04:04.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 describe namespace kubectl-6527'
Sep  5 08:04:05.024: INFO: stderr: ""
Sep  5 08:04:05.025: INFO: stdout: "Name:         kubectl-6527\nLabels:       e2e-framework=kubectl\n              e2e-run=cd157687-fd2b-4739-8214-fd3a982a4770\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:04:05.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6527" for this suite.
Sep  5 08:04:27.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:04:27.152: INFO: namespace kubectl-6527 deletion completed in 22.122010026s

• [SLOW TEST:28.302 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:04:27.152: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2201
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0905 08:05:07.553378      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  5 08:05:07.553: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:05:07.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2201" for this suite.
Sep  5 08:05:15.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:05:15.918: INFO: namespace gc-2201 deletion completed in 8.361241504s

• [SLOW TEST:48.766 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:05:15.921: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  5 08:05:20.480: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:05:20.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7795" for this suite.
Sep  5 08:05:26.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:05:26.656: INFO: namespace container-runtime-7795 deletion completed in 6.110427246s

• [SLOW TEST:10.735 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:05:26.656: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5295
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Sep  5 08:05:26.874: INFO: Waiting up to 5m0s for pod "var-expansion-fc37802d-1bc0-475e-bec2-73b57910ea09" in namespace "var-expansion-5295" to be "success or failure"
Sep  5 08:05:26.879: INFO: Pod "var-expansion-fc37802d-1bc0-475e-bec2-73b57910ea09": Phase="Pending", Reason="", readiness=false. Elapsed: 5.070759ms
Sep  5 08:05:28.896: INFO: Pod "var-expansion-fc37802d-1bc0-475e-bec2-73b57910ea09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022197861s
Sep  5 08:05:30.900: INFO: Pod "var-expansion-fc37802d-1bc0-475e-bec2-73b57910ea09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025740177s
STEP: Saw pod success
Sep  5 08:05:30.900: INFO: Pod "var-expansion-fc37802d-1bc0-475e-bec2-73b57910ea09" satisfied condition "success or failure"
Sep  5 08:05:30.903: INFO: Trying to get logs from node slave1 pod var-expansion-fc37802d-1bc0-475e-bec2-73b57910ea09 container dapi-container: <nil>
STEP: delete the pod
Sep  5 08:05:30.984: INFO: Waiting for pod var-expansion-fc37802d-1bc0-475e-bec2-73b57910ea09 to disappear
Sep  5 08:05:30.995: INFO: Pod var-expansion-fc37802d-1bc0-475e-bec2-73b57910ea09 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:05:30.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5295" for this suite.
Sep  5 08:05:37.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:05:37.104: INFO: namespace var-expansion-5295 deletion completed in 6.103970149s

• [SLOW TEST:10.447 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:05:37.104: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-ddf01ab9-28f0-40c8-8527-42b523679f8e
STEP: Creating a pod to test consume configMaps
Sep  5 08:05:37.347: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c91746a8-beaa-498a-ac93-3143bc5a83ed" in namespace "projected-3670" to be "success or failure"
Sep  5 08:05:37.353: INFO: Pod "pod-projected-configmaps-c91746a8-beaa-498a-ac93-3143bc5a83ed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020015ms
Sep  5 08:05:39.357: INFO: Pod "pod-projected-configmaps-c91746a8-beaa-498a-ac93-3143bc5a83ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009471271s
Sep  5 08:05:41.366: INFO: Pod "pod-projected-configmaps-c91746a8-beaa-498a-ac93-3143bc5a83ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018749063s
STEP: Saw pod success
Sep  5 08:05:41.366: INFO: Pod "pod-projected-configmaps-c91746a8-beaa-498a-ac93-3143bc5a83ed" satisfied condition "success or failure"
Sep  5 08:05:41.369: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-c91746a8-beaa-498a-ac93-3143bc5a83ed container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 08:05:41.413: INFO: Waiting for pod pod-projected-configmaps-c91746a8-beaa-498a-ac93-3143bc5a83ed to disappear
Sep  5 08:05:41.419: INFO: Pod pod-projected-configmaps-c91746a8-beaa-498a-ac93-3143bc5a83ed no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:05:41.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3670" for this suite.
Sep  5 08:05:47.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:05:47.554: INFO: namespace projected-3670 deletion completed in 6.130771955s

• [SLOW TEST:10.450 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:05:47.555: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2701
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Sep  5 08:05:47.798: INFO: Waiting up to 5m0s for pod "client-containers-023c8146-e7a3-448f-859d-c135ea1e77e8" in namespace "containers-2701" to be "success or failure"
Sep  5 08:05:47.802: INFO: Pod "client-containers-023c8146-e7a3-448f-859d-c135ea1e77e8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.940564ms
Sep  5 08:05:49.825: INFO: Pod "client-containers-023c8146-e7a3-448f-859d-c135ea1e77e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027262738s
Sep  5 08:05:51.829: INFO: Pod "client-containers-023c8146-e7a3-448f-859d-c135ea1e77e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030975011s
STEP: Saw pod success
Sep  5 08:05:51.829: INFO: Pod "client-containers-023c8146-e7a3-448f-859d-c135ea1e77e8" satisfied condition "success or failure"
Sep  5 08:05:51.833: INFO: Trying to get logs from node slave1 pod client-containers-023c8146-e7a3-448f-859d-c135ea1e77e8 container test-container: <nil>
STEP: delete the pod
Sep  5 08:05:51.861: INFO: Waiting for pod client-containers-023c8146-e7a3-448f-859d-c135ea1e77e8 to disappear
Sep  5 08:05:51.868: INFO: Pod client-containers-023c8146-e7a3-448f-859d-c135ea1e77e8 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:05:51.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2701" for this suite.
Sep  5 08:05:57.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:05:58.020: INFO: namespace containers-2701 deletion completed in 6.147794023s

• [SLOW TEST:10.465 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:05:58.020: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7644
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep  5 08:05:58.364: INFO: Pod name pod-release: Found 0 pods out of 1
Sep  5 08:06:03.368: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:06:03.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7644" for this suite.
Sep  5 08:06:09.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:06:09.670: INFO: namespace replication-controller-7644 deletion completed in 6.210793481s

• [SLOW TEST:11.651 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:06:09.672: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-7344
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7344 to expose endpoints map[]
Sep  5 08:06:09.958: INFO: Get endpoints failed (7.42679ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Sep  5 08:06:10.961: INFO: successfully validated that service endpoint-test2 in namespace services-7344 exposes endpoints map[] (1.011253947s elapsed)
STEP: Creating pod pod1 in namespace services-7344
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7344 to expose endpoints map[pod1:[80]]
Sep  5 08:06:15.034: INFO: successfully validated that service endpoint-test2 in namespace services-7344 exposes endpoints map[pod1:[80]] (4.065349585s elapsed)
STEP: Creating pod pod2 in namespace services-7344
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7344 to expose endpoints map[pod1:[80] pod2:[80]]
Sep  5 08:06:19.174: INFO: successfully validated that service endpoint-test2 in namespace services-7344 exposes endpoints map[pod1:[80] pod2:[80]] (4.135711518s elapsed)
STEP: Deleting pod pod1 in namespace services-7344
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7344 to expose endpoints map[pod2:[80]]
Sep  5 08:06:20.238: INFO: successfully validated that service endpoint-test2 in namespace services-7344 exposes endpoints map[pod2:[80]] (1.038690815s elapsed)
STEP: Deleting pod pod2 in namespace services-7344
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7344 to expose endpoints map[]
Sep  5 08:06:21.251: INFO: successfully validated that service endpoint-test2 in namespace services-7344 exposes endpoints map[] (1.008179459s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:06:21.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7344" for this suite.
Sep  5 08:06:43.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:06:43.448: INFO: namespace services-7344 deletion completed in 22.109475529s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:33.777 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:06:43.450: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0905 08:06:54.028291      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  5 08:06:54.028: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:06:54.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4964" for this suite.
Sep  5 08:07:00.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:07:00.197: INFO: namespace gc-4964 deletion completed in 6.16477777s

• [SLOW TEST:16.747 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:07:00.197: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2951
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-bb6e70ed-58cf-4e14-b56e-8037ba6e1ebd
STEP: Creating a pod to test consume secrets
Sep  5 08:07:00.428: INFO: Waiting up to 5m0s for pod "pod-secrets-a1cd16e5-757d-4436-9e91-64f3008d6313" in namespace "secrets-2951" to be "success or failure"
Sep  5 08:07:00.435: INFO: Pod "pod-secrets-a1cd16e5-757d-4436-9e91-64f3008d6313": Phase="Pending", Reason="", readiness=false. Elapsed: 7.38123ms
Sep  5 08:07:02.439: INFO: Pod "pod-secrets-a1cd16e5-757d-4436-9e91-64f3008d6313": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011043835s
Sep  5 08:07:04.443: INFO: Pod "pod-secrets-a1cd16e5-757d-4436-9e91-64f3008d6313": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014732739s
STEP: Saw pod success
Sep  5 08:07:04.443: INFO: Pod "pod-secrets-a1cd16e5-757d-4436-9e91-64f3008d6313" satisfied condition "success or failure"
Sep  5 08:07:04.446: INFO: Trying to get logs from node slave1 pod pod-secrets-a1cd16e5-757d-4436-9e91-64f3008d6313 container secret-env-test: <nil>
STEP: delete the pod
Sep  5 08:07:04.514: INFO: Waiting for pod pod-secrets-a1cd16e5-757d-4436-9e91-64f3008d6313 to disappear
Sep  5 08:07:04.525: INFO: Pod pod-secrets-a1cd16e5-757d-4436-9e91-64f3008d6313 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:07:04.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2951" for this suite.
Sep  5 08:07:10.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:07:10.681: INFO: namespace secrets-2951 deletion completed in 6.122378418s

• [SLOW TEST:10.484 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:07:10.682: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9691
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 08:07:32.951: INFO: Container started at 2019-09-05 08:07:13 +0000 UTC, pod became ready at 2019-09-05 08:07:32 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:07:32.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9691" for this suite.
Sep  5 08:07:55.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:07:55.100: INFO: namespace container-probe-9691 deletion completed in 22.103439677s

• [SLOW TEST:44.418 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:07:55.101: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-757
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-757.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-757.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-757.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-757.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 08:08:01.400: INFO: DNS probes using dns-test-c949a33a-5ab0-4e7b-9b2f-3b7da1c5821c succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-757.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-757.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-757.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-757.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 08:08:07.601: INFO: DNS probes using dns-test-ee8a49a2-22d7-43ea-9bc4-f9e43c638c87 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-757.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-757.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-757.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-757.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 08:08:13.815: INFO: DNS probes using dns-test-f386cb92-cb0d-432d-bad9-69035c406f7f succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:08:13.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-757" for this suite.
Sep  5 08:08:19.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:08:20.056: INFO: namespace dns-757 deletion completed in 6.113525969s

• [SLOW TEST:24.956 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:08:20.058: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6589
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-72ec72c3-714c-4f58-97f8-db59f849d963
STEP: Creating a pod to test consume configMaps
Sep  5 08:08:20.276: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a86f94c6-1338-4f19-88c4-be02c6b97f40" in namespace "projected-6589" to be "success or failure"
Sep  5 08:08:20.282: INFO: Pod "pod-projected-configmaps-a86f94c6-1338-4f19-88c4-be02c6b97f40": Phase="Pending", Reason="", readiness=false. Elapsed: 5.737257ms
Sep  5 08:08:22.286: INFO: Pod "pod-projected-configmaps-a86f94c6-1338-4f19-88c4-be02c6b97f40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009365412s
Sep  5 08:08:24.341: INFO: Pod "pod-projected-configmaps-a86f94c6-1338-4f19-88c4-be02c6b97f40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064630216s
STEP: Saw pod success
Sep  5 08:08:24.341: INFO: Pod "pod-projected-configmaps-a86f94c6-1338-4f19-88c4-be02c6b97f40" satisfied condition "success or failure"
Sep  5 08:08:24.344: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-a86f94c6-1338-4f19-88c4-be02c6b97f40 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 08:08:24.367: INFO: Waiting for pod pod-projected-configmaps-a86f94c6-1338-4f19-88c4-be02c6b97f40 to disappear
Sep  5 08:08:24.373: INFO: Pod pod-projected-configmaps-a86f94c6-1338-4f19-88c4-be02c6b97f40 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:08:24.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6589" for this suite.
Sep  5 08:08:30.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:08:30.588: INFO: namespace projected-6589 deletion completed in 6.210142111s

• [SLOW TEST:10.530 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:08:30.588: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4930
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 08:08:36.949: INFO: Waiting up to 5m0s for pod "client-envvars-8e9df063-16bb-40b9-91cf-4e9209453471" in namespace "pods-4930" to be "success or failure"
Sep  5 08:08:37.006: INFO: Pod "client-envvars-8e9df063-16bb-40b9-91cf-4e9209453471": Phase="Pending", Reason="", readiness=false. Elapsed: 57.563265ms
Sep  5 08:08:39.030: INFO: Pod "client-envvars-8e9df063-16bb-40b9-91cf-4e9209453471": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081319896s
Sep  5 08:08:41.034: INFO: Pod "client-envvars-8e9df063-16bb-40b9-91cf-4e9209453471": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.08481725s
STEP: Saw pod success
Sep  5 08:08:41.034: INFO: Pod "client-envvars-8e9df063-16bb-40b9-91cf-4e9209453471" satisfied condition "success or failure"
Sep  5 08:08:41.036: INFO: Trying to get logs from node slave1 pod client-envvars-8e9df063-16bb-40b9-91cf-4e9209453471 container env3cont: <nil>
STEP: delete the pod
Sep  5 08:08:41.065: INFO: Waiting for pod client-envvars-8e9df063-16bb-40b9-91cf-4e9209453471 to disappear
Sep  5 08:08:41.071: INFO: Pod client-envvars-8e9df063-16bb-40b9-91cf-4e9209453471 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:08:41.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4930" for this suite.
Sep  5 08:09:27.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:09:27.182: INFO: namespace pods-4930 deletion completed in 46.105061983s

• [SLOW TEST:56.593 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:09:27.184: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3885
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Sep  5 08:09:27.442: INFO: Waiting up to 5m0s for pod "var-expansion-56514168-ee98-4b2e-ac77-0669fdd392ed" in namespace "var-expansion-3885" to be "success or failure"
Sep  5 08:09:27.449: INFO: Pod "var-expansion-56514168-ee98-4b2e-ac77-0669fdd392ed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.570814ms
Sep  5 08:09:29.453: INFO: Pod "var-expansion-56514168-ee98-4b2e-ac77-0669fdd392ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010147361s
Sep  5 08:09:31.457: INFO: Pod "var-expansion-56514168-ee98-4b2e-ac77-0669fdd392ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014059387s
Sep  5 08:09:33.460: INFO: Pod "var-expansion-56514168-ee98-4b2e-ac77-0669fdd392ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017436414s
STEP: Saw pod success
Sep  5 08:09:33.460: INFO: Pod "var-expansion-56514168-ee98-4b2e-ac77-0669fdd392ed" satisfied condition "success or failure"
Sep  5 08:09:33.463: INFO: Trying to get logs from node slave1 pod var-expansion-56514168-ee98-4b2e-ac77-0669fdd392ed container dapi-container: <nil>
STEP: delete the pod
Sep  5 08:09:33.532: INFO: Waiting for pod var-expansion-56514168-ee98-4b2e-ac77-0669fdd392ed to disappear
Sep  5 08:09:33.539: INFO: Pod var-expansion-56514168-ee98-4b2e-ac77-0669fdd392ed no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:09:33.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3885" for this suite.
Sep  5 08:09:39.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:09:39.662: INFO: namespace var-expansion-3885 deletion completed in 6.118724659s

• [SLOW TEST:12.478 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:09:39.662: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1292
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 08:09:39.877: INFO: (0) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 5.350678ms)
Sep  5 08:09:39.882: INFO: (1) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 5.006839ms)
Sep  5 08:09:39.887: INFO: (2) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.91398ms)
Sep  5 08:09:39.892: INFO: (3) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 5.000019ms)
Sep  5 08:09:39.897: INFO: (4) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.92364ms)
Sep  5 08:09:39.902: INFO: (5) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.87082ms)
Sep  5 08:09:39.907: INFO: (6) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.91668ms)
Sep  5 08:09:39.912: INFO: (7) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.81244ms)
Sep  5 08:09:39.916: INFO: (8) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.88932ms)
Sep  5 08:09:39.921: INFO: (9) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.77248ms)
Sep  5 08:09:39.926: INFO: (10) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.729561ms)
Sep  5 08:09:39.931: INFO: (11) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.7867ms)
Sep  5 08:09:39.936: INFO: (12) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.820141ms)
Sep  5 08:09:39.940: INFO: (13) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.732281ms)
Sep  5 08:09:39.945: INFO: (14) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.736561ms)
Sep  5 08:09:39.950: INFO: (15) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.7943ms)
Sep  5 08:09:39.955: INFO: (16) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.86072ms)
Sep  5 08:09:39.960: INFO: (17) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.82502ms)
Sep  5 08:09:39.964: INFO: (18) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.538062ms)
Sep  5 08:09:39.969: INFO: (19) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.611801ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:09:39.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1292" for this suite.
Sep  5 08:09:46.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:09:46.103: INFO: namespace proxy-1292 deletion completed in 6.129509053s

• [SLOW TEST:6.440 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:09:46.104: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6587
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-ab70a71e-815c-4436-98ef-6368fc083a95
STEP: Creating secret with name s-test-opt-upd-7735d3a0-734f-4c99-adaf-1d348808080b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ab70a71e-815c-4436-98ef-6368fc083a95
STEP: Updating secret s-test-opt-upd-7735d3a0-734f-4c99-adaf-1d348808080b
STEP: Creating secret with name s-test-opt-create-31dca34d-94c0-4bc3-9b20-bf857e6391be
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:09:56.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6587" for this suite.
Sep  5 08:10:18.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:10:18.612: INFO: namespace secrets-6587 deletion completed in 22.117411592s

• [SLOW TEST:32.509 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:10:18.614: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6462
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1309
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3994
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:10:45.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6462" for this suite.
Sep  5 08:10:51.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:10:51.564: INFO: namespace namespaces-6462 deletion completed in 6.106910282s
STEP: Destroying namespace "nsdeletetest-1309" for this suite.
Sep  5 08:10:51.567: INFO: Namespace nsdeletetest-1309 was already deleted
STEP: Destroying namespace "nsdeletetest-3994" for this suite.
Sep  5 08:10:57.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:10:57.675: INFO: namespace nsdeletetest-3994 deletion completed in 6.108236893s

• [SLOW TEST:39.061 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:10:57.676: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-856f00b7-11cc-4560-b400-452a5d7815c7
STEP: Creating a pod to test consume secrets
Sep  5 08:10:57.958: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5c098fe6-4c1a-47f7-a92e-04f0250d2ba7" in namespace "projected-8349" to be "success or failure"
Sep  5 08:10:57.969: INFO: Pod "pod-projected-secrets-5c098fe6-4c1a-47f7-a92e-04f0250d2ba7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.368914ms
Sep  5 08:10:59.974: INFO: Pod "pod-projected-secrets-5c098fe6-4c1a-47f7-a92e-04f0250d2ba7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015924266s
Sep  5 08:11:01.977: INFO: Pod "pod-projected-secrets-5c098fe6-4c1a-47f7-a92e-04f0250d2ba7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019507103s
STEP: Saw pod success
Sep  5 08:11:01.977: INFO: Pod "pod-projected-secrets-5c098fe6-4c1a-47f7-a92e-04f0250d2ba7" satisfied condition "success or failure"
Sep  5 08:11:01.980: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-5c098fe6-4c1a-47f7-a92e-04f0250d2ba7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 08:11:02.020: INFO: Waiting for pod pod-projected-secrets-5c098fe6-4c1a-47f7-a92e-04f0250d2ba7 to disappear
Sep  5 08:11:02.035: INFO: Pod pod-projected-secrets-5c098fe6-4c1a-47f7-a92e-04f0250d2ba7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:11:02.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8349" for this suite.
Sep  5 08:11:08.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:11:08.194: INFO: namespace projected-8349 deletion completed in 6.148905304s

• [SLOW TEST:10.518 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:11:08.195: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9247
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1004
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2728
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:11:14.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9247" for this suite.
Sep  5 08:11:20.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:11:21.080: INFO: namespace namespaces-9247 deletion completed in 6.186142647s
STEP: Destroying namespace "nsdeletetest-1004" for this suite.
Sep  5 08:11:21.082: INFO: Namespace nsdeletetest-1004 was already deleted
STEP: Destroying namespace "nsdeletetest-2728" for this suite.
Sep  5 08:11:27.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:11:27.196: INFO: namespace nsdeletetest-2728 deletion completed in 6.113613542s

• [SLOW TEST:19.001 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:11:27.196: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-605
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Sep  5 08:11:27.416: INFO: Waiting up to 5m0s for pod "pod-a5cd05b5-5a59-4782-903e-f9c979f5ee72" in namespace "emptydir-605" to be "success or failure"
Sep  5 08:11:27.423: INFO: Pod "pod-a5cd05b5-5a59-4782-903e-f9c979f5ee72": Phase="Pending", Reason="", readiness=false. Elapsed: 7.054691ms
Sep  5 08:11:29.613: INFO: Pod "pod-a5cd05b5-5a59-4782-903e-f9c979f5ee72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.197171142s
Sep  5 08:11:31.617: INFO: Pod "pod-a5cd05b5-5a59-4782-903e-f9c979f5ee72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.200809915s
STEP: Saw pod success
Sep  5 08:11:31.617: INFO: Pod "pod-a5cd05b5-5a59-4782-903e-f9c979f5ee72" satisfied condition "success or failure"
Sep  5 08:11:31.620: INFO: Trying to get logs from node slave1 pod pod-a5cd05b5-5a59-4782-903e-f9c979f5ee72 container test-container: <nil>
STEP: delete the pod
Sep  5 08:11:31.682: INFO: Waiting for pod pod-a5cd05b5-5a59-4782-903e-f9c979f5ee72 to disappear
Sep  5 08:11:31.689: INFO: Pod pod-a5cd05b5-5a59-4782-903e-f9c979f5ee72 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:11:31.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-605" for this suite.
Sep  5 08:11:37.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:11:37.796: INFO: namespace emptydir-605 deletion completed in 6.102065545s

• [SLOW TEST:10.600 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:11:37.797: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6210
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  5 08:11:38.108: INFO: Number of nodes with available pods: 0
Sep  5 08:11:38.108: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:39.118: INFO: Number of nodes with available pods: 0
Sep  5 08:11:39.118: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:40.135: INFO: Number of nodes with available pods: 0
Sep  5 08:11:40.135: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:41.124: INFO: Number of nodes with available pods: 0
Sep  5 08:11:41.124: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:42.116: INFO: Number of nodes with available pods: 2
Sep  5 08:11:42.116: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep  5 08:11:42.135: INFO: Number of nodes with available pods: 1
Sep  5 08:11:42.135: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:43.143: INFO: Number of nodes with available pods: 1
Sep  5 08:11:43.144: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:44.234: INFO: Number of nodes with available pods: 1
Sep  5 08:11:44.234: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:45.144: INFO: Number of nodes with available pods: 1
Sep  5 08:11:45.144: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:46.144: INFO: Number of nodes with available pods: 1
Sep  5 08:11:46.144: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:47.152: INFO: Number of nodes with available pods: 1
Sep  5 08:11:47.152: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:48.144: INFO: Number of nodes with available pods: 1
Sep  5 08:11:48.144: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:49.149: INFO: Number of nodes with available pods: 1
Sep  5 08:11:49.149: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:50.144: INFO: Number of nodes with available pods: 1
Sep  5 08:11:50.144: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:51.144: INFO: Number of nodes with available pods: 1
Sep  5 08:11:51.144: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:52.143: INFO: Number of nodes with available pods: 1
Sep  5 08:11:52.143: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:53.144: INFO: Number of nodes with available pods: 1
Sep  5 08:11:53.144: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:54.198: INFO: Number of nodes with available pods: 1
Sep  5 08:11:54.198: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:55.144: INFO: Number of nodes with available pods: 1
Sep  5 08:11:55.144: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:56.143: INFO: Number of nodes with available pods: 1
Sep  5 08:11:56.144: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:57.144: INFO: Number of nodes with available pods: 1
Sep  5 08:11:57.144: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:58.143: INFO: Number of nodes with available pods: 1
Sep  5 08:11:58.143: INFO: Node master1 is running more than one daemon pod
Sep  5 08:11:59.144: INFO: Number of nodes with available pods: 1
Sep  5 08:11:59.144: INFO: Node master1 is running more than one daemon pod
Sep  5 08:12:00.144: INFO: Number of nodes with available pods: 1
Sep  5 08:12:00.144: INFO: Node master1 is running more than one daemon pod
Sep  5 08:12:01.143: INFO: Number of nodes with available pods: 1
Sep  5 08:12:01.144: INFO: Node master1 is running more than one daemon pod
Sep  5 08:12:02.143: INFO: Number of nodes with available pods: 1
Sep  5 08:12:02.143: INFO: Node master1 is running more than one daemon pod
Sep  5 08:12:03.143: INFO: Number of nodes with available pods: 2
Sep  5 08:12:03.143: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6210, will wait for the garbage collector to delete the pods
Sep  5 08:12:03.205: INFO: Deleting DaemonSet.extensions daemon-set took: 5.638117ms
Sep  5 08:12:03.505: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.192053ms
Sep  5 08:12:18.508: INFO: Number of nodes with available pods: 0
Sep  5 08:12:18.508: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 08:12:18.511: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6210/daemonsets","resourceVersion":"233636"},"items":null}

Sep  5 08:12:18.530: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6210/pods","resourceVersion":"233636"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:12:18.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6210" for this suite.
Sep  5 08:12:24.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:12:24.704: INFO: namespace daemonsets-6210 deletion completed in 6.158918496s

• [SLOW TEST:46.907 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:12:24.704: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Sep  5 08:12:24.920: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  5 08:12:24.929: INFO: Waiting for terminating namespaces to be deleted...
Sep  5 08:12:24.932: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Sep  5 08:12:24.945: INFO: netserver-0 from pod-network-test-8884 started at 2019-09-03 11:09:14 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.945: INFO: 	Container webserver ready: true, restart count 6
Sep  5 08:12:24.945: INFO: kube-proxy-master1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 08:12:24.945: INFO: simpletest.rc-wfnx5 from gc-4645 started at 2019-09-05 07:11:20 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.945: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:12:24.945: INFO: kube-scheduler-master1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 08:12:24.945: INFO: metrics-server-687c949fd7-k6ctz from kube-system started at 2019-09-02 10:10:34 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.945: INFO: 	Container metrics-server ready: true, restart count 7
Sep  5 08:12:24.945: INFO: update-demo-nautilus-bwggq from kubectl-3647 started at 2019-09-04 03:18:12 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.945: INFO: 	Container update-demo ready: true, restart count 4
Sep  5 08:12:24.945: INFO: resource-reserver-master1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 08:12:24.945: INFO: coredns-84957c5548-fzgvg from kube-system started at 2019-09-03 09:41:40 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.945: INFO: 	Container coredns ready: true, restart count 6
Sep  5 08:12:24.945: INFO: ss-1 from statefulset-8350 started at 2019-09-04 07:06:28 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.945: INFO: 	Container nginx ready: true, restart count 2
Sep  5 08:12:24.945: INFO: calico-node-vb6dx from kube-system started at 2019-09-04 08:01:05 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.945: INFO: 	Container calico-node ready: true, restart count 2
Sep  5 08:12:24.945: INFO: simpletest.rc-hc5gf from gc-4645 started at 2019-09-05 07:11:19 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.945: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:12:24.945: INFO: kube-apiserver-master1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 08:12:24.945: INFO: calico-kube-controllers-769b56f588-r7cwg from kube-system started at 2019-09-02 10:08:06 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.945: INFO: 	Container calico-kube-controllers ready: true, restart count 7
Sep  5 08:12:24.945: INFO: tiller-deploy-6cfdc48f4f-7pm5m from kube-system started at 2019-09-02 10:09:09 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.945: INFO: 	Container tiller ready: true, restart count 7
Sep  5 08:12:24.945: INFO: simpletest.rc-zvjjm from gc-4645 started at 2019-09-05 07:11:20 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.945: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:12:24.945: INFO: kube-controller-manager-master1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 08:12:24.945: INFO: simpletest.rc-htxhl from gc-4645 started at 2019-09-05 07:11:19 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.945: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:12:24.945: INFO: simpletest.rc-xdhdv from gc-4645 started at 2019-09-05 07:11:19 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.945: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:12:24.945: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Sep  5 08:12:24.959: INFO: simpletest.rc-t6282 from gc-4645 started at 2019-09-05 07:11:27 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:12:24.959: INFO: ss-2 from statefulset-8350 started at 2019-09-04 07:06:18 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container nginx ready: true, restart count 1
Sep  5 08:12:24.959: INFO: simpletest.rc-khvx2 from gc-4645 started at 2019-09-05 07:11:28 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:12:24.959: INFO: simpletest.rc-rmlpm from gc-4645 started at 2019-09-05 07:11:28 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:12:24.959: INFO: netserver-1 from pod-network-test-8884 started at 2019-09-03 11:09:14 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container webserver ready: true, restart count 3
Sep  5 08:12:24.959: INFO: sonobuoy-e2e-job-88e7e0d06e6141c3 from heptio-sonobuoy started at 2019-09-05 07:16:01 +0000 UTC (2 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container e2e ready: true, restart count 0
Sep  5 08:12:24.959: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  5 08:12:24.959: INFO: simpletest.rc-q64hx from gc-4645 started at 2019-09-05 07:11:27 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:12:24.959: INFO: busybox-d60e7c8e-4409-459a-b131-36debda93eda from container-probe-6051 started at 2019-09-04 02:41:13 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container busybox ready: true, restart count 108
Sep  5 08:12:24.959: INFO: simpletest.rc-bbp2f from gc-4645 started at 2019-09-05 07:11:27 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:12:24.959: INFO: update-demo-kitten-gz5rs from kubectl-3647 started at 2019-09-04 03:18:19 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container update-demo ready: true, restart count 2
Sep  5 08:12:24.959: INFO: test-webserver-edbf445f-a7f4-42d8-aa6f-ff39e531c400 from container-probe-1206 started at 2019-09-04 08:35:03 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container test-webserver ready: false, restart count 1
Sep  5 08:12:24.959: INFO: calico-node-tkcp8 from kube-system started at 2019-09-04 08:00:57 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container calico-node ready: true, restart count 1
Sep  5 08:12:24.959: INFO: downwardapi-volume-e50296f7-5d62-48e2-99cc-cd1cbab6f497 from downward-api-6195 started at 2019-09-03 11:57:56 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container client-container ready: false, restart count 0
Sep  5 08:12:24.959: INFO: update-demo-nautilus-tgtmr from kubectl-3647 started at 2019-09-04 03:18:12 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container update-demo ready: true, restart count 2
Sep  5 08:12:24.959: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-05 07:15:58 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  5 08:12:24.959: INFO: kube-proxy-slave1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 08:12:24.959: INFO: ss-0 from statefulset-8350 started at 2019-09-04 07:05:55 +0000 UTC (1 container statuses recorded)
Sep  5 08:12:24.959: INFO: 	Container nginx ready: true, restart count 1
Sep  5 08:12:24.959: INFO: nginx-proxy-slave1 from kube-system started at <nil> (0 container statuses recorded)
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-439d4b1c-9534-4b44-938e-57d2df926482 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-439d4b1c-9534-4b44-938e-57d2df926482 off the node slave1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-439d4b1c-9534-4b44-938e-57d2df926482
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:12:33.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9222" for this suite.
Sep  5 08:12:47.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:12:47.274: INFO: namespace sched-pred-9222 deletion completed in 14.112566879s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:22.570 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:12:47.275: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2862
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 08:12:47.522: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1682a257-6699-4ef4-ae09-a731f4abec07" in namespace "projected-2862" to be "success or failure"
Sep  5 08:12:47.529: INFO: Pod "downwardapi-volume-1682a257-6699-4ef4-ae09-a731f4abec07": Phase="Pending", Reason="", readiness=false. Elapsed: 7.677569ms
Sep  5 08:12:49.533: INFO: Pod "downwardapi-volume-1682a257-6699-4ef4-ae09-a731f4abec07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011674871s
Sep  5 08:12:51.537: INFO: Pod "downwardapi-volume-1682a257-6699-4ef4-ae09-a731f4abec07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015464495s
STEP: Saw pod success
Sep  5 08:12:51.537: INFO: Pod "downwardapi-volume-1682a257-6699-4ef4-ae09-a731f4abec07" satisfied condition "success or failure"
Sep  5 08:12:51.540: INFO: Trying to get logs from node slave1 pod downwardapi-volume-1682a257-6699-4ef4-ae09-a731f4abec07 container client-container: <nil>
STEP: delete the pod
Sep  5 08:12:51.592: INFO: Waiting for pod downwardapi-volume-1682a257-6699-4ef4-ae09-a731f4abec07 to disappear
Sep  5 08:12:51.603: INFO: Pod downwardapi-volume-1682a257-6699-4ef4-ae09-a731f4abec07 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:12:51.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2862" for this suite.
Sep  5 08:12:57.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:12:57.710: INFO: namespace projected-2862 deletion completed in 6.101484299s

• [SLOW TEST:10.435 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:12:57.710: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-552
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Sep  5 08:12:57.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 create -f - --namespace=kubectl-552'
Sep  5 08:13:00.759: INFO: stderr: ""
Sep  5 08:13:00.759: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  5 08:13:01.764: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:13:01.764: INFO: Found 0 / 1
Sep  5 08:13:02.783: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:13:02.783: INFO: Found 0 / 1
Sep  5 08:13:03.764: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:13:03.764: INFO: Found 0 / 1
Sep  5 08:13:04.763: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:13:04.763: INFO: Found 1 / 1
Sep  5 08:13:04.763: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep  5 08:13:04.767: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:13:04.767: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  5 08:13:04.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 patch pod redis-master-zkpqt --namespace=kubectl-552 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  5 08:13:04.947: INFO: stderr: ""
Sep  5 08:13:04.948: INFO: stdout: "pod/redis-master-zkpqt patched\n"
STEP: checking annotations
Sep  5 08:13:04.952: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:13:04.952: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:13:04.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-552" for this suite.
Sep  5 08:13:26.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:13:27.098: INFO: namespace kubectl-552 deletion completed in 22.140147101s

• [SLOW TEST:29.387 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:13:27.098: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-836
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-836
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-836
STEP: Creating statefulset with conflicting port in namespace statefulset-836
STEP: Waiting until pod test-pod will start running in namespace statefulset-836
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-836
Sep  5 08:13:31.364: INFO: Observed stateful pod in namespace: statefulset-836, name: ss-0, uid: 04470d1a-8ae8-4588-8228-1be308187f21, status phase: Pending. Waiting for statefulset controller to delete.
Sep  5 08:13:31.937: INFO: Observed stateful pod in namespace: statefulset-836, name: ss-0, uid: 04470d1a-8ae8-4588-8228-1be308187f21, status phase: Failed. Waiting for statefulset controller to delete.
Sep  5 08:13:31.996: INFO: Observed stateful pod in namespace: statefulset-836, name: ss-0, uid: 04470d1a-8ae8-4588-8228-1be308187f21, status phase: Failed. Waiting for statefulset controller to delete.
Sep  5 08:13:32.001: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-836
STEP: Removing pod with conflicting port in namespace statefulset-836
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-836 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep  5 08:13:38.102: INFO: Deleting all statefulset in ns statefulset-836
Sep  5 08:13:38.106: INFO: Scaling statefulset ss to 0
Sep  5 08:13:58.139: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 08:13:58.142: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:13:58.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-836" for this suite.
Sep  5 08:14:04.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:14:04.348: INFO: namespace statefulset-836 deletion completed in 6.112504752s

• [SLOW TEST:37.250 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:14:04.348: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6905
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:14:04.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6905" for this suite.
Sep  5 08:14:10.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:14:10.700: INFO: namespace services-6905 deletion completed in 6.10516328s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.352 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:14:10.701: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  5 08:14:10.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-1310'
Sep  5 08:14:11.120: INFO: stderr: ""
Sep  5 08:14:11.120: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Sep  5 08:14:16.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pod e2e-test-nginx-pod --namespace=kubectl-1310 -o json'
Sep  5 08:14:16.337: INFO: stderr: ""
Sep  5 08:14:16.337: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-09-05T08:14:03Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-1310\",\n        \"resourceVersion\": \"234163\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1310/pods/e2e-test-nginx-pod\",\n        \"uid\": \"b70f7074-c459-490f-a4aa-14e45c5a9380\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-hlgjb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"slave1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-hlgjb\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-hlgjb\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-05T08:14:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-05T08:14:14Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-05T08:14:14Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-05T08:14:03Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://7c39b2403853e13417062266f9b464691097b1ef7499b64fc8bf6c16336bdd66\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-09-05T08:14:13Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.200.72.30\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.31.51.48\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-09-05T08:14:11Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep  5 08:14:16.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 replace -f - --namespace=kubectl-1310'
Sep  5 08:14:16.777: INFO: stderr: ""
Sep  5 08:14:16.777: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Sep  5 08:14:16.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete pods e2e-test-nginx-pod --namespace=kubectl-1310'
Sep  5 08:14:21.339: INFO: stderr: ""
Sep  5 08:14:21.339: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:14:21.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1310" for this suite.
Sep  5 08:14:27.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:14:27.473: INFO: namespace kubectl-1310 deletion completed in 6.128688358s

• [SLOW TEST:16.772 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:14:27.473: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3962
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  5 08:14:35.753: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 08:14:35.757: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 08:14:37.757: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 08:14:37.760: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 08:14:39.757: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 08:14:39.760: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 08:14:41.757: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 08:14:41.760: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:14:41.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3962" for this suite.
Sep  5 08:15:03.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:15:03.984: INFO: namespace container-lifecycle-hook-3962 deletion completed in 22.208281064s

• [SLOW TEST:36.510 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:15:03.984: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8258
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  5 08:15:04.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-8258'
Sep  5 08:15:04.397: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  5 08:15:04.397: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Sep  5 08:15:06.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8258'
Sep  5 08:15:06.674: INFO: stderr: ""
Sep  5 08:15:06.674: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:15:06.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8258" for this suite.
Sep  5 08:15:28.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:15:28.854: INFO: namespace kubectl-8258 deletion completed in 22.174031655s

• [SLOW TEST:24.870 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:15:28.854: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8185
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Sep  5 08:15:29.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 cluster-info'
Sep  5 08:15:29.440: INFO: stderr: ""
Sep  5 08:15:29.440: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:15:29.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8185" for this suite.
Sep  5 08:15:35.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:15:35.694: INFO: namespace kubectl-8185 deletion completed in 6.196458218s

• [SLOW TEST:6.840 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:15:35.695: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8357
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-0c1e1f45-918f-49f4-bd0b-55453b974476
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:15:40.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8357" for this suite.
Sep  5 08:15:52.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:15:52.198: INFO: namespace configmap-8357 deletion completed in 12.106216361s

• [SLOW TEST:16.504 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:15:52.199: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-188.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-188.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-188.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-188.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-188.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-188.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 08:15:58.573: INFO: DNS probes using dns-188/dns-test-f772ea1d-9ab2-4a36-a2d8-1c8a0ee459a9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:15:58.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-188" for this suite.
Sep  5 08:16:04.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:16:04.830: INFO: namespace dns-188 deletion completed in 6.139599541s

• [SLOW TEST:12.631 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:16:04.831: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2563
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Sep  5 08:16:05.066: INFO: Waiting up to 5m0s for pod "client-containers-e0af416a-8d52-4933-8760-1d8dec5ff6a5" in namespace "containers-2563" to be "success or failure"
Sep  5 08:16:05.130: INFO: Pod "client-containers-e0af416a-8d52-4933-8760-1d8dec5ff6a5": Phase="Pending", Reason="", readiness=false. Elapsed: 63.496199ms
Sep  5 08:16:07.136: INFO: Pod "client-containers-e0af416a-8d52-4933-8760-1d8dec5ff6a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07000783s
Sep  5 08:16:09.140: INFO: Pod "client-containers-e0af416a-8d52-4933-8760-1d8dec5ff6a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073467213s
STEP: Saw pod success
Sep  5 08:16:09.140: INFO: Pod "client-containers-e0af416a-8d52-4933-8760-1d8dec5ff6a5" satisfied condition "success or failure"
Sep  5 08:16:09.143: INFO: Trying to get logs from node slave1 pod client-containers-e0af416a-8d52-4933-8760-1d8dec5ff6a5 container test-container: <nil>
STEP: delete the pod
Sep  5 08:16:09.172: INFO: Waiting for pod client-containers-e0af416a-8d52-4933-8760-1d8dec5ff6a5 to disappear
Sep  5 08:16:09.177: INFO: Pod client-containers-e0af416a-8d52-4933-8760-1d8dec5ff6a5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:16:09.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2563" for this suite.
Sep  5 08:16:15.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:16:15.310: INFO: namespace containers-2563 deletion completed in 6.122904366s

• [SLOW TEST:10.480 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:16:15.311: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5283
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-82be79b1-d875-49f3-a0ef-3629c6fa7497
STEP: Creating a pod to test consume secrets
Sep  5 08:16:15.580: INFO: Waiting up to 5m0s for pod "pod-secrets-3b194c96-7459-48a5-9635-69a7605cb730" in namespace "secrets-5283" to be "success or failure"
Sep  5 08:16:15.671: INFO: Pod "pod-secrets-3b194c96-7459-48a5-9635-69a7605cb730": Phase="Pending", Reason="", readiness=false. Elapsed: 90.284609ms
Sep  5 08:16:17.675: INFO: Pod "pod-secrets-3b194c96-7459-48a5-9635-69a7605cb730": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09410541s
Sep  5 08:16:19.694: INFO: Pod "pod-secrets-3b194c96-7459-48a5-9635-69a7605cb730": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.113333547s
STEP: Saw pod success
Sep  5 08:16:19.694: INFO: Pod "pod-secrets-3b194c96-7459-48a5-9635-69a7605cb730" satisfied condition "success or failure"
Sep  5 08:16:19.697: INFO: Trying to get logs from node slave1 pod pod-secrets-3b194c96-7459-48a5-9635-69a7605cb730 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 08:16:19.770: INFO: Waiting for pod pod-secrets-3b194c96-7459-48a5-9635-69a7605cb730 to disappear
Sep  5 08:16:19.792: INFO: Pod pod-secrets-3b194c96-7459-48a5-9635-69a7605cb730 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:16:19.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5283" for this suite.
Sep  5 08:16:25.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:16:25.959: INFO: namespace secrets-5283 deletion completed in 6.114280638s

• [SLOW TEST:10.649 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:16:25.960: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9347
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9347
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-9347
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9347
Sep  5 08:16:26.225: INFO: Found 0 stateful pods, waiting for 1
Sep  5 08:16:36.235: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep  5 08:16:36.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 08:16:36.750: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 08:16:36.751: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 08:16:36.751: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 08:16:36.755: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  5 08:16:46.803: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 08:16:46.803: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 08:16:46.935: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Sep  5 08:16:46.935: INFO: ss-0  slave1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:18 +0000 UTC  }]
Sep  5 08:16:46.935: INFO: ss-1          Pending         []
Sep  5 08:16:46.935: INFO: 
Sep  5 08:16:46.935: INFO: StatefulSet ss has not reached scale 3, at 2
Sep  5 08:16:47.940: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.973546192s
Sep  5 08:16:48.969: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.968274557s
Sep  5 08:16:49.974: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.939600919s
Sep  5 08:16:50.981: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.934790322s
Sep  5 08:16:51.987: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.927098057s
Sep  5 08:16:52.992: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.921890243s
Sep  5 08:16:53.997: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.916719248s
Sep  5 08:16:55.002: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.911544313s
Sep  5 08:16:56.007: INFO: Verifying statefulset ss doesn't scale past 3 for another 906.224019ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9347
Sep  5 08:16:57.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:16:57.506: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  5 08:16:57.506: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 08:16:57.506: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 08:16:57.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:16:57.988: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  5 08:16:57.988: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 08:16:57.988: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 08:16:57.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:16:58.454: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  5 08:16:58.454: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  5 08:16:58.454: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  5 08:16:58.473: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 08:16:58.473: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 08:16:58.473: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep  5 08:16:58.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 08:16:58.974: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 08:16:58.974: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 08:16:58.974: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 08:16:58.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 08:16:59.463: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 08:16:59.463: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 08:16:59.463: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 08:16:59.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  5 08:17:00.004: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  5 08:17:00.004: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  5 08:17:00.004: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  5 08:17:00.004: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 08:17:00.009: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Sep  5 08:17:10.017: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 08:17:10.017: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 08:17:10.017: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 08:17:10.112: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Sep  5 08:17:10.112: INFO: ss-0  slave1   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:18 +0000 UTC  }]
Sep  5 08:17:10.113: INFO: ss-1  master1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  }]
Sep  5 08:17:10.113: INFO: ss-2  slave1   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  }]
Sep  5 08:17:10.113: INFO: 
Sep  5 08:17:10.113: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 08:17:11.223: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Sep  5 08:17:11.223: INFO: ss-0  slave1   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:18 +0000 UTC  }]
Sep  5 08:17:11.223: INFO: ss-1  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  }]
Sep  5 08:17:11.223: INFO: ss-2  slave1   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  }]
Sep  5 08:17:11.223: INFO: 
Sep  5 08:17:11.223: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 08:17:12.229: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Sep  5 08:17:12.229: INFO: ss-0  slave1   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:18 +0000 UTC  }]
Sep  5 08:17:12.229: INFO: ss-1  master1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  }]
Sep  5 08:17:12.229: INFO: ss-2  slave1   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  }]
Sep  5 08:17:12.229: INFO: 
Sep  5 08:17:12.229: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 08:17:13.297: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Sep  5 08:17:13.297: INFO: ss-0  slave1   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:18 +0000 UTC  }]
Sep  5 08:17:13.297: INFO: ss-1  master1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  }]
Sep  5 08:17:13.297: INFO: ss-2  slave1   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  }]
Sep  5 08:17:13.297: INFO: 
Sep  5 08:17:13.297: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 08:17:14.302: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Sep  5 08:17:14.302: INFO: ss-0  slave1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:18 +0000 UTC  }]
Sep  5 08:17:14.302: INFO: ss-2  slave1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  }]
Sep  5 08:17:14.302: INFO: 
Sep  5 08:17:14.302: INFO: StatefulSet ss has not reached scale 0, at 2
Sep  5 08:17:15.328: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Sep  5 08:17:15.328: INFO: ss-0  slave1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:18 +0000 UTC  }]
Sep  5 08:17:15.328: INFO: ss-2  slave1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  }]
Sep  5 08:17:15.328: INFO: 
Sep  5 08:17:15.328: INFO: StatefulSet ss has not reached scale 0, at 2
Sep  5 08:17:16.333: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Sep  5 08:17:16.333: INFO: ss-0  slave1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:18 +0000 UTC  }]
Sep  5 08:17:16.333: INFO: ss-2  slave1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  }]
Sep  5 08:17:16.333: INFO: 
Sep  5 08:17:16.333: INFO: StatefulSet ss has not reached scale 0, at 2
Sep  5 08:17:17.347: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Sep  5 08:17:17.347: INFO: ss-0  slave1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:18 +0000 UTC  }]
Sep  5 08:17:17.348: INFO: ss-2  slave1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  }]
Sep  5 08:17:17.348: INFO: 
Sep  5 08:17:17.348: INFO: StatefulSet ss has not reached scale 0, at 2
Sep  5 08:17:18.352: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Sep  5 08:17:18.352: INFO: ss-0  slave1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:18 +0000 UTC  }]
Sep  5 08:17:18.352: INFO: ss-2  slave1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  }]
Sep  5 08:17:18.352: INFO: 
Sep  5 08:17:18.352: INFO: StatefulSet ss has not reached scale 0, at 2
Sep  5 08:17:19.357: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Sep  5 08:17:19.357: INFO: ss-0  slave1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:18 +0000 UTC  }]
Sep  5 08:17:19.357: INFO: ss-2  slave1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:17:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:16:38 +0000 UTC  }]
Sep  5 08:17:19.357: INFO: 
Sep  5 08:17:19.357: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9347
Sep  5 08:17:20.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:17:20.651: INFO: rc: 1
Sep  5 08:17:20.652: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0x4002c91410 exit status 1 <nil> <nil> true [0x400050da28 0x400050da60 0x400050dad0] [0x400050da28 0x400050da60 0x400050dad0] [0x400050da48 0x400050daa8] [0x933a78 0x933a78] 0x40033244e0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Sep  5 08:17:30.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:17:30.824: INFO: rc: 1
Sep  5 08:17:30.824: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x40032aae70 exit status 1 <nil> <nil> true [0x40017c4a70 0x40017c4b10 0x40017c4bc0] [0x40017c4a70 0x40017c4b10 0x40017c4bc0] [0x40017c4ac8 0x40017c4b48] [0x933a78 0x933a78] 0x40027d0c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:17:40.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:17:40.994: INFO: rc: 1
Sep  5 08:17:40.994: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4002567a40 exit status 1 <nil> <nil> true [0x4001fc0368 0x4001fc03c0 0x4001fc03d8] [0x4001fc0368 0x4001fc03c0 0x4001fc03d8] [0x4001fc03a8 0x4001fc03d0] [0x933a78 0x933a78] 0x4003395200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:17:50.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:17:51.186: INFO: rc: 1
Sep  5 08:17:51.186: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x40032ab1d0 exit status 1 <nil> <nil> true [0x40017c4be0 0x40017c4c68 0x40017c4d40] [0x40017c4be0 0x40017c4c68 0x40017c4d40] [0x40017c4c28 0x40017c4d08] [0x933a78 0x933a78] 0x40027d0fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:18:01.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:18:01.361: INFO: rc: 1
Sep  5 08:18:01.361: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4002567d70 exit status 1 <nil> <nil> true [0x4001fc03e0 0x4001fc0408 0x4001fc0420] [0x4001fc03e0 0x4001fc0408 0x4001fc0420] [0x4001fc0400 0x4001fc0418] [0x933a78 0x933a78] 0x4003395560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:18:11.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:18:11.532: INFO: rc: 1
Sep  5 08:18:11.532: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x40032ab530 exit status 1 <nil> <nil> true [0x40017c4d60 0x40017c4e80 0x40017c4f28] [0x40017c4d60 0x40017c4e80 0x40017c4f28] [0x40017c4e58 0x40017c4ec0] [0x933a78 0x933a78] 0x40027d1380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:18:21.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:18:21.704: INFO: rc: 1
Sep  5 08:18:21.704: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x40032ab890 exit status 1 <nil> <nil> true [0x40017c4f50 0x40017c4f80 0x40017c5028] [0x40017c4f50 0x40017c4f80 0x40017c5028] [0x40017c4f68 0x40017c5020] [0x933a78 0x933a78] 0x40027d1740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:18:31.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:18:31.879: INFO: rc: 1
Sep  5 08:18:31.879: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x40039340f0 exit status 1 <nil> <nil> true [0x4001fc0428 0x4001fc0450 0x4001fc04b0] [0x4001fc0428 0x4001fc0450 0x4001fc04b0] [0x4001fc0438 0x4001fc0498] [0x933a78 0x933a78] 0x40033958c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:18:41.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:18:42.071: INFO: rc: 1
Sep  5 08:18:42.071: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4003236300 exit status 1 <nil> <nil> true [0x40000ea490 0x40000ea6a0 0x40000ea830] [0x40000ea490 0x40000ea6a0 0x40000ea830] [0x40000ea538 0x40000ea770] [0x933a78 0x933a78] 0x4001326300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:18:52.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:18:52.245: INFO: rc: 1
Sep  5 08:18:52.246: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4003236660 exit status 1 <nil> <nil> true [0x40000ea8e8 0x40000ea990 0x40000eaa90] [0x40000ea8e8 0x40000ea990 0x40000eaa90] [0x40000ea948 0x40000eaa10] [0x933a78 0x933a78] 0x4001326de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:19:02.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:19:02.415: INFO: rc: 1
Sep  5 08:19:02.415: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4002566330 exit status 1 <nil> <nil> true [0x40017c4028 0x40017c4120 0x40017c4268] [0x40017c4028 0x40017c4120 0x40017c4268] [0x40017c4110 0x40017c41c8] [0x933a78 0x933a78] 0x40039fe2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:19:12.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:19:12.591: INFO: rc: 1
Sep  5 08:19:12.591: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4003268360 exit status 1 <nil> <nil> true [0x4001fc0000 0x4001fc0038 0x4001fc0068] [0x4001fc0000 0x4001fc0038 0x4001fc0068] [0x4001fc0030 0x4001fc0060] [0x933a78 0x933a78] 0x40031422a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:19:22.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:19:22.765: INFO: rc: 1
Sep  5 08:19:22.765: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4003268750 exit status 1 <nil> <nil> true [0x4001fc0070 0x4001fc00a8 0x4001fc00e0] [0x4001fc0070 0x4001fc00a8 0x4001fc00e0] [0x4001fc00a0 0x4001fc00c0] [0x933a78 0x933a78] 0x40031428a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:19:32.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:19:32.937: INFO: rc: 1
Sep  5 08:19:32.937: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4003268ae0 exit status 1 <nil> <nil> true [0x4001fc00e8 0x4001fc0118 0x4001fc0148] [0x4001fc00e8 0x4001fc0118 0x4001fc0148] [0x4001fc0108 0x4001fc0140] [0x933a78 0x933a78] 0x4003142c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:19:42.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:19:43.107: INFO: rc: 1
Sep  5 08:19:43.107: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4003268e10 exit status 1 <nil> <nil> true [0x4001fc0170 0x4001fc01c0 0x4001fc01f8] [0x4001fc0170 0x4001fc01c0 0x4001fc01f8] [0x4001fc01a0 0x4001fc01e0] [0x933a78 0x933a78] 0x4003142fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:19:53.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:19:53.279: INFO: rc: 1
Sep  5 08:19:53.279: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x40032691d0 exit status 1 <nil> <nil> true [0x4001fc0200 0x4001fc0240 0x4001fc0290] [0x4001fc0200 0x4001fc0240 0x4001fc0290] [0x4001fc0228 0x4001fc0278] [0x933a78 0x933a78] 0x4003143320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:20:03.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:20:03.449: INFO: rc: 1
Sep  5 08:20:03.449: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4002cee630 exit status 1 <nil> <nil> true [0x400050c028 0x400050c0f8 0x400050c4d8] [0x400050c028 0x400050c0f8 0x400050c4d8] [0x400050c0c8 0x400050c2b0] [0x933a78 0x933a78] 0x40027d0600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:20:13.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:20:13.630: INFO: rc: 1
Sep  5 08:20:13.630: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x40025667e0 exit status 1 <nil> <nil> true [0x40017c42f0 0x40017c43c0 0x40017c44c8] [0x40017c42f0 0x40017c43c0 0x40017c44c8] [0x40017c4358 0x40017c4470] [0x933a78 0x933a78] 0x40039fe600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:20:23.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:20:23.807: INFO: rc: 1
Sep  5 08:20:23.807: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4003269500 exit status 1 <nil> <nil> true [0x4001fc0298 0x4001fc02b0 0x4001fc02e8] [0x4001fc0298 0x4001fc02b0 0x4001fc02e8] [0x4001fc02a8 0x4001fc02d0] [0x933a78 0x933a78] 0x4003143680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:20:33.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:20:33.986: INFO: rc: 1
Sep  5 08:20:33.986: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4003236a80 exit status 1 <nil> <nil> true [0x40000eaad8 0x40000ead90 0x40000eaf38] [0x40000eaad8 0x40000ead90 0x40000eaf38] [0x40000ead40 0x40000eaf18] [0x933a78 0x933a78] 0x4001327440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:20:43.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:20:44.158: INFO: rc: 1
Sep  5 08:20:44.159: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4003236330 exit status 1 <nil> <nil> true [0x40000ea490 0x40000ea6a0 0x40000ea830] [0x40000ea490 0x40000ea6a0 0x40000ea830] [0x40000ea538 0x40000ea770] [0x933a78 0x933a78] 0x4001326300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:20:54.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:20:54.403: INFO: rc: 1
Sep  5 08:20:54.403: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x40032366c0 exit status 1 <nil> <nil> true [0x40000ea8e8 0x40000ea990 0x40000eaa90] [0x40000ea8e8 0x40000ea990 0x40000eaa90] [0x40000ea948 0x40000eaa10] [0x933a78 0x933a78] 0x4001326de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:21:04.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:21:04.573: INFO: rc: 1
Sep  5 08:21:04.573: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4002cee5d0 exit status 1 <nil> <nil> true [0x400050c028 0x400050c0f8 0x400050c4d8] [0x400050c028 0x400050c0f8 0x400050c4d8] [0x400050c0c8 0x400050c2b0] [0x933a78 0x933a78] 0x40027d0600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:21:14.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:21:14.760: INFO: rc: 1
Sep  5 08:21:14.760: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x40032369f0 exit status 1 <nil> <nil> true [0x40000eaad8 0x40000ead90 0x40000eaf38] [0x40000eaad8 0x40000ead90 0x40000eaf38] [0x40000ead40 0x40000eaf18] [0x933a78 0x933a78] 0x4001327440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:21:24.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:21:24.938: INFO: rc: 1
Sep  5 08:21:24.938: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4003236d80 exit status 1 <nil> <nil> true [0x40000eaf80 0x40000eb0e0 0x40000eb1e8] [0x40000eaf80 0x40000eb0e0 0x40000eb1e8] [0x40000eb0c8 0x40000eb1a8] [0x933a78 0x933a78] 0x4001327c80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:21:34.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:21:35.115: INFO: rc: 1
Sep  5 08:21:35.115: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4002566390 exit status 1 <nil> <nil> true [0x4001fc0000 0x4001fc0038 0x4001fc0068] [0x4001fc0000 0x4001fc0038 0x4001fc0068] [0x4001fc0030 0x4001fc0060] [0x933a78 0x933a78] 0x40031422a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:21:45.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:21:45.284: INFO: rc: 1
Sep  5 08:21:45.284: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4002566750 exit status 1 <nil> <nil> true [0x4001fc0070 0x4001fc00a8 0x4001fc00e0] [0x4001fc0070 0x4001fc00a8 0x4001fc00e0] [0x4001fc00a0 0x4001fc00c0] [0x933a78 0x933a78] 0x40031428a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:21:55.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:21:55.455: INFO: rc: 1
Sep  5 08:21:55.456: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4002566ae0 exit status 1 <nil> <nil> true [0x4001fc00e8 0x4001fc0118 0x4001fc0148] [0x4001fc00e8 0x4001fc0118 0x4001fc0148] [0x4001fc0108 0x4001fc0140] [0x933a78 0x933a78] 0x4003142c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:22:05.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:22:05.628: INFO: rc: 1
Sep  5 08:22:05.628: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x4002cee960 exit status 1 <nil> <nil> true [0x400050c500 0x400050c5c8 0x400050c708] [0x400050c500 0x400050c5c8 0x400050c708] [0x400050c5a0 0x400050c6f0] [0x933a78 0x933a78] 0x40027d09c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:22:15.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:22:15.807: INFO: rc: 1
Sep  5 08:22:15.807: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0x40032683c0 exit status 1 <nil> <nil> true [0x40017c4028 0x40017c4120 0x40017c4268] [0x40017c4028 0x40017c4120 0x40017c4268] [0x40017c4110 0x40017c41c8] [0x933a78 0x933a78] 0x40039fe2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 08:22:25.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec --namespace=statefulset-9347 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  5 08:22:25.983: INFO: rc: 1
Sep  5 08:22:25.983: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Sep  5 08:22:25.983: INFO: Scaling statefulset ss to 0
Sep  5 08:22:25.994: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep  5 08:22:25.996: INFO: Deleting all statefulset in ns statefulset-9347
Sep  5 08:22:25.999: INFO: Scaling statefulset ss to 0
Sep  5 08:22:26.009: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 08:22:26.011: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:22:26.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9347" for this suite.
Sep  5 08:22:32.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:22:32.179: INFO: namespace statefulset-9347 deletion completed in 6.109325185s

• [SLOW TEST:366.220 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:22:32.180: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8686
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep  5 08:22:36.951: INFO: Successfully updated pod "annotationupdate7febca69-3866-4528-8259-60d20d1fbbd7"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:22:41.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8686" for this suite.
Sep  5 08:23:03.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:23:03.139: INFO: namespace projected-8686 deletion completed in 22.117852151s

• [SLOW TEST:30.958 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:23:03.140: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-6914/configmap-test-4bb2a2f0-a9c3-40ad-8f3f-a8d81be76746
STEP: Creating a pod to test consume configMaps
Sep  5 08:23:03.371: INFO: Waiting up to 5m0s for pod "pod-configmaps-128583b3-a32a-4ec3-b0e6-61e36f7f61fc" in namespace "configmap-6914" to be "success or failure"
Sep  5 08:23:03.402: INFO: Pod "pod-configmaps-128583b3-a32a-4ec3-b0e6-61e36f7f61fc": Phase="Pending", Reason="", readiness=false. Elapsed: 30.645453ms
Sep  5 08:23:05.425: INFO: Pod "pod-configmaps-128583b3-a32a-4ec3-b0e6-61e36f7f61fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053135515s
Sep  5 08:23:07.432: INFO: Pod "pod-configmaps-128583b3-a32a-4ec3-b0e6-61e36f7f61fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060588859s
STEP: Saw pod success
Sep  5 08:23:07.432: INFO: Pod "pod-configmaps-128583b3-a32a-4ec3-b0e6-61e36f7f61fc" satisfied condition "success or failure"
Sep  5 08:23:07.435: INFO: Trying to get logs from node slave1 pod pod-configmaps-128583b3-a32a-4ec3-b0e6-61e36f7f61fc container env-test: <nil>
STEP: delete the pod
Sep  5 08:23:07.462: INFO: Waiting for pod pod-configmaps-128583b3-a32a-4ec3-b0e6-61e36f7f61fc to disappear
Sep  5 08:23:07.468: INFO: Pod pod-configmaps-128583b3-a32a-4ec3-b0e6-61e36f7f61fc no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:23:07.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6914" for this suite.
Sep  5 08:23:13.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:23:13.579: INFO: namespace configmap-6914 deletion completed in 6.107048321s

• [SLOW TEST:10.440 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:23:13.580: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 08:23:13.817: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6547d8f6-0cb2-4f5a-ac39-c67b5bd37407" in namespace "projected-2512" to be "success or failure"
Sep  5 08:23:13.826: INFO: Pod "downwardapi-volume-6547d8f6-0cb2-4f5a-ac39-c67b5bd37407": Phase="Pending", Reason="", readiness=false. Elapsed: 9.126862ms
Sep  5 08:23:15.830: INFO: Pod "downwardapi-volume-6547d8f6-0cb2-4f5a-ac39-c67b5bd37407": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012551662s
Sep  5 08:23:17.834: INFO: Pod "downwardapi-volume-6547d8f6-0cb2-4f5a-ac39-c67b5bd37407": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01640294s
STEP: Saw pod success
Sep  5 08:23:17.834: INFO: Pod "downwardapi-volume-6547d8f6-0cb2-4f5a-ac39-c67b5bd37407" satisfied condition "success or failure"
Sep  5 08:23:17.836: INFO: Trying to get logs from node slave1 pod downwardapi-volume-6547d8f6-0cb2-4f5a-ac39-c67b5bd37407 container client-container: <nil>
STEP: delete the pod
Sep  5 08:23:17.861: INFO: Waiting for pod downwardapi-volume-6547d8f6-0cb2-4f5a-ac39-c67b5bd37407 to disappear
Sep  5 08:23:17.867: INFO: Pod downwardapi-volume-6547d8f6-0cb2-4f5a-ac39-c67b5bd37407 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:23:17.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2512" for this suite.
Sep  5 08:23:23.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:23:23.994: INFO: namespace projected-2512 deletion completed in 6.122738354s

• [SLOW TEST:10.414 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:23:23.995: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-3587
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3587
I0905 08:23:24.255747      16 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3587, replica count: 1
I0905 08:23:25.306257      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0905 08:23:26.306504      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0905 08:23:27.306699      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0905 08:23:28.306895      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  5 08:23:28.443: INFO: Created: latency-svc-wwzrc
Sep  5 08:23:28.449: INFO: Got endpoints: latency-svc-wwzrc [42.822344ms]
Sep  5 08:23:28.513: INFO: Created: latency-svc-wht4f
Sep  5 08:23:28.552: INFO: Got endpoints: latency-svc-wht4f [102.309698ms]
Sep  5 08:23:28.553: INFO: Created: latency-svc-pmms9
Sep  5 08:23:28.557: INFO: Got endpoints: latency-svc-pmms9 [107.114998ms]
Sep  5 08:23:28.585: INFO: Created: latency-svc-2w4b9
Sep  5 08:23:28.591: INFO: Got endpoints: latency-svc-2w4b9 [140.096902ms]
Sep  5 08:23:28.646: INFO: Created: latency-svc-lsk5z
Sep  5 08:23:28.685: INFO: Got endpoints: latency-svc-lsk5z [233.826876ms]
Sep  5 08:23:28.685: INFO: Created: latency-svc-mbrv2
Sep  5 08:23:28.691: INFO: Got endpoints: latency-svc-mbrv2 [239.280594ms]
Sep  5 08:23:28.719: INFO: Created: latency-svc-qrkhd
Sep  5 08:23:28.725: INFO: Got endpoints: latency-svc-qrkhd [273.174833ms]
Sep  5 08:23:28.771: INFO: Created: latency-svc-7pk5k
Sep  5 08:23:28.803: INFO: Got endpoints: latency-svc-7pk5k [350.395995ms]
Sep  5 08:23:28.804: INFO: Created: latency-svc-gp5s4
Sep  5 08:23:28.808: INFO: Got endpoints: latency-svc-gp5s4 [355.684014ms]
Sep  5 08:23:28.844: INFO: Created: latency-svc-sqftk
Sep  5 08:23:28.849: INFO: Got endpoints: latency-svc-sqftk [396.729864ms]
Sep  5 08:23:28.896: INFO: Created: latency-svc-2j9bv
Sep  5 08:23:28.936: INFO: Created: latency-svc-68srw
Sep  5 08:23:28.936: INFO: Got endpoints: latency-svc-2j9bv [483.827185ms]
Sep  5 08:23:28.941: INFO: Got endpoints: latency-svc-68srw [489.097823ms]
Sep  5 08:23:28.973: INFO: Created: latency-svc-bzvfn
Sep  5 08:23:28.974: INFO: Got endpoints: latency-svc-bzvfn [521.32379ms]
Sep  5 08:23:29.038: INFO: Created: latency-svc-rz6wp
Sep  5 08:23:29.078: INFO: Created: latency-svc-jblql
Sep  5 08:23:29.079: INFO: Got endpoints: latency-svc-rz6wp [626.954255ms]
Sep  5 08:23:29.111: INFO: Got endpoints: latency-svc-jblql [657.895227ms]
Sep  5 08:23:29.171: INFO: Created: latency-svc-9bhkh
Sep  5 08:23:29.203: INFO: Got endpoints: latency-svc-9bhkh [749.860988ms]
Sep  5 08:23:29.204: INFO: Created: latency-svc-ww5bk
Sep  5 08:23:29.217: INFO: Got endpoints: latency-svc-ww5bk [664.927058ms]
Sep  5 08:23:29.245: INFO: Created: latency-svc-bjzjq
Sep  5 08:23:29.258: INFO: Got endpoints: latency-svc-bjzjq [700.539531ms]
Sep  5 08:23:29.312: INFO: Created: latency-svc-7tvl2
Sep  5 08:23:29.344: INFO: Got endpoints: latency-svc-7tvl2 [753.268673ms]
Sep  5 08:23:29.345: INFO: Created: latency-svc-8tvjc
Sep  5 08:23:29.359: INFO: Got endpoints: latency-svc-8tvjc [673.658562ms]
Sep  5 08:23:29.386: INFO: Created: latency-svc-26s89
Sep  5 08:23:29.462: INFO: Got endpoints: latency-svc-26s89 [771.480479ms]
Sep  5 08:23:29.465: INFO: Created: latency-svc-7x7bn
Sep  5 08:23:29.475: INFO: Got endpoints: latency-svc-7x7bn [750.347986ms]
Sep  5 08:23:29.512: INFO: Created: latency-svc-vpqlh
Sep  5 08:23:29.525: INFO: Got endpoints: latency-svc-vpqlh [722.101642ms]
Sep  5 08:23:29.554: INFO: Created: latency-svc-2ccs9
Sep  5 08:23:29.604: INFO: Got endpoints: latency-svc-2ccs9 [796.095697ms]
Sep  5 08:23:29.610: INFO: Created: latency-svc-mb2td
Sep  5 08:23:29.617: INFO: Got endpoints: latency-svc-mb2td [767.041377ms]
Sep  5 08:23:29.646: INFO: Created: latency-svc-cfll7
Sep  5 08:23:29.658: INFO: Got endpoints: latency-svc-cfll7 [721.654984ms]
Sep  5 08:23:29.687: INFO: Created: latency-svc-9tv9h
Sep  5 08:23:29.742: INFO: Got endpoints: latency-svc-9tv9h [800.699978ms]
Sep  5 08:23:29.762: INFO: Created: latency-svc-zb2pp
Sep  5 08:23:29.775: INFO: Got endpoints: latency-svc-zb2pp [800.778777ms]
Sep  5 08:23:29.821: INFO: Created: latency-svc-chclm
Sep  5 08:23:29.833: INFO: Got endpoints: latency-svc-chclm [754.543829ms]
Sep  5 08:23:29.879: INFO: Created: latency-svc-lcdbj
Sep  5 08:23:29.930: INFO: Got endpoints: latency-svc-lcdbj [818.661324ms]
Sep  5 08:23:29.931: INFO: Created: latency-svc-k42c9
Sep  5 08:23:29.943: INFO: Got endpoints: latency-svc-k42c9 [739.222071ms]
Sep  5 08:23:30.014: INFO: Created: latency-svc-h6h4s
Sep  5 08:23:30.056: INFO: Created: latency-svc-qd76j
Sep  5 08:23:30.056: INFO: Got endpoints: latency-svc-h6h4s [839.331019ms]
Sep  5 08:23:30.096: INFO: Got endpoints: latency-svc-qd76j [837.719485ms]
Sep  5 08:23:30.154: INFO: Created: latency-svc-zsb26
Sep  5 08:23:30.188: INFO: Got endpoints: latency-svc-zsb26 [843.639341ms]
Sep  5 08:23:30.189: INFO: Created: latency-svc-j5qhm
Sep  5 08:23:30.201: INFO: Got endpoints: latency-svc-j5qhm [841.971888ms]
Sep  5 08:23:30.230: INFO: Created: latency-svc-pplcl
Sep  5 08:23:30.242: INFO: Got endpoints: latency-svc-pplcl [779.718844ms]
Sep  5 08:23:30.302: INFO: Created: latency-svc-kt895
Sep  5 08:23:30.338: INFO: Got endpoints: latency-svc-kt895 [863.075921ms]
Sep  5 08:23:30.339: INFO: Created: latency-svc-p7dqm
Sep  5 08:23:30.380: INFO: Got endpoints: latency-svc-p7dqm [854.801315ms]
Sep  5 08:23:30.434: INFO: Created: latency-svc-c9kbz
Sep  5 08:23:30.471: INFO: Got endpoints: latency-svc-c9kbz [867.452243ms]
Sep  5 08:23:30.473: INFO: Created: latency-svc-6v49v
Sep  5 08:23:30.484: INFO: Got endpoints: latency-svc-6v49v [867.301744ms]
Sep  5 08:23:30.523: INFO: Created: latency-svc-zd5zg
Sep  5 08:23:30.572: INFO: Got endpoints: latency-svc-zd5zg [913.869251ms]
Sep  5 08:23:30.574: INFO: Created: latency-svc-lxznk
Sep  5 08:23:30.584: INFO: Got endpoints: latency-svc-lxznk [841.672689ms]
Sep  5 08:23:30.623: INFO: Created: latency-svc-z247f
Sep  5 08:23:30.634: INFO: Got endpoints: latency-svc-z247f [858.5701ms]
Sep  5 08:23:30.705: INFO: Created: latency-svc-72qb6
Sep  5 08:23:30.748: INFO: Got endpoints: latency-svc-72qb6 [914.339929ms]
Sep  5 08:23:30.748: INFO: Created: latency-svc-v6lwq
Sep  5 08:23:30.759: INFO: Got endpoints: latency-svc-v6lwq [829.23176ms]
Sep  5 08:23:30.798: INFO: Created: latency-svc-xbbsb
Sep  5 08:23:30.847: INFO: Got endpoints: latency-svc-xbbsb [904.844229ms]
Sep  5 08:23:30.851: INFO: Created: latency-svc-trvsf
Sep  5 08:23:30.859: INFO: Got endpoints: latency-svc-trvsf [802.468251ms]
Sep  5 08:23:30.897: INFO: Created: latency-svc-cqn9s
Sep  5 08:23:30.909: INFO: Got endpoints: latency-svc-cqn9s [813.424005ms]
Sep  5 08:23:30.939: INFO: Created: latency-svc-ndt27
Sep  5 08:23:30.993: INFO: Got endpoints: latency-svc-ndt27 [805.652097ms]
Sep  5 08:23:30.997: INFO: Created: latency-svc-qhcqn
Sep  5 08:23:31.003: INFO: Got endpoints: latency-svc-qhcqn [802.388611ms]
Sep  5 08:23:31.023: INFO: Created: latency-svc-r2d5c
Sep  5 08:23:31.034: INFO: Got endpoints: latency-svc-r2d5c [791.881414ms]
Sep  5 08:23:31.067: INFO: Created: latency-svc-npj66
Sep  5 08:23:31.146: INFO: Created: latency-svc-prwwh
Sep  5 08:23:31.147: INFO: Got endpoints: latency-svc-npj66 [808.530586ms]
Sep  5 08:23:31.173: INFO: Got endpoints: latency-svc-prwwh [792.653891ms]
Sep  5 08:23:31.174: INFO: Created: latency-svc-thm2f
Sep  5 08:23:31.184: INFO: Got endpoints: latency-svc-thm2f [712.675641ms]
Sep  5 08:23:31.287: INFO: Created: latency-svc-dxzr5
Sep  5 08:23:31.315: INFO: Got endpoints: latency-svc-dxzr5 [831.138352ms]
Sep  5 08:23:31.317: INFO: Created: latency-svc-kpphk
Sep  5 08:23:31.326: INFO: Got endpoints: latency-svc-kpphk [753.768252ms]
Sep  5 08:23:31.357: INFO: Created: latency-svc-vwkcp
Sep  5 08:23:31.368: INFO: Got endpoints: latency-svc-vwkcp [783.796948ms]
Sep  5 08:23:31.437: INFO: Created: latency-svc-vbvtg
Sep  5 08:23:31.465: INFO: Got endpoints: latency-svc-vbvtg [830.681295ms]
Sep  5 08:23:31.467: INFO: Created: latency-svc-qbw4k
Sep  5 08:23:31.499: INFO: Got endpoints: latency-svc-qbw4k [750.784544ms]
Sep  5 08:23:31.572: INFO: Created: latency-svc-mttzc
Sep  5 08:23:31.599: INFO: Got endpoints: latency-svc-mttzc [839.630098ms]
Sep  5 08:23:31.603: INFO: Created: latency-svc-ptkqg
Sep  5 08:23:31.610: INFO: Got endpoints: latency-svc-ptkqg [762.209796ms]
Sep  5 08:23:31.633: INFO: Created: latency-svc-l4shq
Sep  5 08:23:31.643: INFO: Got endpoints: latency-svc-l4shq [784.389666ms]
Sep  5 08:23:31.730: INFO: Created: latency-svc-nw77q
Sep  5 08:23:31.757: INFO: Got endpoints: latency-svc-nw77q [847.745624ms]
Sep  5 08:23:31.758: INFO: Created: latency-svc-t9pns
Sep  5 08:23:31.768: INFO: Got endpoints: latency-svc-t9pns [774.265447ms]
Sep  5 08:23:31.791: INFO: Created: latency-svc-q6c6x
Sep  5 08:23:31.802: INFO: Got endpoints: latency-svc-q6c6x [798.585687ms]
Sep  5 08:23:31.824: INFO: Created: latency-svc-z8cps
Sep  5 08:23:31.886: INFO: Got endpoints: latency-svc-z8cps [852.387945ms]
Sep  5 08:23:31.890: INFO: Created: latency-svc-45hx8
Sep  5 08:23:31.916: INFO: Got endpoints: latency-svc-45hx8 [769.634346ms]
Sep  5 08:23:31.949: INFO: Created: latency-svc-854dt
Sep  5 08:23:31.960: INFO: Got endpoints: latency-svc-854dt [787.669432ms]
Sep  5 08:23:31.983: INFO: Created: latency-svc-nx25c
Sep  5 08:23:32.052: INFO: Got endpoints: latency-svc-nx25c [867.661602ms]
Sep  5 08:23:32.055: INFO: Created: latency-svc-5ncpn
Sep  5 08:23:32.060: INFO: Got endpoints: latency-svc-5ncpn [744.967388ms]
Sep  5 08:23:32.091: INFO: Created: latency-svc-fqfrd
Sep  5 08:23:32.102: INFO: Got endpoints: latency-svc-fqfrd [775.668621ms]
Sep  5 08:23:32.126: INFO: Created: latency-svc-5qlt9
Sep  5 08:23:32.135: INFO: Got endpoints: latency-svc-5qlt9 [767.105457ms]
Sep  5 08:23:32.203: INFO: Created: latency-svc-hmcsx
Sep  5 08:23:32.218: INFO: Got endpoints: latency-svc-hmcsx [753.513033ms]
Sep  5 08:23:32.242: INFO: Created: latency-svc-82skj
Sep  5 08:23:32.251: INFO: Got endpoints: latency-svc-82skj [752.770816ms]
Sep  5 08:23:32.277: INFO: Created: latency-svc-qq8l4
Sep  5 08:23:32.285: INFO: Got endpoints: latency-svc-qq8l4 [685.982031ms]
Sep  5 08:23:32.354: INFO: Created: latency-svc-wgb67
Sep  5 08:23:32.383: INFO: Got endpoints: latency-svc-wgb67 [773.55985ms]
Sep  5 08:23:32.385: INFO: Created: latency-svc-qw57v
Sep  5 08:23:32.394: INFO: Got endpoints: latency-svc-qw57v [750.293626ms]
Sep  5 08:23:32.417: INFO: Created: latency-svc-j8bsg
Sep  5 08:23:32.427: INFO: Got endpoints: latency-svc-j8bsg [669.484299ms]
Sep  5 08:23:32.450: INFO: Created: latency-svc-m6z5c
Sep  5 08:23:32.512: INFO: Got endpoints: latency-svc-m6z5c [744.213951ms]
Sep  5 08:23:32.516: INFO: Created: latency-svc-n65pv
Sep  5 08:23:32.527: INFO: Got endpoints: latency-svc-n65pv [725.436428ms]
Sep  5 08:23:32.550: INFO: Created: latency-svc-26wzw
Sep  5 08:23:32.560: INFO: Got endpoints: latency-svc-26wzw [673.874701ms]
Sep  5 08:23:32.585: INFO: Created: latency-svc-sb6wc
Sep  5 08:23:32.594: INFO: Got endpoints: latency-svc-sb6wc [677.628506ms]
Sep  5 08:23:32.654: INFO: Created: latency-svc-xnjjl
Sep  5 08:23:32.684: INFO: Got endpoints: latency-svc-xnjjl [723.995034ms]
Sep  5 08:23:32.686: INFO: Created: latency-svc-nqmwr
Sep  5 08:23:32.694: INFO: Got endpoints: latency-svc-nqmwr [641.835293ms]
Sep  5 08:23:32.718: INFO: Created: latency-svc-j6kfl
Sep  5 08:23:32.727: INFO: Got endpoints: latency-svc-j6kfl [666.991269ms]
Sep  5 08:23:32.751: INFO: Created: latency-svc-kvnmm
Sep  5 08:23:32.805: INFO: Got endpoints: latency-svc-kvnmm [702.857741ms]
Sep  5 08:23:32.811: INFO: Created: latency-svc-d8cpp
Sep  5 08:23:32.820: INFO: Got endpoints: latency-svc-d8cpp [684.867316ms]
Sep  5 08:23:32.844: INFO: Created: latency-svc-lx8fl
Sep  5 08:23:32.868: INFO: Got endpoints: latency-svc-lx8fl [649.510341ms]
Sep  5 08:23:32.893: INFO: Created: latency-svc-rvrc4
Sep  5 08:23:32.902: INFO: Got endpoints: latency-svc-rvrc4 [650.728136ms]
Sep  5 08:23:32.961: INFO: Created: latency-svc-zshx8
Sep  5 08:23:32.985: INFO: Got endpoints: latency-svc-zshx8 [700.417372ms]
Sep  5 08:23:32.986: INFO: Created: latency-svc-p8hc2
Sep  5 08:23:32.994: INFO: Got endpoints: latency-svc-p8hc2 [610.646522ms]
Sep  5 08:23:33.019: INFO: Created: latency-svc-ckddx
Sep  5 08:23:33.028: INFO: Got endpoints: latency-svc-ckddx [633.896726ms]
Sep  5 08:23:33.052: INFO: Created: latency-svc-6ccvd
Sep  5 08:23:33.104: INFO: Got endpoints: latency-svc-6ccvd [676.946268ms]
Sep  5 08:23:33.108: INFO: Created: latency-svc-xcxtx
Sep  5 08:23:33.136: INFO: Got endpoints: latency-svc-xcxtx [623.437269ms]
Sep  5 08:23:33.136: INFO: Created: latency-svc-l9xv4
Sep  5 08:23:33.160: INFO: Got endpoints: latency-svc-l9xv4 [632.675151ms]
Sep  5 08:23:33.185: INFO: Created: latency-svc-n86g6
Sep  5 08:23:33.194: INFO: Got endpoints: latency-svc-n86g6 [633.587887ms]
Sep  5 08:23:33.253: INFO: Created: latency-svc-whtqt
Sep  5 08:23:33.285: INFO: Got endpoints: latency-svc-whtqt [690.891211ms]
Sep  5 08:23:33.286: INFO: Created: latency-svc-zczm9
Sep  5 08:23:33.310: INFO: Got endpoints: latency-svc-zczm9 [625.65696ms]
Sep  5 08:23:33.344: INFO: Created: latency-svc-ccf7p
Sep  5 08:23:33.404: INFO: Got endpoints: latency-svc-ccf7p [709.853912ms]
Sep  5 08:23:33.405: INFO: Created: latency-svc-xvwkt
Sep  5 08:23:33.436: INFO: Got endpoints: latency-svc-xvwkt [708.533178ms]
Sep  5 08:23:33.437: INFO: Created: latency-svc-6dmnh
Sep  5 08:23:33.444: INFO: Got endpoints: latency-svc-6dmnh [639.880541ms]
Sep  5 08:23:33.469: INFO: Created: latency-svc-pb4pz
Sep  5 08:23:33.478: INFO: Got endpoints: latency-svc-pb4pz [658.585044ms]
Sep  5 08:23:33.538: INFO: Created: latency-svc-jbdv9
Sep  5 08:23:33.569: INFO: Got endpoints: latency-svc-jbdv9 [701.108429ms]
Sep  5 08:23:33.570: INFO: Created: latency-svc-c59t8
Sep  5 08:23:33.578: INFO: Got endpoints: latency-svc-c59t8 [675.484494ms]
Sep  5 08:23:33.611: INFO: Created: latency-svc-nfwlw
Sep  5 08:23:33.620: INFO: Got endpoints: latency-svc-nfwlw [634.685782ms]
Sep  5 08:23:33.679: INFO: Created: latency-svc-ssthg
Sep  5 08:23:33.711: INFO: Got endpoints: latency-svc-ssthg [717.325282ms]
Sep  5 08:23:33.714: INFO: Created: latency-svc-fwbk6
Sep  5 08:23:33.721: INFO: Got endpoints: latency-svc-fwbk6 [693.011982ms]
Sep  5 08:23:33.745: INFO: Created: latency-svc-h987p
Sep  5 08:23:33.753: INFO: Got endpoints: latency-svc-h987p [649.719741ms]
Sep  5 08:23:33.779: INFO: Created: latency-svc-qvz78
Sep  5 08:23:33.837: INFO: Got endpoints: latency-svc-qvz78 [700.915609ms]
Sep  5 08:23:33.840: INFO: Created: latency-svc-bl7f7
Sep  5 08:23:33.845: INFO: Got endpoints: latency-svc-bl7f7 [685.319753ms]
Sep  5 08:23:33.870: INFO: Created: latency-svc-6kf9h
Sep  5 08:23:33.905: INFO: Got endpoints: latency-svc-6kf9h [710.36247ms]
Sep  5 08:23:33.929: INFO: Created: latency-svc-v5cs8
Sep  5 08:23:33.987: INFO: Got endpoints: latency-svc-v5cs8 [701.946426ms]
Sep  5 08:23:33.990: INFO: Created: latency-svc-cv4w5
Sep  5 08:23:33.995: INFO: Got endpoints: latency-svc-cv4w5 [684.847276ms]
Sep  5 08:23:34.029: INFO: Created: latency-svc-wpbw5
Sep  5 08:23:34.037: INFO: Got endpoints: latency-svc-wpbw5 [633.090209ms]
Sep  5 08:23:34.063: INFO: Created: latency-svc-945hk
Sep  5 08:23:34.074: INFO: Got endpoints: latency-svc-945hk [638.625566ms]
Sep  5 08:23:34.128: INFO: Created: latency-svc-lzjjw
Sep  5 08:23:34.163: INFO: Created: latency-svc-d4sl4
Sep  5 08:23:34.165: INFO: Got endpoints: latency-svc-lzjjw [720.545108ms]
Sep  5 08:23:34.170: INFO: Got endpoints: latency-svc-d4sl4 [692.023586ms]
Sep  5 08:23:34.205: INFO: Created: latency-svc-hhxz7
Sep  5 08:23:34.212: INFO: Got endpoints: latency-svc-hhxz7 [643.261847ms]
Sep  5 08:23:34.262: INFO: Created: latency-svc-plbsc
Sep  5 08:23:34.296: INFO: Got endpoints: latency-svc-plbsc [718.128918ms]
Sep  5 08:23:34.298: INFO: Created: latency-svc-kr5bm
Sep  5 08:23:34.306: INFO: Got endpoints: latency-svc-kr5bm [686.20877ms]
Sep  5 08:23:34.338: INFO: Created: latency-svc-8tkjs
Sep  5 08:23:34.345: INFO: Got endpoints: latency-svc-8tkjs [633.860966ms]
Sep  5 08:23:34.395: INFO: Created: latency-svc-9vj8g
Sep  5 08:23:34.429: INFO: Got endpoints: latency-svc-9vj8g [708.763037ms]
Sep  5 08:23:34.431: INFO: Created: latency-svc-cr467
Sep  5 08:23:34.454: INFO: Got endpoints: latency-svc-cr467 [700.67835ms]
Sep  5 08:23:34.488: INFO: Created: latency-svc-mwfxf
Sep  5 08:23:34.539: INFO: Got endpoints: latency-svc-mwfxf [701.957685ms]
Sep  5 08:23:34.542: INFO: Created: latency-svc-cqllg
Sep  5 08:23:34.554: INFO: Got endpoints: latency-svc-cqllg [708.985176ms]
Sep  5 08:23:34.589: INFO: Created: latency-svc-8v5c8
Sep  5 08:23:34.596: INFO: Got endpoints: latency-svc-8v5c8 [691.399729ms]
Sep  5 08:23:34.622: INFO: Created: latency-svc-x9fsx
Sep  5 08:23:34.629: INFO: Got endpoints: latency-svc-x9fsx [641.850873ms]
Sep  5 08:23:34.679: INFO: Created: latency-svc-m8w9g
Sep  5 08:23:34.714: INFO: Got endpoints: latency-svc-m8w9g [718.727256ms]
Sep  5 08:23:34.715: INFO: Created: latency-svc-hdkkm
Sep  5 08:23:34.721: INFO: Got endpoints: latency-svc-hdkkm [684.238538ms]
Sep  5 08:23:34.748: INFO: Created: latency-svc-grpvw
Sep  5 08:23:34.754: INFO: Got endpoints: latency-svc-grpvw [679.368278ms]
Sep  5 08:23:34.813: INFO: Created: latency-svc-nw86q
Sep  5 08:23:34.821: INFO: Got endpoints: latency-svc-nw86q [656.196734ms]
Sep  5 08:23:34.847: INFO: Created: latency-svc-p4dgb
Sep  5 08:23:34.855: INFO: Got endpoints: latency-svc-p4dgb [684.077119ms]
Sep  5 08:23:34.889: INFO: Created: latency-svc-tjbkh
Sep  5 08:23:34.896: INFO: Got endpoints: latency-svc-tjbkh [683.554981ms]
Sep  5 08:23:34.947: INFO: Created: latency-svc-bqzhr
Sep  5 08:23:34.980: INFO: Got endpoints: latency-svc-bqzhr [684.267378ms]
Sep  5 08:23:34.981: INFO: Created: latency-svc-9csgd
Sep  5 08:23:34.988: INFO: Got endpoints: latency-svc-9csgd [681.48513ms]
Sep  5 08:23:35.014: INFO: Created: latency-svc-v24pc
Sep  5 08:23:35.021: INFO: Got endpoints: latency-svc-v24pc [675.730073ms]
Sep  5 08:23:35.087: INFO: Created: latency-svc-pvvfx
Sep  5 08:23:35.123: INFO: Got endpoints: latency-svc-pvvfx [693.27142ms]
Sep  5 08:23:35.124: INFO: Created: latency-svc-kzq9x
Sep  5 08:23:35.129: INFO: Got endpoints: latency-svc-kzq9x [675.233295ms]
Sep  5 08:23:35.156: INFO: Created: latency-svc-ptbzb
Sep  5 08:23:35.237: INFO: Got endpoints: latency-svc-ptbzb [698.18674ms]
Sep  5 08:23:35.240: INFO: Created: latency-svc-grbwm
Sep  5 08:23:35.246: INFO: Got endpoints: latency-svc-grbwm [692.008346ms]
Sep  5 08:23:35.282: INFO: Created: latency-svc-lh2x2
Sep  5 08:23:35.288: INFO: Got endpoints: latency-svc-lh2x2 [692.190365ms]
Sep  5 08:23:35.315: INFO: Created: latency-svc-rdfrd
Sep  5 08:23:35.322: INFO: Got endpoints: latency-svc-rdfrd [692.636924ms]
Sep  5 08:23:35.372: INFO: Created: latency-svc-wjvsj
Sep  5 08:23:35.407: INFO: Got endpoints: latency-svc-wjvsj [692.876102ms]
Sep  5 08:23:35.408: INFO: Created: latency-svc-h9jkq
Sep  5 08:23:35.413: INFO: Got endpoints: latency-svc-h9jkq [691.837587ms]
Sep  5 08:23:35.440: INFO: Created: latency-svc-s75h8
Sep  5 08:23:35.536: INFO: Got endpoints: latency-svc-s75h8 [782.247854ms]
Sep  5 08:23:35.539: INFO: Created: latency-svc-t8mrx
Sep  5 08:23:35.546: INFO: Got endpoints: latency-svc-t8mrx [724.88387ms]
Sep  5 08:23:35.582: INFO: Created: latency-svc-xs9r7
Sep  5 08:23:35.588: INFO: Got endpoints: latency-svc-xs9r7 [733.613894ms]
Sep  5 08:23:35.616: INFO: Created: latency-svc-zj6q6
Sep  5 08:23:35.622: INFO: Got endpoints: latency-svc-zj6q6 [725.668288ms]
Sep  5 08:23:35.672: INFO: Created: latency-svc-k85xl
Sep  5 08:23:35.707: INFO: Got endpoints: latency-svc-k85xl [727.079521ms]
Sep  5 08:23:35.708: INFO: Created: latency-svc-d2g87
Sep  5 08:23:35.714: INFO: Got endpoints: latency-svc-d2g87 [726.410745ms]
Sep  5 08:23:35.740: INFO: Created: latency-svc-zxvgw
Sep  5 08:23:35.747: INFO: Got endpoints: latency-svc-zxvgw [725.784887ms]
Sep  5 08:23:35.805: INFO: Created: latency-svc-sxw8c
Sep  5 08:23:35.840: INFO: Got endpoints: latency-svc-sxw8c [717.608041ms]
Sep  5 08:23:35.841: INFO: Created: latency-svc-f798v
Sep  5 08:23:35.846: INFO: Got endpoints: latency-svc-f798v [716.983623ms]
Sep  5 08:23:35.883: INFO: Created: latency-svc-nnc77
Sep  5 08:23:35.888: INFO: Got endpoints: latency-svc-nnc77 [651.485333ms]
Sep  5 08:23:35.953: INFO: Created: latency-svc-4zws7
Sep  5 08:23:35.999: INFO: Got endpoints: latency-svc-4zws7 [752.433877ms]
Sep  5 08:23:35.999: INFO: Created: latency-svc-9q5wn
Sep  5 08:23:36.005: INFO: Got endpoints: latency-svc-9q5wn [716.664024ms]
Sep  5 08:23:36.033: INFO: Created: latency-svc-9gpcg
Sep  5 08:23:36.039: INFO: Got endpoints: latency-svc-9gpcg [717.228142ms]
Sep  5 08:23:36.099: INFO: Created: latency-svc-zr9ql
Sep  5 08:23:36.108: INFO: Got endpoints: latency-svc-zr9ql [701.178768ms]
Sep  5 08:23:36.133: INFO: Created: latency-svc-9xd2c
Sep  5 08:23:36.139: INFO: Got endpoints: latency-svc-9xd2c [725.497588ms]
Sep  5 08:23:36.175: INFO: Created: latency-svc-mf2pg
Sep  5 08:23:36.181: INFO: Got endpoints: latency-svc-mf2pg [644.418383ms]
Sep  5 08:23:36.270: INFO: Created: latency-svc-tc5bk
Sep  5 08:23:36.309: INFO: Got endpoints: latency-svc-tc5bk [762.554155ms]
Sep  5 08:23:36.310: INFO: Created: latency-svc-rm8zh
Sep  5 08:23:36.350: INFO: Got endpoints: latency-svc-rm8zh [761.495519ms]
Sep  5 08:23:36.354: INFO: Created: latency-svc-cqtln
Sep  5 08:23:36.408: INFO: Created: latency-svc-5g4hg
Sep  5 08:23:36.417: INFO: Got endpoints: latency-svc-cqtln [794.933861ms]
Sep  5 08:23:36.417: INFO: Got endpoints: latency-svc-5g4hg [709.540514ms]
Sep  5 08:23:36.442: INFO: Created: latency-svc-rwbm4
Sep  5 08:23:36.455: INFO: Got endpoints: latency-svc-rwbm4 [741.057044ms]
Sep  5 08:23:36.492: INFO: Created: latency-svc-zxhm9
Sep  5 08:23:36.553: INFO: Got endpoints: latency-svc-zxhm9 [805.841596ms]
Sep  5 08:23:36.556: INFO: Created: latency-svc-xm2sr
Sep  5 08:23:36.564: INFO: Got endpoints: latency-svc-xm2sr [723.245537ms]
Sep  5 08:23:36.600: INFO: Created: latency-svc-5zqml
Sep  5 08:23:36.614: INFO: Got endpoints: latency-svc-5zqml [767.226936ms]
Sep  5 08:23:36.651: INFO: Created: latency-svc-dfmvh
Sep  5 08:23:36.712: INFO: Got endpoints: latency-svc-dfmvh [823.133965ms]
Sep  5 08:23:36.714: INFO: Created: latency-svc-smlpn
Sep  5 08:23:36.750: INFO: Got endpoints: latency-svc-smlpn [751.63608ms]
Sep  5 08:23:36.784: INFO: Created: latency-svc-jchqx
Sep  5 08:23:36.801: INFO: Got endpoints: latency-svc-jchqx [796.283496ms]
Sep  5 08:23:36.840: INFO: Created: latency-svc-8z5fp
Sep  5 08:23:36.876: INFO: Got endpoints: latency-svc-8z5fp [836.920729ms]
Sep  5 08:23:36.876: INFO: Created: latency-svc-jwg4g
Sep  5 08:23:36.910: INFO: Got endpoints: latency-svc-jwg4g [801.727353ms]
Sep  5 08:23:36.936: INFO: Created: latency-svc-fnc86
Sep  5 08:23:36.994: INFO: Got endpoints: latency-svc-fnc86 [855.793971ms]
Sep  5 08:23:36.998: INFO: Created: latency-svc-8znh8
Sep  5 08:23:37.006: INFO: Got endpoints: latency-svc-8znh8 [825.375176ms]
Sep  5 08:23:37.043: INFO: Created: latency-svc-8ldvc
Sep  5 08:23:37.056: INFO: Got endpoints: latency-svc-8ldvc [746.72126ms]
Sep  5 08:23:37.093: INFO: Created: latency-svc-6qtwg
Sep  5 08:23:37.153: INFO: Got endpoints: latency-svc-6qtwg [802.778189ms]
Sep  5 08:23:37.156: INFO: Created: latency-svc-v9m59
Sep  5 08:23:37.164: INFO: Got endpoints: latency-svc-v9m59 [747.609957ms]
Sep  5 08:23:37.202: INFO: Created: latency-svc-v8p77
Sep  5 08:23:37.215: INFO: Got endpoints: latency-svc-v8p77 [797.429432ms]
Sep  5 08:23:37.252: INFO: Created: latency-svc-nd4jv
Sep  5 08:23:37.312: INFO: Got endpoints: latency-svc-nd4jv [856.736427ms]
Sep  5 08:23:37.317: INFO: Created: latency-svc-d5cml
Sep  5 08:23:37.323: INFO: Got endpoints: latency-svc-d5cml [770.551782ms]
Sep  5 08:23:37.360: INFO: Created: latency-svc-m9hjp
Sep  5 08:23:37.373: INFO: Got endpoints: latency-svc-m9hjp [808.889504ms]
Sep  5 08:23:37.402: INFO: Created: latency-svc-mvrdc
Sep  5 08:23:37.461: INFO: Got endpoints: latency-svc-mvrdc [847.221086ms]
Sep  5 08:23:37.464: INFO: Created: latency-svc-zj5fd
Sep  5 08:23:37.473: INFO: Got endpoints: latency-svc-zj5fd [761.632199ms]
Sep  5 08:23:37.503: INFO: Created: latency-svc-84kv2
Sep  5 08:23:37.515: INFO: Got endpoints: latency-svc-84kv2 [764.585807ms]
Sep  5 08:23:37.552: INFO: Created: latency-svc-f24tc
Sep  5 08:23:37.610: INFO: Got endpoints: latency-svc-f24tc [808.856104ms]
Sep  5 08:23:37.622: INFO: Created: latency-svc-zpd5g
Sep  5 08:23:37.632: INFO: Got endpoints: latency-svc-zpd5g [755.527584ms]
Sep  5 08:23:37.677: INFO: Created: latency-svc-nzkgm
Sep  5 08:23:37.690: INFO: Got endpoints: latency-svc-nzkgm [780.083203ms]
Sep  5 08:23:37.761: INFO: Created: latency-svc-qkpf5
Sep  5 08:23:37.803: INFO: Got endpoints: latency-svc-qkpf5 [808.182067ms]
Sep  5 08:23:37.803: INFO: Created: latency-svc-2vnb5
Sep  5 08:23:37.815: INFO: Got endpoints: latency-svc-2vnb5 [808.633665ms]
Sep  5 08:23:37.853: INFO: Created: latency-svc-gvthc
Sep  5 08:23:37.912: INFO: Got endpoints: latency-svc-gvthc [856.246189ms]
Sep  5 08:23:37.916: INFO: Created: latency-svc-b9zrm
Sep  5 08:23:37.923: INFO: Got endpoints: latency-svc-b9zrm [770.460043ms]
Sep  5 08:23:37.969: INFO: Created: latency-svc-q5zqn
Sep  5 08:23:37.981: INFO: Got endpoints: latency-svc-q5zqn [817.247729ms]
Sep  5 08:23:38.011: INFO: Created: latency-svc-hnxg7
Sep  5 08:23:38.072: INFO: Created: latency-svc-zfr57
Sep  5 08:23:38.083: INFO: Got endpoints: latency-svc-zfr57 [770.562562ms]
Sep  5 08:23:38.083: INFO: Got endpoints: latency-svc-hnxg7 [868.04282ms]
Sep  5 08:23:38.113: INFO: Created: latency-svc-2x6rw
Sep  5 08:23:38.124: INFO: Got endpoints: latency-svc-2x6rw [800.24018ms]
Sep  5 08:23:38.162: INFO: Created: latency-svc-xjcp4
Sep  5 08:23:38.220: INFO: Got endpoints: latency-svc-xjcp4 [847.550624ms]
Sep  5 08:23:38.224: INFO: Created: latency-svc-76bwr
Sep  5 08:23:38.232: INFO: Got endpoints: latency-svc-76bwr [770.694342ms]
Sep  5 08:23:38.270: INFO: Created: latency-svc-lhcz8
Sep  5 08:23:38.282: INFO: Got endpoints: latency-svc-lhcz8 [808.504405ms]
Sep  5 08:23:38.380: INFO: Created: latency-svc-8vh7p
Sep  5 08:23:38.382: INFO: Created: latency-svc-czgrs
Sep  5 08:23:38.390: INFO: Got endpoints: latency-svc-czgrs [779.715184ms]
Sep  5 08:23:38.390: INFO: Got endpoints: latency-svc-8vh7p [874.934472ms]
Sep  5 08:23:38.390: INFO: Latencies: [102.309698ms 107.114998ms 140.096902ms 233.826876ms 239.280594ms 273.174833ms 350.395995ms 355.684014ms 396.729864ms 483.827185ms 489.097823ms 521.32379ms 610.646522ms 623.437269ms 625.65696ms 626.954255ms 632.675151ms 633.090209ms 633.587887ms 633.860966ms 633.896726ms 634.685782ms 638.625566ms 639.880541ms 641.835293ms 641.850873ms 643.261847ms 644.418383ms 649.510341ms 649.719741ms 650.728136ms 651.485333ms 656.196734ms 657.895227ms 658.585044ms 664.927058ms 666.991269ms 669.484299ms 673.658562ms 673.874701ms 675.233295ms 675.484494ms 675.730073ms 676.946268ms 677.628506ms 679.368278ms 681.48513ms 683.554981ms 684.077119ms 684.238538ms 684.267378ms 684.847276ms 684.867316ms 685.319753ms 685.982031ms 686.20877ms 690.891211ms 691.399729ms 691.837587ms 692.008346ms 692.023586ms 692.190365ms 692.636924ms 692.876102ms 693.011982ms 693.27142ms 698.18674ms 700.417372ms 700.539531ms 700.67835ms 700.915609ms 701.108429ms 701.178768ms 701.946426ms 701.957685ms 702.857741ms 708.533178ms 708.763037ms 708.985176ms 709.540514ms 709.853912ms 710.36247ms 712.675641ms 716.664024ms 716.983623ms 717.228142ms 717.325282ms 717.608041ms 718.128918ms 718.727256ms 720.545108ms 721.654984ms 722.101642ms 723.245537ms 723.995034ms 724.88387ms 725.436428ms 725.497588ms 725.668288ms 725.784887ms 726.410745ms 727.079521ms 733.613894ms 739.222071ms 741.057044ms 744.213951ms 744.967388ms 746.72126ms 747.609957ms 749.860988ms 750.293626ms 750.347986ms 750.784544ms 751.63608ms 752.433877ms 752.770816ms 753.268673ms 753.513033ms 753.768252ms 754.543829ms 755.527584ms 761.495519ms 761.632199ms 762.209796ms 762.554155ms 764.585807ms 767.041377ms 767.105457ms 767.226936ms 769.634346ms 770.460043ms 770.551782ms 770.562562ms 770.694342ms 771.480479ms 773.55985ms 774.265447ms 775.668621ms 779.715184ms 779.718844ms 780.083203ms 782.247854ms 783.796948ms 784.389666ms 787.669432ms 791.881414ms 792.653891ms 794.933861ms 796.095697ms 796.283496ms 797.429432ms 798.585687ms 800.24018ms 800.699978ms 800.778777ms 801.727353ms 802.388611ms 802.468251ms 802.778189ms 805.652097ms 805.841596ms 808.182067ms 808.504405ms 808.530586ms 808.633665ms 808.856104ms 808.889504ms 813.424005ms 817.247729ms 818.661324ms 823.133965ms 825.375176ms 829.23176ms 830.681295ms 831.138352ms 836.920729ms 837.719485ms 839.331019ms 839.630098ms 841.672689ms 841.971888ms 843.639341ms 847.221086ms 847.550624ms 847.745624ms 852.387945ms 854.801315ms 855.793971ms 856.246189ms 856.736427ms 858.5701ms 863.075921ms 867.301744ms 867.452243ms 867.661602ms 868.04282ms 874.934472ms 904.844229ms 913.869251ms 914.339929ms]
Sep  5 08:23:38.390: INFO: 50 %ile: 726.410745ms
Sep  5 08:23:38.390: INFO: 90 %ile: 841.971888ms
Sep  5 08:23:38.391: INFO: 99 %ile: 913.869251ms
Sep  5 08:23:38.391: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:23:38.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3587" for this suite.
Sep  5 08:24:00.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:24:00.604: INFO: namespace svc-latency-3587 deletion completed in 22.203271598s

• [SLOW TEST:36.609 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:24:00.605: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  5 08:24:04.918: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:24:04.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4588" for this suite.
Sep  5 08:24:10.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:24:11.108: INFO: namespace container-runtime-4588 deletion completed in 6.13267152s

• [SLOW TEST:10.503 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:24:11.109: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3103
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 08:24:11.299: INFO: Creating deployment "test-recreate-deployment"
Sep  5 08:24:11.305: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  5 08:24:11.363: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep  5 08:24:13.370: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  5 08:24:13.374: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703268643, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703268643, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703268643, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703268643, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 08:24:15.447: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  5 08:24:15.462: INFO: Updating deployment test-recreate-deployment
Sep  5 08:24:15.462: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep  5 08:24:15.717: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-3103,SelfLink:/apis/apps/v1/namespaces/deployment-3103/deployments/test-recreate-deployment,UID:ef50784d-13c2-404f-bb5e-107e0dc87760,ResourceVersion:237196,Generation:2,CreationTimestamp:2019-09-05 08:24:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-09-05 08:24:07 +0000 UTC 2019-09-05 08:24:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-09-05 08:24:07 +0000 UTC 2019-09-05 08:24:03 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Sep  5 08:24:15.729: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-3103,SelfLink:/apis/apps/v1/namespaces/deployment-3103/replicasets/test-recreate-deployment-5c8c9cc69d,UID:d7993f9f-b5ce-4203-bf1f-876c46e26ec3,ResourceVersion:237194,Generation:1,CreationTimestamp:2019-09-05 08:24:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ef50784d-13c2-404f-bb5e-107e0dc87760 0x400394cd87 0x400394cd88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  5 08:24:15.729: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  5 08:24:15.729: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-3103,SelfLink:/apis/apps/v1/namespaces/deployment-3103/replicasets/test-recreate-deployment-6df85df6b9,UID:9dc9dbb5-6b9c-4238-89b0-89cec2f87035,ResourceVersion:237184,Generation:2,CreationTimestamp:2019-09-05 08:24:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ef50784d-13c2-404f-bb5e-107e0dc87760 0x400394ce57 0x400394ce58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  5 08:24:15.733: INFO: Pod "test-recreate-deployment-5c8c9cc69d-hv9hb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-hv9hb,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-3103,SelfLink:/api/v1/namespaces/deployment-3103/pods/test-recreate-deployment-5c8c9cc69d-hv9hb,UID:53527d6e-7207-4f4f-93f8-1cd690cdba0a,ResourceVersion:237197,Generation:0,CreationTimestamp:2019-09-05 08:24:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d d7993f9f-b5ce-4203-bf1f-876c46e26ec3 0x400394d727 0x400394d728}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-q4v5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-q4v5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-q4v5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x400394d7a0} {node.kubernetes.io/unreachable Exists  NoExecute 0x400394d7c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:24:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:24:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:24:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-05 08:24:07 +0000 UTC  }],Message:,Reason:,HostIP:10.200.72.30,PodIP:,StartTime:2019-09-05 08:24:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:24:15.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3103" for this suite.
Sep  5 08:24:21.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:24:21.910: INFO: namespace deployment-3103 deletion completed in 6.172134974s

• [SLOW TEST:10.801 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:24:21.910: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7019
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Sep  5 08:24:22.165: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  5 08:24:22.173: INFO: Waiting for terminating namespaces to be deleted...
Sep  5 08:24:22.177: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Sep  5 08:24:22.189: INFO: resource-reserver-master1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 08:24:22.189: INFO: coredns-84957c5548-fzgvg from kube-system started at 2019-09-03 09:41:40 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.189: INFO: 	Container coredns ready: true, restart count 6
Sep  5 08:24:22.189: INFO: ss-1 from statefulset-8350 started at 2019-09-04 07:06:28 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.189: INFO: 	Container nginx ready: true, restart count 2
Sep  5 08:24:22.189: INFO: calico-node-vb6dx from kube-system started at 2019-09-04 08:01:05 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.189: INFO: 	Container calico-node ready: true, restart count 2
Sep  5 08:24:22.189: INFO: simpletest.rc-hc5gf from gc-4645 started at 2019-09-05 07:11:19 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.189: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:24:22.189: INFO: kube-apiserver-master1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 08:24:22.189: INFO: calico-kube-controllers-769b56f588-r7cwg from kube-system started at 2019-09-02 10:08:06 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.189: INFO: 	Container calico-kube-controllers ready: true, restart count 7
Sep  5 08:24:22.189: INFO: tiller-deploy-6cfdc48f4f-7pm5m from kube-system started at 2019-09-02 10:09:09 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.189: INFO: 	Container tiller ready: true, restart count 7
Sep  5 08:24:22.189: INFO: simpletest.rc-zvjjm from gc-4645 started at 2019-09-05 07:11:20 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.189: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:24:22.189: INFO: kube-controller-manager-master1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 08:24:22.189: INFO: simpletest.rc-htxhl from gc-4645 started at 2019-09-05 07:11:19 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.189: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:24:22.189: INFO: simpletest.rc-xdhdv from gc-4645 started at 2019-09-05 07:11:19 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.189: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:24:22.189: INFO: netserver-0 from pod-network-test-8884 started at 2019-09-03 11:09:14 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.189: INFO: 	Container webserver ready: true, restart count 6
Sep  5 08:24:22.189: INFO: kube-proxy-master1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 08:24:22.189: INFO: simpletest.rc-wfnx5 from gc-4645 started at 2019-09-05 07:11:20 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.189: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:24:22.189: INFO: kube-scheduler-master1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 08:24:22.189: INFO: metrics-server-687c949fd7-k6ctz from kube-system started at 2019-09-02 10:10:34 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.189: INFO: 	Container metrics-server ready: true, restart count 7
Sep  5 08:24:22.189: INFO: update-demo-nautilus-bwggq from kubectl-3647 started at 2019-09-04 03:18:12 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.189: INFO: 	Container update-demo ready: true, restart count 4
Sep  5 08:24:22.189: INFO: 
Logging pods the kubelet thinks is on node slave1 before test
Sep  5 08:24:22.203: INFO: downwardapi-volume-e50296f7-5d62-48e2-99cc-cd1cbab6f497 from downward-api-6195 started at 2019-09-03 11:57:56 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container client-container ready: false, restart count 0
Sep  5 08:24:22.203: INFO: calico-node-tkcp8 from kube-system started at 2019-09-04 08:00:57 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container calico-node ready: true, restart count 1
Sep  5 08:24:22.203: INFO: kube-proxy-slave1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 08:24:22.203: INFO: update-demo-nautilus-tgtmr from kubectl-3647 started at 2019-09-04 03:18:12 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container update-demo ready: true, restart count 2
Sep  5 08:24:22.203: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-05 07:15:58 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  5 08:24:22.203: INFO: nginx-proxy-slave1 from kube-system started at <nil> (0 container statuses recorded)
Sep  5 08:24:22.203: INFO: ss-0 from statefulset-8350 started at 2019-09-04 07:05:55 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container nginx ready: true, restart count 1
Sep  5 08:24:22.203: INFO: ss-2 from statefulset-8350 started at 2019-09-04 07:06:18 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container nginx ready: true, restart count 1
Sep  5 08:24:22.203: INFO: simpletest.rc-t6282 from gc-4645 started at 2019-09-05 07:11:27 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:24:22.203: INFO: netserver-1 from pod-network-test-8884 started at 2019-09-03 11:09:14 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container webserver ready: true, restart count 3
Sep  5 08:24:22.203: INFO: simpletest.rc-khvx2 from gc-4645 started at 2019-09-05 07:11:28 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:24:22.203: INFO: simpletest.rc-rmlpm from gc-4645 started at 2019-09-05 07:11:28 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:24:22.203: INFO: busybox-d60e7c8e-4409-459a-b131-36debda93eda from container-probe-6051 started at 2019-09-04 02:41:13 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container busybox ready: true, restart count 109
Sep  5 08:24:22.203: INFO: sonobuoy-e2e-job-88e7e0d06e6141c3 from heptio-sonobuoy started at 2019-09-05 07:16:01 +0000 UTC (2 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container e2e ready: true, restart count 0
Sep  5 08:24:22.203: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  5 08:24:22.203: INFO: simpletest.rc-q64hx from gc-4645 started at 2019-09-05 07:11:27 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:24:22.203: INFO: update-demo-kitten-gz5rs from kubectl-3647 started at 2019-09-04 03:18:19 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container update-demo ready: true, restart count 2
Sep  5 08:24:22.203: INFO: simpletest.rc-bbp2f from gc-4645 started at 2019-09-05 07:11:27 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container nginx ready: true, restart count 0
Sep  5 08:24:22.203: INFO: test-webserver-edbf445f-a7f4-42d8-aa6f-ff39e531c400 from container-probe-1206 started at 2019-09-04 08:35:03 +0000 UTC (1 container statuses recorded)
Sep  5 08:24:22.203: INFO: 	Container test-webserver ready: false, restart count 1
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15c17d3ed71372fe], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:24:23.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7019" for this suite.
Sep  5 08:24:29.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:24:29.343: INFO: namespace sched-pred-7019 deletion completed in 6.107059059s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.433 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:24:29.343: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-622
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 08:24:29.593: INFO: (0) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 5.271799ms)
Sep  5 08:24:29.598: INFO: (1) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.99774ms)
Sep  5 08:24:29.603: INFO: (2) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 5.088659ms)
Sep  5 08:24:29.608: INFO: (3) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 5.311838ms)
Sep  5 08:24:29.614: INFO: (4) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 5.154039ms)
Sep  5 08:24:29.619: INFO: (5) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 5.300698ms)
Sep  5 08:24:29.624: INFO: (6) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 5.142999ms)
Sep  5 08:24:29.629: INFO: (7) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 5.265098ms)
Sep  5 08:24:29.634: INFO: (8) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.90418ms)
Sep  5 08:24:29.639: INFO: (9) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.75434ms)
Sep  5 08:24:29.644: INFO: (10) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.8144ms)
Sep  5 08:24:29.649: INFO: (11) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.679521ms)
Sep  5 08:24:29.653: INFO: (12) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.779741ms)
Sep  5 08:24:29.658: INFO: (13) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.75068ms)
Sep  5 08:24:29.663: INFO: (14) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.82884ms)
Sep  5 08:24:29.668: INFO: (15) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.73078ms)
Sep  5 08:24:29.673: INFO: (16) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.77008ms)
Sep  5 08:24:29.678: INFO: (17) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 4.91802ms)
Sep  5 08:24:29.683: INFO: (18) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 5.123239ms)
Sep  5 08:24:29.688: INFO: (19) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="ambari-agent/">ambari-agent/</a>
<... (200; 5.242698ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:24:29.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-622" for this suite.
Sep  5 08:24:35.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:24:35.801: INFO: namespace proxy-622 deletion completed in 6.108045454s

• [SLOW TEST:6.458 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:24:35.802: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1617
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-21ad8da1-629f-4c49-979a-3c62359e1198
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-21ad8da1-629f-4c49-979a-3c62359e1198
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:25:45.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1617" for this suite.
Sep  5 08:26:07.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:26:07.276: INFO: namespace configmap-1617 deletion completed in 22.247623243s

• [SLOW TEST:91.475 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:26:07.276: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4288
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep  5 08:26:12.097: INFO: Successfully updated pod "labelsupdatef5a28a53-77d7-41ac-88a2-8dce4282e199"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:26:16.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4288" for this suite.
Sep  5 08:26:38.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:26:38.416: INFO: namespace downward-api-4288 deletion completed in 22.189336532s

• [SLOW TEST:31.139 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:26:38.416: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6760
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 08:26:38.636: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e460974-a510-4622-aeeb-fe86db788252" in namespace "downward-api-6760" to be "success or failure"
Sep  5 08:26:38.665: INFO: Pod "downwardapi-volume-2e460974-a510-4622-aeeb-fe86db788252": Phase="Pending", Reason="", readiness=false. Elapsed: 28.889461ms
Sep  5 08:26:40.678: INFO: Pod "downwardapi-volume-2e460974-a510-4622-aeeb-fe86db788252": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041856222s
Sep  5 08:26:42.682: INFO: Pod "downwardapi-volume-2e460974-a510-4622-aeeb-fe86db788252": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046124639s
STEP: Saw pod success
Sep  5 08:26:42.682: INFO: Pod "downwardapi-volume-2e460974-a510-4622-aeeb-fe86db788252" satisfied condition "success or failure"
Sep  5 08:26:42.686: INFO: Trying to get logs from node slave1 pod downwardapi-volume-2e460974-a510-4622-aeeb-fe86db788252 container client-container: <nil>
STEP: delete the pod
Sep  5 08:26:42.718: INFO: Waiting for pod downwardapi-volume-2e460974-a510-4622-aeeb-fe86db788252 to disappear
Sep  5 08:26:42.740: INFO: Pod downwardapi-volume-2e460974-a510-4622-aeeb-fe86db788252 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:26:42.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6760" for this suite.
Sep  5 08:26:48.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:26:48.929: INFO: namespace downward-api-6760 deletion completed in 6.183370244s

• [SLOW TEST:10.513 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:26:48.930: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8466
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:26:56.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8466" for this suite.
Sep  5 08:27:18.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:27:18.314: INFO: namespace replication-controller-8466 deletion completed in 22.133638102s

• [SLOW TEST:29.384 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:27:18.314: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3602
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  5 08:27:18.538: INFO: Waiting up to 5m0s for pod "pod-56c6f9c8-806b-4975-987a-d23856aa67bf" in namespace "emptydir-3602" to be "success or failure"
Sep  5 08:27:18.552: INFO: Pod "pod-56c6f9c8-806b-4975-987a-d23856aa67bf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.845062ms
Sep  5 08:27:20.556: INFO: Pod "pod-56c6f9c8-806b-4975-987a-d23856aa67bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01724228s
Sep  5 08:27:22.560: INFO: Pod "pod-56c6f9c8-806b-4975-987a-d23856aa67bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021933171s
STEP: Saw pod success
Sep  5 08:27:22.561: INFO: Pod "pod-56c6f9c8-806b-4975-987a-d23856aa67bf" satisfied condition "success or failure"
Sep  5 08:27:22.598: INFO: Trying to get logs from node slave1 pod pod-56c6f9c8-806b-4975-987a-d23856aa67bf container test-container: <nil>
STEP: delete the pod
Sep  5 08:27:22.637: INFO: Waiting for pod pod-56c6f9c8-806b-4975-987a-d23856aa67bf to disappear
Sep  5 08:27:22.644: INFO: Pod pod-56c6f9c8-806b-4975-987a-d23856aa67bf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:27:22.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3602" for this suite.
Sep  5 08:27:28.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:27:28.767: INFO: namespace emptydir-3602 deletion completed in 6.118145083s

• [SLOW TEST:10.452 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:27:28.767: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  5 08:27:33.569: INFO: Successfully updated pod "pod-update-3b30b961-b15c-47b9-b400-443ecbaa2573"
STEP: verifying the updated pod is in kubernetes
Sep  5 08:27:33.616: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:27:33.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5481" for this suite.
Sep  5 08:27:55.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:27:55.732: INFO: namespace pods-5481 deletion completed in 22.104517985s

• [SLOW TEST:26.965 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:27:55.733: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  5 08:27:55.989: INFO: Number of nodes with available pods: 0
Sep  5 08:27:55.989: INFO: Node master1 is running more than one daemon pod
Sep  5 08:27:56.998: INFO: Number of nodes with available pods: 0
Sep  5 08:27:56.998: INFO: Node master1 is running more than one daemon pod
Sep  5 08:27:58.002: INFO: Number of nodes with available pods: 0
Sep  5 08:27:58.002: INFO: Node master1 is running more than one daemon pod
Sep  5 08:27:59.282: INFO: Number of nodes with available pods: 0
Sep  5 08:27:59.282: INFO: Node master1 is running more than one daemon pod
Sep  5 08:27:59.998: INFO: Number of nodes with available pods: 1
Sep  5 08:27:59.998: INFO: Node master1 is running more than one daemon pod
Sep  5 08:28:00.997: INFO: Number of nodes with available pods: 2
Sep  5 08:28:00.997: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep  5 08:28:01.054: INFO: Number of nodes with available pods: 1
Sep  5 08:28:01.054: INFO: Node slave1 is running more than one daemon pod
Sep  5 08:28:02.065: INFO: Number of nodes with available pods: 1
Sep  5 08:28:02.065: INFO: Node slave1 is running more than one daemon pod
Sep  5 08:28:03.062: INFO: Number of nodes with available pods: 1
Sep  5 08:28:03.062: INFO: Node slave1 is running more than one daemon pod
Sep  5 08:28:04.063: INFO: Number of nodes with available pods: 1
Sep  5 08:28:04.063: INFO: Node slave1 is running more than one daemon pod
Sep  5 08:28:05.063: INFO: Number of nodes with available pods: 1
Sep  5 08:28:05.063: INFO: Node slave1 is running more than one daemon pod
Sep  5 08:28:06.064: INFO: Number of nodes with available pods: 2
Sep  5 08:28:06.064: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2394, will wait for the garbage collector to delete the pods
Sep  5 08:28:06.162: INFO: Deleting DaemonSet.extensions daemon-set took: 6.017835ms
Sep  5 08:28:06.462: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.208878ms
Sep  5 08:28:18.470: INFO: Number of nodes with available pods: 0
Sep  5 08:28:18.470: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 08:28:18.473: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2394/daemonsets","resourceVersion":"237953"},"items":null}

Sep  5 08:28:18.475: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2394/pods","resourceVersion":"237953"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:28:18.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2394" for this suite.
Sep  5 08:28:24.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:28:24.664: INFO: namespace daemonsets-2394 deletion completed in 6.16373774s

• [SLOW TEST:28.931 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:28:24.665: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9479
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 08:28:24.946: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61730b8a-88d7-4786-b75d-0e145bc1f4d5" in namespace "downward-api-9479" to be "success or failure"
Sep  5 08:28:24.953: INFO: Pod "downwardapi-volume-61730b8a-88d7-4786-b75d-0e145bc1f4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.875691ms
Sep  5 08:28:26.957: INFO: Pod "downwardapi-volume-61730b8a-88d7-4786-b75d-0e145bc1f4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010885s
Sep  5 08:28:28.960: INFO: Pod "downwardapi-volume-61730b8a-88d7-4786-b75d-0e145bc1f4d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01442329s
STEP: Saw pod success
Sep  5 08:28:28.960: INFO: Pod "downwardapi-volume-61730b8a-88d7-4786-b75d-0e145bc1f4d5" satisfied condition "success or failure"
Sep  5 08:28:28.963: INFO: Trying to get logs from node slave1 pod downwardapi-volume-61730b8a-88d7-4786-b75d-0e145bc1f4d5 container client-container: <nil>
STEP: delete the pod
Sep  5 08:28:29.012: INFO: Waiting for pod downwardapi-volume-61730b8a-88d7-4786-b75d-0e145bc1f4d5 to disappear
Sep  5 08:28:29.018: INFO: Pod downwardapi-volume-61730b8a-88d7-4786-b75d-0e145bc1f4d5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:28:29.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9479" for this suite.
Sep  5 08:28:35.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:28:35.200: INFO: namespace downward-api-9479 deletion completed in 6.177110861s

• [SLOW TEST:10.535 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:28:35.200: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5877
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
W0905 08:28:36.588427      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  5 08:28:36.588: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:28:36.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5877" for this suite.
Sep  5 08:28:42.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:28:42.722: INFO: namespace gc-5877 deletion completed in 6.129301677s

• [SLOW TEST:7.521 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:28:42.722: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8743
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 08:28:42.951: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ede44d73-c005-41c1-85ed-61e28de7b3d4" in namespace "downward-api-8743" to be "success or failure"
Sep  5 08:28:42.973: INFO: Pod "downwardapi-volume-ede44d73-c005-41c1-85ed-61e28de7b3d4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.229948ms
Sep  5 08:28:44.981: INFO: Pod "downwardapi-volume-ede44d73-c005-41c1-85ed-61e28de7b3d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030502177s
Sep  5 08:28:46.985: INFO: Pod "downwardapi-volume-ede44d73-c005-41c1-85ed-61e28de7b3d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034599344s
STEP: Saw pod success
Sep  5 08:28:46.985: INFO: Pod "downwardapi-volume-ede44d73-c005-41c1-85ed-61e28de7b3d4" satisfied condition "success or failure"
Sep  5 08:28:46.988: INFO: Trying to get logs from node slave1 pod downwardapi-volume-ede44d73-c005-41c1-85ed-61e28de7b3d4 container client-container: <nil>
STEP: delete the pod
Sep  5 08:28:47.043: INFO: Waiting for pod downwardapi-volume-ede44d73-c005-41c1-85ed-61e28de7b3d4 to disappear
Sep  5 08:28:47.049: INFO: Pod downwardapi-volume-ede44d73-c005-41c1-85ed-61e28de7b3d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:28:47.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8743" for this suite.
Sep  5 08:28:53.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:28:53.153: INFO: namespace downward-api-8743 deletion completed in 6.099563317s

• [SLOW TEST:10.431 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:28:53.153: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Sep  5 08:28:53.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 create -f - --namespace=kubectl-2865'
Sep  5 08:28:56.136: INFO: stderr: ""
Sep  5 08:28:56.136: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Sep  5 08:28:57.140: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:28:57.141: INFO: Found 0 / 1
Sep  5 08:28:58.146: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:28:58.146: INFO: Found 0 / 1
Sep  5 08:28:59.140: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:28:59.140: INFO: Found 0 / 1
Sep  5 08:29:00.140: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:29:00.140: INFO: Found 1 / 1
Sep  5 08:29:00.140: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  5 08:29:00.143: INFO: Selector matched 1 pods for map[app:redis]
Sep  5 08:29:00.143: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Sep  5 08:29:00.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 logs redis-master-hbdgk redis-master --namespace=kubectl-2865'
Sep  5 08:29:00.338: INFO: stderr: ""
Sep  5 08:29:00.338: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Sep 08:28:58.779 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Sep 08:28:58.779 # Server started, Redis version 3.2.12\n1:M 05 Sep 08:28:58.779 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Sep 08:28:58.779 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Sep  5 08:29:00.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 log redis-master-hbdgk redis-master --namespace=kubectl-2865 --tail=1'
Sep  5 08:29:00.531: INFO: stderr: ""
Sep  5 08:29:00.531: INFO: stdout: "1:M 05 Sep 08:28:58.779 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Sep  5 08:29:00.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 log redis-master-hbdgk redis-master --namespace=kubectl-2865 --limit-bytes=1'
Sep  5 08:29:00.718: INFO: stderr: ""
Sep  5 08:29:00.718: INFO: stdout: " "
STEP: exposing timestamps
Sep  5 08:29:00.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 log redis-master-hbdgk redis-master --namespace=kubectl-2865 --tail=1 --timestamps'
Sep  5 08:29:00.906: INFO: stderr: ""
Sep  5 08:29:00.906: INFO: stdout: "2019-09-05T08:28:58.782250435Z 1:M 05 Sep 08:28:58.779 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Sep  5 08:29:03.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 log redis-master-hbdgk redis-master --namespace=kubectl-2865 --since=1s'
Sep  5 08:29:03.601: INFO: stderr: ""
Sep  5 08:29:03.601: INFO: stdout: ""
Sep  5 08:29:03.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 log redis-master-hbdgk redis-master --namespace=kubectl-2865 --since=24h'
Sep  5 08:29:03.793: INFO: stderr: ""
Sep  5 08:29:03.793: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Sep 08:28:58.779 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Sep 08:28:58.779 # Server started, Redis version 3.2.12\n1:M 05 Sep 08:28:58.779 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Sep 08:28:58.779 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Sep  5 08:29:03.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete --grace-period=0 --force -f - --namespace=kubectl-2865'
Sep  5 08:29:03.976: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 08:29:03.976: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Sep  5 08:29:03.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get rc,svc -l name=nginx --no-headers --namespace=kubectl-2865'
Sep  5 08:29:04.157: INFO: stderr: "No resources found.\n"
Sep  5 08:29:04.157: INFO: stdout: ""
Sep  5 08:29:04.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -l name=nginx --namespace=kubectl-2865 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 08:29:04.329: INFO: stderr: ""
Sep  5 08:29:04.329: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:29:04.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2865" for this suite.
Sep  5 08:29:10.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:29:10.477: INFO: namespace kubectl-2865 deletion completed in 6.142142176s

• [SLOW TEST:17.324 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:29:10.479: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  5 08:29:10.673: INFO: Waiting up to 5m0s for pod "pod-9f3b0cf8-fd8b-4517-9428-8d66b84516e8" in namespace "emptydir-8694" to be "success or failure"
Sep  5 08:29:10.728: INFO: Pod "pod-9f3b0cf8-fd8b-4517-9428-8d66b84516e8": Phase="Pending", Reason="", readiness=false. Elapsed: 54.440035ms
Sep  5 08:29:12.732: INFO: Pod "pod-9f3b0cf8-fd8b-4517-9428-8d66b84516e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058562239s
Sep  5 08:29:14.798: INFO: Pod "pod-9f3b0cf8-fd8b-4517-9428-8d66b84516e8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.124611086s
Sep  5 08:29:16.802: INFO: Pod "pod-9f3b0cf8-fd8b-4517-9428-8d66b84516e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.128337312s
STEP: Saw pod success
Sep  5 08:29:16.802: INFO: Pod "pod-9f3b0cf8-fd8b-4517-9428-8d66b84516e8" satisfied condition "success or failure"
Sep  5 08:29:16.805: INFO: Trying to get logs from node slave1 pod pod-9f3b0cf8-fd8b-4517-9428-8d66b84516e8 container test-container: <nil>
STEP: delete the pod
Sep  5 08:29:16.860: INFO: Waiting for pod pod-9f3b0cf8-fd8b-4517-9428-8d66b84516e8 to disappear
Sep  5 08:29:16.894: INFO: Pod pod-9f3b0cf8-fd8b-4517-9428-8d66b84516e8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:29:16.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8694" for this suite.
Sep  5 08:29:22.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:29:23.036: INFO: namespace emptydir-8694 deletion completed in 6.137259733s

• [SLOW TEST:12.557 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:29:23.037: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7433
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep  5 08:29:23.270: INFO: Waiting up to 5m0s for pod "downward-api-09c37cd9-5537-44ae-80c0-b1356a81fd3c" in namespace "downward-api-7433" to be "success or failure"
Sep  5 08:29:23.278: INFO: Pod "downward-api-09c37cd9-5537-44ae-80c0-b1356a81fd3c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.177726ms
Sep  5 08:29:25.282: INFO: Pod "downward-api-09c37cd9-5537-44ae-80c0-b1356a81fd3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01214041s
Sep  5 08:29:27.286: INFO: Pod "downward-api-09c37cd9-5537-44ae-80c0-b1356a81fd3c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015822494s
Sep  5 08:29:29.290: INFO: Pod "downward-api-09c37cd9-5537-44ae-80c0-b1356a81fd3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019576658s
STEP: Saw pod success
Sep  5 08:29:29.290: INFO: Pod "downward-api-09c37cd9-5537-44ae-80c0-b1356a81fd3c" satisfied condition "success or failure"
Sep  5 08:29:29.292: INFO: Trying to get logs from node slave1 pod downward-api-09c37cd9-5537-44ae-80c0-b1356a81fd3c container dapi-container: <nil>
STEP: delete the pod
Sep  5 08:29:29.334: INFO: Waiting for pod downward-api-09c37cd9-5537-44ae-80c0-b1356a81fd3c to disappear
Sep  5 08:29:29.344: INFO: Pod downward-api-09c37cd9-5537-44ae-80c0-b1356a81fd3c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:29:29.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7433" for this suite.
Sep  5 08:29:35.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:29:35.448: INFO: namespace downward-api-7433 deletion completed in 6.100416122s

• [SLOW TEST:12.412 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:29:35.449: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9335
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  5 08:29:35.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9335'
Sep  5 08:29:35.839: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  5 08:29:35.839: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Sep  5 08:29:35.913: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-vh9v7]
Sep  5 08:29:35.913: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-vh9v7" in namespace "kubectl-9335" to be "running and ready"
Sep  5 08:29:35.933: INFO: Pod "e2e-test-nginx-rc-vh9v7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.125876ms
Sep  5 08:29:37.936: INFO: Pod "e2e-test-nginx-rc-vh9v7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023616741s
Sep  5 08:29:39.940: INFO: Pod "e2e-test-nginx-rc-vh9v7": Phase="Running", Reason="", readiness=true. Elapsed: 4.027069285s
Sep  5 08:29:39.940: INFO: Pod "e2e-test-nginx-rc-vh9v7" satisfied condition "running and ready"
Sep  5 08:29:39.940: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-vh9v7]
Sep  5 08:29:39.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 logs rc/e2e-test-nginx-rc --namespace=kubectl-9335'
Sep  5 08:29:40.151: INFO: stderr: ""
Sep  5 08:29:40.151: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Sep  5 08:29:40.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete rc e2e-test-nginx-rc --namespace=kubectl-9335'
Sep  5 08:29:40.337: INFO: stderr: ""
Sep  5 08:29:40.337: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:29:40.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9335" for this suite.
Sep  5 08:29:46.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:29:46.443: INFO: namespace kubectl-9335 deletion completed in 6.102051912s

• [SLOW TEST:10.995 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:29:46.444: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6890
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-037350df-01e9-4913-b602-c9aee1c92a7b
STEP: Creating a pod to test consume secrets
Sep  5 08:29:46.677: INFO: Waiting up to 5m0s for pod "pod-secrets-37313a71-1662-47f1-b01e-1387fb7085e0" in namespace "secrets-6890" to be "success or failure"
Sep  5 08:29:46.708: INFO: Pod "pod-secrets-37313a71-1662-47f1-b01e-1387fb7085e0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.397314ms
Sep  5 08:29:48.713: INFO: Pod "pod-secrets-37313a71-1662-47f1-b01e-1387fb7085e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035913569s
Sep  5 08:29:50.721: INFO: Pod "pod-secrets-37313a71-1662-47f1-b01e-1387fb7085e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043890514s
STEP: Saw pod success
Sep  5 08:29:50.721: INFO: Pod "pod-secrets-37313a71-1662-47f1-b01e-1387fb7085e0" satisfied condition "success or failure"
Sep  5 08:29:50.724: INFO: Trying to get logs from node slave1 pod pod-secrets-37313a71-1662-47f1-b01e-1387fb7085e0 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 08:29:50.751: INFO: Waiting for pod pod-secrets-37313a71-1662-47f1-b01e-1387fb7085e0 to disappear
Sep  5 08:29:50.758: INFO: Pod pod-secrets-37313a71-1662-47f1-b01e-1387fb7085e0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:29:50.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6890" for this suite.
Sep  5 08:29:56.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:29:56.869: INFO: namespace secrets-6890 deletion completed in 6.101680351s

• [SLOW TEST:10.426 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:29:56.870: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7663
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Sep  5 08:29:57.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 create -f - --namespace=kubectl-7663'
Sep  5 08:29:57.562: INFO: stderr: ""
Sep  5 08:29:57.562: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 08:29:57.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7663'
Sep  5 08:29:57.750: INFO: stderr: ""
Sep  5 08:29:57.750: INFO: stdout: "update-demo-nautilus-6hxbm update-demo-nautilus-xmtlh "
Sep  5 08:29:57.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-6hxbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7663'
Sep  5 08:29:57.918: INFO: stderr: ""
Sep  5 08:29:57.918: INFO: stdout: ""
Sep  5 08:29:57.918: INFO: update-demo-nautilus-6hxbm is created but not running
Sep  5 08:30:02.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7663'
Sep  5 08:30:03.093: INFO: stderr: ""
Sep  5 08:30:03.093: INFO: stdout: "update-demo-nautilus-6hxbm update-demo-nautilus-xmtlh "
Sep  5 08:30:03.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-6hxbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7663'
Sep  5 08:30:03.264: INFO: stderr: ""
Sep  5 08:30:03.264: INFO: stdout: "true"
Sep  5 08:30:03.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-6hxbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7663'
Sep  5 08:30:03.434: INFO: stderr: ""
Sep  5 08:30:03.434: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 08:30:03.434: INFO: validating pod update-demo-nautilus-6hxbm
Sep  5 08:30:03.499: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 08:30:03.499: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 08:30:03.500: INFO: update-demo-nautilus-6hxbm is verified up and running
Sep  5 08:30:03.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-xmtlh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7663'
Sep  5 08:30:03.669: INFO: stderr: ""
Sep  5 08:30:03.669: INFO: stdout: "true"
Sep  5 08:30:03.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-xmtlh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7663'
Sep  5 08:30:03.836: INFO: stderr: ""
Sep  5 08:30:03.836: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 08:30:03.836: INFO: validating pod update-demo-nautilus-xmtlh
Sep  5 08:30:03.841: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 08:30:03.841: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 08:30:03.841: INFO: update-demo-nautilus-xmtlh is verified up and running
STEP: scaling down the replication controller
Sep  5 08:30:03.844: INFO: scanned /root for discovery docs: <nil>
Sep  5 08:30:03.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7663'
Sep  5 08:30:05.049: INFO: stderr: ""
Sep  5 08:30:05.049: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 08:30:05.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7663'
Sep  5 08:30:05.237: INFO: stderr: ""
Sep  5 08:30:05.237: INFO: stdout: "update-demo-nautilus-6hxbm update-demo-nautilus-xmtlh "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  5 08:30:02.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7663'
Sep  5 08:30:02.334: INFO: stderr: ""
Sep  5 08:30:02.335: INFO: stdout: "update-demo-nautilus-6hxbm update-demo-nautilus-xmtlh "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  5 08:30:07.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7663'
Sep  5 08:30:07.509: INFO: stderr: ""
Sep  5 08:30:07.510: INFO: stdout: "update-demo-nautilus-6hxbm "
Sep  5 08:30:07.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-6hxbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7663'
Sep  5 08:30:07.674: INFO: stderr: ""
Sep  5 08:30:07.674: INFO: stdout: "true"
Sep  5 08:30:07.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-6hxbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7663'
Sep  5 08:30:07.844: INFO: stderr: ""
Sep  5 08:30:07.844: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 08:30:07.844: INFO: validating pod update-demo-nautilus-6hxbm
Sep  5 08:30:07.848: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 08:30:07.848: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 08:30:07.848: INFO: update-demo-nautilus-6hxbm is verified up and running
STEP: scaling up the replication controller
Sep  5 08:30:07.850: INFO: scanned /root for discovery docs: <nil>
Sep  5 08:30:07.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7663'
Sep  5 08:30:09.061: INFO: stderr: ""
Sep  5 08:30:09.061: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 08:30:09.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7663'
Sep  5 08:30:09.243: INFO: stderr: ""
Sep  5 08:30:09.243: INFO: stdout: "update-demo-nautilus-6hxbm update-demo-nautilus-tbf65 "
Sep  5 08:30:09.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-6hxbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7663'
Sep  5 08:30:09.414: INFO: stderr: ""
Sep  5 08:30:09.414: INFO: stdout: "true"
Sep  5 08:30:09.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-6hxbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7663'
Sep  5 08:30:09.639: INFO: stderr: ""
Sep  5 08:30:09.639: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 08:30:09.639: INFO: validating pod update-demo-nautilus-6hxbm
Sep  5 08:30:09.643: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 08:30:09.643: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 08:30:09.643: INFO: update-demo-nautilus-6hxbm is verified up and running
Sep  5 08:30:09.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-tbf65 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7663'
Sep  5 08:30:09.812: INFO: stderr: ""
Sep  5 08:30:09.812: INFO: stdout: ""
Sep  5 08:30:09.812: INFO: update-demo-nautilus-tbf65 is created but not running
Sep  5 08:30:14.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7663'
Sep  5 08:30:14.988: INFO: stderr: ""
Sep  5 08:30:14.988: INFO: stdout: "update-demo-nautilus-6hxbm update-demo-nautilus-tbf65 "
Sep  5 08:30:14.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-6hxbm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7663'
Sep  5 08:30:15.153: INFO: stderr: ""
Sep  5 08:30:15.154: INFO: stdout: "true"
Sep  5 08:30:15.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-6hxbm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7663'
Sep  5 08:30:15.320: INFO: stderr: ""
Sep  5 08:30:15.320: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 08:30:15.320: INFO: validating pod update-demo-nautilus-6hxbm
Sep  5 08:30:15.324: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 08:30:15.324: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 08:30:15.324: INFO: update-demo-nautilus-6hxbm is verified up and running
Sep  5 08:30:15.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-tbf65 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7663'
Sep  5 08:30:15.495: INFO: stderr: ""
Sep  5 08:30:15.495: INFO: stdout: "true"
Sep  5 08:30:15.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods update-demo-nautilus-tbf65 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7663'
Sep  5 08:30:15.660: INFO: stderr: ""
Sep  5 08:30:15.660: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 08:30:15.660: INFO: validating pod update-demo-nautilus-tbf65
Sep  5 08:30:15.720: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 08:30:15.720: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 08:30:15.720: INFO: update-demo-nautilus-tbf65 is verified up and running
STEP: using delete to clean up resources
Sep  5 08:30:15.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete --grace-period=0 --force -f - --namespace=kubectl-7663'
Sep  5 08:30:15.903: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 08:30:15.903: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  5 08:30:15.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7663'
Sep  5 08:30:16.083: INFO: stderr: "No resources found.\n"
Sep  5 08:30:16.083: INFO: stdout: ""
Sep  5 08:30:16.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 get pods -l name=update-demo --namespace=kubectl-7663 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 08:30:16.275: INFO: stderr: ""
Sep  5 08:30:16.275: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:30:16.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7663" for this suite.
Sep  5 08:30:38.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:30:38.388: INFO: namespace kubectl-7663 deletion completed in 22.107265738s

• [SLOW TEST:49.595 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:30:38.389: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3359
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-6cf1d161-079e-41da-ac8b-f09ac2bde7de
STEP: Creating a pod to test consume secrets
Sep  5 08:30:38.645: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8006c5ff-67f7-4c29-b093-fe957909277e" in namespace "projected-3359" to be "success or failure"
Sep  5 08:30:38.656: INFO: Pod "pod-projected-secrets-8006c5ff-67f7-4c29-b093-fe957909277e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.177631ms
Sep  5 08:30:40.694: INFO: Pod "pod-projected-secrets-8006c5ff-67f7-4c29-b093-fe957909277e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048347911s
Sep  5 08:30:42.706: INFO: Pod "pod-projected-secrets-8006c5ff-67f7-4c29-b093-fe957909277e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060923257s
STEP: Saw pod success
Sep  5 08:30:42.706: INFO: Pod "pod-projected-secrets-8006c5ff-67f7-4c29-b093-fe957909277e" satisfied condition "success or failure"
Sep  5 08:30:42.709: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-8006c5ff-67f7-4c29-b093-fe957909277e container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 08:30:42.815: INFO: Waiting for pod pod-projected-secrets-8006c5ff-67f7-4c29-b093-fe957909277e to disappear
Sep  5 08:30:42.822: INFO: Pod pod-projected-secrets-8006c5ff-67f7-4c29-b093-fe957909277e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:30:42.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3359" for this suite.
Sep  5 08:30:48.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:30:48.932: INFO: namespace projected-3359 deletion completed in 6.104712388s

• [SLOW TEST:10.543 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:30:48.932: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7146
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  5 08:30:49.211: INFO: Waiting up to 5m0s for pod "pod-6f71676d-dd3f-4da0-895b-b101b6f9e194" in namespace "emptydir-7146" to be "success or failure"
Sep  5 08:30:49.236: INFO: Pod "pod-6f71676d-dd3f-4da0-895b-b101b6f9e194": Phase="Pending", Reason="", readiness=false. Elapsed: 24.944633ms
Sep  5 08:30:51.242: INFO: Pod "pod-6f71676d-dd3f-4da0-895b-b101b6f9e194": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030410749s
Sep  5 08:30:53.245: INFO: Pod "pod-6f71676d-dd3f-4da0-895b-b101b6f9e194": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033983914s
STEP: Saw pod success
Sep  5 08:30:53.245: INFO: Pod "pod-6f71676d-dd3f-4da0-895b-b101b6f9e194" satisfied condition "success or failure"
Sep  5 08:30:53.248: INFO: Trying to get logs from node slave1 pod pod-6f71676d-dd3f-4da0-895b-b101b6f9e194 container test-container: <nil>
STEP: delete the pod
Sep  5 08:30:53.364: INFO: Waiting for pod pod-6f71676d-dd3f-4da0-895b-b101b6f9e194 to disappear
Sep  5 08:30:53.371: INFO: Pod pod-6f71676d-dd3f-4da0-895b-b101b6f9e194 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:30:53.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7146" for this suite.
Sep  5 08:30:59.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:30:59.505: INFO: namespace emptydir-7146 deletion completed in 6.129379722s

• [SLOW TEST:10.573 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:30:59.505: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9892
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  5 08:31:05.021: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:31:05.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9892" for this suite.
Sep  5 08:31:11.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:31:11.265: INFO: namespace container-runtime-9892 deletion completed in 6.141451889s

• [SLOW TEST:11.761 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:31:11.266: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6030
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep  5 08:31:11.474: INFO: Waiting up to 5m0s for pod "pod-6aa25271-5e1b-4368-a620-6214fb5b96dc" in namespace "emptydir-6030" to be "success or failure"
Sep  5 08:31:11.485: INFO: Pod "pod-6aa25271-5e1b-4368-a620-6214fb5b96dc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.066252ms
Sep  5 08:31:13.502: INFO: Pod "pod-6aa25271-5e1b-4368-a620-6214fb5b96dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027932339s
Sep  5 08:31:15.523: INFO: Pod "pod-6aa25271-5e1b-4368-a620-6214fb5b96dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048200412s
Sep  5 08:31:17.526: INFO: Pod "pod-6aa25271-5e1b-4368-a620-6214fb5b96dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051906776s
STEP: Saw pod success
Sep  5 08:31:17.526: INFO: Pod "pod-6aa25271-5e1b-4368-a620-6214fb5b96dc" satisfied condition "success or failure"
Sep  5 08:31:17.529: INFO: Trying to get logs from node slave1 pod pod-6aa25271-5e1b-4368-a620-6214fb5b96dc container test-container: <nil>
STEP: delete the pod
Sep  5 08:31:17.622: INFO: Waiting for pod pod-6aa25271-5e1b-4368-a620-6214fb5b96dc to disappear
Sep  5 08:31:17.625: INFO: Pod pod-6aa25271-5e1b-4368-a620-6214fb5b96dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:31:17.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6030" for this suite.
Sep  5 08:31:23.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:31:23.732: INFO: namespace emptydir-6030 deletion completed in 6.102634337s

• [SLOW TEST:12.466 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:31:23.732: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4921
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep  5 08:31:24.279: INFO: Waiting up to 5m0s for pod "downward-api-f664d7fd-53bf-4d6c-ba2b-7bd2fad8d20d" in namespace "downward-api-4921" to be "success or failure"
Sep  5 08:31:24.313: INFO: Pod "downward-api-f664d7fd-53bf-4d6c-ba2b-7bd2fad8d20d": Phase="Pending", Reason="", readiness=false. Elapsed: 34.443451ms
Sep  5 08:31:26.370: INFO: Pod "downward-api-f664d7fd-53bf-4d6c-ba2b-7bd2fad8d20d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.090722988s
Sep  5 08:31:28.374: INFO: Pod "downward-api-f664d7fd-53bf-4d6c-ba2b-7bd2fad8d20d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.094711791s
Sep  5 08:31:30.378: INFO: Pod "downward-api-f664d7fd-53bf-4d6c-ba2b-7bd2fad8d20d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.098611214s
STEP: Saw pod success
Sep  5 08:31:30.378: INFO: Pod "downward-api-f664d7fd-53bf-4d6c-ba2b-7bd2fad8d20d" satisfied condition "success or failure"
Sep  5 08:31:30.380: INFO: Trying to get logs from node slave1 pod downward-api-f664d7fd-53bf-4d6c-ba2b-7bd2fad8d20d container dapi-container: <nil>
STEP: delete the pod
Sep  5 08:31:30.410: INFO: Waiting for pod downward-api-f664d7fd-53bf-4d6c-ba2b-7bd2fad8d20d to disappear
Sep  5 08:31:30.416: INFO: Pod downward-api-f664d7fd-53bf-4d6c-ba2b-7bd2fad8d20d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:31:30.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4921" for this suite.
Sep  5 08:31:36.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:31:36.611: INFO: namespace downward-api-4921 deletion completed in 6.190793496s

• [SLOW TEST:12.879 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:31:36.611: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1766
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-1766/configmap-test-e9243263-6a8f-4113-bd6b-51cc53542017
STEP: Creating a pod to test consume configMaps
Sep  5 08:31:36.818: INFO: Waiting up to 5m0s for pod "pod-configmaps-057aa032-0fc9-40b6-8a41-c47e8f88a98c" in namespace "configmap-1766" to be "success or failure"
Sep  5 08:31:36.847: INFO: Pod "pod-configmaps-057aa032-0fc9-40b6-8a41-c47e8f88a98c": Phase="Pending", Reason="", readiness=false. Elapsed: 28.472297ms
Sep  5 08:31:38.851: INFO: Pod "pod-configmaps-057aa032-0fc9-40b6-8a41-c47e8f88a98c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03235506s
Sep  5 08:31:40.854: INFO: Pod "pod-configmaps-057aa032-0fc9-40b6-8a41-c47e8f88a98c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036022004s
STEP: Saw pod success
Sep  5 08:31:40.854: INFO: Pod "pod-configmaps-057aa032-0fc9-40b6-8a41-c47e8f88a98c" satisfied condition "success or failure"
Sep  5 08:31:40.857: INFO: Trying to get logs from node slave1 pod pod-configmaps-057aa032-0fc9-40b6-8a41-c47e8f88a98c container env-test: <nil>
STEP: delete the pod
Sep  5 08:31:40.883: INFO: Waiting for pod pod-configmaps-057aa032-0fc9-40b6-8a41-c47e8f88a98c to disappear
Sep  5 08:31:40.890: INFO: Pod pod-configmaps-057aa032-0fc9-40b6-8a41-c47e8f88a98c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:31:40.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1766" for this suite.
Sep  5 08:31:46.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:31:47.000: INFO: namespace configmap-1766 deletion completed in 6.105621524s

• [SLOW TEST:10.388 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:31:47.000: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9270
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep  5 08:31:47.855: INFO: Pod name wrapped-volume-race-895de29d-a48a-47b6-837b-b91b0a9da87c: Found 0 pods out of 5
Sep  5 08:31:52.864: INFO: Pod name wrapped-volume-race-895de29d-a48a-47b6-837b-b91b0a9da87c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-895de29d-a48a-47b6-837b-b91b0a9da87c in namespace emptydir-wrapper-9270, will wait for the garbage collector to delete the pods
Sep  5 08:32:06.963: INFO: Deleting ReplicationController wrapped-volume-race-895de29d-a48a-47b6-837b-b91b0a9da87c took: 10.022337ms
Sep  5 08:32:07.363: INFO: Terminating ReplicationController wrapped-volume-race-895de29d-a48a-47b6-837b-b91b0a9da87c pods took: 400.189651ms
STEP: Creating RC which spawns configmap-volume pods
Sep  5 08:32:50.906: INFO: Pod name wrapped-volume-race-a9af8ba1-a596-41b7-96bc-a2336a177aad: Found 0 pods out of 5
Sep  5 08:32:55.971: INFO: Pod name wrapped-volume-race-a9af8ba1-a596-41b7-96bc-a2336a177aad: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a9af8ba1-a596-41b7-96bc-a2336a177aad in namespace emptydir-wrapper-9270, will wait for the garbage collector to delete the pods
Sep  5 08:33:10.822: INFO: Deleting ReplicationController wrapped-volume-race-a9af8ba1-a596-41b7-96bc-a2336a177aad took: 7.512628ms
Sep  5 08:33:11.222: INFO: Terminating ReplicationController wrapped-volume-race-a9af8ba1-a596-41b7-96bc-a2336a177aad pods took: 400.179371ms
STEP: Creating RC which spawns configmap-volume pods
Sep  5 08:34:00.867: INFO: Pod name wrapped-volume-race-641ac9f1-dacc-4e17-b464-6d468507224e: Found 0 pods out of 5
Sep  5 08:34:05.876: INFO: Pod name wrapped-volume-race-641ac9f1-dacc-4e17-b464-6d468507224e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-641ac9f1-dacc-4e17-b464-6d468507224e in namespace emptydir-wrapper-9270, will wait for the garbage collector to delete the pods
Sep  5 08:34:19.974: INFO: Deleting ReplicationController wrapped-volume-race-641ac9f1-dacc-4e17-b464-6d468507224e took: 8.868582ms
Sep  5 08:34:20.374: INFO: Terminating ReplicationController wrapped-volume-race-641ac9f1-dacc-4e17-b464-6d468507224e pods took: 400.221411ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:35:01.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9270" for this suite.
Sep  5 08:35:13.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:35:13.600: INFO: namespace emptydir-wrapper-9270 deletion completed in 12.147217825s

• [SLOW TEST:206.600 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:35:13.601: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-143
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  5 08:35:13.871: INFO: Waiting up to 5m0s for pod "pod-2fff6e38-67e7-4c4e-bc44-c761742ae175" in namespace "emptydir-143" to be "success or failure"
Sep  5 08:35:13.876: INFO: Pod "pod-2fff6e38-67e7-4c4e-bc44-c761742ae175": Phase="Pending", Reason="", readiness=false. Elapsed: 5.198238ms
Sep  5 08:35:15.909: INFO: Pod "pod-2fff6e38-67e7-4c4e-bc44-c761742ae175": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038150536s
Sep  5 08:35:17.913: INFO: Pod "pod-2fff6e38-67e7-4c4e-bc44-c761742ae175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041579541s
STEP: Saw pod success
Sep  5 08:35:17.913: INFO: Pod "pod-2fff6e38-67e7-4c4e-bc44-c761742ae175" satisfied condition "success or failure"
Sep  5 08:35:17.915: INFO: Trying to get logs from node slave1 pod pod-2fff6e38-67e7-4c4e-bc44-c761742ae175 container test-container: <nil>
STEP: delete the pod
Sep  5 08:35:17.970: INFO: Waiting for pod pod-2fff6e38-67e7-4c4e-bc44-c761742ae175 to disappear
Sep  5 08:35:17.983: INFO: Pod pod-2fff6e38-67e7-4c4e-bc44-c761742ae175 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:35:17.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-143" for this suite.
Sep  5 08:35:24.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:35:24.130: INFO: namespace emptydir-143 deletion completed in 6.119299165s

• [SLOW TEST:10.529 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:35:24.130: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3278
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Sep  5 08:35:24.991: INFO: created pod pod-service-account-defaultsa
Sep  5 08:35:24.991: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  5 08:35:25.001: INFO: created pod pod-service-account-mountsa
Sep  5 08:35:25.001: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  5 08:35:25.045: INFO: created pod pod-service-account-nomountsa
Sep  5 08:35:25.045: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  5 08:35:25.060: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  5 08:35:25.060: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  5 08:35:25.126: INFO: created pod pod-service-account-mountsa-mountspec
Sep  5 08:35:25.126: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  5 08:35:25.139: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  5 08:35:25.139: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  5 08:35:25.171: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  5 08:35:25.171: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  5 08:35:25.213: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  5 08:35:25.214: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  5 08:35:25.225: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  5 08:35:25.226: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:35:25.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3278" for this suite.
Sep  5 08:35:45.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:35:45.499: INFO: namespace svcaccounts-3278 deletion completed in 20.214000317s

• [SLOW TEST:21.369 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:35:45.500: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-104
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-94843ccd-486b-4f6d-bbaa-9093afe45d78
STEP: Creating configMap with name cm-test-opt-upd-5b2c250a-7536-4a8c-a833-05437103c6b2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-94843ccd-486b-4f6d-bbaa-9093afe45d78
STEP: Updating configmap cm-test-opt-upd-5b2c250a-7536-4a8c-a833-05437103c6b2
STEP: Creating configMap with name cm-test-opt-create-6830347f-c9c1-4339-96c7-672c71fc2b07
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:35:55.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-104" for this suite.
Sep  5 08:36:18.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:36:18.120: INFO: namespace configmap-104 deletion completed in 22.133349405s

• [SLOW TEST:32.620 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:36:18.121: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8665
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  5 08:36:18.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8665'
Sep  5 08:36:18.568: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  5 08:36:18.568: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Sep  5 08:36:20.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8665'
Sep  5 08:36:20.795: INFO: stderr: ""
Sep  5 08:36:20.795: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:36:20.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8665" for this suite.
Sep  5 08:36:42.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:36:43.047: INFO: namespace kubectl-8665 deletion completed in 22.246678596s

• [SLOW TEST:24.926 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:36:43.047: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9223
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Sep  5 08:36:43.259: INFO: Waiting up to 5m0s for pod "var-expansion-842b24c0-4078-4b9c-a3c5-1ae79ded09f2" in namespace "var-expansion-9223" to be "success or failure"
Sep  5 08:36:43.265: INFO: Pod "var-expansion-842b24c0-4078-4b9c-a3c5-1ae79ded09f2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.781095ms
Sep  5 08:36:45.268: INFO: Pod "var-expansion-842b24c0-4078-4b9c-a3c5-1ae79ded09f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00932444s
Sep  5 08:36:47.272: INFO: Pod "var-expansion-842b24c0-4078-4b9c-a3c5-1ae79ded09f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013145424s
STEP: Saw pod success
Sep  5 08:36:47.272: INFO: Pod "var-expansion-842b24c0-4078-4b9c-a3c5-1ae79ded09f2" satisfied condition "success or failure"
Sep  5 08:36:47.275: INFO: Trying to get logs from node slave1 pod var-expansion-842b24c0-4078-4b9c-a3c5-1ae79ded09f2 container dapi-container: <nil>
STEP: delete the pod
Sep  5 08:36:47.299: INFO: Waiting for pod var-expansion-842b24c0-4078-4b9c-a3c5-1ae79ded09f2 to disappear
Sep  5 08:36:47.306: INFO: Pod var-expansion-842b24c0-4078-4b9c-a3c5-1ae79ded09f2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:36:47.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9223" for this suite.
Sep  5 08:36:53.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:36:53.502: INFO: namespace var-expansion-9223 deletion completed in 6.192150351s

• [SLOW TEST:10.455 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:36:53.502: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6428
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-9989
STEP: Creating secret with name secret-test-70b5fab7-5ea0-4968-a46e-5fd65b740d4d
STEP: Creating a pod to test consume secrets
Sep  5 08:36:54.024: INFO: Waiting up to 5m0s for pod "pod-secrets-09e0c47d-cbcf-480c-9cf2-28800a9f8bb8" in namespace "secrets-6428" to be "success or failure"
Sep  5 08:36:54.031: INFO: Pod "pod-secrets-09e0c47d-cbcf-480c-9cf2-28800a9f8bb8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050074ms
Sep  5 08:36:56.034: INFO: Pod "pod-secrets-09e0c47d-cbcf-480c-9cf2-28800a9f8bb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009882278s
Sep  5 08:36:58.038: INFO: Pod "pod-secrets-09e0c47d-cbcf-480c-9cf2-28800a9f8bb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013439122s
STEP: Saw pod success
Sep  5 08:36:58.038: INFO: Pod "pod-secrets-09e0c47d-cbcf-480c-9cf2-28800a9f8bb8" satisfied condition "success or failure"
Sep  5 08:36:58.041: INFO: Trying to get logs from node slave1 pod pod-secrets-09e0c47d-cbcf-480c-9cf2-28800a9f8bb8 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 08:36:58.065: INFO: Waiting for pod pod-secrets-09e0c47d-cbcf-480c-9cf2-28800a9f8bb8 to disappear
Sep  5 08:36:58.086: INFO: Pod pod-secrets-09e0c47d-cbcf-480c-9cf2-28800a9f8bb8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:36:58.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6428" for this suite.
Sep  5 08:37:04.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:37:04.222: INFO: namespace secrets-6428 deletion completed in 6.131012554s
STEP: Destroying namespace "secret-namespace-9989" for this suite.
Sep  5 08:37:10.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:37:10.413: INFO: namespace secret-namespace-9989 deletion completed in 6.190595077s

• [SLOW TEST:16.911 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:37:10.413: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3009
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-58390492-0541-4413-abe0-c88c42d5e37b in namespace container-probe-3009
Sep  5 08:37:16.715: INFO: Started pod busybox-58390492-0541-4413-abe0-c88c42d5e37b in namespace container-probe-3009
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 08:37:16.718: INFO: Initial restart count of pod busybox-58390492-0541-4413-abe0-c88c42d5e37b is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:41:17.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3009" for this suite.
Sep  5 08:41:23.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:41:23.602: INFO: namespace container-probe-3009 deletion completed in 6.192647598s

• [SLOW TEST:253.189 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:41:23.603: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1540
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 08:41:23.836: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c1697b5-7526-47ed-a3f4-da5489066f64" in namespace "downward-api-1540" to be "success or failure"
Sep  5 08:41:23.842: INFO: Pod "downwardapi-volume-2c1697b5-7526-47ed-a3f4-da5489066f64": Phase="Pending", Reason="", readiness=false. Elapsed: 5.936235ms
Sep  5 08:41:25.846: INFO: Pod "downwardapi-volume-2c1697b5-7526-47ed-a3f4-da5489066f64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009926861s
Sep  5 08:41:27.850: INFO: Pod "downwardapi-volume-2c1697b5-7526-47ed-a3f4-da5489066f64": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013594648s
Sep  5 08:41:29.854: INFO: Pod "downwardapi-volume-2c1697b5-7526-47ed-a3f4-da5489066f64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017626754s
STEP: Saw pod success
Sep  5 08:41:29.854: INFO: Pod "downwardapi-volume-2c1697b5-7526-47ed-a3f4-da5489066f64" satisfied condition "success or failure"
Sep  5 08:41:29.857: INFO: Trying to get logs from node slave1 pod downwardapi-volume-2c1697b5-7526-47ed-a3f4-da5489066f64 container client-container: <nil>
STEP: delete the pod
Sep  5 08:41:29.950: INFO: Waiting for pod downwardapi-volume-2c1697b5-7526-47ed-a3f4-da5489066f64 to disappear
Sep  5 08:41:29.957: INFO: Pod downwardapi-volume-2c1697b5-7526-47ed-a3f4-da5489066f64 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:41:29.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1540" for this suite.
Sep  5 08:41:35.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:41:36.062: INFO: namespace downward-api-1540 deletion completed in 6.101300333s

• [SLOW TEST:12.460 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:41:36.063: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0905 08:41:42.574925      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  5 08:41:42.574: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:41:42.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7687" for this suite.
Sep  5 08:41:50.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:41:50.784: INFO: namespace gc-7687 deletion completed in 8.146984998s

• [SLOW TEST:14.721 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:41:50.785: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3607
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-dc8n
STEP: Creating a pod to test atomic-volume-subpath
Sep  5 08:41:51.024: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-dc8n" in namespace "subpath-3607" to be "success or failure"
Sep  5 08:41:51.031: INFO: Pod "pod-subpath-test-configmap-dc8n": Phase="Pending", Reason="", readiness=false. Elapsed: 6.483792ms
Sep  5 08:41:53.053: INFO: Pod "pod-subpath-test-configmap-dc8n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029042118s
Sep  5 08:41:55.057: INFO: Pod "pod-subpath-test-configmap-dc8n": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032951444s
Sep  5 08:41:57.061: INFO: Pod "pod-subpath-test-configmap-dc8n": Phase="Running", Reason="", readiness=true. Elapsed: 6.036835451s
Sep  5 08:41:59.065: INFO: Pod "pod-subpath-test-configmap-dc8n": Phase="Running", Reason="", readiness=true. Elapsed: 8.040730337s
Sep  5 08:42:01.069: INFO: Pod "pod-subpath-test-configmap-dc8n": Phase="Running", Reason="", readiness=true. Elapsed: 10.044295805s
Sep  5 08:42:03.089: INFO: Pod "pod-subpath-test-configmap-dc8n": Phase="Running", Reason="", readiness=true. Elapsed: 12.065165258s
Sep  5 08:42:05.093: INFO: Pod "pod-subpath-test-configmap-dc8n": Phase="Running", Reason="", readiness=true. Elapsed: 14.068369567s
Sep  5 08:42:07.097: INFO: Pod "pod-subpath-test-configmap-dc8n": Phase="Running", Reason="", readiness=true. Elapsed: 16.072608352s
Sep  5 08:42:09.100: INFO: Pod "pod-subpath-test-configmap-dc8n": Phase="Running", Reason="", readiness=true. Elapsed: 18.07612354s
Sep  5 08:42:11.104: INFO: Pod "pod-subpath-test-configmap-dc8n": Phase="Running", Reason="", readiness=true. Elapsed: 20.080151206s
Sep  5 08:42:13.108: INFO: Pod "pod-subpath-test-configmap-dc8n": Phase="Running", Reason="", readiness=true. Elapsed: 22.083696934s
Sep  5 08:42:15.112: INFO: Pod "pod-subpath-test-configmap-dc8n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.087291022s
STEP: Saw pod success
Sep  5 08:42:15.112: INFO: Pod "pod-subpath-test-configmap-dc8n" satisfied condition "success or failure"
Sep  5 08:42:15.114: INFO: Trying to get logs from node slave1 pod pod-subpath-test-configmap-dc8n container test-container-subpath-configmap-dc8n: <nil>
STEP: delete the pod
Sep  5 08:42:15.137: INFO: Waiting for pod pod-subpath-test-configmap-dc8n to disappear
Sep  5 08:42:15.196: INFO: Pod pod-subpath-test-configmap-dc8n no longer exists
STEP: Deleting pod pod-subpath-test-configmap-dc8n
Sep  5 08:42:15.196: INFO: Deleting pod "pod-subpath-test-configmap-dc8n" in namespace "subpath-3607"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:42:15.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3607" for this suite.
Sep  5 08:42:21.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:42:21.316: INFO: namespace subpath-3607 deletion completed in 6.11326714s

• [SLOW TEST:30.530 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:42:21.316: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9177
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0905 08:42:31.918233      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  5 08:42:31.918: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:42:31.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9177" for this suite.
Sep  5 08:42:39.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:42:40.043: INFO: namespace gc-9177 deletion completed in 8.121019208s

• [SLOW TEST:18.727 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:42:40.043: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6245
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 08:42:40.312: INFO: Waiting up to 5m0s for pod "downwardapi-volume-747c3bca-26a5-4c0b-bf4d-85c3b9c3370c" in namespace "projected-6245" to be "success or failure"
Sep  5 08:42:40.316: INFO: Pod "downwardapi-volume-747c3bca-26a5-4c0b-bf4d-85c3b9c3370c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.887263ms
Sep  5 08:42:42.320: INFO: Pod "downwardapi-volume-747c3bca-26a5-4c0b-bf4d-85c3b9c3370c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008175945s
Sep  5 08:42:44.325: INFO: Pod "downwardapi-volume-747c3bca-26a5-4c0b-bf4d-85c3b9c3370c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012330647s
STEP: Saw pod success
Sep  5 08:42:44.325: INFO: Pod "downwardapi-volume-747c3bca-26a5-4c0b-bf4d-85c3b9c3370c" satisfied condition "success or failure"
Sep  5 08:42:44.328: INFO: Trying to get logs from node slave1 pod downwardapi-volume-747c3bca-26a5-4c0b-bf4d-85c3b9c3370c container client-container: <nil>
STEP: delete the pod
Sep  5 08:42:44.394: INFO: Waiting for pod downwardapi-volume-747c3bca-26a5-4c0b-bf4d-85c3b9c3370c to disappear
Sep  5 08:42:44.399: INFO: Pod downwardapi-volume-747c3bca-26a5-4c0b-bf4d-85c3b9c3370c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:42:44.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6245" for this suite.
Sep  5 08:42:50.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:42:50.545: INFO: namespace projected-6245 deletion completed in 6.141481649s

• [SLOW TEST:10.502 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:42:50.546: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6185
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  5 08:42:50.777: INFO: Waiting up to 5m0s for pod "pod-57f0bc36-59cc-4dcd-b739-1e19299c41fe" in namespace "emptydir-6185" to be "success or failure"
Sep  5 08:42:50.790: INFO: Pod "pod-57f0bc36-59cc-4dcd-b739-1e19299c41fe": Phase="Pending", Reason="", readiness=false. Elapsed: 12.610905ms
Sep  5 08:42:52.891: INFO: Pod "pod-57f0bc36-59cc-4dcd-b739-1e19299c41fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.11339017s
Sep  5 08:42:54.940: INFO: Pod "pod-57f0bc36-59cc-4dcd-b739-1e19299c41fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.162627057s
STEP: Saw pod success
Sep  5 08:42:54.940: INFO: Pod "pod-57f0bc36-59cc-4dcd-b739-1e19299c41fe" satisfied condition "success or failure"
Sep  5 08:42:54.943: INFO: Trying to get logs from node slave1 pod pod-57f0bc36-59cc-4dcd-b739-1e19299c41fe container test-container: <nil>
STEP: delete the pod
Sep  5 08:42:54.966: INFO: Waiting for pod pod-57f0bc36-59cc-4dcd-b739-1e19299c41fe to disappear
Sep  5 08:42:54.972: INFO: Pod pod-57f0bc36-59cc-4dcd-b739-1e19299c41fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:42:54.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6185" for this suite.
Sep  5 08:43:00.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:43:01.082: INFO: namespace emptydir-6185 deletion completed in 6.105091427s

• [SLOW TEST:10.536 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:43:01.083: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0905 08:43:31.807101      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  5 08:43:31.807: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:43:31.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5423" for this suite.
Sep  5 08:43:37.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:43:38.018: INFO: namespace gc-5423 deletion completed in 6.207651143s

• [SLOW TEST:36.935 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:43:38.019: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3614
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 08:43:38.295: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00ce3c5b-1739-42a6-b218-d8f19b6616bb" in namespace "projected-3614" to be "success or failure"
Sep  5 08:43:38.355: INFO: Pod "downwardapi-volume-00ce3c5b-1739-42a6-b218-d8f19b6616bb": Phase="Pending", Reason="", readiness=false. Elapsed: 60.160881ms
Sep  5 08:43:40.359: INFO: Pod "downwardapi-volume-00ce3c5b-1739-42a6-b218-d8f19b6616bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063828525s
Sep  5 08:43:42.363: INFO: Pod "downwardapi-volume-00ce3c5b-1739-42a6-b218-d8f19b6616bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067808508s
STEP: Saw pod success
Sep  5 08:43:42.363: INFO: Pod "downwardapi-volume-00ce3c5b-1739-42a6-b218-d8f19b6616bb" satisfied condition "success or failure"
Sep  5 08:43:42.366: INFO: Trying to get logs from node slave1 pod downwardapi-volume-00ce3c5b-1739-42a6-b218-d8f19b6616bb container client-container: <nil>
STEP: delete the pod
Sep  5 08:43:42.421: INFO: Waiting for pod downwardapi-volume-00ce3c5b-1739-42a6-b218-d8f19b6616bb to disappear
Sep  5 08:43:42.449: INFO: Pod downwardapi-volume-00ce3c5b-1739-42a6-b218-d8f19b6616bb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:43:42.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3614" for this suite.
Sep  5 08:43:48.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:43:48.602: INFO: namespace projected-3614 deletion completed in 6.148685498s

• [SLOW TEST:10.584 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:43:48.603: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4263
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-b035792b-332f-404b-8a49-bd3a712c61b9
STEP: Creating a pod to test consume configMaps
Sep  5 08:43:48.868: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-32bfce15-2214-445b-858a-68c1b10ed47d" in namespace "projected-4263" to be "success or failure"
Sep  5 08:43:48.924: INFO: Pod "pod-projected-configmaps-32bfce15-2214-445b-858a-68c1b10ed47d": Phase="Pending", Reason="", readiness=false. Elapsed: 55.65356ms
Sep  5 08:43:50.927: INFO: Pod "pod-projected-configmaps-32bfce15-2214-445b-858a-68c1b10ed47d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059153225s
Sep  5 08:43:52.931: INFO: Pod "pod-projected-configmaps-32bfce15-2214-445b-858a-68c1b10ed47d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062873329s
Sep  5 08:43:54.935: INFO: Pod "pod-projected-configmaps-32bfce15-2214-445b-858a-68c1b10ed47d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066941552s
STEP: Saw pod success
Sep  5 08:43:54.935: INFO: Pod "pod-projected-configmaps-32bfce15-2214-445b-858a-68c1b10ed47d" satisfied condition "success or failure"
Sep  5 08:43:54.938: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-32bfce15-2214-445b-858a-68c1b10ed47d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 08:43:55.082: INFO: Waiting for pod pod-projected-configmaps-32bfce15-2214-445b-858a-68c1b10ed47d to disappear
Sep  5 08:43:55.089: INFO: Pod pod-projected-configmaps-32bfce15-2214-445b-858a-68c1b10ed47d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:43:55.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4263" for this suite.
Sep  5 08:44:01.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:44:01.223: INFO: namespace projected-4263 deletion completed in 6.129420482s

• [SLOW TEST:12.621 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:44:01.224: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3280
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:44:29.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3280" for this suite.
Sep  5 08:44:35.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:44:35.311: INFO: namespace container-runtime-3280 deletion completed in 6.141494809s

• [SLOW TEST:34.087 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:44:35.311: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-12e9a145-cb04-475a-9e54-7dc6038925bd in namespace container-probe-9417
Sep  5 08:44:39.571: INFO: Started pod test-webserver-12e9a145-cb04-475a-9e54-7dc6038925bd in namespace container-probe-9417
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 08:44:39.574: INFO: Initial restart count of pod test-webserver-12e9a145-cb04-475a-9e54-7dc6038925bd is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:48:40.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9417" for this suite.
Sep  5 08:48:46.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:48:46.761: INFO: namespace container-probe-9417 deletion completed in 6.193146906s

• [SLOW TEST:251.450 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:48:46.762: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8698
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-038e6716-d3df-4007-b7e4-6c31371ba8b4
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-038e6716-d3df-4007-b7e4-6c31371ba8b4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:48:55.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8698" for this suite.
Sep  5 08:49:17.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:49:17.340: INFO: namespace projected-8698 deletion completed in 22.117267235s

• [SLOW TEST:30.578 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:49:17.340: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 08:49:17.589: INFO: Waiting up to 5m0s for pod "downwardapi-volume-30bccfd0-6c72-41ba-8f15-db4f0e526997" in namespace "projected-8504" to be "success or failure"
Sep  5 08:49:17.603: INFO: Pod "downwardapi-volume-30bccfd0-6c72-41ba-8f15-db4f0e526997": Phase="Pending", Reason="", readiness=false. Elapsed: 14.00336ms
Sep  5 08:49:19.607: INFO: Pod "downwardapi-volume-30bccfd0-6c72-41ba-8f15-db4f0e526997": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018116822s
Sep  5 08:49:21.611: INFO: Pod "downwardapi-volume-30bccfd0-6c72-41ba-8f15-db4f0e526997": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022208465s
STEP: Saw pod success
Sep  5 08:49:21.611: INFO: Pod "downwardapi-volume-30bccfd0-6c72-41ba-8f15-db4f0e526997" satisfied condition "success or failure"
Sep  5 08:49:21.614: INFO: Trying to get logs from node slave1 pod downwardapi-volume-30bccfd0-6c72-41ba-8f15-db4f0e526997 container client-container: <nil>
STEP: delete the pod
Sep  5 08:49:21.637: INFO: Waiting for pod downwardapi-volume-30bccfd0-6c72-41ba-8f15-db4f0e526997 to disappear
Sep  5 08:49:21.643: INFO: Pod downwardapi-volume-30bccfd0-6c72-41ba-8f15-db4f0e526997 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:49:21.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8504" for this suite.
Sep  5 08:49:27.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:49:27.751: INFO: namespace projected-8504 deletion completed in 6.103182755s

• [SLOW TEST:10.410 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:49:27.752: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1038
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:49:34.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1038" for this suite.
Sep  5 08:50:14.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:50:14.202: INFO: namespace kubelet-test-1038 deletion completed in 40.172545388s

• [SLOW TEST:46.451 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:50:14.203: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9016
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-dca77cd9-994d-497d-afa7-df81a6d7126c
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:50:14.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9016" for this suite.
Sep  5 08:50:20.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:50:20.530: INFO: namespace configmap-9016 deletion completed in 6.1045519s

• [SLOW TEST:6.327 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:50:20.530: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 08:50:20.821: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep  5 08:50:20.838: INFO: Number of nodes with available pods: 0
Sep  5 08:50:20.838: INFO: Node master1 is running more than one daemon pod
Sep  5 08:50:21.847: INFO: Number of nodes with available pods: 0
Sep  5 08:50:21.847: INFO: Node master1 is running more than one daemon pod
Sep  5 08:50:22.922: INFO: Number of nodes with available pods: 0
Sep  5 08:50:22.922: INFO: Node master1 is running more than one daemon pod
Sep  5 08:50:23.863: INFO: Number of nodes with available pods: 1
Sep  5 08:50:23.863: INFO: Node slave1 is running more than one daemon pod
Sep  5 08:50:24.865: INFO: Number of nodes with available pods: 1
Sep  5 08:50:24.865: INFO: Node slave1 is running more than one daemon pod
Sep  5 08:50:25.846: INFO: Number of nodes with available pods: 2
Sep  5 08:50:25.846: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep  5 08:50:25.913: INFO: Wrong image for pod: daemon-set-djdjw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 08:50:25.913: INFO: Wrong image for pod: daemon-set-rvttz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 08:50:26.974: INFO: Wrong image for pod: daemon-set-djdjw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 08:50:26.974: INFO: Wrong image for pod: daemon-set-rvttz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 08:50:27.973: INFO: Wrong image for pod: daemon-set-djdjw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 08:50:27.973: INFO: Wrong image for pod: daemon-set-rvttz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 08:50:27.973: INFO: Pod daemon-set-rvttz is not available
Sep  5 08:50:29.033: INFO: Wrong image for pod: daemon-set-djdjw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 08:50:29.033: INFO: Wrong image for pod: daemon-set-rvttz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 08:50:29.033: INFO: Pod daemon-set-rvttz is not available
Sep  5 08:50:29.974: INFO: Wrong image for pod: daemon-set-djdjw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 08:50:29.974: INFO: Pod daemon-set-g4xjs is not available
Sep  5 08:50:31.058: INFO: Wrong image for pod: daemon-set-djdjw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 08:50:31.058: INFO: Pod daemon-set-g4xjs is not available
Sep  5 08:50:31.981: INFO: Wrong image for pod: daemon-set-djdjw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 08:50:31.981: INFO: Pod daemon-set-g4xjs is not available
Sep  5 08:50:32.974: INFO: Wrong image for pod: daemon-set-djdjw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 08:50:32.974: INFO: Pod daemon-set-g4xjs is not available
Sep  5 08:50:33.973: INFO: Wrong image for pod: daemon-set-djdjw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 08:50:34.973: INFO: Wrong image for pod: daemon-set-djdjw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 08:50:35.973: INFO: Wrong image for pod: daemon-set-djdjw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  5 08:50:35.973: INFO: Pod daemon-set-djdjw is not available
Sep  5 08:50:36.981: INFO: Pod daemon-set-jzcj6 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep  5 08:50:36.994: INFO: Number of nodes with available pods: 1
Sep  5 08:50:36.994: INFO: Node slave1 is running more than one daemon pod
Sep  5 08:50:38.003: INFO: Number of nodes with available pods: 1
Sep  5 08:50:38.003: INFO: Node slave1 is running more than one daemon pod
Sep  5 08:50:39.002: INFO: Number of nodes with available pods: 1
Sep  5 08:50:39.002: INFO: Node slave1 is running more than one daemon pod
Sep  5 08:50:40.002: INFO: Number of nodes with available pods: 1
Sep  5 08:50:40.002: INFO: Node slave1 is running more than one daemon pod
Sep  5 08:50:41.002: INFO: Number of nodes with available pods: 2
Sep  5 08:50:41.002: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9577, will wait for the garbage collector to delete the pods
Sep  5 08:50:41.076: INFO: Deleting DaemonSet.extensions daemon-set took: 5.691176ms
Sep  5 08:50:41.376: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.186902ms
Sep  5 08:50:56.080: INFO: Number of nodes with available pods: 0
Sep  5 08:50:56.080: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 08:50:56.082: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9577/daemonsets","resourceVersion":"243345"},"items":null}

Sep  5 08:50:56.085: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9577/pods","resourceVersion":"243345"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:50:56.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9577" for this suite.
Sep  5 08:51:02.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:51:02.237: INFO: namespace daemonsets-9577 deletion completed in 6.137111859s

• [SLOW TEST:41.706 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:51:02.238: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8249
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep  5 08:51:02.511: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8249,SelfLink:/api/v1/namespaces/watch-8249/configmaps/e2e-watch-test-label-changed,UID:cc877696-4516-448e-a16b-4b3036926f4c,ResourceVersion:243395,Generation:0,CreationTimestamp:2019-09-05 08:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  5 08:51:02.511: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8249,SelfLink:/api/v1/namespaces/watch-8249/configmaps/e2e-watch-test-label-changed,UID:cc877696-4516-448e-a16b-4b3036926f4c,ResourceVersion:243396,Generation:0,CreationTimestamp:2019-09-05 08:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep  5 08:51:02.511: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8249,SelfLink:/api/v1/namespaces/watch-8249/configmaps/e2e-watch-test-label-changed,UID:cc877696-4516-448e-a16b-4b3036926f4c,ResourceVersion:243397,Generation:0,CreationTimestamp:2019-09-05 08:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep  5 08:51:12.558: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8249,SelfLink:/api/v1/namespaces/watch-8249/configmaps/e2e-watch-test-label-changed,UID:cc877696-4516-448e-a16b-4b3036926f4c,ResourceVersion:243412,Generation:0,CreationTimestamp:2019-09-05 08:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  5 08:51:12.559: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8249,SelfLink:/api/v1/namespaces/watch-8249/configmaps/e2e-watch-test-label-changed,UID:cc877696-4516-448e-a16b-4b3036926f4c,ResourceVersion:243413,Generation:0,CreationTimestamp:2019-09-05 08:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Sep  5 08:51:12.559: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8249,SelfLink:/api/v1/namespaces/watch-8249/configmaps/e2e-watch-test-label-changed,UID:cc877696-4516-448e-a16b-4b3036926f4c,ResourceVersion:243414,Generation:0,CreationTimestamp:2019-09-05 08:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:51:12.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8249" for this suite.
Sep  5 08:51:18.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:51:18.712: INFO: namespace watch-8249 deletion completed in 6.146036461s

• [SLOW TEST:16.474 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:51:18.712: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep  5 08:51:18.924: INFO: PodSpec: initContainers in spec.initContainers
Sep  5 08:52:08.140: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-59347aac-9f6c-46dd-a8e6-af6d106ca681", GenerateName:"", Namespace:"init-container-5306", SelfLink:"/api/v1/namespaces/init-container-5306/pods/pod-init-59347aac-9f6c-46dd-a8e6-af6d106ca681", UID:"91e0ddb7-0a18-4ef5-846a-7c6997a29ca0", ResourceVersion:"243536", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63703270278, loc:(*time.Location)(0x791aa60)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"924347561"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-tkc62", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0x4001134b80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-tkc62", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-tkc62", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-tkc62", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0x40028f04f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"slave1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0x4002a9c360), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0x40028f0580)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0x40028f05a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0x40028f05a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0x40028f05ac), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703270279, loc:(*time.Location)(0x791aa60)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703270279, loc:(*time.Location)(0x791aa60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703270279, loc:(*time.Location)(0x791aa60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703270278, loc:(*time.Location)(0x791aa60)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.200.72.30", PodIP:"172.31.51.147", StartTime:(*v1.Time)(0x4002b4a2e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0x4002931180)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0x40029311f0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://9a47e1e9e958576c40dc6c818c1fcc452cb1a2204a73709798802c3679fb82ff"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0x4002b4a320), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0x4002b4a300), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:52:08.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5306" for this suite.
Sep  5 08:52:30.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:52:30.276: INFO: namespace init-container-5306 deletion completed in 22.106854328s

• [SLOW TEST:71.564 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:52:30.276: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-5949
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Sep  5 08:52:30.508: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Sep  5 08:52:32.026: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep  5 08:52:34.179: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703270352, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703270352, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703270352, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703270351, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 08:52:36.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703270352, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703270352, loc:(*time.Location)(0x791aa60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703270352, loc:(*time.Location)(0x791aa60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703270351, loc:(*time.Location)(0x791aa60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 08:52:49.912: INFO: Waited 11.655178594s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:52:50.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5949" for this suite.
Sep  5 08:52:56.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:52:56.564: INFO: namespace aggregator-5949 deletion completed in 6.226585233s

• [SLOW TEST:26.288 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:52:56.565: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3057
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep  5 08:53:00.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 exec pod-sharedvolume-30e4dcf7-d9e3-4fff-98b5-9fe6807f0918 -c busybox-main-container --namespace=emptydir-3057 -- cat /usr/share/volumeshare/shareddata.txt'
Sep  5 08:53:03.989: INFO: stderr: ""
Sep  5 08:53:03.990: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:53:03.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3057" for this suite.
Sep  5 08:53:10.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:53:10.164: INFO: namespace emptydir-3057 deletion completed in 6.169296561s

• [SLOW TEST:13.599 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:53:10.165: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6235
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6235
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  5 08:53:10.399: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  5 08:53:32.624: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.31.51.149:8080/dial?request=hostName&protocol=udp&host=172.31.51.146&port=8081&tries=1'] Namespace:pod-network-test-6235 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 08:53:32.624: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 08:53:32.904: INFO: Waiting for endpoints: map[]
Sep  5 08:53:32.908: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.31.51.149:8080/dial?request=hostName&protocol=udp&host=172.31.161.121&port=8081&tries=1'] Namespace:pod-network-test-6235 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 08:53:32.908: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 08:53:33.193: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:53:33.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6235" for this suite.
Sep  5 08:53:57.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:53:57.408: INFO: namespace pod-network-test-6235 deletion completed in 24.209730296s

• [SLOW TEST:47.243 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:53:57.409: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7370
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 08:53:57.637: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a0bf886-1eb8-450e-a7aa-5c353b340e16" in namespace "downward-api-7370" to be "success or failure"
Sep  5 08:53:57.666: INFO: Pod "downwardapi-volume-6a0bf886-1eb8-450e-a7aa-5c353b340e16": Phase="Pending", Reason="", readiness=false. Elapsed: 29.073214ms
Sep  5 08:53:59.685: INFO: Pod "downwardapi-volume-6a0bf886-1eb8-450e-a7aa-5c353b340e16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047589354s
Sep  5 08:54:01.688: INFO: Pod "downwardapi-volume-6a0bf886-1eb8-450e-a7aa-5c353b340e16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051269219s
STEP: Saw pod success
Sep  5 08:54:01.688: INFO: Pod "downwardapi-volume-6a0bf886-1eb8-450e-a7aa-5c353b340e16" satisfied condition "success or failure"
Sep  5 08:54:01.691: INFO: Trying to get logs from node slave1 pod downwardapi-volume-6a0bf886-1eb8-450e-a7aa-5c353b340e16 container client-container: <nil>
STEP: delete the pod
Sep  5 08:54:01.734: INFO: Waiting for pod downwardapi-volume-6a0bf886-1eb8-450e-a7aa-5c353b340e16 to disappear
Sep  5 08:54:01.758: INFO: Pod downwardapi-volume-6a0bf886-1eb8-450e-a7aa-5c353b340e16 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:54:01.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7370" for this suite.
Sep  5 08:54:07.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:54:07.867: INFO: namespace downward-api-7370 deletion completed in 6.103481814s

• [SLOW TEST:10.459 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:54:07.868: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2406
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-80a84cb0-6efe-4410-8ef5-0bde51d87688
STEP: Creating a pod to test consume configMaps
Sep  5 08:54:08.118: INFO: Waiting up to 5m0s for pod "pod-configmaps-68a66cfa-ff9f-48d6-9f09-1cbb3db4784a" in namespace "configmap-2406" to be "success or failure"
Sep  5 08:54:08.132: INFO: Pod "pod-configmaps-68a66cfa-ff9f-48d6-9f09-1cbb3db4784a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.92832ms
Sep  5 08:54:10.136: INFO: Pod "pod-configmaps-68a66cfa-ff9f-48d6-9f09-1cbb3db4784a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017628364s
Sep  5 08:54:12.140: INFO: Pod "pod-configmaps-68a66cfa-ff9f-48d6-9f09-1cbb3db4784a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021397348s
Sep  5 08:54:14.144: INFO: Pod "pod-configmaps-68a66cfa-ff9f-48d6-9f09-1cbb3db4784a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025219211s
STEP: Saw pod success
Sep  5 08:54:14.144: INFO: Pod "pod-configmaps-68a66cfa-ff9f-48d6-9f09-1cbb3db4784a" satisfied condition "success or failure"
Sep  5 08:54:14.147: INFO: Trying to get logs from node slave1 pod pod-configmaps-68a66cfa-ff9f-48d6-9f09-1cbb3db4784a container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 08:54:14.199: INFO: Waiting for pod pod-configmaps-68a66cfa-ff9f-48d6-9f09-1cbb3db4784a to disappear
Sep  5 08:54:14.202: INFO: Pod pod-configmaps-68a66cfa-ff9f-48d6-9f09-1cbb3db4784a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:54:14.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2406" for this suite.
Sep  5 08:54:20.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:54:20.308: INFO: namespace configmap-2406 deletion completed in 6.100961784s

• [SLOW TEST:12.440 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:54:20.308: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9774
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Sep  5 08:54:20.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815508381 api-versions'
Sep  5 08:54:20.843: INFO: stderr: ""
Sep  5 08:54:20.843: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:54:20.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9774" for this suite.
Sep  5 08:54:26.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:54:26.984: INFO: namespace kubectl-9774 deletion completed in 6.122766611s

• [SLOW TEST:6.676 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:54:26.985: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1082
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  5 08:54:27.237: INFO: Waiting up to 5m0s for pod "pod-1447931b-a414-4bf4-bd9e-aec6355d040e" in namespace "emptydir-1082" to be "success or failure"
Sep  5 08:54:27.247: INFO: Pod "pod-1447931b-a414-4bf4-bd9e-aec6355d040e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.100681ms
Sep  5 08:54:29.250: INFO: Pod "pod-1447931b-a414-4bf4-bd9e-aec6355d040e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012585946s
Sep  5 08:54:31.254: INFO: Pod "pod-1447931b-a414-4bf4-bd9e-aec6355d040e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016047951s
STEP: Saw pod success
Sep  5 08:54:31.254: INFO: Pod "pod-1447931b-a414-4bf4-bd9e-aec6355d040e" satisfied condition "success or failure"
Sep  5 08:54:31.271: INFO: Trying to get logs from node slave1 pod pod-1447931b-a414-4bf4-bd9e-aec6355d040e container test-container: <nil>
STEP: delete the pod
Sep  5 08:54:31.323: INFO: Waiting for pod pod-1447931b-a414-4bf4-bd9e-aec6355d040e to disappear
Sep  5 08:54:31.329: INFO: Pod pod-1447931b-a414-4bf4-bd9e-aec6355d040e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:54:31.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1082" for this suite.
Sep  5 08:54:37.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:54:37.510: INFO: namespace emptydir-1082 deletion completed in 6.176316438s

• [SLOW TEST:10.525 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:54:37.512: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-547
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  5 08:54:45.855: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 08:54:45.861: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 08:54:47.861: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 08:54:47.864: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 08:54:49.861: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 08:54:49.865: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 08:54:51.861: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 08:54:51.866: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 08:54:53.861: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 08:54:53.865: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 08:54:55.861: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 08:54:55.865: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 08:54:57.861: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 08:54:57.865: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:54:57.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-547" for this suite.
Sep  5 08:55:19.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:55:19.984: INFO: namespace container-lifecycle-hook-547 deletion completed in 22.114864205s

• [SLOW TEST:42.473 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:55:19.985: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2159
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2159
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  5 08:55:20.197: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  5 08:55:46.361: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.31.51.152 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2159 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 08:55:46.361: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 08:55:47.657: INFO: Found all expected endpoints: [netserver-0]
Sep  5 08:55:47.660: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.31.161.122 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2159 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 08:55:47.660: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
Sep  5 08:55:48.930: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:55:48.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2159" for this suite.
Sep  5 08:56:10.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:56:11.071: INFO: namespace pod-network-test-2159 deletion completed in 22.135902774s

• [SLOW TEST:51.086 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:56:11.072: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 08:56:11.302: INFO: Waiting up to 5m0s for pod "downwardapi-volume-981ab8e2-9c81-461c-97e1-1eefc142866e" in namespace "downward-api-988" to be "success or failure"
Sep  5 08:56:11.310: INFO: Pod "downwardapi-volume-981ab8e2-9c81-461c-97e1-1eefc142866e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.168209ms
Sep  5 08:56:13.313: INFO: Pod "downwardapi-volume-981ab8e2-9c81-461c-97e1-1eefc142866e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011039193s
Sep  5 08:56:15.317: INFO: Pod "downwardapi-volume-981ab8e2-9c81-461c-97e1-1eefc142866e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015006116s
Sep  5 08:56:17.321: INFO: Pod "downwardapi-volume-981ab8e2-9c81-461c-97e1-1eefc142866e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01873262s
STEP: Saw pod success
Sep  5 08:56:17.321: INFO: Pod "downwardapi-volume-981ab8e2-9c81-461c-97e1-1eefc142866e" satisfied condition "success or failure"
Sep  5 08:56:17.324: INFO: Trying to get logs from node slave1 pod downwardapi-volume-981ab8e2-9c81-461c-97e1-1eefc142866e container client-container: <nil>
STEP: delete the pod
Sep  5 08:56:17.359: INFO: Waiting for pod downwardapi-volume-981ab8e2-9c81-461c-97e1-1eefc142866e to disappear
Sep  5 08:56:17.366: INFO: Pod downwardapi-volume-981ab8e2-9c81-461c-97e1-1eefc142866e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:56:17.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-988" for this suite.
Sep  5 08:56:23.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:56:23.504: INFO: namespace downward-api-988 deletion completed in 6.133135046s

• [SLOW TEST:12.432 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:56:23.504: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3395
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:56:29.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3395" for this suite.
Sep  5 08:56:35.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:56:35.507: INFO: namespace watch-3395 deletion completed in 6.279155094s

• [SLOW TEST:12.002 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:56:35.507: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-218
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Sep  5 08:56:35.779: INFO: Waiting up to 5m0s for pod "client-containers-598d65a4-13ac-4c50-a354-7d6be6f1f309" in namespace "containers-218" to be "success or failure"
Sep  5 08:56:35.810: INFO: Pod "client-containers-598d65a4-13ac-4c50-a354-7d6be6f1f309": Phase="Pending", Reason="", readiness=false. Elapsed: 31.227685ms
Sep  5 08:56:37.814: INFO: Pod "client-containers-598d65a4-13ac-4c50-a354-7d6be6f1f309": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035010649s
Sep  5 08:56:39.818: INFO: Pod "client-containers-598d65a4-13ac-4c50-a354-7d6be6f1f309": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038819692s
Sep  5 08:56:41.821: INFO: Pod "client-containers-598d65a4-13ac-4c50-a354-7d6be6f1f309": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042219618s
STEP: Saw pod success
Sep  5 08:56:41.822: INFO: Pod "client-containers-598d65a4-13ac-4c50-a354-7d6be6f1f309" satisfied condition "success or failure"
Sep  5 08:56:41.824: INFO: Trying to get logs from node slave1 pod client-containers-598d65a4-13ac-4c50-a354-7d6be6f1f309 container test-container: <nil>
STEP: delete the pod
Sep  5 08:56:41.864: INFO: Waiting for pod client-containers-598d65a4-13ac-4c50-a354-7d6be6f1f309 to disappear
Sep  5 08:56:41.892: INFO: Pod client-containers-598d65a4-13ac-4c50-a354-7d6be6f1f309 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:56:41.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-218" for this suite.
Sep  5 08:56:47.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:56:48.048: INFO: namespace containers-218 deletion completed in 6.151338266s

• [SLOW TEST:12.541 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:56:48.049: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7295
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Sep  5 08:56:48.264: INFO: Waiting up to 5m0s for pod "client-containers-65737586-d20c-4f28-99c8-564e1524ead2" in namespace "containers-7295" to be "success or failure"
Sep  5 08:56:48.270: INFO: Pod "client-containers-65737586-d20c-4f28-99c8-564e1524ead2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006135ms
Sep  5 08:56:50.274: INFO: Pod "client-containers-65737586-d20c-4f28-99c8-564e1524ead2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009521139s
Sep  5 08:56:52.278: INFO: Pod "client-containers-65737586-d20c-4f28-99c8-564e1524ead2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013454723s
Sep  5 08:56:54.281: INFO: Pod "client-containers-65737586-d20c-4f28-99c8-564e1524ead2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017221146s
STEP: Saw pod success
Sep  5 08:56:54.281: INFO: Pod "client-containers-65737586-d20c-4f28-99c8-564e1524ead2" satisfied condition "success or failure"
Sep  5 08:56:54.284: INFO: Trying to get logs from node slave1 pod client-containers-65737586-d20c-4f28-99c8-564e1524ead2 container test-container: <nil>
STEP: delete the pod
Sep  5 08:56:54.313: INFO: Waiting for pod client-containers-65737586-d20c-4f28-99c8-564e1524ead2 to disappear
Sep  5 08:56:54.319: INFO: Pod client-containers-65737586-d20c-4f28-99c8-564e1524ead2 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:56:54.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7295" for this suite.
Sep  5 08:57:00.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:57:00.509: INFO: namespace containers-7295 deletion completed in 6.18531034s

• [SLOW TEST:12.460 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:57:00.509: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2603
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep  5 08:57:00.757: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:57:08.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2603" for this suite.
Sep  5 08:57:30.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:57:30.434: INFO: namespace init-container-2603 deletion completed in 22.207598045s

• [SLOW TEST:29.925 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:57:30.435: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3195
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  5 08:57:30.659: INFO: Waiting up to 5m0s for pod "pod-57bcc853-fb81-4992-a865-9a14b1f68035" in namespace "emptydir-3195" to be "success or failure"
Sep  5 08:57:30.690: INFO: Pod "pod-57bcc853-fb81-4992-a865-9a14b1f68035": Phase="Pending", Reason="", readiness=false. Elapsed: 31.663344ms
Sep  5 08:57:32.694: INFO: Pod "pod-57bcc853-fb81-4992-a865-9a14b1f68035": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035528767s
Sep  5 08:57:34.699: INFO: Pod "pod-57bcc853-fb81-4992-a865-9a14b1f68035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039863768s
STEP: Saw pod success
Sep  5 08:57:34.699: INFO: Pod "pod-57bcc853-fb81-4992-a865-9a14b1f68035" satisfied condition "success or failure"
Sep  5 08:57:34.701: INFO: Trying to get logs from node slave1 pod pod-57bcc853-fb81-4992-a865-9a14b1f68035 container test-container: <nil>
STEP: delete the pod
Sep  5 08:57:34.732: INFO: Waiting for pod pod-57bcc853-fb81-4992-a865-9a14b1f68035 to disappear
Sep  5 08:57:34.738: INFO: Pod pod-57bcc853-fb81-4992-a865-9a14b1f68035 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:57:34.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3195" for this suite.
Sep  5 08:57:40.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:57:40.910: INFO: namespace emptydir-3195 deletion completed in 6.167299557s

• [SLOW TEST:10.475 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:57:40.911: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1371
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-7mpc
STEP: Creating a pod to test atomic-volume-subpath
Sep  5 08:57:41.148: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7mpc" in namespace "subpath-1371" to be "success or failure"
Sep  5 08:57:41.155: INFO: Pod "pod-subpath-test-configmap-7mpc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.163313ms
Sep  5 08:57:43.159: INFO: Pod "pod-subpath-test-configmap-7mpc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009963777s
Sep  5 08:57:45.162: INFO: Pod "pod-subpath-test-configmap-7mpc": Phase="Running", Reason="", readiness=true. Elapsed: 4.013607141s
Sep  5 08:57:47.166: INFO: Pod "pod-subpath-test-configmap-7mpc": Phase="Running", Reason="", readiness=true. Elapsed: 6.017264326s
Sep  5 08:57:49.169: INFO: Pod "pod-subpath-test-configmap-7mpc": Phase="Running", Reason="", readiness=true. Elapsed: 8.02090001s
Sep  5 08:57:51.173: INFO: Pod "pod-subpath-test-configmap-7mpc": Phase="Running", Reason="", readiness=true. Elapsed: 10.024624214s
Sep  5 08:57:53.177: INFO: Pod "pod-subpath-test-configmap-7mpc": Phase="Running", Reason="", readiness=true. Elapsed: 12.028061899s
Sep  5 08:57:55.180: INFO: Pod "pod-subpath-test-configmap-7mpc": Phase="Running", Reason="", readiness=true. Elapsed: 14.031545104s
Sep  5 08:57:57.184: INFO: Pod "pod-subpath-test-configmap-7mpc": Phase="Running", Reason="", readiness=true. Elapsed: 16.035275908s
Sep  5 08:57:59.187: INFO: Pod "pod-subpath-test-configmap-7mpc": Phase="Running", Reason="", readiness=true. Elapsed: 18.038826693s
Sep  5 08:58:01.191: INFO: Pod "pod-subpath-test-configmap-7mpc": Phase="Running", Reason="", readiness=true. Elapsed: 20.042392838s
Sep  5 08:58:03.195: INFO: Pod "pod-subpath-test-configmap-7mpc": Phase="Running", Reason="", readiness=true. Elapsed: 22.045952563s
Sep  5 08:58:05.198: INFO: Pod "pod-subpath-test-configmap-7mpc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.049426248s
STEP: Saw pod success
Sep  5 08:58:05.198: INFO: Pod "pod-subpath-test-configmap-7mpc" satisfied condition "success or failure"
Sep  5 08:58:05.201: INFO: Trying to get logs from node slave1 pod pod-subpath-test-configmap-7mpc container test-container-subpath-configmap-7mpc: <nil>
STEP: delete the pod
Sep  5 08:58:05.227: INFO: Waiting for pod pod-subpath-test-configmap-7mpc to disappear
Sep  5 08:58:05.234: INFO: Pod pod-subpath-test-configmap-7mpc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7mpc
Sep  5 08:58:05.234: INFO: Deleting pod "pod-subpath-test-configmap-7mpc" in namespace "subpath-1371"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:58:05.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1371" for this suite.
Sep  5 08:58:11.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:58:11.396: INFO: namespace subpath-1371 deletion completed in 6.149988512s

• [SLOW TEST:30.485 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:58:11.397: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1412
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 08:58:11.641: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b8259ef5-f047-4e69-af4f-0b6906a177fa" in namespace "projected-1412" to be "success or failure"
Sep  5 08:58:11.650: INFO: Pod "downwardapi-volume-b8259ef5-f047-4e69-af4f-0b6906a177fa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.489983ms
Sep  5 08:58:13.654: INFO: Pod "downwardapi-volume-b8259ef5-f047-4e69-af4f-0b6906a177fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012203827s
Sep  5 08:58:15.657: INFO: Pod "downwardapi-volume-b8259ef5-f047-4e69-af4f-0b6906a177fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015918311s
Sep  5 08:58:17.661: INFO: Pod "downwardapi-volume-b8259ef5-f047-4e69-af4f-0b6906a177fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019739755s
STEP: Saw pod success
Sep  5 08:58:17.661: INFO: Pod "downwardapi-volume-b8259ef5-f047-4e69-af4f-0b6906a177fa" satisfied condition "success or failure"
Sep  5 08:58:17.664: INFO: Trying to get logs from node slave1 pod downwardapi-volume-b8259ef5-f047-4e69-af4f-0b6906a177fa container client-container: <nil>
STEP: delete the pod
Sep  5 08:58:17.701: INFO: Waiting for pod downwardapi-volume-b8259ef5-f047-4e69-af4f-0b6906a177fa to disappear
Sep  5 08:58:17.707: INFO: Pod downwardapi-volume-b8259ef5-f047-4e69-af4f-0b6906a177fa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:58:17.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1412" for this suite.
Sep  5 08:58:23.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:58:23.905: INFO: namespace projected-1412 deletion completed in 6.192695808s

• [SLOW TEST:12.508 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:58:23.905: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1925
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-43768fa5-fbd6-4b1d-97a9-f6d6e55dba9b
STEP: Creating a pod to test consume configMaps
Sep  5 08:58:24.155: INFO: Waiting up to 5m0s for pod "pod-configmaps-70add95e-1902-409f-a1ef-e0df187a54a5" in namespace "configmap-1925" to be "success or failure"
Sep  5 08:58:24.165: INFO: Pod "pod-configmaps-70add95e-1902-409f-a1ef-e0df187a54a5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.976777ms
Sep  5 08:58:26.169: INFO: Pod "pod-configmaps-70add95e-1902-409f-a1ef-e0df187a54a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013674741s
Sep  5 08:58:28.172: INFO: Pod "pod-configmaps-70add95e-1902-409f-a1ef-e0df187a54a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017345645s
STEP: Saw pod success
Sep  5 08:58:28.172: INFO: Pod "pod-configmaps-70add95e-1902-409f-a1ef-e0df187a54a5" satisfied condition "success or failure"
Sep  5 08:58:28.175: INFO: Trying to get logs from node slave1 pod pod-configmaps-70add95e-1902-409f-a1ef-e0df187a54a5 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 08:58:28.249: INFO: Waiting for pod pod-configmaps-70add95e-1902-409f-a1ef-e0df187a54a5 to disappear
Sep  5 08:58:28.256: INFO: Pod pod-configmaps-70add95e-1902-409f-a1ef-e0df187a54a5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:58:28.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1925" for this suite.
Sep  5 08:58:34.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:58:34.380: INFO: namespace configmap-1925 deletion completed in 6.119291925s

• [SLOW TEST:10.475 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:58:34.381: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  5 08:58:39.672: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:58:39.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6687" for this suite.
Sep  5 08:58:45.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:58:45.918: INFO: namespace container-runtime-6687 deletion completed in 6.175769402s

• [SLOW TEST:11.537 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:58:45.919: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6111
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep  5 08:58:46.159: INFO: Waiting up to 5m0s for pod "downward-api-23732de8-ebd5-44cd-9f1a-5d6c4a3bdc69" in namespace "downward-api-6111" to be "success or failure"
Sep  5 08:58:46.170: INFO: Pod "downward-api-23732de8-ebd5-44cd-9f1a-5d6c4a3bdc69": Phase="Pending", Reason="", readiness=false. Elapsed: 11.58647ms
Sep  5 08:58:48.174: INFO: Pod "downward-api-23732de8-ebd5-44cd-9f1a-5d6c4a3bdc69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015490933s
Sep  5 08:58:50.178: INFO: Pod "downward-api-23732de8-ebd5-44cd-9f1a-5d6c4a3bdc69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018800259s
STEP: Saw pod success
Sep  5 08:58:50.178: INFO: Pod "downward-api-23732de8-ebd5-44cd-9f1a-5d6c4a3bdc69" satisfied condition "success or failure"
Sep  5 08:58:50.180: INFO: Trying to get logs from node slave1 pod downward-api-23732de8-ebd5-44cd-9f1a-5d6c4a3bdc69 container dapi-container: <nil>
STEP: delete the pod
Sep  5 08:58:50.213: INFO: Waiting for pod downward-api-23732de8-ebd5-44cd-9f1a-5d6c4a3bdc69 to disappear
Sep  5 08:58:50.246: INFO: Pod downward-api-23732de8-ebd5-44cd-9f1a-5d6c4a3bdc69 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:58:50.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6111" for this suite.
Sep  5 08:58:56.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 08:58:56.407: INFO: namespace downward-api-6111 deletion completed in 6.143487301s

• [SLOW TEST:10.488 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 08:58:56.407: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-533
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 08:59:56.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-533" for this suite.
Sep  5 09:00:18.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 09:00:18.776: INFO: namespace container-probe-533 deletion completed in 22.117839827s

• [SLOW TEST:82.369 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 09:00:18.776: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8386
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  5 09:00:18.991: INFO: Waiting up to 5m0s for pod "pod-19c1c0a5-743e-4b3f-b923-d2db6c88d933" in namespace "emptydir-8386" to be "success or failure"
Sep  5 09:00:18.997: INFO: Pod "pod-19c1c0a5-743e-4b3f-b923-d2db6c88d933": Phase="Pending", Reason="", readiness=false. Elapsed: 5.881115ms
Sep  5 09:00:21.001: INFO: Pod "pod-19c1c0a5-743e-4b3f-b923-d2db6c88d933": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009563496s
Sep  5 09:00:23.005: INFO: Pod "pod-19c1c0a5-743e-4b3f-b923-d2db6c88d933": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013299037s
Sep  5 09:00:25.008: INFO: Pod "pod-19c1c0a5-743e-4b3f-b923-d2db6c88d933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016883539s
STEP: Saw pod success
Sep  5 09:00:25.008: INFO: Pod "pod-19c1c0a5-743e-4b3f-b923-d2db6c88d933" satisfied condition "success or failure"
Sep  5 09:00:25.011: INFO: Trying to get logs from node slave1 pod pod-19c1c0a5-743e-4b3f-b923-d2db6c88d933 container test-container: <nil>
STEP: delete the pod
Sep  5 09:00:25.089: INFO: Waiting for pod pod-19c1c0a5-743e-4b3f-b923-d2db6c88d933 to disappear
Sep  5 09:00:25.097: INFO: Pod pod-19c1c0a5-743e-4b3f-b923-d2db6c88d933 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 09:00:25.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8386" for this suite.
Sep  5 09:00:31.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 09:00:31.267: INFO: namespace emptydir-8386 deletion completed in 6.160333259s

• [SLOW TEST:12.491 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 09:00:31.268: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2912
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 09:00:31.491: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2f270a3e-df33-4908-abb2-d601f80c243b" in namespace "downward-api-2912" to be "success or failure"
Sep  5 09:00:31.496: INFO: Pod "downwardapi-volume-2f270a3e-df33-4908-abb2-d601f80c243b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.529981ms
Sep  5 09:00:33.501: INFO: Pod "downwardapi-volume-2f270a3e-df33-4908-abb2-d601f80c243b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009227838s
Sep  5 09:00:35.504: INFO: Pod "downwardapi-volume-2f270a3e-df33-4908-abb2-d601f80c243b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013190338s
STEP: Saw pod success
Sep  5 09:00:35.505: INFO: Pod "downwardapi-volume-2f270a3e-df33-4908-abb2-d601f80c243b" satisfied condition "success or failure"
Sep  5 09:00:35.507: INFO: Trying to get logs from node slave1 pod downwardapi-volume-2f270a3e-df33-4908-abb2-d601f80c243b container client-container: <nil>
STEP: delete the pod
Sep  5 09:00:35.531: INFO: Waiting for pod downwardapi-volume-2f270a3e-df33-4908-abb2-d601f80c243b to disappear
Sep  5 09:00:35.537: INFO: Pod downwardapi-volume-2f270a3e-df33-4908-abb2-d601f80c243b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 09:00:35.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2912" for this suite.
Sep  5 09:00:41.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 09:00:41.685: INFO: namespace downward-api-2912 deletion completed in 6.143358112s

• [SLOW TEST:10.417 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 09:00:41.685: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6023
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep  5 09:00:46.467: INFO: Successfully updated pod "annotationupdatecacd705e-83da-47ec-98e6-e948118bcd17"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 09:00:48.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6023" for this suite.
Sep  5 09:01:10.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 09:01:10.650: INFO: namespace downward-api-6023 deletion completed in 22.107897804s

• [SLOW TEST:28.965 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 09:01:10.651: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7193
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Sep  5 09:01:10.928: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-815508381 proxy --unix-socket=/tmp/kubectl-proxy-unix364984076/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 09:01:11.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7193" for this suite.
Sep  5 09:01:17.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 09:01:17.203: INFO: namespace kubectl-7193 deletion completed in 6.136691081s

• [SLOW TEST:6.552 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 09:01:17.203: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  5 09:01:25.541: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:25.599: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:27.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:27.603: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:29.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:29.603: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:31.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:31.603: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:33.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:33.603: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:35.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:35.603: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:37.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:37.603: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:39.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:39.603: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:41.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:41.603: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:43.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:43.603: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:45.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:45.603: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:47.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:47.603: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:49.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:49.603: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:51.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:51.603: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:53.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:53.603: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:55.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:55.603: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 09:01:57.599: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 09:01:57.603: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 09:01:57.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-141" for this suite.
Sep  5 09:02:19.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 09:02:19.728: INFO: namespace container-lifecycle-hook-141 deletion completed in 22.121080798s

• [SLOW TEST:62.525 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 09:02:19.729: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3700
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  5 09:02:19.968: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b7738bb-beb5-4cc6-85dd-114f0788761d" in namespace "projected-3700" to be "success or failure"
Sep  5 09:02:19.996: INFO: Pod "downwardapi-volume-7b7738bb-beb5-4cc6-85dd-114f0788761d": Phase="Pending", Reason="", readiness=false. Elapsed: 28.164898ms
Sep  5 09:02:22.013: INFO: Pod "downwardapi-volume-7b7738bb-beb5-4cc6-85dd-114f0788761d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045090225s
Sep  5 09:02:24.017: INFO: Pod "downwardapi-volume-7b7738bb-beb5-4cc6-85dd-114f0788761d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049127648s
STEP: Saw pod success
Sep  5 09:02:24.017: INFO: Pod "downwardapi-volume-7b7738bb-beb5-4cc6-85dd-114f0788761d" satisfied condition "success or failure"
Sep  5 09:02:24.020: INFO: Trying to get logs from node slave1 pod downwardapi-volume-7b7738bb-beb5-4cc6-85dd-114f0788761d container client-container: <nil>
STEP: delete the pod
Sep  5 09:02:24.106: INFO: Waiting for pod downwardapi-volume-7b7738bb-beb5-4cc6-85dd-114f0788761d to disappear
Sep  5 09:02:24.113: INFO: Pod downwardapi-volume-7b7738bb-beb5-4cc6-85dd-114f0788761d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 09:02:24.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3700" for this suite.
Sep  5 09:02:30.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 09:02:30.311: INFO: namespace projected-3700 deletion completed in 6.193648124s

• [SLOW TEST:10.583 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 09:02:30.312: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-2541
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2541 to expose endpoints map[]
Sep  5 09:02:30.588: INFO: Get endpoints failed (6.540792ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Sep  5 09:02:31.592: INFO: successfully validated that service multi-endpoint-test in namespace services-2541 exposes endpoints map[] (1.010601295s elapsed)
STEP: Creating pod pod1 in namespace services-2541
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2541 to expose endpoints map[pod1:[100]]
Sep  5 09:02:35.666: INFO: successfully validated that service multi-endpoint-test in namespace services-2541 exposes endpoints map[pod1:[100]] (4.043155593s elapsed)
STEP: Creating pod pod2 in namespace services-2541
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2541 to expose endpoints map[pod1:[100] pod2:[101]]
Sep  5 09:02:39.776: INFO: successfully validated that service multi-endpoint-test in namespace services-2541 exposes endpoints map[pod1:[100] pod2:[101]] (4.103958791s elapsed)
STEP: Deleting pod pod1 in namespace services-2541
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2541 to expose endpoints map[pod2:[101]]
Sep  5 09:02:40.829: INFO: successfully validated that service multi-endpoint-test in namespace services-2541 exposes endpoints map[pod2:[101]] (1.048027973s elapsed)
STEP: Deleting pod pod2 in namespace services-2541
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2541 to expose endpoints map[]
Sep  5 09:02:41.879: INFO: successfully validated that service multi-endpoint-test in namespace services-2541 exposes endpoints map[] (1.044576068s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 09:02:41.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2541" for this suite.
Sep  5 09:02:47.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 09:02:48.112: INFO: namespace services-2541 deletion completed in 6.156447725s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:17.800 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 09:02:48.112: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3594
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep  5 09:02:48.380: INFO: Waiting up to 5m0s for pod "downward-api-0af60f37-3d62-4714-b6a0-0412e94b5ce5" in namespace "downward-api-3594" to be "success or failure"
Sep  5 09:02:48.386: INFO: Pod "downward-api-0af60f37-3d62-4714-b6a0-0412e94b5ce5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.540416ms
Sep  5 09:02:50.408: INFO: Pod "downward-api-0af60f37-3d62-4714-b6a0-0412e94b5ce5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027757101s
Sep  5 09:02:52.412: INFO: Pod "downward-api-0af60f37-3d62-4714-b6a0-0412e94b5ce5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031981742s
Sep  5 09:02:54.417: INFO: Pod "downward-api-0af60f37-3d62-4714-b6a0-0412e94b5ce5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037080301s
STEP: Saw pod success
Sep  5 09:02:54.417: INFO: Pod "downward-api-0af60f37-3d62-4714-b6a0-0412e94b5ce5" satisfied condition "success or failure"
Sep  5 09:02:54.420: INFO: Trying to get logs from node slave1 pod downward-api-0af60f37-3d62-4714-b6a0-0412e94b5ce5 container dapi-container: <nil>
STEP: delete the pod
Sep  5 09:02:54.445: INFO: Waiting for pod downward-api-0af60f37-3d62-4714-b6a0-0412e94b5ce5 to disappear
Sep  5 09:02:54.451: INFO: Pod downward-api-0af60f37-3d62-4714-b6a0-0412e94b5ce5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 09:02:54.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3594" for this suite.
Sep  5 09:03:00.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 09:03:00.603: INFO: namespace downward-api-3594 deletion completed in 6.147820921s

• [SLOW TEST:12.491 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 09:03:00.604: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-4488
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-4488
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-4488
STEP: Deleting pre-stop pod
Sep  5 09:03:15.930: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 09:03:16.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4488" for this suite.
Sep  5 09:03:58.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 09:03:58.138: INFO: namespace prestop-4488 deletion completed in 42.112719336s

• [SLOW TEST:57.534 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 09:03:58.138: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-146
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-e7183c83-80c2-4737-aaf3-a1a3fbb5f29a
STEP: Creating a pod to test consume configMaps
Sep  5 09:03:58.404: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-13db701c-08a7-425c-b0f4-5042f4b94c48" in namespace "projected-146" to be "success or failure"
Sep  5 09:03:58.417: INFO: Pod "pod-projected-configmaps-13db701c-08a7-425c-b0f4-5042f4b94c48": Phase="Pending", Reason="", readiness=false. Elapsed: 12.955844ms
Sep  5 09:04:00.421: INFO: Pod "pod-projected-configmaps-13db701c-08a7-425c-b0f4-5042f4b94c48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016957046s
Sep  5 09:04:02.425: INFO: Pod "pod-projected-configmaps-13db701c-08a7-425c-b0f4-5042f4b94c48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020936869s
STEP: Saw pod success
Sep  5 09:04:02.425: INFO: Pod "pod-projected-configmaps-13db701c-08a7-425c-b0f4-5042f4b94c48" satisfied condition "success or failure"
Sep  5 09:04:02.428: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-13db701c-08a7-425c-b0f4-5042f4b94c48 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 09:04:02.479: INFO: Waiting for pod pod-projected-configmaps-13db701c-08a7-425c-b0f4-5042f4b94c48 to disappear
Sep  5 09:04:02.492: INFO: Pod pod-projected-configmaps-13db701c-08a7-425c-b0f4-5042f4b94c48 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 09:04:02.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-146" for this suite.
Sep  5 09:04:08.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 09:04:08.603: INFO: namespace projected-146 deletion completed in 6.106479821s

• [SLOW TEST:10.466 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 09:04:08.603: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8416
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-d513fa9c-1c3c-498b-bab5-a388655454f8
STEP: Creating a pod to test consume configMaps
Sep  5 09:04:08.854: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-16e21f26-02e6-4e5d-b25e-f94fa5491d68" in namespace "projected-8416" to be "success or failure"
Sep  5 09:04:08.866: INFO: Pod "pod-projected-configmaps-16e21f26-02e6-4e5d-b25e-f94fa5491d68": Phase="Pending", Reason="", readiness=false. Elapsed: 12.043508ms
Sep  5 09:04:10.870: INFO: Pod "pod-projected-configmaps-16e21f26-02e6-4e5d-b25e-f94fa5491d68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015953291s
Sep  5 09:04:12.902: INFO: Pod "pod-projected-configmaps-16e21f26-02e6-4e5d-b25e-f94fa5491d68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047718914s
STEP: Saw pod success
Sep  5 09:04:12.902: INFO: Pod "pod-projected-configmaps-16e21f26-02e6-4e5d-b25e-f94fa5491d68" satisfied condition "success or failure"
Sep  5 09:04:12.916: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-16e21f26-02e6-4e5d-b25e-f94fa5491d68 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 09:04:12.984: INFO: Waiting for pod pod-projected-configmaps-16e21f26-02e6-4e5d-b25e-f94fa5491d68 to disappear
Sep  5 09:04:12.999: INFO: Pod pod-projected-configmaps-16e21f26-02e6-4e5d-b25e-f94fa5491d68 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 09:04:12.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8416" for this suite.
Sep  5 09:04:19.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 09:04:19.138: INFO: namespace projected-8416 deletion completed in 6.1343566s

• [SLOW TEST:10.534 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 09:04:19.138: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1013
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep  5 09:04:19.446: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1013,SelfLink:/api/v1/namespaces/watch-1013/configmaps/e2e-watch-test-watch-closed,UID:9c7cee0f-0a48-406d-ace5-35458656c1de,ResourceVersion:246150,Generation:0,CreationTimestamp:2019-09-05 09:04:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  5 09:04:19.446: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1013,SelfLink:/api/v1/namespaces/watch-1013/configmaps/e2e-watch-test-watch-closed,UID:9c7cee0f-0a48-406d-ace5-35458656c1de,ResourceVersion:246151,Generation:0,CreationTimestamp:2019-09-05 09:04:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep  5 09:04:19.467: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1013,SelfLink:/api/v1/namespaces/watch-1013/configmaps/e2e-watch-test-watch-closed,UID:9c7cee0f-0a48-406d-ace5-35458656c1de,ResourceVersion:246152,Generation:0,CreationTimestamp:2019-09-05 09:04:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  5 09:04:19.467: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1013,SelfLink:/api/v1/namespaces/watch-1013/configmaps/e2e-watch-test-watch-closed,UID:9c7cee0f-0a48-406d-ace5-35458656c1de,ResourceVersion:246153,Generation:0,CreationTimestamp:2019-09-05 09:04:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 09:04:19.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1013" for this suite.
Sep  5 09:04:25.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 09:04:25.571: INFO: namespace watch-1013 deletion completed in 6.097150921s

• [SLOW TEST:6.433 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  5 09:04:25.572: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-629
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  5 09:04:25.805: INFO: >>> kubeConfig: /tmp/kubeconfig-815508381
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  5 09:04:29.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-629" for this suite.
Sep  5 09:05:09.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  5 09:05:10.006: INFO: namespace pods-629 deletion completed in 40.149334218s

• [SLOW TEST:44.435 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSep  5 09:05:10.008: INFO: Running AfterSuite actions on all nodes
Sep  5 09:05:10.028: INFO: Running AfterSuite actions on node 1
Sep  5 09:05:10.028: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 6540.906 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h49m4.8089153s
Test Suite Passed
