I0324 02:44:55.257002      18 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-811275467
I0324 02:44:55.257113      18 e2e.go:243] Starting e2e run "4cef418f-46f9-4d79-89d9-c27b3098160c" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1585017894 - Will randomize all specs
Will run 215 of 4412 specs

Mar 24 02:44:55.352: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 02:44:55.356: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 24 02:44:55.367: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 24 02:44:55.390: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 24 02:44:55.390: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Mar 24 02:44:55.390: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 24 02:44:55.396: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kindnet' (0 seconds elapsed)
Mar 24 02:44:55.396: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar 24 02:44:55.396: INFO: e2e test version: v1.15.12-beta.0.7+7f18f85e0e8bcc
Mar 24 02:44:55.397: INFO: kube-apiserver version: v1.15.12-beta.0.7+7f18f85e0e8bcc
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:44:55.397: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename subpath
Mar 24 02:44:55.418: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Mar 24 02:44:55.426: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1013
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-4vth
STEP: Creating a pod to test atomic-volume-subpath
Mar 24 02:44:55.544: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-4vth" in namespace "subpath-1013" to be "success or failure"
Mar 24 02:44:55.546: INFO: Pod "pod-subpath-test-downwardapi-4vth": Phase="Pending", Reason="", readiness=false. Elapsed: 1.676711ms
Mar 24 02:44:57.549: INFO: Pod "pod-subpath-test-downwardapi-4vth": Phase="Running", Reason="", readiness=true. Elapsed: 2.004474287s
Mar 24 02:44:59.551: INFO: Pod "pod-subpath-test-downwardapi-4vth": Phase="Running", Reason="", readiness=true. Elapsed: 4.007353891s
Mar 24 02:45:01.555: INFO: Pod "pod-subpath-test-downwardapi-4vth": Phase="Running", Reason="", readiness=true. Elapsed: 6.010445563s
Mar 24 02:45:03.557: INFO: Pod "pod-subpath-test-downwardapi-4vth": Phase="Running", Reason="", readiness=true. Elapsed: 8.013383098s
Mar 24 02:45:05.561: INFO: Pod "pod-subpath-test-downwardapi-4vth": Phase="Running", Reason="", readiness=true. Elapsed: 10.016664405s
Mar 24 02:45:07.564: INFO: Pod "pod-subpath-test-downwardapi-4vth": Phase="Running", Reason="", readiness=true. Elapsed: 12.019584227s
Mar 24 02:45:09.566: INFO: Pod "pod-subpath-test-downwardapi-4vth": Phase="Running", Reason="", readiness=true. Elapsed: 14.022335646s
Mar 24 02:45:11.569: INFO: Pod "pod-subpath-test-downwardapi-4vth": Phase="Running", Reason="", readiness=true. Elapsed: 16.024700952s
Mar 24 02:45:13.572: INFO: Pod "pod-subpath-test-downwardapi-4vth": Phase="Running", Reason="", readiness=true. Elapsed: 18.027571051s
Mar 24 02:45:15.575: INFO: Pod "pod-subpath-test-downwardapi-4vth": Phase="Running", Reason="", readiness=true. Elapsed: 20.030705082s
Mar 24 02:45:17.578: INFO: Pod "pod-subpath-test-downwardapi-4vth": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.033957892s
STEP: Saw pod success
Mar 24 02:45:17.578: INFO: Pod "pod-subpath-test-downwardapi-4vth" satisfied condition "success or failure"
Mar 24 02:45:17.580: INFO: Trying to get logs from node kind-worker pod pod-subpath-test-downwardapi-4vth container test-container-subpath-downwardapi-4vth: <nil>
STEP: delete the pod
Mar 24 02:45:17.603: INFO: Waiting for pod pod-subpath-test-downwardapi-4vth to disappear
Mar 24 02:45:17.606: INFO: Pod pod-subpath-test-downwardapi-4vth no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-4vth
Mar 24 02:45:17.606: INFO: Deleting pod "pod-subpath-test-downwardapi-4vth" in namespace "subpath-1013"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:45:17.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1013" for this suite.
Mar 24 02:45:23.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:45:23.680: INFO: namespace subpath-1013 deletion completed in 6.069630272s

• [SLOW TEST:28.282 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:45:23.680: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1031
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 02:45:25.833: INFO: Waiting up to 5m0s for pod "client-envvars-e3cb6851-182e-4a7d-920c-903cd93b9e42" in namespace "pods-1031" to be "success or failure"
Mar 24 02:45:25.836: INFO: Pod "client-envvars-e3cb6851-182e-4a7d-920c-903cd93b9e42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.547009ms
Mar 24 02:45:27.839: INFO: Pod "client-envvars-e3cb6851-182e-4a7d-920c-903cd93b9e42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005317111s
Mar 24 02:45:29.841: INFO: Pod "client-envvars-e3cb6851-182e-4a7d-920c-903cd93b9e42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008128391s
STEP: Saw pod success
Mar 24 02:45:29.841: INFO: Pod "client-envvars-e3cb6851-182e-4a7d-920c-903cd93b9e42" satisfied condition "success or failure"
Mar 24 02:45:29.843: INFO: Trying to get logs from node kind-worker pod client-envvars-e3cb6851-182e-4a7d-920c-903cd93b9e42 container env3cont: <nil>
STEP: delete the pod
Mar 24 02:45:29.854: INFO: Waiting for pod client-envvars-e3cb6851-182e-4a7d-920c-903cd93b9e42 to disappear
Mar 24 02:45:29.856: INFO: Pod client-envvars-e3cb6851-182e-4a7d-920c-903cd93b9e42 no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:45:29.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1031" for this suite.
Mar 24 02:46:13.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:46:13.921: INFO: namespace pods-1031 deletion completed in 44.062750202s

• [SLOW TEST:50.241 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:46:13.921: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-35
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1557
[It] should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 24 02:46:14.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-35'
Mar 24 02:46:14.276: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 24 02:46:14.276: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Mar 24 02:46:16.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete deployment e2e-test-nginx-deployment --namespace=kubectl-35'
Mar 24 02:46:16.369: INFO: stderr: ""
Mar 24 02:46:16.369: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:46:16.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-35" for this suite.
Mar 24 02:46:22.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:46:22.436: INFO: namespace kubectl-35 deletion completed in 6.064638735s

• [SLOW TEST:8.515 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:46:22.437: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-2770
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2770
I0324 02:46:22.564323      18 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2770, replica count: 1
I0324 02:46:23.614726      18 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0324 02:46:24.614956      18 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 02:46:24.725: INFO: Created: latency-svc-hn2mt
Mar 24 02:46:24.731: INFO: Got endpoints: latency-svc-hn2mt [16.280134ms]
Mar 24 02:46:24.742: INFO: Created: latency-svc-pfml9
Mar 24 02:46:24.745: INFO: Got endpoints: latency-svc-pfml9 [13.816809ms]
Mar 24 02:46:24.755: INFO: Created: latency-svc-grbhq
Mar 24 02:46:24.755: INFO: Got endpoints: latency-svc-grbhq [23.639774ms]
Mar 24 02:46:24.759: INFO: Created: latency-svc-9fdf7
Mar 24 02:46:24.763: INFO: Got endpoints: latency-svc-9fdf7 [32.131881ms]
Mar 24 02:46:24.771: INFO: Created: latency-svc-cz4vv
Mar 24 02:46:24.774: INFO: Got endpoints: latency-svc-cz4vv [42.65664ms]
Mar 24 02:46:24.782: INFO: Created: latency-svc-285dc
Mar 24 02:46:24.784: INFO: Got endpoints: latency-svc-285dc [52.891976ms]
Mar 24 02:46:24.789: INFO: Created: latency-svc-6c2fm
Mar 24 02:46:24.793: INFO: Got endpoints: latency-svc-6c2fm [60.975349ms]
Mar 24 02:46:24.799: INFO: Created: latency-svc-hdgv7
Mar 24 02:46:24.802: INFO: Got endpoints: latency-svc-hdgv7 [70.049142ms]
Mar 24 02:46:24.811: INFO: Created: latency-svc-llshp
Mar 24 02:46:24.822: INFO: Got endpoints: latency-svc-llshp [90.144172ms]
Mar 24 02:46:24.822: INFO: Created: latency-svc-2vcml
Mar 24 02:46:24.825: INFO: Got endpoints: latency-svc-2vcml [93.258492ms]
Mar 24 02:46:24.831: INFO: Created: latency-svc-v9rrs
Mar 24 02:46:24.838: INFO: Got endpoints: latency-svc-v9rrs [106.104903ms]
Mar 24 02:46:24.839: INFO: Created: latency-svc-pvw9m
Mar 24 02:46:24.846: INFO: Got endpoints: latency-svc-pvw9m [114.452564ms]
Mar 24 02:46:24.848: INFO: Created: latency-svc-mw477
Mar 24 02:46:24.850: INFO: Got endpoints: latency-svc-mw477 [118.664965ms]
Mar 24 02:46:24.856: INFO: Created: latency-svc-r7l4b
Mar 24 02:46:24.862: INFO: Got endpoints: latency-svc-r7l4b [130.848457ms]
Mar 24 02:46:24.863: INFO: Created: latency-svc-xcrcm
Mar 24 02:46:24.867: INFO: Got endpoints: latency-svc-xcrcm [135.316828ms]
Mar 24 02:46:24.871: INFO: Created: latency-svc-fq9t5
Mar 24 02:46:24.875: INFO: Got endpoints: latency-svc-fq9t5 [143.228672ms]
Mar 24 02:46:24.878: INFO: Created: latency-svc-j5xs5
Mar 24 02:46:24.880: INFO: Got endpoints: latency-svc-j5xs5 [13.86441ms]
Mar 24 02:46:24.887: INFO: Created: latency-svc-cgkvg
Mar 24 02:46:24.890: INFO: Got endpoints: latency-svc-cgkvg [144.978886ms]
Mar 24 02:46:24.905: INFO: Created: latency-svc-z95rh
Mar 24 02:46:24.905: INFO: Got endpoints: latency-svc-z95rh [149.525763ms]
Mar 24 02:46:24.911: INFO: Created: latency-svc-2sh4w
Mar 24 02:46:24.913: INFO: Got endpoints: latency-svc-2sh4w [149.270693ms]
Mar 24 02:46:24.917: INFO: Created: latency-svc-dlkq8
Mar 24 02:46:24.930: INFO: Got endpoints: latency-svc-dlkq8 [156.219164ms]
Mar 24 02:46:24.933: INFO: Created: latency-svc-vl9rd
Mar 24 02:46:24.938: INFO: Got endpoints: latency-svc-vl9rd [153.639779ms]
Mar 24 02:46:24.944: INFO: Created: latency-svc-2kfn8
Mar 24 02:46:24.949: INFO: Got endpoints: latency-svc-2kfn8 [155.972779ms]
Mar 24 02:46:24.955: INFO: Created: latency-svc-plmhv
Mar 24 02:46:24.958: INFO: Got endpoints: latency-svc-plmhv [156.765986ms]
Mar 24 02:46:24.970: INFO: Created: latency-svc-ff4zm
Mar 24 02:46:24.970: INFO: Got endpoints: latency-svc-ff4zm [148.214216ms]
Mar 24 02:46:24.978: INFO: Created: latency-svc-t9smq
Mar 24 02:46:24.982: INFO: Got endpoints: latency-svc-t9smq [156.629986ms]
Mar 24 02:46:24.991: INFO: Created: latency-svc-94npv
Mar 24 02:46:24.997: INFO: Got endpoints: latency-svc-94npv [159.1813ms]
Mar 24 02:46:25.000: INFO: Created: latency-svc-79fhm
Mar 24 02:46:25.005: INFO: Got endpoints: latency-svc-79fhm [158.914088ms]
Mar 24 02:46:25.015: INFO: Created: latency-svc-gngnd
Mar 24 02:46:25.044: INFO: Got endpoints: latency-svc-gngnd [193.872562ms]
Mar 24 02:46:25.050: INFO: Created: latency-svc-724h4
Mar 24 02:46:25.054: INFO: Got endpoints: latency-svc-724h4 [191.116981ms]
Mar 24 02:46:25.060: INFO: Created: latency-svc-5hkn7
Mar 24 02:46:25.063: INFO: Got endpoints: latency-svc-5hkn7 [187.665094ms]
Mar 24 02:46:25.069: INFO: Created: latency-svc-t84lh
Mar 24 02:46:25.072: INFO: Got endpoints: latency-svc-t84lh [191.25707ms]
Mar 24 02:46:25.079: INFO: Created: latency-svc-pfhtn
Mar 24 02:46:25.083: INFO: Got endpoints: latency-svc-pfhtn [192.506957ms]
Mar 24 02:46:25.090: INFO: Created: latency-svc-w6mtf
Mar 24 02:46:25.093: INFO: Got endpoints: latency-svc-w6mtf [188.279337ms]
Mar 24 02:46:25.099: INFO: Created: latency-svc-qs6jt
Mar 24 02:46:25.103: INFO: Got endpoints: latency-svc-qs6jt [189.559396ms]
Mar 24 02:46:25.110: INFO: Created: latency-svc-c82np
Mar 24 02:46:25.110: INFO: Got endpoints: latency-svc-c82np [179.933926ms]
Mar 24 02:46:25.116: INFO: Created: latency-svc-4pkks
Mar 24 02:46:25.117: INFO: Got endpoints: latency-svc-4pkks [179.319974ms]
Mar 24 02:46:25.122: INFO: Created: latency-svc-v2stz
Mar 24 02:46:25.127: INFO: Created: latency-svc-gf7tz
Mar 24 02:46:25.131: INFO: Got endpoints: latency-svc-v2stz [182.116703ms]
Mar 24 02:46:25.137: INFO: Created: latency-svc-98rzc
Mar 24 02:46:25.153: INFO: Created: latency-svc-bm79k
Mar 24 02:46:25.159: INFO: Created: latency-svc-8hmlb
Mar 24 02:46:25.165: INFO: Created: latency-svc-4wvs5
Mar 24 02:46:25.171: INFO: Created: latency-svc-mrz2k
Mar 24 02:46:25.177: INFO: Created: latency-svc-p7v4g
Mar 24 02:46:25.186: INFO: Got endpoints: latency-svc-gf7tz [227.540659ms]
Mar 24 02:46:25.186: INFO: Created: latency-svc-6njc7
Mar 24 02:46:25.197: INFO: Created: latency-svc-8t5zt
Mar 24 02:46:25.203: INFO: Created: latency-svc-ckw8r
Mar 24 02:46:25.208: INFO: Created: latency-svc-plfbh
Mar 24 02:46:25.214: INFO: Created: latency-svc-qf4z9
Mar 24 02:46:25.220: INFO: Created: latency-svc-x6jg4
Mar 24 02:46:25.229: INFO: Created: latency-svc-vq9rz
Mar 24 02:46:25.231: INFO: Got endpoints: latency-svc-98rzc [260.921082ms]
Mar 24 02:46:25.238: INFO: Created: latency-svc-v7cbj
Mar 24 02:46:25.245: INFO: Created: latency-svc-djlzb
Mar 24 02:46:25.261: INFO: Created: latency-svc-ppptl
Mar 24 02:46:25.281: INFO: Got endpoints: latency-svc-bm79k [299.304617ms]
Mar 24 02:46:25.304: INFO: Created: latency-svc-zh87s
Mar 24 02:46:25.330: INFO: Got endpoints: latency-svc-8hmlb [333.297017ms]
Mar 24 02:46:25.342: INFO: Created: latency-svc-vmw82
Mar 24 02:46:25.381: INFO: Got endpoints: latency-svc-4wvs5 [376.365135ms]
Mar 24 02:46:25.393: INFO: Created: latency-svc-8znq2
Mar 24 02:46:25.431: INFO: Got endpoints: latency-svc-mrz2k [386.48507ms]
Mar 24 02:46:25.442: INFO: Created: latency-svc-5wfpk
Mar 24 02:46:25.482: INFO: Got endpoints: latency-svc-p7v4g [428.079551ms]
Mar 24 02:46:25.493: INFO: Created: latency-svc-prz4d
Mar 24 02:46:25.532: INFO: Got endpoints: latency-svc-6njc7 [469.285794ms]
Mar 24 02:46:25.544: INFO: Created: latency-svc-zhz2m
Mar 24 02:46:25.581: INFO: Got endpoints: latency-svc-8t5zt [509.12181ms]
Mar 24 02:46:25.594: INFO: Created: latency-svc-dvnqt
Mar 24 02:46:25.632: INFO: Got endpoints: latency-svc-ckw8r [548.748127ms]
Mar 24 02:46:25.644: INFO: Created: latency-svc-nr8bk
Mar 24 02:46:25.686: INFO: Got endpoints: latency-svc-plfbh [593.024573ms]
Mar 24 02:46:25.697: INFO: Created: latency-svc-rf94w
Mar 24 02:46:25.732: INFO: Got endpoints: latency-svc-qf4z9 [628.861265ms]
Mar 24 02:46:25.745: INFO: Created: latency-svc-cbvsv
Mar 24 02:46:25.782: INFO: Got endpoints: latency-svc-x6jg4 [671.767464ms]
Mar 24 02:46:25.798: INFO: Created: latency-svc-sjwm6
Mar 24 02:46:25.831: INFO: Got endpoints: latency-svc-vq9rz [713.950863ms]
Mar 24 02:46:25.842: INFO: Created: latency-svc-8w2xr
Mar 24 02:46:25.881: INFO: Got endpoints: latency-svc-v7cbj [749.790434ms]
Mar 24 02:46:25.894: INFO: Created: latency-svc-t4n74
Mar 24 02:46:25.931: INFO: Got endpoints: latency-svc-djlzb [744.781279ms]
Mar 24 02:46:25.943: INFO: Created: latency-svc-spd6m
Mar 24 02:46:25.981: INFO: Got endpoints: latency-svc-ppptl [749.939381ms]
Mar 24 02:46:25.992: INFO: Created: latency-svc-t4tjf
Mar 24 02:46:26.033: INFO: Got endpoints: latency-svc-zh87s [751.6864ms]
Mar 24 02:46:26.045: INFO: Created: latency-svc-sq74m
Mar 24 02:46:26.081: INFO: Got endpoints: latency-svc-vmw82 [750.516872ms]
Mar 24 02:46:26.092: INFO: Created: latency-svc-spt9j
Mar 24 02:46:26.131: INFO: Got endpoints: latency-svc-8znq2 [750.107674ms]
Mar 24 02:46:26.143: INFO: Created: latency-svc-9shfn
Mar 24 02:46:26.182: INFO: Got endpoints: latency-svc-5wfpk [750.560755ms]
Mar 24 02:46:26.192: INFO: Created: latency-svc-cxrt2
Mar 24 02:46:26.232: INFO: Got endpoints: latency-svc-prz4d [749.663812ms]
Mar 24 02:46:26.244: INFO: Created: latency-svc-vvmrw
Mar 24 02:46:26.281: INFO: Got endpoints: latency-svc-zhz2m [749.118051ms]
Mar 24 02:46:26.296: INFO: Created: latency-svc-6w882
Mar 24 02:46:26.331: INFO: Got endpoints: latency-svc-dvnqt [749.975318ms]
Mar 24 02:46:26.341: INFO: Created: latency-svc-ghgtd
Mar 24 02:46:26.382: INFO: Got endpoints: latency-svc-nr8bk [750.146043ms]
Mar 24 02:46:26.394: INFO: Created: latency-svc-vq57z
Mar 24 02:46:26.433: INFO: Got endpoints: latency-svc-rf94w [747.300069ms]
Mar 24 02:46:26.463: INFO: Created: latency-svc-ttf8w
Mar 24 02:46:26.481: INFO: Got endpoints: latency-svc-cbvsv [749.413823ms]
Mar 24 02:46:26.496: INFO: Created: latency-svc-2hfzh
Mar 24 02:46:26.532: INFO: Got endpoints: latency-svc-sjwm6 [749.335994ms]
Mar 24 02:46:26.545: INFO: Created: latency-svc-p4xlg
Mar 24 02:46:26.581: INFO: Got endpoints: latency-svc-8w2xr [749.151221ms]
Mar 24 02:46:26.590: INFO: Created: latency-svc-sdzk5
Mar 24 02:46:26.631: INFO: Got endpoints: latency-svc-t4n74 [750.689356ms]
Mar 24 02:46:26.644: INFO: Created: latency-svc-zf9f2
Mar 24 02:46:26.681: INFO: Got endpoints: latency-svc-spd6m [749.67866ms]
Mar 24 02:46:26.692: INFO: Created: latency-svc-xsrrv
Mar 24 02:46:26.731: INFO: Got endpoints: latency-svc-t4tjf [749.999841ms]
Mar 24 02:46:26.742: INFO: Created: latency-svc-rwslf
Mar 24 02:46:26.781: INFO: Got endpoints: latency-svc-sq74m [748.565191ms]
Mar 24 02:46:26.792: INFO: Created: latency-svc-wjr52
Mar 24 02:46:26.831: INFO: Got endpoints: latency-svc-spt9j [749.672018ms]
Mar 24 02:46:26.842: INFO: Created: latency-svc-6v955
Mar 24 02:46:26.881: INFO: Got endpoints: latency-svc-9shfn [749.435266ms]
Mar 24 02:46:26.891: INFO: Created: latency-svc-wbbn8
Mar 24 02:46:26.930: INFO: Got endpoints: latency-svc-cxrt2 [748.841344ms]
Mar 24 02:46:26.942: INFO: Created: latency-svc-9vdnk
Mar 24 02:46:26.981: INFO: Got endpoints: latency-svc-vvmrw [749.54405ms]
Mar 24 02:46:26.991: INFO: Created: latency-svc-7tr5z
Mar 24 02:46:27.031: INFO: Got endpoints: latency-svc-6w882 [749.521724ms]
Mar 24 02:46:27.043: INFO: Created: latency-svc-89jwg
Mar 24 02:46:27.081: INFO: Got endpoints: latency-svc-ghgtd [749.441329ms]
Mar 24 02:46:27.092: INFO: Created: latency-svc-56h2m
Mar 24 02:46:27.131: INFO: Got endpoints: latency-svc-vq57z [749.314473ms]
Mar 24 02:46:27.148: INFO: Created: latency-svc-rxpgc
Mar 24 02:46:27.181: INFO: Got endpoints: latency-svc-ttf8w [747.520757ms]
Mar 24 02:46:27.195: INFO: Created: latency-svc-jbb2q
Mar 24 02:46:27.231: INFO: Got endpoints: latency-svc-2hfzh [749.709755ms]
Mar 24 02:46:27.244: INFO: Created: latency-svc-w8pxm
Mar 24 02:46:27.281: INFO: Got endpoints: latency-svc-p4xlg [749.520087ms]
Mar 24 02:46:27.293: INFO: Created: latency-svc-8kv2r
Mar 24 02:46:27.331: INFO: Got endpoints: latency-svc-sdzk5 [750.010049ms]
Mar 24 02:46:27.342: INFO: Created: latency-svc-4rdjx
Mar 24 02:46:27.382: INFO: Got endpoints: latency-svc-zf9f2 [750.675696ms]
Mar 24 02:46:27.391: INFO: Created: latency-svc-glwl5
Mar 24 02:46:27.431: INFO: Got endpoints: latency-svc-xsrrv [749.917183ms]
Mar 24 02:46:27.440: INFO: Created: latency-svc-g6ljd
Mar 24 02:46:27.486: INFO: Got endpoints: latency-svc-rwslf [754.754176ms]
Mar 24 02:46:27.496: INFO: Created: latency-svc-v4wtt
Mar 24 02:46:27.531: INFO: Got endpoints: latency-svc-wjr52 [749.374383ms]
Mar 24 02:46:27.540: INFO: Created: latency-svc-b9q6k
Mar 24 02:46:27.581: INFO: Got endpoints: latency-svc-6v955 [750.697449ms]
Mar 24 02:46:27.593: INFO: Created: latency-svc-6php9
Mar 24 02:46:27.631: INFO: Got endpoints: latency-svc-wbbn8 [749.874009ms]
Mar 24 02:46:27.640: INFO: Created: latency-svc-8wdp5
Mar 24 02:46:27.681: INFO: Got endpoints: latency-svc-9vdnk [750.226868ms]
Mar 24 02:46:27.690: INFO: Created: latency-svc-g4kng
Mar 24 02:46:27.731: INFO: Got endpoints: latency-svc-7tr5z [749.751616ms]
Mar 24 02:46:27.741: INFO: Created: latency-svc-vs87k
Mar 24 02:46:27.781: INFO: Got endpoints: latency-svc-89jwg [749.895774ms]
Mar 24 02:46:27.795: INFO: Created: latency-svc-z4qtm
Mar 24 02:46:27.831: INFO: Got endpoints: latency-svc-56h2m [749.959886ms]
Mar 24 02:46:27.841: INFO: Created: latency-svc-xjh4r
Mar 24 02:46:27.882: INFO: Got endpoints: latency-svc-rxpgc [751.264763ms]
Mar 24 02:46:27.892: INFO: Created: latency-svc-cvnth
Mar 24 02:46:27.931: INFO: Got endpoints: latency-svc-jbb2q [749.362024ms]
Mar 24 02:46:27.939: INFO: Created: latency-svc-6rfdj
Mar 24 02:46:27.981: INFO: Got endpoints: latency-svc-w8pxm [749.819636ms]
Mar 24 02:46:27.991: INFO: Created: latency-svc-4g58d
Mar 24 02:46:28.031: INFO: Got endpoints: latency-svc-8kv2r [749.403511ms]
Mar 24 02:46:28.039: INFO: Created: latency-svc-9q4cd
Mar 24 02:46:28.081: INFO: Got endpoints: latency-svc-4rdjx [750.106439ms]
Mar 24 02:46:28.092: INFO: Created: latency-svc-wtdlq
Mar 24 02:46:28.131: INFO: Got endpoints: latency-svc-glwl5 [748.304483ms]
Mar 24 02:46:28.144: INFO: Created: latency-svc-bs8wv
Mar 24 02:46:28.181: INFO: Got endpoints: latency-svc-g6ljd [749.602931ms]
Mar 24 02:46:28.190: INFO: Created: latency-svc-4lt6c
Mar 24 02:46:28.231: INFO: Got endpoints: latency-svc-v4wtt [744.893318ms]
Mar 24 02:46:28.240: INFO: Created: latency-svc-bb7qp
Mar 24 02:46:28.281: INFO: Got endpoints: latency-svc-b9q6k [749.710008ms]
Mar 24 02:46:28.290: INFO: Created: latency-svc-vz8gh
Mar 24 02:46:28.331: INFO: Got endpoints: latency-svc-6php9 [749.516136ms]
Mar 24 02:46:28.339: INFO: Created: latency-svc-z9rcp
Mar 24 02:46:28.381: INFO: Got endpoints: latency-svc-8wdp5 [750.555606ms]
Mar 24 02:46:28.392: INFO: Created: latency-svc-mrzqq
Mar 24 02:46:28.435: INFO: Got endpoints: latency-svc-g4kng [753.63423ms]
Mar 24 02:46:28.449: INFO: Created: latency-svc-5p7gx
Mar 24 02:46:28.481: INFO: Got endpoints: latency-svc-vs87k [749.793388ms]
Mar 24 02:46:28.491: INFO: Created: latency-svc-rtr9j
Mar 24 02:46:28.531: INFO: Got endpoints: latency-svc-z4qtm [749.934039ms]
Mar 24 02:46:28.542: INFO: Created: latency-svc-8dftp
Mar 24 02:46:28.581: INFO: Got endpoints: latency-svc-xjh4r [749.749178ms]
Mar 24 02:46:28.590: INFO: Created: latency-svc-t8q46
Mar 24 02:46:28.632: INFO: Got endpoints: latency-svc-cvnth [749.102767ms]
Mar 24 02:46:28.645: INFO: Created: latency-svc-wlxvw
Mar 24 02:46:28.681: INFO: Got endpoints: latency-svc-6rfdj [749.868787ms]
Mar 24 02:46:28.690: INFO: Created: latency-svc-5mbpg
Mar 24 02:46:28.730: INFO: Got endpoints: latency-svc-4g58d [749.546226ms]
Mar 24 02:46:28.740: INFO: Created: latency-svc-nq2rb
Mar 24 02:46:28.781: INFO: Got endpoints: latency-svc-9q4cd [749.971657ms]
Mar 24 02:46:28.789: INFO: Created: latency-svc-v7q2s
Mar 24 02:46:28.831: INFO: Got endpoints: latency-svc-wtdlq [749.57226ms]
Mar 24 02:46:28.843: INFO: Created: latency-svc-c5dzm
Mar 24 02:46:28.881: INFO: Got endpoints: latency-svc-bs8wv [750.498142ms]
Mar 24 02:46:28.890: INFO: Created: latency-svc-wwsbr
Mar 24 02:46:28.934: INFO: Got endpoints: latency-svc-4lt6c [752.960044ms]
Mar 24 02:46:28.967: INFO: Created: latency-svc-prj54
Mar 24 02:46:28.981: INFO: Got endpoints: latency-svc-bb7qp [750.134395ms]
Mar 24 02:46:28.991: INFO: Created: latency-svc-zq9zz
Mar 24 02:46:29.031: INFO: Got endpoints: latency-svc-vz8gh [750.216105ms]
Mar 24 02:46:29.043: INFO: Created: latency-svc-x7hwv
Mar 24 02:46:29.081: INFO: Got endpoints: latency-svc-z9rcp [749.631792ms]
Mar 24 02:46:29.091: INFO: Created: latency-svc-j7rxw
Mar 24 02:46:29.131: INFO: Got endpoints: latency-svc-mrzqq [749.306531ms]
Mar 24 02:46:29.141: INFO: Created: latency-svc-lf4h9
Mar 24 02:46:29.182: INFO: Got endpoints: latency-svc-5p7gx [747.233278ms]
Mar 24 02:46:29.196: INFO: Created: latency-svc-4jbrh
Mar 24 02:46:29.231: INFO: Got endpoints: latency-svc-rtr9j [749.892181ms]
Mar 24 02:46:29.241: INFO: Created: latency-svc-btfcn
Mar 24 02:46:29.283: INFO: Got endpoints: latency-svc-8dftp [752.088789ms]
Mar 24 02:46:29.292: INFO: Created: latency-svc-j6tdq
Mar 24 02:46:29.331: INFO: Got endpoints: latency-svc-t8q46 [750.232415ms]
Mar 24 02:46:29.340: INFO: Created: latency-svc-d77lm
Mar 24 02:46:29.381: INFO: Got endpoints: latency-svc-wlxvw [749.254454ms]
Mar 24 02:46:29.391: INFO: Created: latency-svc-jh98q
Mar 24 02:46:29.431: INFO: Got endpoints: latency-svc-5mbpg [750.429736ms]
Mar 24 02:46:29.442: INFO: Created: latency-svc-nvtgh
Mar 24 02:46:29.481: INFO: Got endpoints: latency-svc-nq2rb [750.776329ms]
Mar 24 02:46:29.496: INFO: Created: latency-svc-4jlx7
Mar 24 02:46:29.531: INFO: Got endpoints: latency-svc-v7q2s [750.603656ms]
Mar 24 02:46:29.541: INFO: Created: latency-svc-z9bxs
Mar 24 02:46:29.581: INFO: Got endpoints: latency-svc-c5dzm [749.967052ms]
Mar 24 02:46:29.594: INFO: Created: latency-svc-x9cmg
Mar 24 02:46:29.631: INFO: Got endpoints: latency-svc-wwsbr [750.082639ms]
Mar 24 02:46:29.641: INFO: Created: latency-svc-hvhfz
Mar 24 02:46:29.682: INFO: Got endpoints: latency-svc-prj54 [747.901479ms]
Mar 24 02:46:29.692: INFO: Created: latency-svc-xj2x6
Mar 24 02:46:29.731: INFO: Got endpoints: latency-svc-zq9zz [750.107197ms]
Mar 24 02:46:29.741: INFO: Created: latency-svc-bvn8j
Mar 24 02:46:29.781: INFO: Got endpoints: latency-svc-x7hwv [750.328583ms]
Mar 24 02:46:29.792: INFO: Created: latency-svc-rcr5x
Mar 24 02:46:29.831: INFO: Got endpoints: latency-svc-j7rxw [750.371392ms]
Mar 24 02:46:29.841: INFO: Created: latency-svc-ns5sw
Mar 24 02:46:29.881: INFO: Got endpoints: latency-svc-lf4h9 [750.439096ms]
Mar 24 02:46:29.892: INFO: Created: latency-svc-4sgfp
Mar 24 02:46:29.931: INFO: Got endpoints: latency-svc-4jbrh [748.448419ms]
Mar 24 02:46:29.943: INFO: Created: latency-svc-4vf8m
Mar 24 02:46:29.981: INFO: Got endpoints: latency-svc-btfcn [749.784671ms]
Mar 24 02:46:29.991: INFO: Created: latency-svc-gk67g
Mar 24 02:46:30.031: INFO: Got endpoints: latency-svc-j6tdq [748.260835ms]
Mar 24 02:46:30.042: INFO: Created: latency-svc-4vnd7
Mar 24 02:46:30.081: INFO: Got endpoints: latency-svc-d77lm [750.347024ms]
Mar 24 02:46:30.091: INFO: Created: latency-svc-vcrtb
Mar 24 02:46:30.131: INFO: Got endpoints: latency-svc-jh98q [749.979823ms]
Mar 24 02:46:30.140: INFO: Created: latency-svc-k2sqp
Mar 24 02:46:30.181: INFO: Got endpoints: latency-svc-nvtgh [749.886061ms]
Mar 24 02:46:30.191: INFO: Created: latency-svc-8md79
Mar 24 02:46:30.235: INFO: Got endpoints: latency-svc-4jlx7 [753.335689ms]
Mar 24 02:46:30.244: INFO: Created: latency-svc-x8phb
Mar 24 02:46:30.281: INFO: Got endpoints: latency-svc-z9bxs [749.327687ms]
Mar 24 02:46:30.289: INFO: Created: latency-svc-kqmdm
Mar 24 02:46:30.331: INFO: Got endpoints: latency-svc-x9cmg [750.015838ms]
Mar 24 02:46:30.342: INFO: Created: latency-svc-n5jk8
Mar 24 02:46:30.381: INFO: Got endpoints: latency-svc-hvhfz [749.42583ms]
Mar 24 02:46:30.392: INFO: Created: latency-svc-dnxvn
Mar 24 02:46:30.431: INFO: Got endpoints: latency-svc-xj2x6 [748.874806ms]
Mar 24 02:46:30.440: INFO: Created: latency-svc-chnsb
Mar 24 02:46:30.481: INFO: Got endpoints: latency-svc-bvn8j [749.175836ms]
Mar 24 02:46:30.490: INFO: Created: latency-svc-r5fvq
Mar 24 02:46:30.531: INFO: Got endpoints: latency-svc-rcr5x [749.674758ms]
Mar 24 02:46:30.541: INFO: Created: latency-svc-vlkth
Mar 24 02:46:30.581: INFO: Got endpoints: latency-svc-ns5sw [749.943401ms]
Mar 24 02:46:30.592: INFO: Created: latency-svc-xvbnb
Mar 24 02:46:30.631: INFO: Got endpoints: latency-svc-4sgfp [749.711423ms]
Mar 24 02:46:30.641: INFO: Created: latency-svc-qxwkk
Mar 24 02:46:30.681: INFO: Got endpoints: latency-svc-4vf8m [750.333263ms]
Mar 24 02:46:30.690: INFO: Created: latency-svc-mfbvs
Mar 24 02:46:30.731: INFO: Got endpoints: latency-svc-gk67g [750.54618ms]
Mar 24 02:46:30.741: INFO: Created: latency-svc-qcqp8
Mar 24 02:46:30.781: INFO: Got endpoints: latency-svc-4vnd7 [749.429766ms]
Mar 24 02:46:30.790: INFO: Created: latency-svc-qglrg
Mar 24 02:46:30.831: INFO: Got endpoints: latency-svc-vcrtb [749.337731ms]
Mar 24 02:46:30.841: INFO: Created: latency-svc-qbf5q
Mar 24 02:46:30.881: INFO: Got endpoints: latency-svc-k2sqp [750.072156ms]
Mar 24 02:46:30.890: INFO: Created: latency-svc-zwhfd
Mar 24 02:46:30.931: INFO: Got endpoints: latency-svc-8md79 [749.681874ms]
Mar 24 02:46:30.941: INFO: Created: latency-svc-bwx79
Mar 24 02:46:30.981: INFO: Got endpoints: latency-svc-x8phb [746.081721ms]
Mar 24 02:46:30.994: INFO: Created: latency-svc-fgrcj
Mar 24 02:46:31.031: INFO: Got endpoints: latency-svc-kqmdm [749.872757ms]
Mar 24 02:46:31.039: INFO: Created: latency-svc-7cgct
Mar 24 02:46:31.081: INFO: Got endpoints: latency-svc-n5jk8 [750.245885ms]
Mar 24 02:46:31.090: INFO: Created: latency-svc-dpc7v
Mar 24 02:46:31.130: INFO: Got endpoints: latency-svc-dnxvn [749.401575ms]
Mar 24 02:46:31.140: INFO: Created: latency-svc-wr522
Mar 24 02:46:31.185: INFO: Got endpoints: latency-svc-chnsb [754.25938ms]
Mar 24 02:46:31.194: INFO: Created: latency-svc-vkgjw
Mar 24 02:46:31.231: INFO: Got endpoints: latency-svc-r5fvq [749.886425ms]
Mar 24 02:46:31.241: INFO: Created: latency-svc-dcdkb
Mar 24 02:46:31.281: INFO: Got endpoints: latency-svc-vlkth [749.680599ms]
Mar 24 02:46:31.292: INFO: Created: latency-svc-pb8df
Mar 24 02:46:31.331: INFO: Got endpoints: latency-svc-xvbnb [749.164275ms]
Mar 24 02:46:31.339: INFO: Created: latency-svc-nwn69
Mar 24 02:46:31.381: INFO: Got endpoints: latency-svc-qxwkk [749.085226ms]
Mar 24 02:46:31.409: INFO: Created: latency-svc-sgf9p
Mar 24 02:46:31.430: INFO: Got endpoints: latency-svc-mfbvs [749.378944ms]
Mar 24 02:46:31.439: INFO: Created: latency-svc-pm5f6
Mar 24 02:46:31.481: INFO: Got endpoints: latency-svc-qcqp8 [749.079159ms]
Mar 24 02:46:31.489: INFO: Created: latency-svc-bm68z
Mar 24 02:46:31.531: INFO: Got endpoints: latency-svc-qglrg [750.106661ms]
Mar 24 02:46:31.541: INFO: Created: latency-svc-t6tnt
Mar 24 02:46:31.581: INFO: Got endpoints: latency-svc-qbf5q [749.985401ms]
Mar 24 02:46:31.590: INFO: Created: latency-svc-w5kpc
Mar 24 02:46:31.631: INFO: Got endpoints: latency-svc-zwhfd [749.963554ms]
Mar 24 02:46:31.643: INFO: Created: latency-svc-76zxd
Mar 24 02:46:31.681: INFO: Got endpoints: latency-svc-bwx79 [750.552524ms]
Mar 24 02:46:31.693: INFO: Created: latency-svc-qzt8h
Mar 24 02:46:31.731: INFO: Got endpoints: latency-svc-fgrcj [750.235873ms]
Mar 24 02:46:31.742: INFO: Created: latency-svc-llb9q
Mar 24 02:46:31.781: INFO: Got endpoints: latency-svc-7cgct [750.229548ms]
Mar 24 02:46:31.793: INFO: Created: latency-svc-p7g6g
Mar 24 02:46:31.831: INFO: Got endpoints: latency-svc-dpc7v [749.861966ms]
Mar 24 02:46:31.841: INFO: Created: latency-svc-xxdbk
Mar 24 02:46:31.882: INFO: Got endpoints: latency-svc-wr522 [751.274675ms]
Mar 24 02:46:31.890: INFO: Created: latency-svc-b8ftd
Mar 24 02:46:31.931: INFO: Got endpoints: latency-svc-vkgjw [746.012863ms]
Mar 24 02:46:31.940: INFO: Created: latency-svc-k292g
Mar 24 02:46:31.981: INFO: Got endpoints: latency-svc-dcdkb [749.746112ms]
Mar 24 02:46:31.990: INFO: Created: latency-svc-k7jdt
Mar 24 02:46:32.039: INFO: Got endpoints: latency-svc-pb8df [758.3355ms]
Mar 24 02:46:32.052: INFO: Created: latency-svc-x6vv9
Mar 24 02:46:32.081: INFO: Got endpoints: latency-svc-nwn69 [750.037715ms]
Mar 24 02:46:32.090: INFO: Created: latency-svc-brrsq
Mar 24 02:46:32.131: INFO: Got endpoints: latency-svc-sgf9p [749.910439ms]
Mar 24 02:46:32.145: INFO: Created: latency-svc-cvnpv
Mar 24 02:46:32.181: INFO: Got endpoints: latency-svc-pm5f6 [750.627423ms]
Mar 24 02:46:32.190: INFO: Created: latency-svc-c9zzb
Mar 24 02:46:32.231: INFO: Got endpoints: latency-svc-bm68z [750.021041ms]
Mar 24 02:46:32.240: INFO: Created: latency-svc-p2dd4
Mar 24 02:46:32.281: INFO: Got endpoints: latency-svc-t6tnt [750.121185ms]
Mar 24 02:46:32.290: INFO: Created: latency-svc-mcvjm
Mar 24 02:46:32.331: INFO: Got endpoints: latency-svc-w5kpc [750.314605ms]
Mar 24 02:46:32.340: INFO: Created: latency-svc-bcrsh
Mar 24 02:46:32.381: INFO: Got endpoints: latency-svc-76zxd [749.57371ms]
Mar 24 02:46:32.390: INFO: Created: latency-svc-z6r7d
Mar 24 02:46:32.431: INFO: Got endpoints: latency-svc-qzt8h [749.887173ms]
Mar 24 02:46:32.440: INFO: Created: latency-svc-qsvpx
Mar 24 02:46:32.481: INFO: Got endpoints: latency-svc-llb9q [750.383807ms]
Mar 24 02:46:32.492: INFO: Created: latency-svc-hmg7p
Mar 24 02:46:32.531: INFO: Got endpoints: latency-svc-p7g6g [749.643465ms]
Mar 24 02:46:32.539: INFO: Created: latency-svc-z45jj
Mar 24 02:46:32.581: INFO: Got endpoints: latency-svc-xxdbk [749.997117ms]
Mar 24 02:46:32.631: INFO: Got endpoints: latency-svc-b8ftd [748.502153ms]
Mar 24 02:46:32.682: INFO: Got endpoints: latency-svc-k292g [750.337861ms]
Mar 24 02:46:32.731: INFO: Got endpoints: latency-svc-k7jdt [749.714762ms]
Mar 24 02:46:32.781: INFO: Got endpoints: latency-svc-x6vv9 [741.587147ms]
Mar 24 02:46:32.832: INFO: Got endpoints: latency-svc-brrsq [750.526551ms]
Mar 24 02:46:32.881: INFO: Got endpoints: latency-svc-cvnpv [750.010258ms]
Mar 24 02:46:32.931: INFO: Got endpoints: latency-svc-c9zzb [750.038585ms]
Mar 24 02:46:32.981: INFO: Got endpoints: latency-svc-p2dd4 [749.952411ms]
Mar 24 02:46:33.031: INFO: Got endpoints: latency-svc-mcvjm [749.849705ms]
Mar 24 02:46:33.082: INFO: Got endpoints: latency-svc-bcrsh [750.287547ms]
Mar 24 02:46:33.131: INFO: Got endpoints: latency-svc-z6r7d [750.276256ms]
Mar 24 02:46:33.181: INFO: Got endpoints: latency-svc-qsvpx [749.414757ms]
Mar 24 02:46:33.231: INFO: Got endpoints: latency-svc-hmg7p [749.434644ms]
Mar 24 02:46:33.281: INFO: Got endpoints: latency-svc-z45jj [750.154572ms]
Mar 24 02:46:33.281: INFO: Latencies: [13.816809ms 13.86441ms 23.639774ms 32.131881ms 42.65664ms 52.891976ms 60.975349ms 70.049142ms 90.144172ms 93.258492ms 106.104903ms 114.452564ms 118.664965ms 130.848457ms 135.316828ms 143.228672ms 144.978886ms 148.214216ms 149.270693ms 149.525763ms 153.639779ms 155.972779ms 156.219164ms 156.629986ms 156.765986ms 158.914088ms 159.1813ms 179.319974ms 179.933926ms 182.116703ms 187.665094ms 188.279337ms 189.559396ms 191.116981ms 191.25707ms 192.506957ms 193.872562ms 227.540659ms 260.921082ms 299.304617ms 333.297017ms 376.365135ms 386.48507ms 428.079551ms 469.285794ms 509.12181ms 548.748127ms 593.024573ms 628.861265ms 671.767464ms 713.950863ms 741.587147ms 744.781279ms 744.893318ms 746.012863ms 746.081721ms 747.233278ms 747.300069ms 747.520757ms 747.901479ms 748.260835ms 748.304483ms 748.448419ms 748.502153ms 748.565191ms 748.841344ms 748.874806ms 749.079159ms 749.085226ms 749.102767ms 749.118051ms 749.151221ms 749.164275ms 749.175836ms 749.254454ms 749.306531ms 749.314473ms 749.327687ms 749.335994ms 749.337731ms 749.362024ms 749.374383ms 749.378944ms 749.401575ms 749.403511ms 749.413823ms 749.414757ms 749.42583ms 749.429766ms 749.434644ms 749.435266ms 749.441329ms 749.516136ms 749.520087ms 749.521724ms 749.54405ms 749.546226ms 749.57226ms 749.57371ms 749.602931ms 749.631792ms 749.643465ms 749.663812ms 749.672018ms 749.674758ms 749.67866ms 749.680599ms 749.681874ms 749.709755ms 749.710008ms 749.711423ms 749.714762ms 749.746112ms 749.749178ms 749.751616ms 749.784671ms 749.790434ms 749.793388ms 749.819636ms 749.849705ms 749.861966ms 749.868787ms 749.872757ms 749.874009ms 749.886061ms 749.886425ms 749.887173ms 749.892181ms 749.895774ms 749.910439ms 749.917183ms 749.934039ms 749.939381ms 749.943401ms 749.952411ms 749.959886ms 749.963554ms 749.967052ms 749.971657ms 749.975318ms 749.979823ms 749.985401ms 749.997117ms 749.999841ms 750.010049ms 750.010258ms 750.015838ms 750.021041ms 750.037715ms 750.038585ms 750.072156ms 750.082639ms 750.106439ms 750.106661ms 750.107197ms 750.107674ms 750.121185ms 750.134395ms 750.146043ms 750.154572ms 750.216105ms 750.226868ms 750.229548ms 750.232415ms 750.235873ms 750.245885ms 750.276256ms 750.287547ms 750.314605ms 750.328583ms 750.333263ms 750.337861ms 750.347024ms 750.371392ms 750.383807ms 750.429736ms 750.439096ms 750.498142ms 750.516872ms 750.526551ms 750.54618ms 750.552524ms 750.555606ms 750.560755ms 750.603656ms 750.627423ms 750.675696ms 750.689356ms 750.697449ms 750.776329ms 751.264763ms 751.274675ms 751.6864ms 752.088789ms 752.960044ms 753.335689ms 753.63423ms 754.25938ms 754.754176ms 758.3355ms]
Mar 24 02:46:33.282: INFO: 50 %ile: 749.631792ms
Mar 24 02:46:33.282: INFO: 90 %ile: 750.54618ms
Mar 24 02:46:33.282: INFO: 99 %ile: 754.754176ms
Mar 24 02:46:33.282: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:46:33.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2770" for this suite.
Mar 24 02:46:53.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:46:53.358: INFO: namespace svc-latency-2770 deletion completed in 20.07324577s

• [SLOW TEST:30.921 seconds]
[sig-network] Service endpoints latency
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:46:53.358: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7135
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-82e13362-a1cf-4993-83f7-3fae75b397fc
STEP: Creating a pod to test consume configMaps
Mar 24 02:46:53.489: INFO: Waiting up to 5m0s for pod "pod-configmaps-9f526e9c-e62a-4324-93dd-b733b73fe055" in namespace "configmap-7135" to be "success or failure"
Mar 24 02:46:53.491: INFO: Pod "pod-configmaps-9f526e9c-e62a-4324-93dd-b733b73fe055": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039283ms
Mar 24 02:46:55.494: INFO: Pod "pod-configmaps-9f526e9c-e62a-4324-93dd-b733b73fe055": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005008011s
Mar 24 02:46:57.498: INFO: Pod "pod-configmaps-9f526e9c-e62a-4324-93dd-b733b73fe055": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008246183s
STEP: Saw pod success
Mar 24 02:46:57.498: INFO: Pod "pod-configmaps-9f526e9c-e62a-4324-93dd-b733b73fe055" satisfied condition "success or failure"
Mar 24 02:46:57.500: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-9f526e9c-e62a-4324-93dd-b733b73fe055 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 02:46:57.522: INFO: Waiting for pod pod-configmaps-9f526e9c-e62a-4324-93dd-b733b73fe055 to disappear
Mar 24 02:46:57.524: INFO: Pod pod-configmaps-9f526e9c-e62a-4324-93dd-b733b73fe055 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:46:57.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7135" for this suite.
Mar 24 02:47:03.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:47:03.586: INFO: namespace configmap-7135 deletion completed in 6.059537918s

• [SLOW TEST:10.227 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:47:03.586: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-194
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-0895e9a3-0159-4610-a3b3-fdeeedab1984
STEP: Creating a pod to test consume configMaps
Mar 24 02:47:03.718: INFO: Waiting up to 5m0s for pod "pod-configmaps-56cb14e6-179a-44ef-af7d-e354ba55f180" in namespace "configmap-194" to be "success or failure"
Mar 24 02:47:03.726: INFO: Pod "pod-configmaps-56cb14e6-179a-44ef-af7d-e354ba55f180": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032987ms
Mar 24 02:47:05.729: INFO: Pod "pod-configmaps-56cb14e6-179a-44ef-af7d-e354ba55f180": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011023201s
STEP: Saw pod success
Mar 24 02:47:05.729: INFO: Pod "pod-configmaps-56cb14e6-179a-44ef-af7d-e354ba55f180" satisfied condition "success or failure"
Mar 24 02:47:05.731: INFO: Trying to get logs from node kind-worker pod pod-configmaps-56cb14e6-179a-44ef-af7d-e354ba55f180 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 02:47:05.745: INFO: Waiting for pod pod-configmaps-56cb14e6-179a-44ef-af7d-e354ba55f180 to disappear
Mar 24 02:47:05.747: INFO: Pod pod-configmaps-56cb14e6-179a-44ef-af7d-e354ba55f180 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:47:05.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-194" for this suite.
Mar 24 02:47:11.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:47:11.811: INFO: namespace configmap-194 deletion completed in 6.061941437s

• [SLOW TEST:8.226 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:47:11.812: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7071
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:47:11.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7071" for this suite.
Mar 24 02:47:17.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:47:18.004: INFO: namespace services-7071 deletion completed in 6.064447927s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.193 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:47:18.005: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9877
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 24 02:47:18.134: INFO: Waiting up to 5m0s for pod "pod-49ec737e-3361-4678-a37e-f9f8f6e5dd7a" in namespace "emptydir-9877" to be "success or failure"
Mar 24 02:47:18.136: INFO: Pod "pod-49ec737e-3361-4678-a37e-f9f8f6e5dd7a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.914176ms
Mar 24 02:47:20.138: INFO: Pod "pod-49ec737e-3361-4678-a37e-f9f8f6e5dd7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00465501s
STEP: Saw pod success
Mar 24 02:47:20.138: INFO: Pod "pod-49ec737e-3361-4678-a37e-f9f8f6e5dd7a" satisfied condition "success or failure"
Mar 24 02:47:20.140: INFO: Trying to get logs from node kind-worker2 pod pod-49ec737e-3361-4678-a37e-f9f8f6e5dd7a container test-container: <nil>
STEP: delete the pod
Mar 24 02:47:20.150: INFO: Waiting for pod pod-49ec737e-3361-4678-a37e-f9f8f6e5dd7a to disappear
Mar 24 02:47:20.153: INFO: Pod pod-49ec737e-3361-4678-a37e-f9f8f6e5dd7a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:47:20.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9877" for this suite.
Mar 24 02:47:26.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:47:26.225: INFO: namespace emptydir-9877 deletion completed in 6.068937398s

• [SLOW TEST:8.220 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:47:26.225: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2949
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 02:47:26.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c56c6de-14b3-4150-be54-f4fe3a687c44" in namespace "projected-2949" to be "success or failure"
Mar 24 02:47:26.357: INFO: Pod "downwardapi-volume-0c56c6de-14b3-4150-be54-f4fe3a687c44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.424955ms
Mar 24 02:47:28.360: INFO: Pod "downwardapi-volume-0c56c6de-14b3-4150-be54-f4fe3a687c44": Phase="Running", Reason="", readiness=true. Elapsed: 2.005296612s
Mar 24 02:47:30.363: INFO: Pod "downwardapi-volume-0c56c6de-14b3-4150-be54-f4fe3a687c44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00808877s
STEP: Saw pod success
Mar 24 02:47:30.363: INFO: Pod "downwardapi-volume-0c56c6de-14b3-4150-be54-f4fe3a687c44" satisfied condition "success or failure"
Mar 24 02:47:30.365: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-0c56c6de-14b3-4150-be54-f4fe3a687c44 container client-container: <nil>
STEP: delete the pod
Mar 24 02:47:30.375: INFO: Waiting for pod downwardapi-volume-0c56c6de-14b3-4150-be54-f4fe3a687c44 to disappear
Mar 24 02:47:30.377: INFO: Pod downwardapi-volume-0c56c6de-14b3-4150-be54-f4fe3a687c44 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:47:30.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2949" for this suite.
Mar 24 02:47:36.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:47:36.446: INFO: namespace projected-2949 deletion completed in 6.067027293s

• [SLOW TEST:10.221 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:47:36.446: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6485
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:47:39.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6485" for this suite.
Mar 24 02:48:01.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:48:01.653: INFO: namespace replication-controller-6485 deletion completed in 22.061540435s

• [SLOW TEST:25.207 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:48:01.653: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6163
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-a79e23f1-8867-4150-a2d1-de72804dc237
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-a79e23f1-8867-4150-a2d1-de72804dc237
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:48:05.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6163" for this suite.
Mar 24 02:48:27.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:48:27.874: INFO: namespace configmap-6163 deletion completed in 22.064400525s

• [SLOW TEST:26.220 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:48:27.875: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8991
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 02:48:27.998: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:48:32.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8991" for this suite.
Mar 24 02:49:12.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:49:12.091: INFO: namespace pods-8991 deletion completed in 40.064386721s

• [SLOW TEST:44.216 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:49:12.091: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-233
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-0cbaa3f1-1167-4ce5-933e-1004d724c39b
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:49:12.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-233" for this suite.
Mar 24 02:49:18.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:49:18.284: INFO: namespace secrets-233 deletion completed in 6.063783704s

• [SLOW TEST:6.192 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:49:18.284: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7659
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Mar 24 02:49:18.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 create -f - --namespace=kubectl-7659'
Mar 24 02:49:18.654: INFO: stderr: ""
Mar 24 02:49:18.654: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 24 02:49:18.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7659'
Mar 24 02:49:18.735: INFO: stderr: ""
Mar 24 02:49:18.735: INFO: stdout: "update-demo-nautilus-k55vh update-demo-nautilus-sjh46 "
Mar 24 02:49:18.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-k55vh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7659'
Mar 24 02:49:18.811: INFO: stderr: ""
Mar 24 02:49:18.811: INFO: stdout: ""
Mar 24 02:49:18.811: INFO: update-demo-nautilus-k55vh is created but not running
Mar 24 02:49:23.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7659'
Mar 24 02:49:23.887: INFO: stderr: ""
Mar 24 02:49:23.887: INFO: stdout: "update-demo-nautilus-k55vh update-demo-nautilus-sjh46 "
Mar 24 02:49:23.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-k55vh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7659'
Mar 24 02:49:23.958: INFO: stderr: ""
Mar 24 02:49:23.958: INFO: stdout: "true"
Mar 24 02:49:23.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-k55vh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7659'
Mar 24 02:49:24.031: INFO: stderr: ""
Mar 24 02:49:24.031: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 02:49:24.031: INFO: validating pod update-demo-nautilus-k55vh
Mar 24 02:49:24.034: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 02:49:24.034: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 02:49:24.035: INFO: update-demo-nautilus-k55vh is verified up and running
Mar 24 02:49:24.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-sjh46 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7659'
Mar 24 02:49:24.110: INFO: stderr: ""
Mar 24 02:49:24.110: INFO: stdout: "true"
Mar 24 02:49:24.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-sjh46 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7659'
Mar 24 02:49:24.182: INFO: stderr: ""
Mar 24 02:49:24.182: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 02:49:24.182: INFO: validating pod update-demo-nautilus-sjh46
Mar 24 02:49:24.185: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 02:49:24.185: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 02:49:24.185: INFO: update-demo-nautilus-sjh46 is verified up and running
STEP: rolling-update to new replication controller
Mar 24 02:49:24.187: INFO: scanned /root for discovery docs: <nil>
Mar 24 02:49:24.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7659'
Mar 24 02:49:46.540: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 24 02:49:46.540: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 24 02:49:46.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7659'
Mar 24 02:49:46.620: INFO: stderr: ""
Mar 24 02:49:46.620: INFO: stdout: "update-demo-kitten-2v75d update-demo-kitten-fgnwr "
Mar 24 02:49:46.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-kitten-2v75d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7659'
Mar 24 02:49:46.693: INFO: stderr: ""
Mar 24 02:49:46.693: INFO: stdout: "true"
Mar 24 02:49:46.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-kitten-2v75d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7659'
Mar 24 02:49:46.767: INFO: stderr: ""
Mar 24 02:49:46.767: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 24 02:49:46.767: INFO: validating pod update-demo-kitten-2v75d
Mar 24 02:49:46.770: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 24 02:49:46.770: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 24 02:49:46.770: INFO: update-demo-kitten-2v75d is verified up and running
Mar 24 02:49:46.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-kitten-fgnwr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7659'
Mar 24 02:49:46.843: INFO: stderr: ""
Mar 24 02:49:46.843: INFO: stdout: "true"
Mar 24 02:49:46.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-kitten-fgnwr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7659'
Mar 24 02:49:46.914: INFO: stderr: ""
Mar 24 02:49:46.914: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 24 02:49:46.914: INFO: validating pod update-demo-kitten-fgnwr
Mar 24 02:49:46.918: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 24 02:49:46.918: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 24 02:49:46.918: INFO: update-demo-kitten-fgnwr is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:49:46.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7659" for this suite.
Mar 24 02:50:02.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:50:02.981: INFO: namespace kubectl-7659 deletion completed in 16.060932892s

• [SLOW TEST:44.697 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:50:02.982: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar 24 02:50:03.106: INFO: PodSpec: initContainers in spec.initContainers
Mar 24 02:50:48.692: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-183f6090-8659-4f18-ade0-36b36819f0fa", GenerateName:"", Namespace:"init-container-6755", SelfLink:"/api/v1/namespaces/init-container-6755/pods/pod-init-183f6090-8659-4f18-ade0-36b36819f0fa", UID:"37a418e7-3500-43ba-84ea-e5fc4e8f160e", ResourceVersion:"3211", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63720615003, loc:(*time.Location)(0x7eafa20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"106624065"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-qt96r", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001c620c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qt96r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qt96r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qt96r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0023a4098), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kind-worker", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001e04060), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0023a4130)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0023a4160)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0023a4168), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0023a416c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720615003, loc:(*time.Location)(0x7eafa20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720615003, loc:(*time.Location)(0x7eafa20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720615003, loc:(*time.Location)(0x7eafa20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720615003, loc:(*time.Location)(0x7eafa20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.17.0.4", PodIP:"10.244.2.10", StartTime:(*v1.Time)(0xc001538060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0025800e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0025801c0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://ed832495309cffda5c61f1ee183702958555ce07237e021d14a4c34cb9b00ac0"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0015380a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001538080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:50:48.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6755" for this suite.
Mar 24 02:51:10.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:51:10.759: INFO: namespace init-container-6755 deletion completed in 22.063106346s

• [SLOW TEST:67.777 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:51:10.759: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8974
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 24 02:51:13.403: INFO: Successfully updated pod "pod-update-f18b6ef5-6048-4aec-a718-60e4ee5441ab"
STEP: verifying the updated pod is in kubernetes
Mar 24 02:51:13.407: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:51:13.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8974" for this suite.
Mar 24 02:51:35.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:51:35.479: INFO: namespace pods-8974 deletion completed in 22.069470916s

• [SLOW TEST:24.720 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:51:35.480: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1697
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1456
[It] should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 24 02:51:35.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1697'
Mar 24 02:51:35.696: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 24 02:51:35.696: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Mar 24 02:51:35.709: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-4tfmq]
Mar 24 02:51:35.709: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-4tfmq" in namespace "kubectl-1697" to be "running and ready"
Mar 24 02:51:35.711: INFO: Pod "e2e-test-nginx-rc-4tfmq": Phase="Pending", Reason="", readiness=false. Elapsed: 1.816912ms
Mar 24 02:51:37.713: INFO: Pod "e2e-test-nginx-rc-4tfmq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004537107s
Mar 24 02:51:39.716: INFO: Pod "e2e-test-nginx-rc-4tfmq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007320805s
Mar 24 02:51:41.719: INFO: Pod "e2e-test-nginx-rc-4tfmq": Phase="Running", Reason="", readiness=true. Elapsed: 6.0100625s
Mar 24 02:51:41.719: INFO: Pod "e2e-test-nginx-rc-4tfmq" satisfied condition "running and ready"
Mar 24 02:51:41.719: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-4tfmq]
Mar 24 02:51:41.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 logs rc/e2e-test-nginx-rc --namespace=kubectl-1697'
Mar 24 02:51:41.812: INFO: stderr: ""
Mar 24 02:51:41.812: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1461
Mar 24 02:51:41.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete rc e2e-test-nginx-rc --namespace=kubectl-1697'
Mar 24 02:51:41.893: INFO: stderr: ""
Mar 24 02:51:41.893: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:51:41.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1697" for this suite.
Mar 24 02:51:47.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:51:47.958: INFO: namespace kubectl-1697 deletion completed in 6.06246964s

• [SLOW TEST:12.479 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:51:47.959: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5287
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-8794
STEP: Creating secret with name secret-test-4ce4ae0b-6061-47d3-91db-5dac7183e60c
STEP: Creating a pod to test consume secrets
Mar 24 02:51:48.214: INFO: Waiting up to 5m0s for pod "pod-secrets-27ff9be9-b503-4a66-a087-365167152389" in namespace "secrets-5287" to be "success or failure"
Mar 24 02:51:48.216: INFO: Pod "pod-secrets-27ff9be9-b503-4a66-a087-365167152389": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063055ms
Mar 24 02:51:50.219: INFO: Pod "pod-secrets-27ff9be9-b503-4a66-a087-365167152389": Phase="Running", Reason="", readiness=true. Elapsed: 2.004644887s
Mar 24 02:51:52.222: INFO: Pod "pod-secrets-27ff9be9-b503-4a66-a087-365167152389": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007492686s
STEP: Saw pod success
Mar 24 02:51:52.222: INFO: Pod "pod-secrets-27ff9be9-b503-4a66-a087-365167152389" satisfied condition "success or failure"
Mar 24 02:51:52.224: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-27ff9be9-b503-4a66-a087-365167152389 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 02:51:52.233: INFO: Waiting for pod pod-secrets-27ff9be9-b503-4a66-a087-365167152389 to disappear
Mar 24 02:51:52.235: INFO: Pod pod-secrets-27ff9be9-b503-4a66-a087-365167152389 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:51:52.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5287" for this suite.
Mar 24 02:51:58.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:51:58.300: INFO: namespace secrets-5287 deletion completed in 6.062914061s
STEP: Destroying namespace "secret-namespace-8794" for this suite.
Mar 24 02:52:04.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:52:04.391: INFO: namespace secret-namespace-8794 deletion completed in 6.091220825s

• [SLOW TEST:16.432 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:52:04.391: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 24 02:52:04.532: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:52:04.534: INFO: Number of nodes with available pods: 0
Mar 24 02:52:04.534: INFO: Node kind-worker is running more than one daemon pod
Mar 24 02:52:05.538: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:52:05.540: INFO: Number of nodes with available pods: 0
Mar 24 02:52:05.540: INFO: Node kind-worker is running more than one daemon pod
Mar 24 02:52:06.537: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:52:06.540: INFO: Number of nodes with available pods: 2
Mar 24 02:52:06.540: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 24 02:52:06.550: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:52:06.552: INFO: Number of nodes with available pods: 1
Mar 24 02:52:06.552: INFO: Node kind-worker is running more than one daemon pod
Mar 24 02:52:07.555: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:52:07.557: INFO: Number of nodes with available pods: 1
Mar 24 02:52:07.557: INFO: Node kind-worker is running more than one daemon pod
Mar 24 02:52:08.555: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:52:08.558: INFO: Number of nodes with available pods: 1
Mar 24 02:52:08.558: INFO: Node kind-worker is running more than one daemon pod
Mar 24 02:52:09.555: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:52:09.557: INFO: Number of nodes with available pods: 1
Mar 24 02:52:09.557: INFO: Node kind-worker is running more than one daemon pod
Mar 24 02:52:10.555: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:52:10.557: INFO: Number of nodes with available pods: 1
Mar 24 02:52:10.557: INFO: Node kind-worker is running more than one daemon pod
Mar 24 02:52:11.555: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:52:11.558: INFO: Number of nodes with available pods: 1
Mar 24 02:52:11.558: INFO: Node kind-worker is running more than one daemon pod
Mar 24 02:52:12.555: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:52:12.557: INFO: Number of nodes with available pods: 2
Mar 24 02:52:12.557: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5986, will wait for the garbage collector to delete the pods
Mar 24 02:52:12.616: INFO: Deleting DaemonSet.extensions daemon-set took: 5.078859ms
Mar 24 02:52:12.917: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.299122ms
Mar 24 02:52:21.320: INFO: Number of nodes with available pods: 0
Mar 24 02:52:21.320: INFO: Number of running nodes: 0, number of available pods: 0
Mar 24 02:52:21.323: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5986/daemonsets","resourceVersion":"3521"},"items":null}

Mar 24 02:52:21.325: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5986/pods","resourceVersion":"3521"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:52:21.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5986" for this suite.
Mar 24 02:52:27.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:52:27.397: INFO: namespace daemonsets-5986 deletion completed in 6.062029175s

• [SLOW TEST:23.005 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:52:27.397: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar 24 02:52:30.044: INFO: Successfully updated pod "annotationupdate4e28a99d-c51b-4d57-a05d-c5e04f47c843"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:52:34.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3909" for this suite.
Mar 24 02:52:56.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:52:56.138: INFO: namespace downward-api-3909 deletion completed in 22.076162426s

• [SLOW TEST:28.741 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:52:56.139: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6080
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6080
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-6080
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6080
Mar 24 02:52:56.278: INFO: Found 0 stateful pods, waiting for 1
Mar 24 02:53:06.282: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 24 02:53:06.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-6080 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 24 02:53:06.454: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 24 02:53:06.454: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 24 02:53:06.454: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 24 02:53:06.457: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 24 02:53:16.460: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 02:53:16.460: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 02:53:16.469: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar 24 02:53:16.469: INFO: ss-0  kind-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:52:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:52:56 +0000 UTC  }]
Mar 24 02:53:16.469: INFO: 
Mar 24 02:53:16.469: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 24 02:53:17.473: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997461996s
Mar 24 02:53:18.476: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993238599s
Mar 24 02:53:19.479: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990367344s
Mar 24 02:53:20.482: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.987336837s
Mar 24 02:53:21.486: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.984353266s
Mar 24 02:53:22.489: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.981039588s
Mar 24 02:53:23.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.977889759s
Mar 24 02:53:24.495: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.974667413s
Mar 24 02:53:25.499: INFO: Verifying statefulset ss doesn't scale past 3 for another 971.494964ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6080
Mar 24 02:53:26.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-6080 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 24 02:53:26.667: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar 24 02:53:26.667: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 24 02:53:26.667: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 24 02:53:26.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-6080 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 24 02:53:26.838: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 24 02:53:26.838: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 24 02:53:26.838: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 24 02:53:26.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-6080 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 24 02:53:27.003: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 24 02:53:27.003: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 24 02:53:27.003: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 24 02:53:27.006: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar 24 02:53:37.010: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 02:53:37.010: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 02:53:37.010: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 24 02:53:37.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-6080 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 24 02:53:37.185: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 24 02:53:37.185: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 24 02:53:37.185: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 24 02:53:37.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-6080 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 24 02:53:37.356: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 24 02:53:37.356: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 24 02:53:37.356: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 24 02:53:37.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-6080 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 24 02:53:37.548: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 24 02:53:37.548: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 24 02:53:37.548: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 24 02:53:37.548: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 02:53:37.550: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Mar 24 02:53:47.556: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 02:53:47.556: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 02:53:47.556: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 02:53:47.563: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar 24 02:53:47.563: INFO: ss-0  kind-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:52:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:52:56 +0000 UTC  }]
Mar 24 02:53:47.564: INFO: ss-1  kind-worker   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:16 +0000 UTC  }]
Mar 24 02:53:47.564: INFO: ss-2  kind-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:16 +0000 UTC  }]
Mar 24 02:53:47.564: INFO: 
Mar 24 02:53:47.564: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 24 02:53:48.567: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar 24 02:53:48.567: INFO: ss-0  kind-worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:52:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:52:56 +0000 UTC  }]
Mar 24 02:53:48.567: INFO: ss-1  kind-worker   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:16 +0000 UTC  }]
Mar 24 02:53:48.567: INFO: ss-2  kind-worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:16 +0000 UTC  }]
Mar 24 02:53:48.567: INFO: 
Mar 24 02:53:48.567: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 24 02:53:49.570: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Mar 24 02:53:49.570: INFO: ss-1  kind-worker  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:16 +0000 UTC  }]
Mar 24 02:53:49.570: INFO: 
Mar 24 02:53:49.570: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 24 02:53:50.573: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Mar 24 02:53:50.573: INFO: ss-1  kind-worker  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:16 +0000 UTC  }]
Mar 24 02:53:50.573: INFO: 
Mar 24 02:53:50.573: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 24 02:53:51.576: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Mar 24 02:53:51.576: INFO: ss-1  kind-worker  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 02:53:16 +0000 UTC  }]
Mar 24 02:53:51.576: INFO: 
Mar 24 02:53:51.576: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 24 02:53:52.579: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.984431372s
Mar 24 02:53:53.582: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.981520683s
Mar 24 02:53:54.585: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.978637427s
Mar 24 02:53:55.588: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.975860891s
Mar 24 02:53:56.591: INFO: Verifying statefulset ss doesn't scale past 0 for another 972.993437ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6080
Mar 24 02:53:57.594: INFO: Scaling statefulset ss to 0
Mar 24 02:53:57.601: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar 24 02:53:57.602: INFO: Deleting all statefulset in ns statefulset-6080
Mar 24 02:53:57.604: INFO: Scaling statefulset ss to 0
Mar 24 02:53:57.610: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 02:53:57.612: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:53:57.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6080" for this suite.
Mar 24 02:54:03.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:54:03.706: INFO: namespace statefulset-6080 deletion completed in 6.084138727s

• [SLOW TEST:67.567 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:54:03.706: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4342
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 02:54:03.836: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9e8517f-957d-4112-a7d8-36b80c91fadf" in namespace "downward-api-4342" to be "success or failure"
Mar 24 02:54:03.838: INFO: Pod "downwardapi-volume-d9e8517f-957d-4112-a7d8-36b80c91fadf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.9425ms
Mar 24 02:54:05.841: INFO: Pod "downwardapi-volume-d9e8517f-957d-4112-a7d8-36b80c91fadf": Phase="Running", Reason="", readiness=true. Elapsed: 2.004839036s
Mar 24 02:54:07.844: INFO: Pod "downwardapi-volume-d9e8517f-957d-4112-a7d8-36b80c91fadf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007945545s
STEP: Saw pod success
Mar 24 02:54:07.844: INFO: Pod "downwardapi-volume-d9e8517f-957d-4112-a7d8-36b80c91fadf" satisfied condition "success or failure"
Mar 24 02:54:07.847: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-d9e8517f-957d-4112-a7d8-36b80c91fadf container client-container: <nil>
STEP: delete the pod
Mar 24 02:54:07.856: INFO: Waiting for pod downwardapi-volume-d9e8517f-957d-4112-a7d8-36b80c91fadf to disappear
Mar 24 02:54:07.858: INFO: Pod downwardapi-volume-d9e8517f-957d-4112-a7d8-36b80c91fadf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:54:07.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4342" for this suite.
Mar 24 02:54:13.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:54:13.923: INFO: namespace downward-api-4342 deletion completed in 6.062466558s

• [SLOW TEST:10.216 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:54:13.923: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5446
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 02:54:14.051: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba064796-8a2a-4c2a-b458-1a618b886bb5" in namespace "projected-5446" to be "success or failure"
Mar 24 02:54:14.053: INFO: Pod "downwardapi-volume-ba064796-8a2a-4c2a-b458-1a618b886bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.334268ms
Mar 24 02:54:16.056: INFO: Pod "downwardapi-volume-ba064796-8a2a-4c2a-b458-1a618b886bb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005131855s
STEP: Saw pod success
Mar 24 02:54:16.056: INFO: Pod "downwardapi-volume-ba064796-8a2a-4c2a-b458-1a618b886bb5" satisfied condition "success or failure"
Mar 24 02:54:16.058: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-ba064796-8a2a-4c2a-b458-1a618b886bb5 container client-container: <nil>
STEP: delete the pod
Mar 24 02:54:16.072: INFO: Waiting for pod downwardapi-volume-ba064796-8a2a-4c2a-b458-1a618b886bb5 to disappear
Mar 24 02:54:16.074: INFO: Pod downwardapi-volume-ba064796-8a2a-4c2a-b458-1a618b886bb5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:54:16.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5446" for this suite.
Mar 24 02:54:22.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:54:22.141: INFO: namespace projected-5446 deletion completed in 6.064758933s

• [SLOW TEST:8.218 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:54:22.141: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7023
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 24 02:54:22.270: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7023,SelfLink:/api/v1/namespaces/watch-7023/configmaps/e2e-watch-test-configmap-a,UID:ede68d6a-7ce3-496f-9580-9b4d31a0c1d6,ResourceVersion:3970,Generation:0,CreationTimestamp:2020-03-24 02:54:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 24 02:54:22.270: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7023,SelfLink:/api/v1/namespaces/watch-7023/configmaps/e2e-watch-test-configmap-a,UID:ede68d6a-7ce3-496f-9580-9b4d31a0c1d6,ResourceVersion:3970,Generation:0,CreationTimestamp:2020-03-24 02:54:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 24 02:54:32.276: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7023,SelfLink:/api/v1/namespaces/watch-7023/configmaps/e2e-watch-test-configmap-a,UID:ede68d6a-7ce3-496f-9580-9b4d31a0c1d6,ResourceVersion:3985,Generation:0,CreationTimestamp:2020-03-24 02:54:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 24 02:54:32.276: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7023,SelfLink:/api/v1/namespaces/watch-7023/configmaps/e2e-watch-test-configmap-a,UID:ede68d6a-7ce3-496f-9580-9b4d31a0c1d6,ResourceVersion:3985,Generation:0,CreationTimestamp:2020-03-24 02:54:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 24 02:54:42.282: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7023,SelfLink:/api/v1/namespaces/watch-7023/configmaps/e2e-watch-test-configmap-a,UID:ede68d6a-7ce3-496f-9580-9b4d31a0c1d6,ResourceVersion:4001,Generation:0,CreationTimestamp:2020-03-24 02:54:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 24 02:54:42.282: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7023,SelfLink:/api/v1/namespaces/watch-7023/configmaps/e2e-watch-test-configmap-a,UID:ede68d6a-7ce3-496f-9580-9b4d31a0c1d6,ResourceVersion:4001,Generation:0,CreationTimestamp:2020-03-24 02:54:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 24 02:54:52.287: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7023,SelfLink:/api/v1/namespaces/watch-7023/configmaps/e2e-watch-test-configmap-a,UID:ede68d6a-7ce3-496f-9580-9b4d31a0c1d6,ResourceVersion:4015,Generation:0,CreationTimestamp:2020-03-24 02:54:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 24 02:54:52.287: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7023,SelfLink:/api/v1/namespaces/watch-7023/configmaps/e2e-watch-test-configmap-a,UID:ede68d6a-7ce3-496f-9580-9b4d31a0c1d6,ResourceVersion:4015,Generation:0,CreationTimestamp:2020-03-24 02:54:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 24 02:55:02.293: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7023,SelfLink:/api/v1/namespaces/watch-7023/configmaps/e2e-watch-test-configmap-b,UID:9926a3fe-cbe8-414c-a907-24f8106eb95b,ResourceVersion:4030,Generation:0,CreationTimestamp:2020-03-24 02:55:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 24 02:55:02.293: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7023,SelfLink:/api/v1/namespaces/watch-7023/configmaps/e2e-watch-test-configmap-b,UID:9926a3fe-cbe8-414c-a907-24f8106eb95b,ResourceVersion:4030,Generation:0,CreationTimestamp:2020-03-24 02:55:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 24 02:55:12.298: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7023,SelfLink:/api/v1/namespaces/watch-7023/configmaps/e2e-watch-test-configmap-b,UID:9926a3fe-cbe8-414c-a907-24f8106eb95b,ResourceVersion:4048,Generation:0,CreationTimestamp:2020-03-24 02:55:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 24 02:55:12.298: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7023,SelfLink:/api/v1/namespaces/watch-7023/configmaps/e2e-watch-test-configmap-b,UID:9926a3fe-cbe8-414c-a907-24f8106eb95b,ResourceVersion:4048,Generation:0,CreationTimestamp:2020-03-24 02:55:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:55:22.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7023" for this suite.
Mar 24 02:55:28.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:55:28.364: INFO: namespace watch-7023 deletion completed in 6.063000449s

• [SLOW TEST:66.223 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:55:28.364: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8954
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 24 02:55:36.512: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 24 02:55:36.514: INFO: Pod pod-with-prestop-http-hook still exists
Mar 24 02:55:38.514: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 24 02:55:38.517: INFO: Pod pod-with-prestop-http-hook still exists
Mar 24 02:55:40.514: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 24 02:55:40.517: INFO: Pod pod-with-prestop-http-hook still exists
Mar 24 02:55:42.514: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 24 02:55:42.517: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:55:42.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8954" for this suite.
Mar 24 02:56:04.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:56:04.588: INFO: namespace container-lifecycle-hook-8954 deletion completed in 22.063519716s

• [SLOW TEST:36.224 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:56:04.589: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9921
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9921
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 24 02:56:04.712: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 24 02:56:26.755: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.18 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9921 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 02:56:26.756: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 02:56:27.838: INFO: Found all expected endpoints: [netserver-0]
Mar 24 02:56:27.841: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.18 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9921 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 02:56:27.841: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 02:56:28.926: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:56:28.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9921" for this suite.
Mar 24 02:56:50.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:56:50.993: INFO: namespace pod-network-test-9921 deletion completed in 22.063742193s

• [SLOW TEST:46.404 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:56:50.993: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7481
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7481.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7481.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 02:57:01.150: INFO: DNS probes using dns-7481/dns-test-aeb5725a-d1af-4a1a-a6bb-bd4f1d16a618 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:57:01.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7481" for this suite.
Mar 24 02:57:07.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:57:07.229: INFO: namespace dns-7481 deletion completed in 6.067923172s

• [SLOW TEST:16.237 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:57:07.230: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4432
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4432
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 24 02:57:07.354: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 24 02:57:31.395: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.20:8080/dial?request=hostName&protocol=http&host=10.244.2.21&port=8080&tries=1'] Namespace:pod-network-test-4432 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 02:57:31.395: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 02:57:31.478: INFO: Waiting for endpoints: map[]
Mar 24 02:57:31.481: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.20:8080/dial?request=hostName&protocol=http&host=10.244.1.19&port=8080&tries=1'] Namespace:pod-network-test-4432 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 02:57:31.481: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 02:57:31.562: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:57:31.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4432" for this suite.
Mar 24 02:57:53.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:57:53.631: INFO: namespace pod-network-test-4432 deletion completed in 22.065443987s

• [SLOW TEST:46.401 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:57:53.631: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:57:53.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1604" for this suite.
Mar 24 02:57:59.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:57:59.848: INFO: namespace kubelet-test-1604 deletion completed in 6.074666669s

• [SLOW TEST:6.217 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:57:59.849: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8870
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Mar 24 02:57:59.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 --namespace=kubectl-8870 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar 24 02:58:02.428: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar 24 02:58:02.428: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:58:04.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8870" for this suite.
Mar 24 02:58:10.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:58:10.496: INFO: namespace kubectl-8870 deletion completed in 6.061048442s

• [SLOW TEST:10.647 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:58:10.496: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7341
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar 24 02:58:10.626: INFO: Waiting up to 5m0s for pod "downward-api-685a5c0d-286f-4f4f-85bd-d2fd69beb55e" in namespace "downward-api-7341" to be "success or failure"
Mar 24 02:58:10.628: INFO: Pod "downward-api-685a5c0d-286f-4f4f-85bd-d2fd69beb55e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.977858ms
Mar 24 02:58:12.631: INFO: Pod "downward-api-685a5c0d-286f-4f4f-85bd-d2fd69beb55e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005050118s
STEP: Saw pod success
Mar 24 02:58:12.631: INFO: Pod "downward-api-685a5c0d-286f-4f4f-85bd-d2fd69beb55e" satisfied condition "success or failure"
Mar 24 02:58:12.633: INFO: Trying to get logs from node kind-worker2 pod downward-api-685a5c0d-286f-4f4f-85bd-d2fd69beb55e container dapi-container: <nil>
STEP: delete the pod
Mar 24 02:58:12.644: INFO: Waiting for pod downward-api-685a5c0d-286f-4f4f-85bd-d2fd69beb55e to disappear
Mar 24 02:58:12.645: INFO: Pod downward-api-685a5c0d-286f-4f4f-85bd-d2fd69beb55e no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 02:58:12.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7341" for this suite.
Mar 24 02:58:18.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 02:58:18.708: INFO: namespace downward-api-7341 deletion completed in 6.060847633s

• [SLOW TEST:8.212 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 02:58:18.709: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6486
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6486
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Mar 24 02:58:18.846: INFO: Found 0 stateful pods, waiting for 3
Mar 24 02:58:28.849: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 02:58:28.849: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 02:58:28.849: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 02:58:28.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-6486 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 24 02:58:29.023: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 24 02:58:29.023: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 24 02:58:29.023: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar 24 02:58:39.049: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 24 02:58:49.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-6486 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 24 02:58:49.235: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar 24 02:58:49.235: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 24 02:58:49.235: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 24 02:58:59.251: INFO: Waiting for StatefulSet statefulset-6486/ss2 to complete update
Mar 24 02:58:59.251: INFO: Waiting for Pod statefulset-6486/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Mar 24 02:58:59.251: INFO: Waiting for Pod statefulset-6486/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Mar 24 02:58:59.251: INFO: Waiting for Pod statefulset-6486/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Mar 24 02:59:09.258: INFO: Waiting for StatefulSet statefulset-6486/ss2 to complete update
Mar 24 02:59:09.258: INFO: Waiting for Pod statefulset-6486/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Mar 24 02:59:09.258: INFO: Waiting for Pod statefulset-6486/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Mar 24 02:59:19.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-6486 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 24 02:59:19.420: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 24 02:59:19.420: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 24 02:59:19.420: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 24 02:59:29.445: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 24 02:59:39.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-6486 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 24 02:59:39.625: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar 24 02:59:39.625: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 24 02:59:39.625: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 24 02:59:49.640: INFO: Waiting for StatefulSet statefulset-6486/ss2 to complete update
Mar 24 02:59:49.640: INFO: Waiting for Pod statefulset-6486/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar 24 02:59:59.646: INFO: Deleting all statefulset in ns statefulset-6486
Mar 24 02:59:59.648: INFO: Scaling statefulset ss2 to 0
Mar 24 03:00:39.658: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 03:00:39.660: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:00:39.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6486" for this suite.
Mar 24 03:00:45.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:00:45.733: INFO: namespace statefulset-6486 deletion completed in 6.061608339s

• [SLOW TEST:147.024 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:00:45.733: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1028
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 24 03:00:49.885: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 03:00:49.887: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 03:00:51.887: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 03:00:51.890: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 03:00:53.887: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 03:00:53.890: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 03:00:55.887: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 03:00:55.890: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 03:00:57.887: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 03:00:57.890: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 03:00:59.887: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 03:00:59.890: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 03:01:01.887: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 03:01:01.890: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 03:01:03.887: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 03:01:03.890: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 03:01:05.887: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 03:01:05.890: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 03:01:07.887: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 03:01:07.890: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 03:01:09.887: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 03:01:09.890: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 03:01:11.887: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 03:01:11.890: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:01:11.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1028" for this suite.
Mar 24 03:01:33.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:01:33.954: INFO: namespace container-lifecycle-hook-1028 deletion completed in 22.0616s

• [SLOW TEST:48.222 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:01:33.955: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-88
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar 24 03:01:34.078: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:01:37.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-88" for this suite.
Mar 24 03:01:43.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:01:43.260: INFO: namespace init-container-88 deletion completed in 6.069137596s

• [SLOW TEST:9.305 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:01:43.260: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6547
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:01:43.395: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 24 03:01:43.400: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:43.403: INFO: Number of nodes with available pods: 0
Mar 24 03:01:43.403: INFO: Node kind-worker is running more than one daemon pod
Mar 24 03:01:44.406: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:44.409: INFO: Number of nodes with available pods: 0
Mar 24 03:01:44.409: INFO: Node kind-worker is running more than one daemon pod
Mar 24 03:01:45.406: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:45.409: INFO: Number of nodes with available pods: 2
Mar 24 03:01:45.409: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 24 03:01:45.428: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:45.428: INFO: Wrong image for pod: daemon-set-dw6s2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:45.430: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:46.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:46.433: INFO: Wrong image for pod: daemon-set-dw6s2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:46.436: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:47.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:47.433: INFO: Wrong image for pod: daemon-set-dw6s2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:47.435: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:48.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:48.433: INFO: Wrong image for pod: daemon-set-dw6s2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:48.433: INFO: Pod daemon-set-dw6s2 is not available
Mar 24 03:01:48.436: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:49.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:49.433: INFO: Wrong image for pod: daemon-set-dw6s2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:49.433: INFO: Pod daemon-set-dw6s2 is not available
Mar 24 03:01:49.436: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:50.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:50.433: INFO: Wrong image for pod: daemon-set-dw6s2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:50.433: INFO: Pod daemon-set-dw6s2 is not available
Mar 24 03:01:50.435: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:51.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:51.433: INFO: Pod daemon-set-7fqvd is not available
Mar 24 03:01:51.435: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:52.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:52.433: INFO: Pod daemon-set-7fqvd is not available
Mar 24 03:01:52.436: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:53.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:53.436: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:54.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:54.433: INFO: Pod daemon-set-2zbgt is not available
Mar 24 03:01:54.436: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:55.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:55.433: INFO: Pod daemon-set-2zbgt is not available
Mar 24 03:01:55.436: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:56.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:56.433: INFO: Pod daemon-set-2zbgt is not available
Mar 24 03:01:56.436: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:57.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:57.433: INFO: Pod daemon-set-2zbgt is not available
Mar 24 03:01:57.435: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:58.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:58.433: INFO: Pod daemon-set-2zbgt is not available
Mar 24 03:01:58.436: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:01:59.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:01:59.433: INFO: Pod daemon-set-2zbgt is not available
Mar 24 03:01:59.436: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:02:00.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:02:00.433: INFO: Pod daemon-set-2zbgt is not available
Mar 24 03:02:00.436: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:02:01.433: INFO: Wrong image for pod: daemon-set-2zbgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 24 03:02:01.433: INFO: Pod daemon-set-2zbgt is not available
Mar 24 03:02:01.435: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:02:02.435: INFO: Pod daemon-set-8p5qf is not available
Mar 24 03:02:02.438: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 24 03:02:02.441: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:02:02.443: INFO: Number of nodes with available pods: 1
Mar 24 03:02:02.443: INFO: Node kind-worker is running more than one daemon pod
Mar 24 03:02:03.446: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:02:03.448: INFO: Number of nodes with available pods: 1
Mar 24 03:02:03.448: INFO: Node kind-worker is running more than one daemon pod
Mar 24 03:02:04.446: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:02:04.448: INFO: Number of nodes with available pods: 2
Mar 24 03:02:04.448: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6547, will wait for the garbage collector to delete the pods
Mar 24 03:02:04.516: INFO: Deleting DaemonSet.extensions daemon-set took: 5.785853ms
Mar 24 03:02:04.816: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.282002ms
Mar 24 03:02:11.718: INFO: Number of nodes with available pods: 0
Mar 24 03:02:11.718: INFO: Number of running nodes: 0, number of available pods: 0
Mar 24 03:02:11.720: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6547/daemonsets","resourceVersion":"5478"},"items":null}

Mar 24 03:02:11.722: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6547/pods","resourceVersion":"5478"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:02:11.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6547" for this suite.
Mar 24 03:02:17.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:02:17.790: INFO: namespace daemonsets-6547 deletion completed in 6.060245121s

• [SLOW TEST:34.530 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:02:17.790: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8275
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 24 03:02:21.938: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:21.940: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:02:23.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:23.943: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:02:25.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:25.943: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:02:27.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:27.943: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:02:29.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:29.943: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:02:31.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:31.943: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:02:33.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:33.943: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:02:35.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:35.943: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:02:37.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:37.943: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:02:39.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:39.947: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:02:41.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:41.943: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:02:43.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:43.943: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:02:45.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:45.943: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:02:47.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:47.943: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:02:49.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:49.943: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:02:51.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:02:51.943: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:02:51.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8275" for this suite.
Mar 24 03:03:13.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:03:14.014: INFO: namespace container-lifecycle-hook-8275 deletion completed in 22.063267204s

• [SLOW TEST:56.224 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:03:14.014: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5175
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Mar 24 03:03:14.146: INFO: Waiting up to 5m0s for pod "client-containers-67a06d67-07dc-4a6a-9484-a8d9f5b32379" in namespace "containers-5175" to be "success or failure"
Mar 24 03:03:14.148: INFO: Pod "client-containers-67a06d67-07dc-4a6a-9484-a8d9f5b32379": Phase="Pending", Reason="", readiness=false. Elapsed: 2.206874ms
Mar 24 03:03:16.151: INFO: Pod "client-containers-67a06d67-07dc-4a6a-9484-a8d9f5b32379": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005138462s
STEP: Saw pod success
Mar 24 03:03:16.151: INFO: Pod "client-containers-67a06d67-07dc-4a6a-9484-a8d9f5b32379" satisfied condition "success or failure"
Mar 24 03:03:16.153: INFO: Trying to get logs from node kind-worker pod client-containers-67a06d67-07dc-4a6a-9484-a8d9f5b32379 container test-container: <nil>
STEP: delete the pod
Mar 24 03:03:16.165: INFO: Waiting for pod client-containers-67a06d67-07dc-4a6a-9484-a8d9f5b32379 to disappear
Mar 24 03:03:16.167: INFO: Pod client-containers-67a06d67-07dc-4a6a-9484-a8d9f5b32379 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:03:16.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5175" for this suite.
Mar 24 03:03:22.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:03:22.235: INFO: namespace containers-5175 deletion completed in 6.064919486s

• [SLOW TEST:8.221 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:03:22.236: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1991
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Mar 24 03:03:22.364: INFO: Waiting up to 5m0s for pod "client-containers-35ef0eec-c52f-4a48-9e55-0f3be68ad8ef" in namespace "containers-1991" to be "success or failure"
Mar 24 03:03:22.366: INFO: Pod "client-containers-35ef0eec-c52f-4a48-9e55-0f3be68ad8ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004588ms
Mar 24 03:03:24.369: INFO: Pod "client-containers-35ef0eec-c52f-4a48-9e55-0f3be68ad8ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004802383s
STEP: Saw pod success
Mar 24 03:03:24.369: INFO: Pod "client-containers-35ef0eec-c52f-4a48-9e55-0f3be68ad8ef" satisfied condition "success or failure"
Mar 24 03:03:24.371: INFO: Trying to get logs from node kind-worker2 pod client-containers-35ef0eec-c52f-4a48-9e55-0f3be68ad8ef container test-container: <nil>
STEP: delete the pod
Mar 24 03:03:24.382: INFO: Waiting for pod client-containers-35ef0eec-c52f-4a48-9e55-0f3be68ad8ef to disappear
Mar 24 03:03:24.384: INFO: Pod client-containers-35ef0eec-c52f-4a48-9e55-0f3be68ad8ef no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:03:24.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1991" for this suite.
Mar 24 03:03:30.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:03:30.448: INFO: namespace containers-1991 deletion completed in 6.0616785s

• [SLOW TEST:8.212 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:03:30.448: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Mar 24 03:03:30.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 cluster-info'
Mar 24 03:03:30.649: INFO: stderr: ""
Mar 24 03:03:30.649: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:03:30.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9477" for this suite.
Mar 24 03:03:36.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:03:36.713: INFO: namespace kubectl-9477 deletion completed in 6.061661748s

• [SLOW TEST:6.265 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:03:36.713: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5398
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Mar 24 03:03:38.855: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-811275467 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar 24 03:03:53.931: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:03:53.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5398" for this suite.
Mar 24 03:03:59.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:03:59.995: INFO: namespace pods-5398 deletion completed in 6.058847518s

• [SLOW TEST:23.282 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:03:59.996: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1068
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 24 03:04:00.130: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1068,SelfLink:/api/v1/namespaces/watch-1068/configmaps/e2e-watch-test-label-changed,UID:2d427e49-c26f-4191-ac50-96284985e3fc,ResourceVersion:5810,Generation:0,CreationTimestamp:2020-03-24 03:04:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 24 03:04:00.130: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1068,SelfLink:/api/v1/namespaces/watch-1068/configmaps/e2e-watch-test-label-changed,UID:2d427e49-c26f-4191-ac50-96284985e3fc,ResourceVersion:5811,Generation:0,CreationTimestamp:2020-03-24 03:04:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 24 03:04:00.130: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1068,SelfLink:/api/v1/namespaces/watch-1068/configmaps/e2e-watch-test-label-changed,UID:2d427e49-c26f-4191-ac50-96284985e3fc,ResourceVersion:5812,Generation:0,CreationTimestamp:2020-03-24 03:04:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 24 03:04:10.146: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1068,SelfLink:/api/v1/namespaces/watch-1068/configmaps/e2e-watch-test-label-changed,UID:2d427e49-c26f-4191-ac50-96284985e3fc,ResourceVersion:5829,Generation:0,CreationTimestamp:2020-03-24 03:04:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 24 03:04:10.146: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1068,SelfLink:/api/v1/namespaces/watch-1068/configmaps/e2e-watch-test-label-changed,UID:2d427e49-c26f-4191-ac50-96284985e3fc,ResourceVersion:5830,Generation:0,CreationTimestamp:2020-03-24 03:04:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar 24 03:04:10.146: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1068,SelfLink:/api/v1/namespaces/watch-1068/configmaps/e2e-watch-test-label-changed,UID:2d427e49-c26f-4191-ac50-96284985e3fc,ResourceVersion:5831,Generation:0,CreationTimestamp:2020-03-24 03:04:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:04:10.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1068" for this suite.
Mar 24 03:04:16.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:04:16.222: INFO: namespace watch-1068 deletion completed in 6.073099183s

• [SLOW TEST:16.226 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:04:16.222: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6761
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 24 03:04:19.365: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:04:19.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6761" for this suite.
Mar 24 03:04:25.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:04:25.447: INFO: namespace container-runtime-6761 deletion completed in 6.069428399s

• [SLOW TEST:9.225 seconds]
[k8s.io] Container Runtime
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:04:25.448: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9515
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-6b3af742-ed1c-4b3d-a844-be1f3e25ff3a
STEP: Creating configMap with name cm-test-opt-upd-60ac728b-f813-4440-904b-6e322f47540c
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-6b3af742-ed1c-4b3d-a844-be1f3e25ff3a
STEP: Updating configmap cm-test-opt-upd-60ac728b-f813-4440-904b-6e322f47540c
STEP: Creating configMap with name cm-test-opt-create-9dee76c8-ae72-46c1-9b16-983f375ab403
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:05:35.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9515" for this suite.
Mar 24 03:05:57.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:05:57.884: INFO: namespace projected-9515 deletion completed in 22.06065931s

• [SLOW TEST:92.436 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:05:57.885: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-1612
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar 24 03:05:58.009: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Mar 24 03:05:58.985: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 24 03:06:01.015: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720615958, loc:(*time.Location)(0x7eafa20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720615958, loc:(*time.Location)(0x7eafa20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720615958, loc:(*time.Location)(0x7eafa20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720615958, loc:(*time.Location)(0x7eafa20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:06:03.018: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720615958, loc:(*time.Location)(0x7eafa20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720615958, loc:(*time.Location)(0x7eafa20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720615958, loc:(*time.Location)(0x7eafa20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720615958, loc:(*time.Location)(0x7eafa20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:06:05.743: INFO: Waited 720.11027ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:06:06.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1612" for this suite.
Mar 24 03:06:12.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:06:12.339: INFO: namespace aggregator-1612 deletion completed in 6.157818408s

• [SLOW TEST:14.454 seconds]
[sig-api-machinery] Aggregator
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:06:12.339: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3261
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1292
STEP: creating an rc
Mar 24 03:06:12.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 create -f - --namespace=kubectl-3261'
Mar 24 03:06:12.726: INFO: stderr: ""
Mar 24 03:06:12.726: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Mar 24 03:06:13.730: INFO: Selector matched 1 pods for map[app:redis]
Mar 24 03:06:13.730: INFO: Found 0 / 1
Mar 24 03:06:14.729: INFO: Selector matched 1 pods for map[app:redis]
Mar 24 03:06:14.730: INFO: Found 1 / 1
Mar 24 03:06:14.730: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 24 03:06:14.732: INFO: Selector matched 1 pods for map[app:redis]
Mar 24 03:06:14.732: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Mar 24 03:06:14.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 logs redis-master-d4b8v redis-master --namespace=kubectl-3261'
Mar 24 03:06:14.815: INFO: stderr: ""
Mar 24 03:06:14.815: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 24 Mar 03:06:13.778 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 24 Mar 03:06:13.778 # Server started, Redis version 3.2.12\n1:M 24 Mar 03:06:13.778 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 24 Mar 03:06:13.778 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Mar 24 03:06:14.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 logs redis-master-d4b8v redis-master --namespace=kubectl-3261 --tail=1'
Mar 24 03:06:14.897: INFO: stderr: ""
Mar 24 03:06:14.897: INFO: stdout: "1:M 24 Mar 03:06:13.778 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Mar 24 03:06:14.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 logs redis-master-d4b8v redis-master --namespace=kubectl-3261 --limit-bytes=1'
Mar 24 03:06:14.978: INFO: stderr: ""
Mar 24 03:06:14.978: INFO: stdout: " "
STEP: exposing timestamps
Mar 24 03:06:14.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 logs redis-master-d4b8v redis-master --namespace=kubectl-3261 --tail=1 --timestamps'
Mar 24 03:06:15.064: INFO: stderr: ""
Mar 24 03:06:15.064: INFO: stdout: "2020-03-24T03:06:13.778473441Z 1:M 24 Mar 03:06:13.778 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Mar 24 03:06:17.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 logs redis-master-d4b8v redis-master --namespace=kubectl-3261 --since=1s'
Mar 24 03:06:17.647: INFO: stderr: ""
Mar 24 03:06:17.647: INFO: stdout: ""
Mar 24 03:06:17.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 logs redis-master-d4b8v redis-master --namespace=kubectl-3261 --since=24h'
Mar 24 03:06:17.730: INFO: stderr: ""
Mar 24 03:06:17.730: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 24 Mar 03:06:13.778 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 24 Mar 03:06:13.778 # Server started, Redis version 3.2.12\n1:M 24 Mar 03:06:13.778 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 24 Mar 03:06:13.778 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
STEP: using delete to clean up resources
Mar 24 03:06:17.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete --grace-period=0 --force -f - --namespace=kubectl-3261'
Mar 24 03:06:17.805: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:06:17.805: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Mar 24 03:06:17.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get rc,svc -l name=nginx --no-headers --namespace=kubectl-3261'
Mar 24 03:06:17.887: INFO: stderr: "No resources found.\n"
Mar 24 03:06:17.887: INFO: stdout: ""
Mar 24 03:06:17.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -l name=nginx --namespace=kubectl-3261 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 24 03:06:17.961: INFO: stderr: ""
Mar 24 03:06:17.961: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:06:17.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3261" for this suite.
Mar 24 03:06:23.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:06:24.026: INFO: namespace kubectl-3261 deletion completed in 6.062478477s

• [SLOW TEST:11.687 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:06:24.027: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5891
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-36362618-d429-423a-a4c9-79817eb615ac in namespace container-probe-5891
Mar 24 03:06:26.162: INFO: Started pod busybox-36362618-d429-423a-a4c9-79817eb615ac in namespace container-probe-5891
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 03:06:26.164: INFO: Initial restart count of pod busybox-36362618-d429-423a-a4c9-79817eb615ac is 0
Mar 24 03:07:20.247: INFO: Restart count of pod container-probe-5891/busybox-36362618-d429-423a-a4c9-79817eb615ac is now 1 (54.082941163s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:07:20.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5891" for this suite.
Mar 24 03:07:26.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:07:26.319: INFO: namespace container-probe-5891 deletion completed in 6.062862929s

• [SLOW TEST:62.292 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:07:26.320: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9991
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9991.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9991.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9991.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9991.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9991.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9991.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9991.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9991.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9991.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9991.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9991.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9991.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9991.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 205.4.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.4.205_udp@PTR;check="$$(dig +tcp +noall +answer +search 205.4.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.4.205_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9991.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9991.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9991.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9991.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9991.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9991.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9991.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9991.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9991.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9991.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9991.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9991.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9991.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 205.4.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.4.205_udp@PTR;check="$$(dig +tcp +noall +answer +search 205.4.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.4.205_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 03:07:28.473: INFO: Unable to read wheezy_udp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:28.475: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:28.478: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:28.480: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:28.495: INFO: Unable to read jessie_udp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:28.497: INFO: Unable to read jessie_tcp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:28.498: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:28.501: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:28.512: INFO: Lookups using dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8 failed for: [wheezy_udp@dns-test-service.dns-9991.svc.cluster.local wheezy_tcp@dns-test-service.dns-9991.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9991.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9991.svc.cluster.local jessie_udp@dns-test-service.dns-9991.svc.cluster.local jessie_tcp@dns-test-service.dns-9991.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9991.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9991.svc.cluster.local]

Mar 24 03:07:33.517: INFO: Unable to read wheezy_udp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:33.519: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:33.538: INFO: Unable to read jessie_udp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:33.540: INFO: Unable to read jessie_tcp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:33.557: INFO: Lookups using dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8 failed for: [wheezy_udp@dns-test-service.dns-9991.svc.cluster.local wheezy_tcp@dns-test-service.dns-9991.svc.cluster.local jessie_udp@dns-test-service.dns-9991.svc.cluster.local jessie_tcp@dns-test-service.dns-9991.svc.cluster.local]

Mar 24 03:07:38.516: INFO: Unable to read wheezy_udp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:38.518: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:38.538: INFO: Unable to read jessie_udp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:38.540: INFO: Unable to read jessie_tcp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:38.556: INFO: Lookups using dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8 failed for: [wheezy_udp@dns-test-service.dns-9991.svc.cluster.local wheezy_tcp@dns-test-service.dns-9991.svc.cluster.local jessie_udp@dns-test-service.dns-9991.svc.cluster.local jessie_tcp@dns-test-service.dns-9991.svc.cluster.local]

Mar 24 03:07:43.516: INFO: Unable to read wheezy_udp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:43.518: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:43.538: INFO: Unable to read jessie_udp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:43.540: INFO: Unable to read jessie_tcp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:43.556: INFO: Lookups using dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8 failed for: [wheezy_udp@dns-test-service.dns-9991.svc.cluster.local wheezy_tcp@dns-test-service.dns-9991.svc.cluster.local jessie_udp@dns-test-service.dns-9991.svc.cluster.local jessie_tcp@dns-test-service.dns-9991.svc.cluster.local]

Mar 24 03:07:48.516: INFO: Unable to read wheezy_udp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:48.519: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:48.540: INFO: Unable to read jessie_udp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:48.543: INFO: Unable to read jessie_tcp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:48.560: INFO: Lookups using dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8 failed for: [wheezy_udp@dns-test-service.dns-9991.svc.cluster.local wheezy_tcp@dns-test-service.dns-9991.svc.cluster.local jessie_udp@dns-test-service.dns-9991.svc.cluster.local jessie_tcp@dns-test-service.dns-9991.svc.cluster.local]

Mar 24 03:07:53.516: INFO: Unable to read wheezy_udp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:53.518: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:53.539: INFO: Unable to read jessie_udp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:53.541: INFO: Unable to read jessie_tcp@dns-test-service.dns-9991.svc.cluster.local from pod dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8: the server could not find the requested resource (get pods dns-test-f619fb07-157d-4ab1-9eac-da47111445d8)
Mar 24 03:07:53.556: INFO: Lookups using dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8 failed for: [wheezy_udp@dns-test-service.dns-9991.svc.cluster.local wheezy_tcp@dns-test-service.dns-9991.svc.cluster.local jessie_udp@dns-test-service.dns-9991.svc.cluster.local jessie_tcp@dns-test-service.dns-9991.svc.cluster.local]

Mar 24 03:07:58.556: INFO: DNS probes using dns-9991/dns-test-f619fb07-157d-4ab1-9eac-da47111445d8 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:07:58.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9991" for this suite.
Mar 24 03:08:04.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:08:04.671: INFO: namespace dns-9991 deletion completed in 6.064820014s

• [SLOW TEST:38.351 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:08:04.672: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Mar 24 03:08:04.800: INFO: Waiting up to 5m0s for pod "var-expansion-ddf3a633-470f-4d28-b7e1-c663797e281d" in namespace "var-expansion-5483" to be "success or failure"
Mar 24 03:08:04.802: INFO: Pod "var-expansion-ddf3a633-470f-4d28-b7e1-c663797e281d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.717176ms
Mar 24 03:08:06.804: INFO: Pod "var-expansion-ddf3a633-470f-4d28-b7e1-c663797e281d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004389294s
STEP: Saw pod success
Mar 24 03:08:06.805: INFO: Pod "var-expansion-ddf3a633-470f-4d28-b7e1-c663797e281d" satisfied condition "success or failure"
Mar 24 03:08:06.807: INFO: Trying to get logs from node kind-worker2 pod var-expansion-ddf3a633-470f-4d28-b7e1-c663797e281d container dapi-container: <nil>
STEP: delete the pod
Mar 24 03:08:06.818: INFO: Waiting for pod var-expansion-ddf3a633-470f-4d28-b7e1-c663797e281d to disappear
Mar 24 03:08:06.819: INFO: Pod var-expansion-ddf3a633-470f-4d28-b7e1-c663797e281d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:08:06.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5483" for this suite.
Mar 24 03:08:12.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:08:12.884: INFO: namespace var-expansion-5483 deletion completed in 6.062298238s

• [SLOW TEST:8.212 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:08:12.884: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9013
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 24 03:08:13.025: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:08:13.027: INFO: Number of nodes with available pods: 0
Mar 24 03:08:13.027: INFO: Node kind-worker is running more than one daemon pod
Mar 24 03:08:14.030: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:08:14.033: INFO: Number of nodes with available pods: 0
Mar 24 03:08:14.033: INFO: Node kind-worker is running more than one daemon pod
Mar 24 03:08:15.030: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:08:15.033: INFO: Number of nodes with available pods: 1
Mar 24 03:08:15.033: INFO: Node kind-worker is running more than one daemon pod
Mar 24 03:08:16.032: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:08:16.035: INFO: Number of nodes with available pods: 2
Mar 24 03:08:16.035: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 24 03:08:16.047: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:08:16.049: INFO: Number of nodes with available pods: 1
Mar 24 03:08:16.049: INFO: Node kind-worker2 is running more than one daemon pod
Mar 24 03:08:17.053: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:08:17.055: INFO: Number of nodes with available pods: 1
Mar 24 03:08:17.055: INFO: Node kind-worker2 is running more than one daemon pod
Mar 24 03:08:18.053: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:08:18.055: INFO: Number of nodes with available pods: 2
Mar 24 03:08:18.055: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9013, will wait for the garbage collector to delete the pods
Mar 24 03:08:18.114: INFO: Deleting DaemonSet.extensions daemon-set took: 3.743084ms
Mar 24 03:08:18.414: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.288831ms
Mar 24 03:08:21.817: INFO: Number of nodes with available pods: 0
Mar 24 03:08:21.818: INFO: Number of running nodes: 0, number of available pods: 0
Mar 24 03:08:21.821: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9013/daemonsets","resourceVersion":"6564"},"items":null}

Mar 24 03:08:21.823: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9013/pods","resourceVersion":"6564"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:08:21.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9013" for this suite.
Mar 24 03:08:27.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:08:27.893: INFO: namespace daemonsets-9013 deletion completed in 6.061792669s

• [SLOW TEST:15.009 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:08:27.894: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:08:28.017: INFO: Creating deployment "nginx-deployment"
Mar 24 03:08:28.020: INFO: Waiting for observed generation 1
Mar 24 03:08:30.025: INFO: Waiting for all required pods to come up
Mar 24 03:08:30.028: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 24 03:08:32.035: INFO: Waiting for deployment "nginx-deployment" to complete
Mar 24 03:08:32.039: INFO: Updating deployment "nginx-deployment" with a non-existent image
Mar 24 03:08:32.043: INFO: Updating deployment nginx-deployment
Mar 24 03:08:32.043: INFO: Waiting for observed generation 2
Mar 24 03:08:34.047: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 24 03:08:34.049: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 24 03:08:34.051: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar 24 03:08:34.057: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 24 03:08:34.057: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 24 03:08:34.060: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar 24 03:08:34.063: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Mar 24 03:08:34.063: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Mar 24 03:08:34.067: INFO: Updating deployment nginx-deployment
Mar 24 03:08:34.068: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Mar 24 03:08:34.072: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 24 03:08:34.076: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar 24 03:08:34.098: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-3867,SelfLink:/apis/apps/v1/namespaces/deployment-3867/deployments/nginx-deployment,UID:b5918b3d-1fb4-4e41-b7fd-09a65f1ef53c,ResourceVersion:6800,Generation:3,CreationTimestamp:2020-03-24 03:08:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2020-03-24 03:08:32 +0000 UTC 2020-03-24 03:08:28 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2020-03-24 03:08:34 +0000 UTC 2020-03-24 03:08:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Mar 24 03:08:34.108: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-3867,SelfLink:/apis/apps/v1/namespaces/deployment-3867/replicasets/nginx-deployment-55fb7cb77f,UID:5ec4d1e1-e4d0-4c6b-85ed-d7d7f3cea04f,ResourceVersion:6795,Generation:3,CreationTimestamp:2020-03-24 03:08:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment b5918b3d-1fb4-4e41-b7fd-09a65f1ef53c 0xc001f61287 0xc001f61288}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 24 03:08:34.108: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Mar 24 03:08:34.109: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-3867,SelfLink:/apis/apps/v1/namespaces/deployment-3867/replicasets/nginx-deployment-7b8c6f4498,UID:4111b484-c925-427a-be7f-d00ef11d9e9b,ResourceVersion:6793,Generation:3,CreationTimestamp:2020-03-24 03:08:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment b5918b3d-1fb4-4e41-b7fd-09a65f1ef53c 0xc001f61357 0xc001f61358}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Mar 24 03:08:34.138: INFO: Pod "nginx-deployment-55fb7cb77f-2tk6v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2tk6v,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-55fb7cb77f-2tk6v,UID:71557ea9-e36e-40f8-9db2-2e6e6a56a1a6,ResourceVersion:6761,Generation:0,CreationTimestamp:2020-03-24 03:08:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5ec4d1e1-e4d0-4c6b-85ed-d7d7f3cea04f 0xc001f61d10 0xc001f61d11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f61d90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f61db0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.4,PodIP:,StartTime:2020-03-24 03:08:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.138: INFO: Pod "nginx-deployment-55fb7cb77f-6whzt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6whzt,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-55fb7cb77f-6whzt,UID:14ae1e9e-e0f0-4bb2-919d-1a92cbf534b7,ResourceVersion:6810,Generation:0,CreationTimestamp:2020-03-24 03:08:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5ec4d1e1-e4d0-4c6b-85ed-d7d7f3cea04f 0xc001f61e80 0xc001f61e81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f61f00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f61f20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.139: INFO: Pod "nginx-deployment-55fb7cb77f-c6trw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-c6trw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-55fb7cb77f-c6trw,UID:9714020e-49b1-48ff-b364-3f7ce2c13228,ResourceVersion:6751,Generation:0,CreationTimestamp:2020-03-24 03:08:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5ec4d1e1-e4d0-4c6b-85ed-d7d7f3cea04f 0xc001f61fa0 0xc001f61fa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f46040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f46060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.4,PodIP:,StartTime:2020-03-24 03:08:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.139: INFO: Pod "nginx-deployment-55fb7cb77f-cstm6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cstm6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-55fb7cb77f-cstm6,UID:6fa8af1b-f627-429f-93be-4c284c7c9947,ResourceVersion:6789,Generation:0,CreationTimestamp:2020-03-24 03:08:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5ec4d1e1-e4d0-4c6b-85ed-d7d7f3cea04f 0xc001f461d0 0xc001f461d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f462d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f462f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.2,PodIP:10.244.1.42,StartTime:2020-03-24 03:08:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/nginx:404": failed to resolve reference "docker.io/library/nginx:404": docker.io/library/nginx:404: not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.139: INFO: Pod "nginx-deployment-55fb7cb77f-dgzs7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-dgzs7,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-55fb7cb77f-dgzs7,UID:56f8347f-efe2-432d-bc3c-fb2455314333,ResourceVersion:6777,Generation:0,CreationTimestamp:2020-03-24 03:08:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5ec4d1e1-e4d0-4c6b-85ed-d7d7f3cea04f 0xc001f46420 0xc001f46421}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f464a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f464c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.2,PodIP:,StartTime:2020-03-24 03:08:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.139: INFO: Pod "nginx-deployment-55fb7cb77f-j5rwv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-j5rwv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-55fb7cb77f-j5rwv,UID:10dd7de1-1cf6-4c51-9f8e-32164fbc6b0b,ResourceVersion:6823,Generation:0,CreationTimestamp:2020-03-24 03:08:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5ec4d1e1-e4d0-4c6b-85ed-d7d7f3cea04f 0xc001f46660 0xc001f46661}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f46800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f468b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.139: INFO: Pod "nginx-deployment-55fb7cb77f-lpv97" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-lpv97,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-55fb7cb77f-lpv97,UID:4760cfe7-a9fc-4063-8deb-15c398305586,ResourceVersion:6826,Generation:0,CreationTimestamp:2020-03-24 03:08:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5ec4d1e1-e4d0-4c6b-85ed-d7d7f3cea04f 0xc001f46917 0xc001f46918}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f469e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f46a00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.140: INFO: Pod "nginx-deployment-55fb7cb77f-qzd5p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-qzd5p,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-55fb7cb77f-qzd5p,UID:d402167b-f69d-4705-a8ca-53cf87c19e86,ResourceVersion:6774,Generation:0,CreationTimestamp:2020-03-24 03:08:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5ec4d1e1-e4d0-4c6b-85ed-d7d7f3cea04f 0xc001f46a97 0xc001f46a98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f46b10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f46b30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:32 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.2,PodIP:,StartTime:2020-03-24 03:08:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.140: INFO: Pod "nginx-deployment-7b8c6f4498-4nhnz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4nhnz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-4nhnz,UID:d027f367-b24c-4b12-ac65-a11272acec52,ResourceVersion:6821,Generation:0,CreationTimestamp:2020-03-24 03:08:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f46c10 0xc001f46c11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f46c80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f46ca0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.140: INFO: Pod "nginx-deployment-7b8c6f4498-62lpk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-62lpk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-62lpk,UID:6cb0c8ed-2ffa-457b-8607-e281ec3a5a6e,ResourceVersion:6815,Generation:0,CreationTimestamp:2020-03-24 03:08:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f46d20 0xc001f46d21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f46d90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f46db0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:34 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.4,PodIP:,StartTime:2020-03-24 03:08:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.140: INFO: Pod "nginx-deployment-7b8c6f4498-65j4b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-65j4b,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-65j4b,UID:2c13bde9-13ec-405e-9211-727d7093c306,ResourceVersion:6819,Generation:0,CreationTimestamp:2020-03-24 03:08:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f46e70 0xc001f46e71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f46ee0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f46f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.140: INFO: Pod "nginx-deployment-7b8c6f4498-6vr9l" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-6vr9l,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-6vr9l,UID:cf95f981-3a1b-45b9-9b2c-f3fe3d22ba91,ResourceVersion:6724,Generation:0,CreationTimestamp:2020-03-24 03:08:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f46f80 0xc001f46f81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f46ff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f47010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.2,PodIP:10.244.1.41,StartTime:2020-03-24 03:08:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-24 03:08:29 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://2ee20659cece8595b6dc541eb4a65cb6ffaf3715722b940694c0a041437b05e2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.141: INFO: Pod "nginx-deployment-7b8c6f4498-8q4xw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8q4xw,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-8q4xw,UID:0ffaf5a4-31b3-4109-9861-a4aeb643928a,ResourceVersion:6825,Generation:0,CreationTimestamp:2020-03-24 03:08:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f470e0 0xc001f470e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f47150} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f47170}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.141: INFO: Pod "nginx-deployment-7b8c6f4498-998dz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-998dz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-998dz,UID:571196d2-9547-4151-85bb-feaa26eb979a,ResourceVersion:6716,Generation:0,CreationTimestamp:2020-03-24 03:08:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f471f0 0xc001f471f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f47260} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f47280}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.4,PodIP:10.244.2.43,StartTime:2020-03-24 03:08:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-24 03:08:29 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://f13ae0ff86c38b87d7cd12c9eab0ef8d359f380e2053a75059c1c261e79e9938}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.141: INFO: Pod "nginx-deployment-7b8c6f4498-crt7z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-crt7z,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-crt7z,UID:36213edc-6d77-40fb-b538-4aed25e7fa86,ResourceVersion:6818,Generation:0,CreationTimestamp:2020-03-24 03:08:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f47350 0xc001f47351}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f473c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f473e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.141: INFO: Pod "nginx-deployment-7b8c6f4498-gg8jk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gg8jk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-gg8jk,UID:73efd20a-6623-4db6-978c-d035fd235e81,ResourceVersion:6816,Generation:0,CreationTimestamp:2020-03-24 03:08:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f47460 0xc001f47461}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f474c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f474e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.141: INFO: Pod "nginx-deployment-7b8c6f4498-hcchf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hcchf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-hcchf,UID:5cba8bbb-adea-454c-a6b4-25a31e93811f,ResourceVersion:6712,Generation:0,CreationTimestamp:2020-03-24 03:08:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f47547 0xc001f47548}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f475c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f475e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.4,PodIP:10.244.2.42,StartTime:2020-03-24 03:08:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-24 03:08:29 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://2e37ecbb3de66f04faaf9ad00100d61f13413f384caa78c36c217728e9053696}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.142: INFO: Pod "nginx-deployment-7b8c6f4498-hhsth" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hhsth,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-hhsth,UID:6fb45245-70ae-4fc4-92b1-c5d62b4a879b,ResourceVersion:6695,Generation:0,CreationTimestamp:2020-03-24 03:08:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f476b0 0xc001f476b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f47720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f47740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.2,PodIP:10.244.1.39,StartTime:2020-03-24 03:08:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-24 03:08:29 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://d79413b32acbbb355b020b388c9384806d0926addda50a41f1fdd5704e685d7b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.142: INFO: Pod "nginx-deployment-7b8c6f4498-jdxnd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jdxnd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-jdxnd,UID:e2d74f43-f9e5-44fe-b106-e90e9d467974,ResourceVersion:6698,Generation:0,CreationTimestamp:2020-03-24 03:08:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f47810 0xc001f47811}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f47880} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f478a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.2,PodIP:10.244.1.37,StartTime:2020-03-24 03:08:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-24 03:08:29 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://ecfa4608ea62789c3526c79f3e940bbfe4d0b1d26c32ce6c980c4b1c3abdeaa2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.142: INFO: Pod "nginx-deployment-7b8c6f4498-kq7t8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kq7t8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-kq7t8,UID:c5070383-f8f3-4147-9181-c8649b1ca95d,ResourceVersion:6707,Generation:0,CreationTimestamp:2020-03-24 03:08:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f47970 0xc001f47971}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f479e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f47a00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.4,PodIP:10.244.2.40,StartTime:2020-03-24 03:08:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-24 03:08:29 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://0147cd42e2df11f0b006f0df93311154501de3112aa1a056e8f1853edab1cc43}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.142: INFO: Pod "nginx-deployment-7b8c6f4498-krs9n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-krs9n,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-krs9n,UID:d861c21e-56f6-48fe-8953-f710215cced0,ResourceVersion:6820,Generation:0,CreationTimestamp:2020-03-24 03:08:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f47ad0 0xc001f47ad1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f47b30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f47b50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.142: INFO: Pod "nginx-deployment-7b8c6f4498-m88sv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-m88sv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-m88sv,UID:2605f473-f9a2-421b-9522-c8b5a5aed445,ResourceVersion:6822,Generation:0,CreationTimestamp:2020-03-24 03:08:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f47bb7 0xc001f47bb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f47c20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f47c40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.143: INFO: Pod "nginx-deployment-7b8c6f4498-qrd7m" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qrd7m,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-qrd7m,UID:d60633b8-3485-4045-be74-9fb8dcb9e61d,ResourceVersion:6675,Generation:0,CreationTimestamp:2020-03-24 03:08:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f47ca7 0xc001f47ca8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f47d20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f47d40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.4,PodIP:10.244.2.39,StartTime:2020-03-24 03:08:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-24 03:08:29 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://087dd96a8a8f7dc0a7b5f4218d8c7d6edfe5e5ea50a5bf8492267829aff54133}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.143: INFO: Pod "nginx-deployment-7b8c6f4498-sp74l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-sp74l,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-sp74l,UID:625f83ec-e21f-4058-94fe-a5b9475cc9a7,ResourceVersion:6824,Generation:0,CreationTimestamp:2020-03-24 03:08:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f47e10 0xc001f47e11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f47e70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f47e90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.143: INFO: Pod "nginx-deployment-7b8c6f4498-tmgdx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tmgdx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-tmgdx,UID:7ad7f8ed-9205-4c3f-9b16-31651a545cad,ResourceVersion:6808,Generation:0,CreationTimestamp:2020-03-24 03:08:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc001f47ef7 0xc001f47ef8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f47f70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f47f90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.143: INFO: Pod "nginx-deployment-7b8c6f4498-w485j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-w485j,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-w485j,UID:30f13a2f-fe1d-42d8-b5e5-4471142ec896,ResourceVersion:6817,Generation:0,CreationTimestamp:2020-03-24 03:08:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc002dca010 0xc002dca011}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dca070} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dca090}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.143: INFO: Pod "nginx-deployment-7b8c6f4498-xbwtn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xbwtn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-xbwtn,UID:94abe7e6-9a71-4b5b-9efa-23420e17a02c,ResourceVersion:6701,Generation:0,CreationTimestamp:2020-03-24 03:08:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc002dca0f7 0xc002dca0f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dca170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dca190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:28 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.2,PodIP:10.244.1.38,StartTime:2020-03-24 03:08:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-24 03:08:29 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://9a5d3a715a531a62ea711aa5336af18861f1c4a5c10f8843ead555481282cf77}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:08:34.143: INFO: Pod "nginx-deployment-7b8c6f4498-z56qs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-z56qs,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3867,SelfLink:/api/v1/namespaces/deployment-3867/pods/nginx-deployment-7b8c6f4498-z56qs,UID:3d54c5e1-c24d-448c-9fcb-c32cfb6a2f0d,ResourceVersion:6813,Generation:0,CreationTimestamp:2020-03-24 03:08:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4111b484-c925-427a-be7f-d00ef11d9e9b 0xc002dca260 0xc002dca261}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n26rj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n26rj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n26rj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dca2d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dca2f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:34 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.2,PodIP:,StartTime:2020-03-24 03:08:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:08:34.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3867" for this suite.
Mar 24 03:08:40.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:08:40.304: INFO: namespace deployment-3867 deletion completed in 6.137432467s

• [SLOW TEST:12.411 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:08:40.304: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1564
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
STEP: creating the pod
Mar 24 03:08:40.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 create -f - --namespace=kubectl-1564'
Mar 24 03:08:40.977: INFO: stderr: ""
Mar 24 03:08:40.977: INFO: stdout: "pod/pause created\n"
Mar 24 03:08:40.977: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 24 03:08:40.977: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1564" to be "running and ready"
Mar 24 03:08:40.980: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.981323ms
Mar 24 03:08:42.988: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010781355s
Mar 24 03:08:44.991: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.013178174s
Mar 24 03:08:44.991: INFO: Pod "pause" satisfied condition "running and ready"
Mar 24 03:08:44.991: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 24 03:08:44.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 label pods pause testing-label=testing-label-value --namespace=kubectl-1564'
Mar 24 03:08:45.072: INFO: stderr: ""
Mar 24 03:08:45.072: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 24 03:08:45.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pod pause -L testing-label --namespace=kubectl-1564'
Mar 24 03:08:45.146: INFO: stderr: ""
Mar 24 03:08:45.146: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 24 03:08:45.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 label pods pause testing-label- --namespace=kubectl-1564'
Mar 24 03:08:45.225: INFO: stderr: ""
Mar 24 03:08:45.225: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 24 03:08:45.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pod pause -L testing-label --namespace=kubectl-1564'
Mar 24 03:08:45.303: INFO: stderr: ""
Mar 24 03:08:45.303: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1217
STEP: using delete to clean up resources
Mar 24 03:08:45.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete --grace-period=0 --force -f - --namespace=kubectl-1564'
Mar 24 03:08:45.386: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:08:45.386: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 24 03:08:45.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get rc,svc -l name=pause --no-headers --namespace=kubectl-1564'
Mar 24 03:08:45.487: INFO: stderr: "No resources found.\n"
Mar 24 03:08:45.487: INFO: stdout: ""
Mar 24 03:08:45.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -l name=pause --namespace=kubectl-1564 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 24 03:08:45.582: INFO: stderr: ""
Mar 24 03:08:45.582: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:08:45.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1564" for this suite.
Mar 24 03:08:51.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:08:51.648: INFO: namespace kubectl-1564 deletion completed in 6.063839188s

• [SLOW TEST:11.344 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:08:51.648: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4542
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:08:51.773: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 24 03:08:51.777: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 24 03:08:56.781: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 24 03:08:56.781: INFO: Creating deployment "test-rolling-update-deployment"
Mar 24 03:08:56.784: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 24 03:08:56.789: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 24 03:08:58.794: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 24 03:08:58.796: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar 24 03:08:58.801: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-4542,SelfLink:/apis/apps/v1/namespaces/deployment-4542/deployments/test-rolling-update-deployment,UID:8a7d2764-064e-4273-8289-7b49d0132f55,ResourceVersion:7179,Generation:1,CreationTimestamp:2020-03-24 03:08:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-03-24 03:08:56 +0000 UTC 2020-03-24 03:08:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-03-24 03:08:58 +0000 UTC 2020-03-24 03:08:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar 24 03:08:58.804: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-4542,SelfLink:/apis/apps/v1/namespaces/deployment-4542/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:2de9dafc-fba8-479a-b333-2acc12d9912c,ResourceVersion:7168,Generation:1,CreationTimestamp:2020-03-24 03:08:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 8a7d2764-064e-4273-8289-7b49d0132f55 0xc002da4097 0xc002da4098}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar 24 03:08:58.804: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 24 03:08:58.804: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-4542,SelfLink:/apis/apps/v1/namespaces/deployment-4542/replicasets/test-rolling-update-controller,UID:cdfddb12-6102-42b8-bf20-f102e0ac5ed0,ResourceVersion:7178,Generation:2,CreationTimestamp:2020-03-24 03:08:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 8a7d2764-064e-4273-8289-7b49d0132f55 0xc0025ddfbf 0xc0025ddfd0}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 24 03:08:58.806: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-rqrh8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-rqrh8,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-4542,SelfLink:/api/v1/namespaces/deployment-4542/pods/test-rolling-update-deployment-79f6b9d75c-rqrh8,UID:197fa6db-0639-4742-a33a-fc9cf120c181,ResourceVersion:7167,Generation:0,CreationTimestamp:2020-03-24 03:08:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 2de9dafc-fba8-479a-b333-2acc12d9912c 0xc0016a3f37 0xc0016a3f38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7rvjc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7rvjc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-7rvjc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016a3fb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016a3fd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:08:56 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.4,PodIP:10.244.2.57,StartTime:2020-03-24 03:08:56 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-03-24 03:08:57 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://fe8e7582b50331b198f14d280962e8dfab41e23177841129e262c51361e344b9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:08:58.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4542" for this suite.
Mar 24 03:09:04.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:09:04.870: INFO: namespace deployment-4542 deletion completed in 6.062228416s

• [SLOW TEST:13.222 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:09:04.871: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3884
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-e581965d-5966-406f-b555-34fbf2bbbe76
STEP: Creating secret with name s-test-opt-upd-f502a09b-72e5-45b8-8a7e-99b8512b2d65
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e581965d-5966-406f-b555-34fbf2bbbe76
STEP: Updating secret s-test-opt-upd-f502a09b-72e5-45b8-8a7e-99b8512b2d65
STEP: Creating secret with name s-test-opt-create-da04fce4-7e6c-4dd4-9b1e-01bd3f28bb78
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:09:09.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3884" for this suite.
Mar 24 03:09:31.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:09:31.123: INFO: namespace projected-3884 deletion completed in 22.068945452s

• [SLOW TEST:26.252 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:09:31.124: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1559.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1559.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1559.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1559.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1559.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1559.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 03:09:35.281: INFO: DNS probes using dns-1559/dns-test-726e8295-d4af-4c59-999b-6272286e2484 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:09:35.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1559" for this suite.
Mar 24 03:09:41.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:09:41.354: INFO: namespace dns-1559 deletion completed in 6.064375549s

• [SLOW TEST:10.231 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:09:41.355: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3690
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-f1c84f63-3f19-4c23-8657-306eae2f9293
STEP: Creating a pod to test consume configMaps
Mar 24 03:09:41.486: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f8b4df41-1f92-4d3d-9884-21231406b712" in namespace "projected-3690" to be "success or failure"
Mar 24 03:09:41.488: INFO: Pod "pod-projected-configmaps-f8b4df41-1f92-4d3d-9884-21231406b712": Phase="Pending", Reason="", readiness=false. Elapsed: 1.881106ms
Mar 24 03:09:43.491: INFO: Pod "pod-projected-configmaps-f8b4df41-1f92-4d3d-9884-21231406b712": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004509755s
STEP: Saw pod success
Mar 24 03:09:43.491: INFO: Pod "pod-projected-configmaps-f8b4df41-1f92-4d3d-9884-21231406b712" satisfied condition "success or failure"
Mar 24 03:09:43.493: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-f8b4df41-1f92-4d3d-9884-21231406b712 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 03:09:43.505: INFO: Waiting for pod pod-projected-configmaps-f8b4df41-1f92-4d3d-9884-21231406b712 to disappear
Mar 24 03:09:43.507: INFO: Pod pod-projected-configmaps-f8b4df41-1f92-4d3d-9884-21231406b712 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:09:43.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3690" for this suite.
Mar 24 03:09:49.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:09:49.576: INFO: namespace projected-3690 deletion completed in 6.066460014s

• [SLOW TEST:8.221 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:09:49.576: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1719
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5270
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8794
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:10:03.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1719" for this suite.
Mar 24 03:10:09.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:10:10.035: INFO: namespace namespaces-1719 deletion completed in 6.067169425s
STEP: Destroying namespace "nsdeletetest-5270" for this suite.
Mar 24 03:10:10.037: INFO: Namespace nsdeletetest-5270 was already deleted
STEP: Destroying namespace "nsdeletetest-8794" for this suite.
Mar 24 03:10:16.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:10:16.102: INFO: namespace nsdeletetest-8794 deletion completed in 6.064619477s

• [SLOW TEST:26.526 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:10:16.102: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4801
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 24 03:10:16.241: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-4801,SelfLink:/api/v1/namespaces/watch-4801/configmaps/e2e-watch-test-resource-version,UID:10809c9b-48c4-48c4-9115-14fe14813506,ResourceVersion:7489,Generation:0,CreationTimestamp:2020-03-24 03:10:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 24 03:10:16.241: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-4801,SelfLink:/api/v1/namespaces/watch-4801/configmaps/e2e-watch-test-resource-version,UID:10809c9b-48c4-48c4-9115-14fe14813506,ResourceVersion:7490,Generation:0,CreationTimestamp:2020-03-24 03:10:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:10:16.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4801" for this suite.
Mar 24 03:10:22.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:10:22.307: INFO: namespace watch-4801 deletion completed in 6.063879189s

• [SLOW TEST:6.205 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:10:22.308: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6765
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:10:40.444: INFO: Container started at 2020-03-24 03:10:23 +0000 UTC, pod became ready at 2020-03-24 03:10:38 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:10:40.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6765" for this suite.
Mar 24 03:11:02.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:11:02.509: INFO: namespace container-probe-6765 deletion completed in 22.062658491s

• [SLOW TEST:40.202 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:11:02.510: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7839
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 24 03:11:02.639: INFO: Waiting up to 5m0s for pod "pod-61eee730-6147-4069-96c8-c5044531cb7a" in namespace "emptydir-7839" to be "success or failure"
Mar 24 03:11:02.641: INFO: Pod "pod-61eee730-6147-4069-96c8-c5044531cb7a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.825259ms
Mar 24 03:11:04.644: INFO: Pod "pod-61eee730-6147-4069-96c8-c5044531cb7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004424454s
STEP: Saw pod success
Mar 24 03:11:04.644: INFO: Pod "pod-61eee730-6147-4069-96c8-c5044531cb7a" satisfied condition "success or failure"
Mar 24 03:11:04.646: INFO: Trying to get logs from node kind-worker pod pod-61eee730-6147-4069-96c8-c5044531cb7a container test-container: <nil>
STEP: delete the pod
Mar 24 03:11:04.659: INFO: Waiting for pod pod-61eee730-6147-4069-96c8-c5044531cb7a to disappear
Mar 24 03:11:04.661: INFO: Pod pod-61eee730-6147-4069-96c8-c5044531cb7a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:11:04.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7839" for this suite.
Mar 24 03:11:10.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:11:10.726: INFO: namespace emptydir-7839 deletion completed in 6.062829473s

• [SLOW TEST:8.217 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:11:10.727: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-506
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar 24 03:11:12.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec pod-sharedvolume-d84e955f-8780-48df-a224-d98b1e0935bc -c busybox-main-container --namespace=emptydir-506 -- cat /usr/share/volumeshare/shareddata.txt'
Mar 24 03:11:13.049: INFO: stderr: ""
Mar 24 03:11:13.049: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:11:13.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-506" for this suite.
Mar 24 03:11:19.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:11:19.117: INFO: namespace emptydir-506 deletion completed in 6.064842036s

• [SLOW TEST:8.391 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:11:19.118: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-4387
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4387 to expose endpoints map[]
Mar 24 03:11:19.254: INFO: Get endpoints failed (2.610089ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar 24 03:11:20.257: INFO: successfully validated that service endpoint-test2 in namespace services-4387 exposes endpoints map[] (1.00555562s elapsed)
STEP: Creating pod pod1 in namespace services-4387
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4387 to expose endpoints map[pod1:[80]]
Mar 24 03:11:22.278: INFO: successfully validated that service endpoint-test2 in namespace services-4387 exposes endpoints map[pod1:[80]] (2.01640446s elapsed)
STEP: Creating pod pod2 in namespace services-4387
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4387 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 24 03:11:24.301: INFO: successfully validated that service endpoint-test2 in namespace services-4387 exposes endpoints map[pod1:[80] pod2:[80]] (2.019676825s elapsed)
STEP: Deleting pod pod1 in namespace services-4387
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4387 to expose endpoints map[pod2:[80]]
Mar 24 03:11:25.315: INFO: successfully validated that service endpoint-test2 in namespace services-4387 exposes endpoints map[pod2:[80]] (1.010984147s elapsed)
STEP: Deleting pod pod2 in namespace services-4387
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4387 to expose endpoints map[]
Mar 24 03:11:26.324: INFO: successfully validated that service endpoint-test2 in namespace services-4387 exposes endpoints map[] (1.005893674s elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:11:26.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4387" for this suite.
Mar 24 03:11:48.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:11:48.401: INFO: namespace services-4387 deletion completed in 22.063940677s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:29.284 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:11:48.402: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3485
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar 24 03:11:48.526: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:11:52.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3485" for this suite.
Mar 24 03:11:58.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:11:58.754: INFO: namespace init-container-3485 deletion completed in 6.062919806s

• [SLOW TEST:10.352 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:11:58.754: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2784
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:11:58.884: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c01bd10-578e-4724-9313-40004963e850" in namespace "projected-2784" to be "success or failure"
Mar 24 03:11:58.886: INFO: Pod "downwardapi-volume-8c01bd10-578e-4724-9313-40004963e850": Phase="Pending", Reason="", readiness=false. Elapsed: 1.820635ms
Mar 24 03:12:00.889: INFO: Pod "downwardapi-volume-8c01bd10-578e-4724-9313-40004963e850": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004639227s
STEP: Saw pod success
Mar 24 03:12:00.889: INFO: Pod "downwardapi-volume-8c01bd10-578e-4724-9313-40004963e850" satisfied condition "success or failure"
Mar 24 03:12:00.891: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-8c01bd10-578e-4724-9313-40004963e850 container client-container: <nil>
STEP: delete the pod
Mar 24 03:12:00.903: INFO: Waiting for pod downwardapi-volume-8c01bd10-578e-4724-9313-40004963e850 to disappear
Mar 24 03:12:00.905: INFO: Pod downwardapi-volume-8c01bd10-578e-4724-9313-40004963e850 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:12:00.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2784" for this suite.
Mar 24 03:12:06.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:12:06.968: INFO: namespace projected-2784 deletion completed in 6.061169202s

• [SLOW TEST:8.214 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:12:06.968: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9262
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar 24 03:12:07.097: INFO: Waiting up to 5m0s for pod "downward-api-4bc38640-d12e-43e8-b24e-a5c6026ba84e" in namespace "downward-api-9262" to be "success or failure"
Mar 24 03:12:07.099: INFO: Pod "downward-api-4bc38640-d12e-43e8-b24e-a5c6026ba84e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.197414ms
Mar 24 03:12:09.102: INFO: Pod "downward-api-4bc38640-d12e-43e8-b24e-a5c6026ba84e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005084406s
STEP: Saw pod success
Mar 24 03:12:09.102: INFO: Pod "downward-api-4bc38640-d12e-43e8-b24e-a5c6026ba84e" satisfied condition "success or failure"
Mar 24 03:12:09.104: INFO: Trying to get logs from node kind-worker pod downward-api-4bc38640-d12e-43e8-b24e-a5c6026ba84e container dapi-container: <nil>
STEP: delete the pod
Mar 24 03:12:09.114: INFO: Waiting for pod downward-api-4bc38640-d12e-43e8-b24e-a5c6026ba84e to disappear
Mar 24 03:12:09.116: INFO: Pod downward-api-4bc38640-d12e-43e8-b24e-a5c6026ba84e no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:12:09.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9262" for this suite.
Mar 24 03:12:15.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:12:15.182: INFO: namespace downward-api-9262 deletion completed in 6.06347833s

• [SLOW TEST:8.213 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:12:15.182: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Mar 24 03:12:15.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 create -f - --namespace=kubectl-1844'
Mar 24 03:12:15.525: INFO: stderr: ""
Mar 24 03:12:15.525: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 24 03:12:16.529: INFO: Selector matched 1 pods for map[app:redis]
Mar 24 03:12:16.529: INFO: Found 0 / 1
Mar 24 03:12:17.529: INFO: Selector matched 1 pods for map[app:redis]
Mar 24 03:12:17.529: INFO: Found 1 / 1
Mar 24 03:12:17.529: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 24 03:12:17.532: INFO: Selector matched 1 pods for map[app:redis]
Mar 24 03:12:17.532: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 24 03:12:17.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 patch pod redis-master-l7l92 --namespace=kubectl-1844 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 24 03:12:17.612: INFO: stderr: ""
Mar 24 03:12:17.612: INFO: stdout: "pod/redis-master-l7l92 patched\n"
STEP: checking annotations
Mar 24 03:12:17.614: INFO: Selector matched 1 pods for map[app:redis]
Mar 24 03:12:17.614: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:12:17.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1844" for this suite.
Mar 24 03:12:39.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:12:39.678: INFO: namespace kubectl-1844 deletion completed in 22.062092073s

• [SLOW TEST:24.496 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:12:39.679: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8474
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-676ac81c-10ba-4dcf-8c3a-01f3da84b2e4
STEP: Creating a pod to test consume secrets
Mar 24 03:12:39.810: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b98b4498-ed95-465e-a013-1dd6bc82c857" in namespace "projected-8474" to be "success or failure"
Mar 24 03:12:39.813: INFO: Pod "pod-projected-secrets-b98b4498-ed95-465e-a013-1dd6bc82c857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.750178ms
Mar 24 03:12:41.815: INFO: Pod "pod-projected-secrets-b98b4498-ed95-465e-a013-1dd6bc82c857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005071262s
STEP: Saw pod success
Mar 24 03:12:41.815: INFO: Pod "pod-projected-secrets-b98b4498-ed95-465e-a013-1dd6bc82c857" satisfied condition "success or failure"
Mar 24 03:12:41.817: INFO: Trying to get logs from node kind-worker pod pod-projected-secrets-b98b4498-ed95-465e-a013-1dd6bc82c857 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:12:41.828: INFO: Waiting for pod pod-projected-secrets-b98b4498-ed95-465e-a013-1dd6bc82c857 to disappear
Mar 24 03:12:41.832: INFO: Pod pod-projected-secrets-b98b4498-ed95-465e-a013-1dd6bc82c857 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:12:41.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8474" for this suite.
Mar 24 03:12:47.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:12:47.895: INFO: namespace projected-8474 deletion completed in 6.060331368s

• [SLOW TEST:8.215 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:12:47.895: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-82
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar 24 03:12:48.018: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:12:52.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-82" for this suite.
Mar 24 03:13:14.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:13:14.379: INFO: namespace init-container-82 deletion completed in 22.066818847s

• [SLOW TEST:26.484 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:13:14.379: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-653
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:13:16.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-653" for this suite.
Mar 24 03:14:06.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:14:06.592: INFO: namespace kubelet-test-653 deletion completed in 50.068351045s

• [SLOW TEST:52.213 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:14:06.592: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1850
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:14:08.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1850" for this suite.
Mar 24 03:14:46.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:14:46.808: INFO: namespace kubelet-test-1850 deletion completed in 38.070356699s

• [SLOW TEST:40.216 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:14:46.809: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5196
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Mar 24 03:14:46.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 api-versions'
Mar 24 03:14:47.013: INFO: stderr: ""
Mar 24 03:14:47.013: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:14:47.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5196" for this suite.
Mar 24 03:14:53.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:14:53.079: INFO: namespace kubectl-5196 deletion completed in 6.063325217s

• [SLOW TEST:6.271 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:14:53.080: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar 24 03:14:59.224: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:14:59.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0324 03:14:59.224936      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-9145" for this suite.
Mar 24 03:15:05.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:15:05.297: INFO: namespace gc-9145 deletion completed in 6.070365668s

• [SLOW TEST:12.218 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:15:05.298: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-207
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-7cf2cbb7-6d22-4fa3-ab67-97cfec7afe5a
STEP: Creating a pod to test consume secrets
Mar 24 03:15:05.433: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-58235281-6243-4257-835c-5b868856ae59" in namespace "projected-207" to be "success or failure"
Mar 24 03:15:05.435: INFO: Pod "pod-projected-secrets-58235281-6243-4257-835c-5b868856ae59": Phase="Pending", Reason="", readiness=false. Elapsed: 1.884081ms
Mar 24 03:15:07.438: INFO: Pod "pod-projected-secrets-58235281-6243-4257-835c-5b868856ae59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004952516s
STEP: Saw pod success
Mar 24 03:15:07.438: INFO: Pod "pod-projected-secrets-58235281-6243-4257-835c-5b868856ae59" satisfied condition "success or failure"
Mar 24 03:15:07.440: INFO: Trying to get logs from node kind-worker pod pod-projected-secrets-58235281-6243-4257-835c-5b868856ae59 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:15:07.452: INFO: Waiting for pod pod-projected-secrets-58235281-6243-4257-835c-5b868856ae59 to disappear
Mar 24 03:15:07.454: INFO: Pod pod-projected-secrets-58235281-6243-4257-835c-5b868856ae59 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:15:07.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-207" for this suite.
Mar 24 03:15:13.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:15:13.517: INFO: namespace projected-207 deletion completed in 6.061176414s

• [SLOW TEST:8.220 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:15:13.517: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8664
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 24 03:15:13.646: INFO: Waiting up to 5m0s for pod "pod-b1255860-e611-40bd-aaa1-da71be3a56cf" in namespace "emptydir-8664" to be "success or failure"
Mar 24 03:15:13.648: INFO: Pod "pod-b1255860-e611-40bd-aaa1-da71be3a56cf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.816654ms
Mar 24 03:15:15.651: INFO: Pod "pod-b1255860-e611-40bd-aaa1-da71be3a56cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004875617s
STEP: Saw pod success
Mar 24 03:15:15.651: INFO: Pod "pod-b1255860-e611-40bd-aaa1-da71be3a56cf" satisfied condition "success or failure"
Mar 24 03:15:15.654: INFO: Trying to get logs from node kind-worker2 pod pod-b1255860-e611-40bd-aaa1-da71be3a56cf container test-container: <nil>
STEP: delete the pod
Mar 24 03:15:15.665: INFO: Waiting for pod pod-b1255860-e611-40bd-aaa1-da71be3a56cf to disappear
Mar 24 03:15:15.667: INFO: Pod pod-b1255860-e611-40bd-aaa1-da71be3a56cf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:15:15.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8664" for this suite.
Mar 24 03:15:21.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:15:21.733: INFO: namespace emptydir-8664 deletion completed in 6.063576141s

• [SLOW TEST:8.216 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:15:21.734: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2825
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 24 03:15:24.875: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:15:24.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2825" for this suite.
Mar 24 03:15:30.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:15:30.947: INFO: namespace container-runtime-2825 deletion completed in 6.062037095s

• [SLOW TEST:9.214 seconds]
[k8s.io] Container Runtime
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:15:30.948: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1192
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Mar 24 03:15:31.077: INFO: Waiting up to 5m0s for pod "client-containers-536ca336-4b40-4713-bb78-89cf9b959f0a" in namespace "containers-1192" to be "success or failure"
Mar 24 03:15:31.080: INFO: Pod "client-containers-536ca336-4b40-4713-bb78-89cf9b959f0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.265647ms
Mar 24 03:15:33.083: INFO: Pod "client-containers-536ca336-4b40-4713-bb78-89cf9b959f0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005258265s
STEP: Saw pod success
Mar 24 03:15:33.083: INFO: Pod "client-containers-536ca336-4b40-4713-bb78-89cf9b959f0a" satisfied condition "success or failure"
Mar 24 03:15:33.085: INFO: Trying to get logs from node kind-worker2 pod client-containers-536ca336-4b40-4713-bb78-89cf9b959f0a container test-container: <nil>
STEP: delete the pod
Mar 24 03:15:33.095: INFO: Waiting for pod client-containers-536ca336-4b40-4713-bb78-89cf9b959f0a to disappear
Mar 24 03:15:33.097: INFO: Pod client-containers-536ca336-4b40-4713-bb78-89cf9b959f0a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:15:33.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1192" for this suite.
Mar 24 03:15:39.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:15:39.160: INFO: namespace containers-1192 deletion completed in 6.06132007s

• [SLOW TEST:8.212 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:15:39.160: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3877
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-17a27067-3cc0-4672-8637-3ac096df4d9a
STEP: Creating a pod to test consume configMaps
Mar 24 03:15:39.290: INFO: Waiting up to 5m0s for pod "pod-configmaps-c5ff30e0-3e59-4beb-8c67-9e14941c7892" in namespace "configmap-3877" to be "success or failure"
Mar 24 03:15:39.292: INFO: Pod "pod-configmaps-c5ff30e0-3e59-4beb-8c67-9e14941c7892": Phase="Pending", Reason="", readiness=false. Elapsed: 1.762482ms
Mar 24 03:15:41.295: INFO: Pod "pod-configmaps-c5ff30e0-3e59-4beb-8c67-9e14941c7892": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004681629s
STEP: Saw pod success
Mar 24 03:15:41.295: INFO: Pod "pod-configmaps-c5ff30e0-3e59-4beb-8c67-9e14941c7892" satisfied condition "success or failure"
Mar 24 03:15:41.297: INFO: Trying to get logs from node kind-worker pod pod-configmaps-c5ff30e0-3e59-4beb-8c67-9e14941c7892 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 03:15:41.308: INFO: Waiting for pod pod-configmaps-c5ff30e0-3e59-4beb-8c67-9e14941c7892 to disappear
Mar 24 03:15:41.309: INFO: Pod pod-configmaps-c5ff30e0-3e59-4beb-8c67-9e14941c7892 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:15:41.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3877" for this suite.
Mar 24 03:15:47.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:15:47.372: INFO: namespace configmap-3877 deletion completed in 6.060148615s

• [SLOW TEST:8.212 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:15:47.372: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-396
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-176f91aa-b6c1-4d66-8bd9-fcab1fd498b6
STEP: Creating a pod to test consume secrets
Mar 24 03:15:47.503: INFO: Waiting up to 5m0s for pod "pod-secrets-7f523fcf-1749-4a1c-bd7c-c21f17fc3279" in namespace "secrets-396" to be "success or failure"
Mar 24 03:15:47.504: INFO: Pod "pod-secrets-7f523fcf-1749-4a1c-bd7c-c21f17fc3279": Phase="Pending", Reason="", readiness=false. Elapsed: 1.808157ms
Mar 24 03:15:49.507: INFO: Pod "pod-secrets-7f523fcf-1749-4a1c-bd7c-c21f17fc3279": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004543336s
Mar 24 03:15:51.510: INFO: Pod "pod-secrets-7f523fcf-1749-4a1c-bd7c-c21f17fc3279": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007741662s
STEP: Saw pod success
Mar 24 03:15:51.510: INFO: Pod "pod-secrets-7f523fcf-1749-4a1c-bd7c-c21f17fc3279" satisfied condition "success or failure"
Mar 24 03:15:51.512: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-7f523fcf-1749-4a1c-bd7c-c21f17fc3279 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:15:51.522: INFO: Waiting for pod pod-secrets-7f523fcf-1749-4a1c-bd7c-c21f17fc3279 to disappear
Mar 24 03:15:51.524: INFO: Pod pod-secrets-7f523fcf-1749-4a1c-bd7c-c21f17fc3279 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:15:51.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-396" for this suite.
Mar 24 03:15:57.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:15:57.588: INFO: namespace secrets-396 deletion completed in 6.061926771s

• [SLOW TEST:10.216 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:15:57.588: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-xh6j
STEP: Creating a pod to test atomic-volume-subpath
Mar 24 03:15:57.723: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xh6j" in namespace "subpath-6622" to be "success or failure"
Mar 24 03:15:57.725: INFO: Pod "pod-subpath-test-configmap-xh6j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109784ms
Mar 24 03:15:59.728: INFO: Pod "pod-subpath-test-configmap-xh6j": Phase="Running", Reason="", readiness=true. Elapsed: 2.005017809s
Mar 24 03:16:01.731: INFO: Pod "pod-subpath-test-configmap-xh6j": Phase="Running", Reason="", readiness=true. Elapsed: 4.008035935s
Mar 24 03:16:03.733: INFO: Pod "pod-subpath-test-configmap-xh6j": Phase="Running", Reason="", readiness=true. Elapsed: 6.010744182s
Mar 24 03:16:05.736: INFO: Pod "pod-subpath-test-configmap-xh6j": Phase="Running", Reason="", readiness=true. Elapsed: 8.013509353s
Mar 24 03:16:07.739: INFO: Pod "pod-subpath-test-configmap-xh6j": Phase="Running", Reason="", readiness=true. Elapsed: 10.016240794s
Mar 24 03:16:09.742: INFO: Pod "pod-subpath-test-configmap-xh6j": Phase="Running", Reason="", readiness=true. Elapsed: 12.019033102s
Mar 24 03:16:11.745: INFO: Pod "pod-subpath-test-configmap-xh6j": Phase="Running", Reason="", readiness=true. Elapsed: 14.022423406s
Mar 24 03:16:13.748: INFO: Pod "pod-subpath-test-configmap-xh6j": Phase="Running", Reason="", readiness=true. Elapsed: 16.025488725s
Mar 24 03:16:15.752: INFO: Pod "pod-subpath-test-configmap-xh6j": Phase="Running", Reason="", readiness=true. Elapsed: 18.028986529s
Mar 24 03:16:17.754: INFO: Pod "pod-subpath-test-configmap-xh6j": Phase="Running", Reason="", readiness=true. Elapsed: 20.031719389s
Mar 24 03:16:19.757: INFO: Pod "pod-subpath-test-configmap-xh6j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.034576216s
STEP: Saw pod success
Mar 24 03:16:19.757: INFO: Pod "pod-subpath-test-configmap-xh6j" satisfied condition "success or failure"
Mar 24 03:16:19.759: INFO: Trying to get logs from node kind-worker pod pod-subpath-test-configmap-xh6j container test-container-subpath-configmap-xh6j: <nil>
STEP: delete the pod
Mar 24 03:16:19.770: INFO: Waiting for pod pod-subpath-test-configmap-xh6j to disappear
Mar 24 03:16:19.772: INFO: Pod pod-subpath-test-configmap-xh6j no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xh6j
Mar 24 03:16:19.772: INFO: Deleting pod "pod-subpath-test-configmap-xh6j" in namespace "subpath-6622"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:16:19.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6622" for this suite.
Mar 24 03:16:25.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:16:25.840: INFO: namespace subpath-6622 deletion completed in 6.064403873s

• [SLOW TEST:28.252 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:16:25.840: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-9400
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9400 to expose endpoints map[]
Mar 24 03:16:25.978: INFO: successfully validated that service multi-endpoint-test in namespace services-9400 exposes endpoints map[] (5.143176ms elapsed)
STEP: Creating pod pod1 in namespace services-9400
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9400 to expose endpoints map[pod1:[100]]
Mar 24 03:16:27.998: INFO: successfully validated that service multi-endpoint-test in namespace services-9400 exposes endpoints map[pod1:[100]] (2.015720649s elapsed)
STEP: Creating pod pod2 in namespace services-9400
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9400 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 24 03:16:31.030: INFO: successfully validated that service multi-endpoint-test in namespace services-9400 exposes endpoints map[pod1:[100] pod2:[101]] (3.029016386s elapsed)
STEP: Deleting pod pod1 in namespace services-9400
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9400 to expose endpoints map[pod2:[101]]
Mar 24 03:16:31.042: INFO: successfully validated that service multi-endpoint-test in namespace services-9400 exposes endpoints map[pod2:[101]] (9.01546ms elapsed)
STEP: Deleting pod pod2 in namespace services-9400
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9400 to expose endpoints map[]
Mar 24 03:16:32.050: INFO: successfully validated that service multi-endpoint-test in namespace services-9400 exposes endpoints map[] (1.004876159s elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:16:32.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9400" for this suite.
Mar 24 03:16:54.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:16:54.135: INFO: namespace services-9400 deletion completed in 22.064500311s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:28.294 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:16:54.135: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7069
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:16:54.264: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6fe5d98-ad6e-4128-96e0-9d2669d0c160" in namespace "projected-7069" to be "success or failure"
Mar 24 03:16:54.266: INFO: Pod "downwardapi-volume-b6fe5d98-ad6e-4128-96e0-9d2669d0c160": Phase="Pending", Reason="", readiness=false. Elapsed: 1.997512ms
Mar 24 03:16:56.269: INFO: Pod "downwardapi-volume-b6fe5d98-ad6e-4128-96e0-9d2669d0c160": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004918822s
STEP: Saw pod success
Mar 24 03:16:56.269: INFO: Pod "downwardapi-volume-b6fe5d98-ad6e-4128-96e0-9d2669d0c160" satisfied condition "success or failure"
Mar 24 03:16:56.271: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-b6fe5d98-ad6e-4128-96e0-9d2669d0c160 container client-container: <nil>
STEP: delete the pod
Mar 24 03:16:56.284: INFO: Waiting for pod downwardapi-volume-b6fe5d98-ad6e-4128-96e0-9d2669d0c160 to disappear
Mar 24 03:16:56.286: INFO: Pod downwardapi-volume-b6fe5d98-ad6e-4128-96e0-9d2669d0c160 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:16:56.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7069" for this suite.
Mar 24 03:17:02.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:17:02.350: INFO: namespace projected-7069 deletion completed in 6.062020088s

• [SLOW TEST:8.215 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:17:02.350: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2213
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Mar 24 03:17:02.478: INFO: Waiting up to 5m0s for pod "var-expansion-ee61bf30-327a-431a-9a01-d975f96a6df9" in namespace "var-expansion-2213" to be "success or failure"
Mar 24 03:17:02.480: INFO: Pod "var-expansion-ee61bf30-327a-431a-9a01-d975f96a6df9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.860721ms
Mar 24 03:17:04.483: INFO: Pod "var-expansion-ee61bf30-327a-431a-9a01-d975f96a6df9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004777926s
STEP: Saw pod success
Mar 24 03:17:04.483: INFO: Pod "var-expansion-ee61bf30-327a-431a-9a01-d975f96a6df9" satisfied condition "success or failure"
Mar 24 03:17:04.485: INFO: Trying to get logs from node kind-worker pod var-expansion-ee61bf30-327a-431a-9a01-d975f96a6df9 container dapi-container: <nil>
STEP: delete the pod
Mar 24 03:17:04.496: INFO: Waiting for pod var-expansion-ee61bf30-327a-431a-9a01-d975f96a6df9 to disappear
Mar 24 03:17:04.498: INFO: Pod var-expansion-ee61bf30-327a-431a-9a01-d975f96a6df9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:17:04.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2213" for this suite.
Mar 24 03:17:10.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:17:10.562: INFO: namespace var-expansion-2213 deletion completed in 6.06138739s

• [SLOW TEST:8.212 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:17:10.562: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4284
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar 24 03:17:14.700: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-9454bb58-26d4-4260-aa37-c615ff7fe905,GenerateName:,Namespace:events-4284,SelfLink:/api/v1/namespaces/events-4284/pods/send-events-9454bb58-26d4-4260-aa37-c615ff7fe905,UID:9a8cb4df-0b56-48c5-b4c3-3fa638bae2b5,ResourceVersion:9026,Generation:0,CreationTimestamp:2020-03-24 03:17:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 686687341,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wvbzs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wvbzs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-wvbzs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025dd590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025dd5b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:17:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:17:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:17:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:17:10 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.2,PodIP:10.244.1.73,StartTime:2020-03-24 03:17:10 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2020-03-24 03:17:11 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 containerd://cd9e87303c9a155932e07a0fb72af1032f274c9ebd0ecb3019940fd0be6b5b51}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Mar 24 03:17:16.704: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar 24 03:17:18.707: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:17:18.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4284" for this suite.
Mar 24 03:18:02.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:18:02.778: INFO: namespace events-4284 deletion completed in 44.063774189s

• [SLOW TEST:52.216 seconds]
[k8s.io] [sig-node] Events
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:18:02.778: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8802
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Mar 24 03:18:02.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 create -f - --namespace=kubectl-8802'
Mar 24 03:18:03.122: INFO: stderr: ""
Mar 24 03:18:03.122: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 24 03:18:03.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8802'
Mar 24 03:18:03.203: INFO: stderr: ""
Mar 24 03:18:03.203: INFO: stdout: "update-demo-nautilus-dtjcn update-demo-nautilus-j62kb "
Mar 24 03:18:03.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-dtjcn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8802'
Mar 24 03:18:03.283: INFO: stderr: ""
Mar 24 03:18:03.283: INFO: stdout: ""
Mar 24 03:18:03.283: INFO: update-demo-nautilus-dtjcn is created but not running
Mar 24 03:18:08.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8802'
Mar 24 03:18:08.365: INFO: stderr: ""
Mar 24 03:18:08.365: INFO: stdout: "update-demo-nautilus-dtjcn update-demo-nautilus-j62kb "
Mar 24 03:18:08.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-dtjcn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8802'
Mar 24 03:18:08.445: INFO: stderr: ""
Mar 24 03:18:08.445: INFO: stdout: "true"
Mar 24 03:18:08.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-dtjcn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8802'
Mar 24 03:18:08.523: INFO: stderr: ""
Mar 24 03:18:08.523: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 03:18:08.523: INFO: validating pod update-demo-nautilus-dtjcn
Mar 24 03:18:08.526: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 03:18:08.527: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 03:18:08.527: INFO: update-demo-nautilus-dtjcn is verified up and running
Mar 24 03:18:08.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-j62kb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8802'
Mar 24 03:18:08.605: INFO: stderr: ""
Mar 24 03:18:08.605: INFO: stdout: "true"
Mar 24 03:18:08.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-j62kb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8802'
Mar 24 03:18:08.680: INFO: stderr: ""
Mar 24 03:18:08.680: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 03:18:08.680: INFO: validating pod update-demo-nautilus-j62kb
Mar 24 03:18:08.683: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 03:18:08.683: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 03:18:08.683: INFO: update-demo-nautilus-j62kb is verified up and running
STEP: scaling down the replication controller
Mar 24 03:18:08.685: INFO: scanned /root for discovery docs: <nil>
Mar 24 03:18:08.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-8802'
Mar 24 03:18:09.781: INFO: stderr: ""
Mar 24 03:18:09.781: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 24 03:18:09.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8802'
Mar 24 03:18:09.859: INFO: stderr: ""
Mar 24 03:18:09.859: INFO: stdout: "update-demo-nautilus-dtjcn update-demo-nautilus-j62kb "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 24 03:18:14.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8802'
Mar 24 03:18:14.938: INFO: stderr: ""
Mar 24 03:18:14.938: INFO: stdout: "update-demo-nautilus-j62kb "
Mar 24 03:18:14.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-j62kb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8802'
Mar 24 03:18:15.017: INFO: stderr: ""
Mar 24 03:18:15.017: INFO: stdout: "true"
Mar 24 03:18:15.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-j62kb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8802'
Mar 24 03:18:15.092: INFO: stderr: ""
Mar 24 03:18:15.092: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 03:18:15.092: INFO: validating pod update-demo-nautilus-j62kb
Mar 24 03:18:15.095: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 03:18:15.095: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 03:18:15.095: INFO: update-demo-nautilus-j62kb is verified up and running
STEP: scaling up the replication controller
Mar 24 03:18:15.097: INFO: scanned /root for discovery docs: <nil>
Mar 24 03:18:15.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-8802'
Mar 24 03:18:16.192: INFO: stderr: ""
Mar 24 03:18:16.192: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 24 03:18:16.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8802'
Mar 24 03:18:16.278: INFO: stderr: ""
Mar 24 03:18:16.278: INFO: stdout: "update-demo-nautilus-j62kb update-demo-nautilus-lcb8m "
Mar 24 03:18:16.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-j62kb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8802'
Mar 24 03:18:16.369: INFO: stderr: ""
Mar 24 03:18:16.369: INFO: stdout: "true"
Mar 24 03:18:16.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-j62kb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8802'
Mar 24 03:18:16.446: INFO: stderr: ""
Mar 24 03:18:16.446: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 03:18:16.446: INFO: validating pod update-demo-nautilus-j62kb
Mar 24 03:18:16.448: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 03:18:16.448: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 03:18:16.448: INFO: update-demo-nautilus-j62kb is verified up and running
Mar 24 03:18:16.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-lcb8m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8802'
Mar 24 03:18:16.525: INFO: stderr: ""
Mar 24 03:18:16.525: INFO: stdout: ""
Mar 24 03:18:16.526: INFO: update-demo-nautilus-lcb8m is created but not running
Mar 24 03:18:21.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8802'
Mar 24 03:18:21.607: INFO: stderr: ""
Mar 24 03:18:21.607: INFO: stdout: "update-demo-nautilus-j62kb update-demo-nautilus-lcb8m "
Mar 24 03:18:21.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-j62kb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8802'
Mar 24 03:18:21.684: INFO: stderr: ""
Mar 24 03:18:21.684: INFO: stdout: "true"
Mar 24 03:18:21.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-j62kb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8802'
Mar 24 03:18:21.762: INFO: stderr: ""
Mar 24 03:18:21.762: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 03:18:21.762: INFO: validating pod update-demo-nautilus-j62kb
Mar 24 03:18:21.766: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 03:18:21.766: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 03:18:21.766: INFO: update-demo-nautilus-j62kb is verified up and running
Mar 24 03:18:21.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-lcb8m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8802'
Mar 24 03:18:21.846: INFO: stderr: ""
Mar 24 03:18:21.846: INFO: stdout: "true"
Mar 24 03:18:21.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-lcb8m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8802'
Mar 24 03:18:21.938: INFO: stderr: ""
Mar 24 03:18:21.938: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 03:18:21.938: INFO: validating pod update-demo-nautilus-lcb8m
Mar 24 03:18:21.941: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 03:18:21.941: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 03:18:21.941: INFO: update-demo-nautilus-lcb8m is verified up and running
STEP: using delete to clean up resources
Mar 24 03:18:21.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete --grace-period=0 --force -f - --namespace=kubectl-8802'
Mar 24 03:18:22.027: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:18:22.027: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 24 03:18:22.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8802'
Mar 24 03:18:22.114: INFO: stderr: "No resources found.\n"
Mar 24 03:18:22.114: INFO: stdout: ""
Mar 24 03:18:22.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -l name=update-demo --namespace=kubectl-8802 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 24 03:18:22.195: INFO: stderr: ""
Mar 24 03:18:22.195: INFO: stdout: "update-demo-nautilus-j62kb\nupdate-demo-nautilus-lcb8m\n"
Mar 24 03:18:22.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8802'
Mar 24 03:18:22.781: INFO: stderr: "No resources found.\n"
Mar 24 03:18:22.781: INFO: stdout: ""
Mar 24 03:18:22.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -l name=update-demo --namespace=kubectl-8802 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 24 03:18:22.876: INFO: stderr: ""
Mar 24 03:18:22.876: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:18:22.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8802" for this suite.
Mar 24 03:18:28.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:18:28.940: INFO: namespace kubectl-8802 deletion completed in 6.061391894s

• [SLOW TEST:26.162 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:18:28.940: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6656
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-423
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-648
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:18:35.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6656" for this suite.
Mar 24 03:18:41.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:18:41.397: INFO: namespace namespaces-6656 deletion completed in 6.065558841s
STEP: Destroying namespace "nsdeletetest-423" for this suite.
Mar 24 03:18:41.398: INFO: Namespace nsdeletetest-423 was already deleted
STEP: Destroying namespace "nsdeletetest-648" for this suite.
Mar 24 03:18:47.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:18:47.460: INFO: namespace nsdeletetest-648 deletion completed in 6.061433496s

• [SLOW TEST:18.519 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:18:47.460: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4807
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:18:47.589: INFO: Waiting up to 5m0s for pod "downwardapi-volume-883de55d-16aa-4c9f-84ab-2498ec3e1f7c" in namespace "downward-api-4807" to be "success or failure"
Mar 24 03:18:47.592: INFO: Pod "downwardapi-volume-883de55d-16aa-4c9f-84ab-2498ec3e1f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.450061ms
Mar 24 03:18:49.595: INFO: Pod "downwardapi-volume-883de55d-16aa-4c9f-84ab-2498ec3e1f7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005521062s
STEP: Saw pod success
Mar 24 03:18:49.595: INFO: Pod "downwardapi-volume-883de55d-16aa-4c9f-84ab-2498ec3e1f7c" satisfied condition "success or failure"
Mar 24 03:18:49.597: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-883de55d-16aa-4c9f-84ab-2498ec3e1f7c container client-container: <nil>
STEP: delete the pod
Mar 24 03:18:49.609: INFO: Waiting for pod downwardapi-volume-883de55d-16aa-4c9f-84ab-2498ec3e1f7c to disappear
Mar 24 03:18:49.611: INFO: Pod downwardapi-volume-883de55d-16aa-4c9f-84ab-2498ec3e1f7c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:18:49.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4807" for this suite.
Mar 24 03:18:55.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:18:55.677: INFO: namespace downward-api-4807 deletion completed in 6.063740188s

• [SLOW TEST:8.217 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:18:55.677: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:18:55.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-849cce38-c2a9-46c2-a49f-1a61bce01ae1" in namespace "downward-api-5144" to be "success or failure"
Mar 24 03:18:55.809: INFO: Pod "downwardapi-volume-849cce38-c2a9-46c2-a49f-1a61bce01ae1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07196ms
Mar 24 03:18:57.812: INFO: Pod "downwardapi-volume-849cce38-c2a9-46c2-a49f-1a61bce01ae1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005450313s
STEP: Saw pod success
Mar 24 03:18:57.813: INFO: Pod "downwardapi-volume-849cce38-c2a9-46c2-a49f-1a61bce01ae1" satisfied condition "success or failure"
Mar 24 03:18:57.815: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-849cce38-c2a9-46c2-a49f-1a61bce01ae1 container client-container: <nil>
STEP: delete the pod
Mar 24 03:18:57.826: INFO: Waiting for pod downwardapi-volume-849cce38-c2a9-46c2-a49f-1a61bce01ae1 to disappear
Mar 24 03:18:57.828: INFO: Pod downwardapi-volume-849cce38-c2a9-46c2-a49f-1a61bce01ae1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:18:57.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5144" for this suite.
Mar 24 03:19:03.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:19:03.892: INFO: namespace downward-api-5144 deletion completed in 6.062037421s

• [SLOW TEST:8.214 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:19:03.892: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-862
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-6thl
STEP: Creating a pod to test atomic-volume-subpath
Mar 24 03:19:04.026: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6thl" in namespace "subpath-862" to be "success or failure"
Mar 24 03:19:04.028: INFO: Pod "pod-subpath-test-projected-6thl": Phase="Pending", Reason="", readiness=false. Elapsed: 1.935512ms
Mar 24 03:19:06.031: INFO: Pod "pod-subpath-test-projected-6thl": Phase="Running", Reason="", readiness=true. Elapsed: 2.00513306s
Mar 24 03:19:08.034: INFO: Pod "pod-subpath-test-projected-6thl": Phase="Running", Reason="", readiness=true. Elapsed: 4.00813035s
Mar 24 03:19:10.037: INFO: Pod "pod-subpath-test-projected-6thl": Phase="Running", Reason="", readiness=true. Elapsed: 6.010978492s
Mar 24 03:19:12.040: INFO: Pod "pod-subpath-test-projected-6thl": Phase="Running", Reason="", readiness=true. Elapsed: 8.014082652s
Mar 24 03:19:14.043: INFO: Pod "pod-subpath-test-projected-6thl": Phase="Running", Reason="", readiness=true. Elapsed: 10.016792194s
Mar 24 03:19:16.046: INFO: Pod "pod-subpath-test-projected-6thl": Phase="Running", Reason="", readiness=true. Elapsed: 12.019628398s
Mar 24 03:19:18.049: INFO: Pod "pod-subpath-test-projected-6thl": Phase="Running", Reason="", readiness=true. Elapsed: 14.022764425s
Mar 24 03:19:20.052: INFO: Pod "pod-subpath-test-projected-6thl": Phase="Running", Reason="", readiness=true. Elapsed: 16.025906696s
Mar 24 03:19:22.055: INFO: Pod "pod-subpath-test-projected-6thl": Phase="Running", Reason="", readiness=true. Elapsed: 18.028706699s
Mar 24 03:19:24.058: INFO: Pod "pod-subpath-test-projected-6thl": Phase="Running", Reason="", readiness=true. Elapsed: 20.031504934s
Mar 24 03:19:26.061: INFO: Pod "pod-subpath-test-projected-6thl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.034510784s
STEP: Saw pod success
Mar 24 03:19:26.061: INFO: Pod "pod-subpath-test-projected-6thl" satisfied condition "success or failure"
Mar 24 03:19:26.063: INFO: Trying to get logs from node kind-worker2 pod pod-subpath-test-projected-6thl container test-container-subpath-projected-6thl: <nil>
STEP: delete the pod
Mar 24 03:19:26.076: INFO: Waiting for pod pod-subpath-test-projected-6thl to disappear
Mar 24 03:19:26.079: INFO: Pod pod-subpath-test-projected-6thl no longer exists
STEP: Deleting pod pod-subpath-test-projected-6thl
Mar 24 03:19:26.079: INFO: Deleting pod "pod-subpath-test-projected-6thl" in namespace "subpath-862"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:19:26.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-862" for this suite.
Mar 24 03:19:32.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:19:32.145: INFO: namespace subpath-862 deletion completed in 6.061712023s

• [SLOW TEST:28.252 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:19:32.145: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4825
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:19:32.274: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e60d091e-f78d-4804-aa30-238c04c5f421" in namespace "projected-4825" to be "success or failure"
Mar 24 03:19:32.276: INFO: Pod "downwardapi-volume-e60d091e-f78d-4804-aa30-238c04c5f421": Phase="Pending", Reason="", readiness=false. Elapsed: 2.401254ms
Mar 24 03:19:34.279: INFO: Pod "downwardapi-volume-e60d091e-f78d-4804-aa30-238c04c5f421": Phase="Running", Reason="", readiness=true. Elapsed: 2.00548465s
Mar 24 03:19:36.282: INFO: Pod "downwardapi-volume-e60d091e-f78d-4804-aa30-238c04c5f421": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008462899s
STEP: Saw pod success
Mar 24 03:19:36.282: INFO: Pod "downwardapi-volume-e60d091e-f78d-4804-aa30-238c04c5f421" satisfied condition "success or failure"
Mar 24 03:19:36.285: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-e60d091e-f78d-4804-aa30-238c04c5f421 container client-container: <nil>
STEP: delete the pod
Mar 24 03:19:36.295: INFO: Waiting for pod downwardapi-volume-e60d091e-f78d-4804-aa30-238c04c5f421 to disappear
Mar 24 03:19:36.297: INFO: Pod downwardapi-volume-e60d091e-f78d-4804-aa30-238c04c5f421 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:19:36.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4825" for this suite.
Mar 24 03:19:42.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:19:42.363: INFO: namespace projected-4825 deletion completed in 6.062526453s

• [SLOW TEST:10.218 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:19:42.363: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8608
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar 24 03:19:42.492: INFO: Waiting up to 5m0s for pod "downward-api-955ab210-126d-4981-a10a-93dd03b691db" in namespace "downward-api-8608" to be "success or failure"
Mar 24 03:19:42.494: INFO: Pod "downward-api-955ab210-126d-4981-a10a-93dd03b691db": Phase="Pending", Reason="", readiness=false. Elapsed: 1.859437ms
Mar 24 03:19:44.497: INFO: Pod "downward-api-955ab210-126d-4981-a10a-93dd03b691db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004601193s
STEP: Saw pod success
Mar 24 03:19:44.497: INFO: Pod "downward-api-955ab210-126d-4981-a10a-93dd03b691db" satisfied condition "success or failure"
Mar 24 03:19:44.499: INFO: Trying to get logs from node kind-worker2 pod downward-api-955ab210-126d-4981-a10a-93dd03b691db container dapi-container: <nil>
STEP: delete the pod
Mar 24 03:19:44.510: INFO: Waiting for pod downward-api-955ab210-126d-4981-a10a-93dd03b691db to disappear
Mar 24 03:19:44.512: INFO: Pod downward-api-955ab210-126d-4981-a10a-93dd03b691db no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:19:44.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8608" for this suite.
Mar 24 03:19:50.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:19:50.575: INFO: namespace downward-api-8608 deletion completed in 6.060202234s

• [SLOW TEST:8.211 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:19:50.575: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 24 03:19:50.932: INFO: Pod name wrapped-volume-race-a8204b67-08a5-464e-a92f-326727d9c2b5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a8204b67-08a5-464e-a92f-326727d9c2b5 in namespace emptydir-wrapper-9652, will wait for the garbage collector to delete the pods
Mar 24 03:20:05.048: INFO: Deleting ReplicationController wrapped-volume-race-a8204b67-08a5-464e-a92f-326727d9c2b5 took: 4.339914ms
Mar 24 03:20:05.348: INFO: Terminating ReplicationController wrapped-volume-race-a8204b67-08a5-464e-a92f-326727d9c2b5 pods took: 300.267778ms
STEP: Creating RC which spawns configmap-volume pods
Mar 24 03:20:41.761: INFO: Pod name wrapped-volume-race-73d0e9c4-f403-422c-ad3e-baa4ac50a89f: Found 0 pods out of 5
Mar 24 03:20:46.765: INFO: Pod name wrapped-volume-race-73d0e9c4-f403-422c-ad3e-baa4ac50a89f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-73d0e9c4-f403-422c-ad3e-baa4ac50a89f in namespace emptydir-wrapper-9652, will wait for the garbage collector to delete the pods
Mar 24 03:20:56.836: INFO: Deleting ReplicationController wrapped-volume-race-73d0e9c4-f403-422c-ad3e-baa4ac50a89f took: 4.643118ms
Mar 24 03:20:57.137: INFO: Terminating ReplicationController wrapped-volume-race-73d0e9c4-f403-422c-ad3e-baa4ac50a89f pods took: 300.295227ms
STEP: Creating RC which spawns configmap-volume pods
Mar 24 03:21:41.749: INFO: Pod name wrapped-volume-race-f6685422-a1e8-45df-8eb4-6f238b30d391: Found 0 pods out of 5
Mar 24 03:21:46.753: INFO: Pod name wrapped-volume-race-f6685422-a1e8-45df-8eb4-6f238b30d391: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f6685422-a1e8-45df-8eb4-6f238b30d391 in namespace emptydir-wrapper-9652, will wait for the garbage collector to delete the pods
Mar 24 03:21:56.826: INFO: Deleting ReplicationController wrapped-volume-race-f6685422-a1e8-45df-8eb4-6f238b30d391 took: 4.909518ms
Mar 24 03:21:57.126: INFO: Terminating ReplicationController wrapped-volume-race-f6685422-a1e8-45df-8eb4-6f238b30d391 pods took: 300.332389ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:22:31.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9652" for this suite.
Mar 24 03:22:37.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:22:37.965: INFO: namespace emptydir-wrapper-9652 deletion completed in 6.072300601s

• [SLOW TEST:167.390 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:22:37.966: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-714
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-714/configmap-test-e0fe48e5-3998-47c7-9402-15e1e5a3c65a
STEP: Creating a pod to test consume configMaps
Mar 24 03:22:38.102: INFO: Waiting up to 5m0s for pod "pod-configmaps-07311151-fa44-43e5-aa37-8f84ebe64ea3" in namespace "configmap-714" to be "success or failure"
Mar 24 03:22:38.104: INFO: Pod "pod-configmaps-07311151-fa44-43e5-aa37-8f84ebe64ea3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.996299ms
Mar 24 03:22:40.107: INFO: Pod "pod-configmaps-07311151-fa44-43e5-aa37-8f84ebe64ea3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004875663s
STEP: Saw pod success
Mar 24 03:22:40.107: INFO: Pod "pod-configmaps-07311151-fa44-43e5-aa37-8f84ebe64ea3" satisfied condition "success or failure"
Mar 24 03:22:40.109: INFO: Trying to get logs from node kind-worker pod pod-configmaps-07311151-fa44-43e5-aa37-8f84ebe64ea3 container env-test: <nil>
STEP: delete the pod
Mar 24 03:22:40.123: INFO: Waiting for pod pod-configmaps-07311151-fa44-43e5-aa37-8f84ebe64ea3 to disappear
Mar 24 03:22:40.125: INFO: Pod pod-configmaps-07311151-fa44-43e5-aa37-8f84ebe64ea3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:22:40.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-714" for this suite.
Mar 24 03:22:46.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:22:46.190: INFO: namespace configmap-714 deletion completed in 6.062279569s

• [SLOW TEST:8.224 seconds]
[sig-node] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:22:46.190: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9140
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9140
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Mar 24 03:22:46.324: INFO: Found 0 stateful pods, waiting for 3
Mar 24 03:22:56.327: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 03:22:56.327: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 03:22:56.327: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar 24 03:22:56.348: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 24 03:23:06.374: INFO: Updating stateful set ss2
Mar 24 03:23:06.377: INFO: Waiting for Pod statefulset-9140/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Mar 24 03:23:16.425: INFO: Found 2 stateful pods, waiting for 3
Mar 24 03:23:26.428: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 03:23:26.428: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 03:23:26.428: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 24 03:23:26.448: INFO: Updating stateful set ss2
Mar 24 03:23:26.452: INFO: Waiting for Pod statefulset-9140/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Mar 24 03:23:36.473: INFO: Updating stateful set ss2
Mar 24 03:23:36.477: INFO: Waiting for StatefulSet statefulset-9140/ss2 to complete update
Mar 24 03:23:36.477: INFO: Waiting for Pod statefulset-9140/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar 24 03:23:46.482: INFO: Deleting all statefulset in ns statefulset-9140
Mar 24 03:23:46.484: INFO: Scaling statefulset ss2 to 0
Mar 24 03:24:06.496: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 03:24:06.498: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:24:06.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9140" for this suite.
Mar 24 03:24:12.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:24:12.571: INFO: namespace statefulset-9140 deletion completed in 6.062081303s

• [SLOW TEST:86.381 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:24:12.571: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-828
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
[It] should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 24 03:24:12.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-828'
Mar 24 03:24:12.949: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 24 03:24:12.949: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Mar 24 03:24:12.954: INFO: scanned /root for discovery docs: <nil>
Mar 24 03:24:12.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-828'
Mar 24 03:24:28.699: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 24 03:24:28.699: INFO: stdout: "Created e2e-test-nginx-rc-7fcae73310694322f02dea1ff0a59206\nScaling up e2e-test-nginx-rc-7fcae73310694322f02dea1ff0a59206 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-7fcae73310694322f02dea1ff0a59206 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-7fcae73310694322f02dea1ff0a59206 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Mar 24 03:24:28.699: INFO: stdout: "Created e2e-test-nginx-rc-7fcae73310694322f02dea1ff0a59206\nScaling up e2e-test-nginx-rc-7fcae73310694322f02dea1ff0a59206 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-7fcae73310694322f02dea1ff0a59206 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-7fcae73310694322f02dea1ff0a59206 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Mar 24 03:24:28.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-828'
Mar 24 03:24:28.775: INFO: stderr: ""
Mar 24 03:24:28.775: INFO: stdout: "e2e-test-nginx-rc-7fcae73310694322f02dea1ff0a59206-nmgdc "
Mar 24 03:24:28.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods e2e-test-nginx-rc-7fcae73310694322f02dea1ff0a59206-nmgdc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-828'
Mar 24 03:24:28.846: INFO: stderr: ""
Mar 24 03:24:28.846: INFO: stdout: "true"
Mar 24 03:24:28.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods e2e-test-nginx-rc-7fcae73310694322f02dea1ff0a59206-nmgdc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-828'
Mar 24 03:24:28.918: INFO: stderr: ""
Mar 24 03:24:28.918: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Mar 24 03:24:28.918: INFO: e2e-test-nginx-rc-7fcae73310694322f02dea1ff0a59206-nmgdc is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1522
Mar 24 03:24:28.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete rc e2e-test-nginx-rc --namespace=kubectl-828'
Mar 24 03:24:28.997: INFO: stderr: ""
Mar 24 03:24:28.997: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:24:28.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-828" for this suite.
Mar 24 03:24:51.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:24:51.064: INFO: namespace kubectl-828 deletion completed in 22.06413818s

• [SLOW TEST:38.493 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:24:51.064: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4433
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:24:51.198: INFO: (0) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.651835ms)
Mar 24 03:24:51.201: INFO: (1) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.614492ms)
Mar 24 03:24:51.203: INFO: (2) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.258287ms)
Mar 24 03:24:51.205: INFO: (3) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.302977ms)
Mar 24 03:24:51.208: INFO: (4) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.251505ms)
Mar 24 03:24:51.210: INFO: (5) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.123209ms)
Mar 24 03:24:51.212: INFO: (6) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.297676ms)
Mar 24 03:24:51.214: INFO: (7) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.054575ms)
Mar 24 03:24:51.216: INFO: (8) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.897907ms)
Mar 24 03:24:51.218: INFO: (9) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.892841ms)
Mar 24 03:24:51.220: INFO: (10) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.274545ms)
Mar 24 03:24:51.222: INFO: (11) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.110965ms)
Mar 24 03:24:51.225: INFO: (12) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.43393ms)
Mar 24 03:24:51.228: INFO: (13) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.568768ms)
Mar 24 03:24:51.230: INFO: (14) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.100073ms)
Mar 24 03:24:51.232: INFO: (15) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.031516ms)
Mar 24 03:24:51.234: INFO: (16) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.008496ms)
Mar 24 03:24:51.236: INFO: (17) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.308732ms)
Mar 24 03:24:51.238: INFO: (18) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.111019ms)
Mar 24 03:24:51.240: INFO: (19) /api/v1/nodes/kind-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.278328ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:24:51.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4433" for this suite.
Mar 24 03:24:57.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:24:57.303: INFO: namespace proxy-4433 deletion completed in 6.060913809s

• [SLOW TEST:6.240 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:24:57.304: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9538
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9538
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 24 03:24:57.428: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 24 03:25:13.469: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.104:8080/dial?request=hostName&protocol=udp&host=10.244.1.83&port=8081&tries=1'] Namespace:pod-network-test-9538 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 03:25:13.469: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 03:25:13.566: INFO: Waiting for endpoints: map[]
Mar 24 03:25:13.568: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.104:8080/dial?request=hostName&protocol=udp&host=10.244.2.103&port=8081&tries=1'] Namespace:pod-network-test-9538 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 03:25:13.568: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 03:25:13.673: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:25:13.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9538" for this suite.
Mar 24 03:25:35.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:25:35.738: INFO: namespace pod-network-test-9538 deletion completed in 22.062444786s

• [SLOW TEST:38.434 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:25:35.739: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8760
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8760.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8760.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8760.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8760.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 03:25:45.882: INFO: DNS probes using dns-test-71e7afea-e420-4dbd-be16-becfb1ed4700 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8760.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8760.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8760.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8760.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 03:25:49.910: INFO: File wheezy_udp@dns-test-service-3.dns-8760.svc.cluster.local from pod  dns-8760/dns-test-864c9fbe-0bf5-4e91-8152-229051ee21eb contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:25:49.913: INFO: File jessie_udp@dns-test-service-3.dns-8760.svc.cluster.local from pod  dns-8760/dns-test-864c9fbe-0bf5-4e91-8152-229051ee21eb contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:25:49.913: INFO: Lookups using dns-8760/dns-test-864c9fbe-0bf5-4e91-8152-229051ee21eb failed for: [wheezy_udp@dns-test-service-3.dns-8760.svc.cluster.local jessie_udp@dns-test-service-3.dns-8760.svc.cluster.local]

Mar 24 03:25:54.917: INFO: File wheezy_udp@dns-test-service-3.dns-8760.svc.cluster.local from pod  dns-8760/dns-test-864c9fbe-0bf5-4e91-8152-229051ee21eb contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:25:54.919: INFO: File jessie_udp@dns-test-service-3.dns-8760.svc.cluster.local from pod  dns-8760/dns-test-864c9fbe-0bf5-4e91-8152-229051ee21eb contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:25:54.919: INFO: Lookups using dns-8760/dns-test-864c9fbe-0bf5-4e91-8152-229051ee21eb failed for: [wheezy_udp@dns-test-service-3.dns-8760.svc.cluster.local jessie_udp@dns-test-service-3.dns-8760.svc.cluster.local]

Mar 24 03:25:59.916: INFO: File wheezy_udp@dns-test-service-3.dns-8760.svc.cluster.local from pod  dns-8760/dns-test-864c9fbe-0bf5-4e91-8152-229051ee21eb contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:25:59.919: INFO: File jessie_udp@dns-test-service-3.dns-8760.svc.cluster.local from pod  dns-8760/dns-test-864c9fbe-0bf5-4e91-8152-229051ee21eb contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:25:59.919: INFO: Lookups using dns-8760/dns-test-864c9fbe-0bf5-4e91-8152-229051ee21eb failed for: [wheezy_udp@dns-test-service-3.dns-8760.svc.cluster.local jessie_udp@dns-test-service-3.dns-8760.svc.cluster.local]

Mar 24 03:26:04.916: INFO: File wheezy_udp@dns-test-service-3.dns-8760.svc.cluster.local from pod  dns-8760/dns-test-864c9fbe-0bf5-4e91-8152-229051ee21eb contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:26:04.919: INFO: File jessie_udp@dns-test-service-3.dns-8760.svc.cluster.local from pod  dns-8760/dns-test-864c9fbe-0bf5-4e91-8152-229051ee21eb contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:26:04.919: INFO: Lookups using dns-8760/dns-test-864c9fbe-0bf5-4e91-8152-229051ee21eb failed for: [wheezy_udp@dns-test-service-3.dns-8760.svc.cluster.local jessie_udp@dns-test-service-3.dns-8760.svc.cluster.local]

Mar 24 03:26:09.918: INFO: File jessie_udp@dns-test-service-3.dns-8760.svc.cluster.local from pod  dns-8760/dns-test-864c9fbe-0bf5-4e91-8152-229051ee21eb contains '' instead of 'bar.example.com.'
Mar 24 03:26:09.918: INFO: Lookups using dns-8760/dns-test-864c9fbe-0bf5-4e91-8152-229051ee21eb failed for: [jessie_udp@dns-test-service-3.dns-8760.svc.cluster.local]

Mar 24 03:26:14.918: INFO: DNS probes using dns-test-864c9fbe-0bf5-4e91-8152-229051ee21eb succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8760.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8760.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8760.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8760.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 03:26:18.978: INFO: DNS probes using dns-test-4e632da2-dfa8-400c-8304-5bd347fe0f1b succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:26:18.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8760" for this suite.
Mar 24 03:26:25.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:26:25.067: INFO: namespace dns-8760 deletion completed in 6.064690013s

• [SLOW TEST:49.328 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:26:25.067: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5329
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 24 03:26:27.206: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:26:27.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5329" for this suite.
Mar 24 03:26:33.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:26:33.278: INFO: namespace container-runtime-5329 deletion completed in 6.062909794s

• [SLOW TEST:8.211 seconds]
[k8s.io] Container Runtime
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:26:33.278: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1428
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar 24 03:26:35.917: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1428 pod-service-account-670e0fa2-7690-4254-9ffd-e2b62edbe404 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar 24 03:26:36.119: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1428 pod-service-account-670e0fa2-7690-4254-9ffd-e2b62edbe404 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar 24 03:26:36.309: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1428 pod-service-account-670e0fa2-7690-4254-9ffd-e2b62edbe404 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:26:36.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1428" for this suite.
Mar 24 03:26:42.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:26:42.585: INFO: namespace svcaccounts-1428 deletion completed in 6.062790793s

• [SLOW TEST:9.306 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:26:42.585: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9116
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 24 03:26:42.714: INFO: Waiting up to 5m0s for pod "pod-01eae1fc-5ecb-4342-bbef-260df189bc15" in namespace "emptydir-9116" to be "success or failure"
Mar 24 03:26:42.716: INFO: Pod "pod-01eae1fc-5ecb-4342-bbef-260df189bc15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.540544ms
Mar 24 03:26:44.719: INFO: Pod "pod-01eae1fc-5ecb-4342-bbef-260df189bc15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00560387s
STEP: Saw pod success
Mar 24 03:26:44.719: INFO: Pod "pod-01eae1fc-5ecb-4342-bbef-260df189bc15" satisfied condition "success or failure"
Mar 24 03:26:44.721: INFO: Trying to get logs from node kind-worker pod pod-01eae1fc-5ecb-4342-bbef-260df189bc15 container test-container: <nil>
STEP: delete the pod
Mar 24 03:26:44.731: INFO: Waiting for pod pod-01eae1fc-5ecb-4342-bbef-260df189bc15 to disappear
Mar 24 03:26:44.733: INFO: Pod pod-01eae1fc-5ecb-4342-bbef-260df189bc15 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:26:44.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9116" for this suite.
Mar 24 03:26:50.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:26:50.801: INFO: namespace emptydir-9116 deletion completed in 6.065587588s

• [SLOW TEST:8.216 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:26:50.802: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1861
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-68bceb14-0902-4a00-863b-e5bc0fabfd33
STEP: Creating a pod to test consume secrets
Mar 24 03:26:50.933: INFO: Waiting up to 5m0s for pod "pod-secrets-62036931-c5c7-46cf-9fe3-d442670c0d68" in namespace "secrets-1861" to be "success or failure"
Mar 24 03:26:50.935: INFO: Pod "pod-secrets-62036931-c5c7-46cf-9fe3-d442670c0d68": Phase="Pending", Reason="", readiness=false. Elapsed: 1.952878ms
Mar 24 03:26:52.938: INFO: Pod "pod-secrets-62036931-c5c7-46cf-9fe3-d442670c0d68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005109951s
STEP: Saw pod success
Mar 24 03:26:52.938: INFO: Pod "pod-secrets-62036931-c5c7-46cf-9fe3-d442670c0d68" satisfied condition "success or failure"
Mar 24 03:26:52.940: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-62036931-c5c7-46cf-9fe3-d442670c0d68 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:26:52.951: INFO: Waiting for pod pod-secrets-62036931-c5c7-46cf-9fe3-d442670c0d68 to disappear
Mar 24 03:26:52.953: INFO: Pod pod-secrets-62036931-c5c7-46cf-9fe3-d442670c0d68 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:26:52.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1861" for this suite.
Mar 24 03:26:58.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:26:59.015: INFO: namespace secrets-1861 deletion completed in 6.060561575s

• [SLOW TEST:8.214 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:26:59.016: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4734
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4734
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 24 03:26:59.142: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 24 03:27:21.187: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.88:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4734 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 03:27:21.187: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 03:27:21.284: INFO: Found all expected endpoints: [netserver-0]
Mar 24 03:27:21.286: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.108:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4734 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 03:27:21.286: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 03:27:21.376: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:27:21.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4734" for this suite.
Mar 24 03:27:43.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:27:43.442: INFO: namespace pod-network-test-4734 deletion completed in 22.062196006s

• [SLOW TEST:44.426 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:27:43.442: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8823
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 24 03:27:43.570: INFO: Waiting up to 5m0s for pod "pod-2543f87e-16da-40dd-b889-07603f8df043" in namespace "emptydir-8823" to be "success or failure"
Mar 24 03:27:43.572: INFO: Pod "pod-2543f87e-16da-40dd-b889-07603f8df043": Phase="Pending", Reason="", readiness=false. Elapsed: 1.998667ms
Mar 24 03:27:45.574: INFO: Pod "pod-2543f87e-16da-40dd-b889-07603f8df043": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004848489s
STEP: Saw pod success
Mar 24 03:27:45.574: INFO: Pod "pod-2543f87e-16da-40dd-b889-07603f8df043" satisfied condition "success or failure"
Mar 24 03:27:45.576: INFO: Trying to get logs from node kind-worker pod pod-2543f87e-16da-40dd-b889-07603f8df043 container test-container: <nil>
STEP: delete the pod
Mar 24 03:27:45.588: INFO: Waiting for pod pod-2543f87e-16da-40dd-b889-07603f8df043 to disappear
Mar 24 03:27:45.590: INFO: Pod pod-2543f87e-16da-40dd-b889-07603f8df043 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:27:45.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8823" for this suite.
Mar 24 03:27:51.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:27:51.655: INFO: namespace emptydir-8823 deletion completed in 6.061570877s

• [SLOW TEST:8.214 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:27:51.656: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9328
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-d43a8121-8361-4882-aa80-4d75b71514fb
STEP: Creating a pod to test consume secrets
Mar 24 03:27:51.786: INFO: Waiting up to 5m0s for pod "pod-secrets-90af2b77-3ca3-4155-96b1-67f1326855ff" in namespace "secrets-9328" to be "success or failure"
Mar 24 03:27:51.788: INFO: Pod "pod-secrets-90af2b77-3ca3-4155-96b1-67f1326855ff": Phase="Pending", Reason="", readiness=false. Elapsed: 1.635399ms
Mar 24 03:27:53.791: INFO: Pod "pod-secrets-90af2b77-3ca3-4155-96b1-67f1326855ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004661213s
STEP: Saw pod success
Mar 24 03:27:53.791: INFO: Pod "pod-secrets-90af2b77-3ca3-4155-96b1-67f1326855ff" satisfied condition "success or failure"
Mar 24 03:27:53.793: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-90af2b77-3ca3-4155-96b1-67f1326855ff container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:27:53.803: INFO: Waiting for pod pod-secrets-90af2b77-3ca3-4155-96b1-67f1326855ff to disappear
Mar 24 03:27:53.805: INFO: Pod pod-secrets-90af2b77-3ca3-4155-96b1-67f1326855ff no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:27:53.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9328" for this suite.
Mar 24 03:27:59.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:27:59.870: INFO: namespace secrets-9328 deletion completed in 6.06136713s

• [SLOW TEST:8.214 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:27:59.870: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3954
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Mar 24 03:27:59.994: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-811275467 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:28:00.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3954" for this suite.
Mar 24 03:28:06.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:28:06.144: INFO: namespace kubectl-3954 deletion completed in 6.071900368s

• [SLOW TEST:6.274 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:28:06.144: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3472
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:28:06.277: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4bf3cf62-16a3-4dcb-a90a-e9681ccb0ccb" in namespace "projected-3472" to be "success or failure"
Mar 24 03:28:06.280: INFO: Pod "downwardapi-volume-4bf3cf62-16a3-4dcb-a90a-e9681ccb0ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.126504ms
Mar 24 03:28:08.282: INFO: Pod "downwardapi-volume-4bf3cf62-16a3-4dcb-a90a-e9681ccb0ccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004953805s
STEP: Saw pod success
Mar 24 03:28:08.282: INFO: Pod "downwardapi-volume-4bf3cf62-16a3-4dcb-a90a-e9681ccb0ccb" satisfied condition "success or failure"
Mar 24 03:28:08.285: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-4bf3cf62-16a3-4dcb-a90a-e9681ccb0ccb container client-container: <nil>
STEP: delete the pod
Mar 24 03:28:08.296: INFO: Waiting for pod downwardapi-volume-4bf3cf62-16a3-4dcb-a90a-e9681ccb0ccb to disappear
Mar 24 03:28:08.297: INFO: Pod downwardapi-volume-4bf3cf62-16a3-4dcb-a90a-e9681ccb0ccb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:28:08.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3472" for this suite.
Mar 24 03:28:14.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:28:14.361: INFO: namespace projected-3472 deletion completed in 6.061411737s

• [SLOW TEST:8.218 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:28:14.362: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8912
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-1a502baa-e8bb-4e1c-8516-33a0e07c7680
STEP: Creating a pod to test consume configMaps
Mar 24 03:28:14.492: INFO: Waiting up to 5m0s for pod "pod-configmaps-c70267a2-ea3a-4c7e-b6da-bb8265ac30b4" in namespace "configmap-8912" to be "success or failure"
Mar 24 03:28:14.494: INFO: Pod "pod-configmaps-c70267a2-ea3a-4c7e-b6da-bb8265ac30b4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.680351ms
Mar 24 03:28:16.497: INFO: Pod "pod-configmaps-c70267a2-ea3a-4c7e-b6da-bb8265ac30b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004407999s
Mar 24 03:28:18.499: INFO: Pod "pod-configmaps-c70267a2-ea3a-4c7e-b6da-bb8265ac30b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007099625s
STEP: Saw pod success
Mar 24 03:28:18.499: INFO: Pod "pod-configmaps-c70267a2-ea3a-4c7e-b6da-bb8265ac30b4" satisfied condition "success or failure"
Mar 24 03:28:18.502: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-c70267a2-ea3a-4c7e-b6da-bb8265ac30b4 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 03:28:18.511: INFO: Waiting for pod pod-configmaps-c70267a2-ea3a-4c7e-b6da-bb8265ac30b4 to disappear
Mar 24 03:28:18.514: INFO: Pod pod-configmaps-c70267a2-ea3a-4c7e-b6da-bb8265ac30b4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:28:18.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8912" for this suite.
Mar 24 03:28:24.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:28:24.582: INFO: namespace configmap-8912 deletion completed in 6.066219641s

• [SLOW TEST:10.220 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:28:24.582: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2810
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:28:24.710: INFO: Waiting up to 5m0s for pod "downwardapi-volume-97a73e9f-c3ff-4ffc-965c-64d38c39b2d0" in namespace "downward-api-2810" to be "success or failure"
Mar 24 03:28:24.712: INFO: Pod "downwardapi-volume-97a73e9f-c3ff-4ffc-965c-64d38c39b2d0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.904793ms
Mar 24 03:28:26.714: INFO: Pod "downwardapi-volume-97a73e9f-c3ff-4ffc-965c-64d38c39b2d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004458841s
STEP: Saw pod success
Mar 24 03:28:26.714: INFO: Pod "downwardapi-volume-97a73e9f-c3ff-4ffc-965c-64d38c39b2d0" satisfied condition "success or failure"
Mar 24 03:28:26.716: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-97a73e9f-c3ff-4ffc-965c-64d38c39b2d0 container client-container: <nil>
STEP: delete the pod
Mar 24 03:28:26.726: INFO: Waiting for pod downwardapi-volume-97a73e9f-c3ff-4ffc-965c-64d38c39b2d0 to disappear
Mar 24 03:28:26.728: INFO: Pod downwardapi-volume-97a73e9f-c3ff-4ffc-965c-64d38c39b2d0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:28:26.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2810" for this suite.
Mar 24 03:28:32.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:28:32.794: INFO: namespace downward-api-2810 deletion completed in 6.064261256s

• [SLOW TEST:8.212 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:28:32.794: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar 24 03:28:32.922: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:28:41.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7166" for this suite.
Mar 24 03:28:47.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:28:47.312: INFO: namespace pods-7166 deletion completed in 6.06161335s

• [SLOW TEST:14.518 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:28:47.313: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3143
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-64bc2803-8793-44df-8272-d0d453aff0d2
STEP: Creating a pod to test consume configMaps
Mar 24 03:28:47.444: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-08565f4a-d490-4857-9295-2015dc881a4b" in namespace "projected-3143" to be "success or failure"
Mar 24 03:28:47.446: INFO: Pod "pod-projected-configmaps-08565f4a-d490-4857-9295-2015dc881a4b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.597783ms
Mar 24 03:28:49.449: INFO: Pod "pod-projected-configmaps-08565f4a-d490-4857-9295-2015dc881a4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004586562s
STEP: Saw pod success
Mar 24 03:28:49.449: INFO: Pod "pod-projected-configmaps-08565f4a-d490-4857-9295-2015dc881a4b" satisfied condition "success or failure"
Mar 24 03:28:49.451: INFO: Trying to get logs from node kind-worker pod pod-projected-configmaps-08565f4a-d490-4857-9295-2015dc881a4b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 03:28:49.462: INFO: Waiting for pod pod-projected-configmaps-08565f4a-d490-4857-9295-2015dc881a4b to disappear
Mar 24 03:28:49.465: INFO: Pod pod-projected-configmaps-08565f4a-d490-4857-9295-2015dc881a4b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:28:49.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3143" for this suite.
Mar 24 03:28:55.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:28:55.542: INFO: namespace projected-3143 deletion completed in 6.074243167s

• [SLOW TEST:8.229 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:28:55.542: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:28:55.672: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2bc0902f-ecdf-4ecb-b985-e47aa2cb044a" in namespace "downward-api-2742" to be "success or failure"
Mar 24 03:28:55.674: INFO: Pod "downwardapi-volume-2bc0902f-ecdf-4ecb-b985-e47aa2cb044a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015186ms
Mar 24 03:28:57.677: INFO: Pod "downwardapi-volume-2bc0902f-ecdf-4ecb-b985-e47aa2cb044a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004837928s
Mar 24 03:28:59.680: INFO: Pod "downwardapi-volume-2bc0902f-ecdf-4ecb-b985-e47aa2cb044a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007571931s
STEP: Saw pod success
Mar 24 03:28:59.680: INFO: Pod "downwardapi-volume-2bc0902f-ecdf-4ecb-b985-e47aa2cb044a" satisfied condition "success or failure"
Mar 24 03:28:59.682: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-2bc0902f-ecdf-4ecb-b985-e47aa2cb044a container client-container: <nil>
STEP: delete the pod
Mar 24 03:28:59.692: INFO: Waiting for pod downwardapi-volume-2bc0902f-ecdf-4ecb-b985-e47aa2cb044a to disappear
Mar 24 03:28:59.694: INFO: Pod downwardapi-volume-2bc0902f-ecdf-4ecb-b985-e47aa2cb044a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:28:59.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2742" for this suite.
Mar 24 03:29:05.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:29:05.760: INFO: namespace downward-api-2742 deletion completed in 6.064411969s

• [SLOW TEST:10.218 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:29:05.761: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:29:05.889: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dc450c36-01e8-49dd-aeb4-fffe1e329b26" in namespace "projected-2579" to be "success or failure"
Mar 24 03:29:05.891: INFO: Pod "downwardapi-volume-dc450c36-01e8-49dd-aeb4-fffe1e329b26": Phase="Pending", Reason="", readiness=false. Elapsed: 1.729404ms
Mar 24 03:29:07.894: INFO: Pod "downwardapi-volume-dc450c36-01e8-49dd-aeb4-fffe1e329b26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005027435s
STEP: Saw pod success
Mar 24 03:29:07.894: INFO: Pod "downwardapi-volume-dc450c36-01e8-49dd-aeb4-fffe1e329b26" satisfied condition "success or failure"
Mar 24 03:29:07.896: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-dc450c36-01e8-49dd-aeb4-fffe1e329b26 container client-container: <nil>
STEP: delete the pod
Mar 24 03:29:07.908: INFO: Waiting for pod downwardapi-volume-dc450c36-01e8-49dd-aeb4-fffe1e329b26 to disappear
Mar 24 03:29:07.910: INFO: Pod downwardapi-volume-dc450c36-01e8-49dd-aeb4-fffe1e329b26 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:29:07.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2579" for this suite.
Mar 24 03:29:13.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:29:13.972: INFO: namespace projected-2579 deletion completed in 6.060553317s

• [SLOW TEST:8.211 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:29:13.973: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-629
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:30:14.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-629" for this suite.
Mar 24 03:30:36.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:30:36.167: INFO: namespace container-probe-629 deletion completed in 22.062061448s

• [SLOW TEST:82.195 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:30:36.168: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6182
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 24 03:30:36.296: INFO: Waiting up to 5m0s for pod "pod-2e72732e-2c3c-4b88-99ce-d514236842f2" in namespace "emptydir-6182" to be "success or failure"
Mar 24 03:30:36.298: INFO: Pod "pod-2e72732e-2c3c-4b88-99ce-d514236842f2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.810967ms
Mar 24 03:30:38.301: INFO: Pod "pod-2e72732e-2c3c-4b88-99ce-d514236842f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004674037s
Mar 24 03:30:40.304: INFO: Pod "pod-2e72732e-2c3c-4b88-99ce-d514236842f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007792699s
STEP: Saw pod success
Mar 24 03:30:40.304: INFO: Pod "pod-2e72732e-2c3c-4b88-99ce-d514236842f2" satisfied condition "success or failure"
Mar 24 03:30:40.306: INFO: Trying to get logs from node kind-worker pod pod-2e72732e-2c3c-4b88-99ce-d514236842f2 container test-container: <nil>
STEP: delete the pod
Mar 24 03:30:40.315: INFO: Waiting for pod pod-2e72732e-2c3c-4b88-99ce-d514236842f2 to disappear
Mar 24 03:30:40.317: INFO: Pod pod-2e72732e-2c3c-4b88-99ce-d514236842f2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:30:40.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6182" for this suite.
Mar 24 03:30:46.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:30:46.379: INFO: namespace emptydir-6182 deletion completed in 6.059773222s

• [SLOW TEST:10.211 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:30:46.379: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5567
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-af620304-02d6-4a7c-b954-705e0260af66
STEP: Creating a pod to test consume configMaps
Mar 24 03:30:46.508: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4cbc5122-fe19-428a-a5c6-1f2efd14abdb" in namespace "projected-5567" to be "success or failure"
Mar 24 03:30:46.511: INFO: Pod "pod-projected-configmaps-4cbc5122-fe19-428a-a5c6-1f2efd14abdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.346778ms
Mar 24 03:30:48.513: INFO: Pod "pod-projected-configmaps-4cbc5122-fe19-428a-a5c6-1f2efd14abdb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005132234s
STEP: Saw pod success
Mar 24 03:30:48.513: INFO: Pod "pod-projected-configmaps-4cbc5122-fe19-428a-a5c6-1f2efd14abdb" satisfied condition "success or failure"
Mar 24 03:30:48.516: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-4cbc5122-fe19-428a-a5c6-1f2efd14abdb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 03:30:48.527: INFO: Waiting for pod pod-projected-configmaps-4cbc5122-fe19-428a-a5c6-1f2efd14abdb to disappear
Mar 24 03:30:48.529: INFO: Pod pod-projected-configmaps-4cbc5122-fe19-428a-a5c6-1f2efd14abdb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:30:48.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5567" for this suite.
Mar 24 03:30:54.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:30:54.597: INFO: namespace projected-5567 deletion completed in 6.065269785s

• [SLOW TEST:8.218 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:30:54.598: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3852
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 24 03:30:58.754: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 24 03:30:58.757: INFO: Pod pod-with-poststart-http-hook still exists
Mar 24 03:31:00.757: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 24 03:31:00.760: INFO: Pod pod-with-poststart-http-hook still exists
Mar 24 03:31:02.757: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 24 03:31:02.760: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:31:02.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3852" for this suite.
Mar 24 03:31:24.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:31:24.823: INFO: namespace container-lifecycle-hook-3852 deletion completed in 22.060853611s

• [SLOW TEST:30.225 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:31:24.823: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1423
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Mar 24 03:31:24.948: INFO: namespace kubectl-1423
Mar 24 03:31:24.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 create -f - --namespace=kubectl-1423'
Mar 24 03:31:25.175: INFO: stderr: ""
Mar 24 03:31:25.175: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 24 03:31:26.179: INFO: Selector matched 1 pods for map[app:redis]
Mar 24 03:31:26.179: INFO: Found 0 / 1
Mar 24 03:31:27.178: INFO: Selector matched 1 pods for map[app:redis]
Mar 24 03:31:27.178: INFO: Found 1 / 1
Mar 24 03:31:27.178: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 24 03:31:27.181: INFO: Selector matched 1 pods for map[app:redis]
Mar 24 03:31:27.181: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 24 03:31:27.181: INFO: wait on redis-master startup in kubectl-1423 
Mar 24 03:31:27.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 logs redis-master-t7bhf redis-master --namespace=kubectl-1423'
Mar 24 03:31:27.266: INFO: stderr: ""
Mar 24 03:31:27.266: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 24 Mar 03:31:26.282 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 24 Mar 03:31:26.283 # Server started, Redis version 3.2.12\n1:M 24 Mar 03:31:26.283 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 24 Mar 03:31:26.283 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Mar 24 03:31:27.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1423'
Mar 24 03:31:27.364: INFO: stderr: ""
Mar 24 03:31:27.364: INFO: stdout: "service/rm2 exposed\n"
Mar 24 03:31:27.366: INFO: Service rm2 in namespace kubectl-1423 found.
STEP: exposing service
Mar 24 03:31:29.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1423'
Mar 24 03:31:29.479: INFO: stderr: ""
Mar 24 03:31:29.480: INFO: stdout: "service/rm3 exposed\n"
Mar 24 03:31:29.483: INFO: Service rm3 in namespace kubectl-1423 found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:31:31.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1423" for this suite.
Mar 24 03:31:53.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:31:53.550: INFO: namespace kubectl-1423 deletion completed in 22.06029472s

• [SLOW TEST:28.727 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:31:53.550: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2876
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 24 03:31:53.679: INFO: Waiting up to 5m0s for pod "pod-1845ae1c-e7d2-449d-a9b2-ff26637380dc" in namespace "emptydir-2876" to be "success or failure"
Mar 24 03:31:53.681: INFO: Pod "pod-1845ae1c-e7d2-449d-a9b2-ff26637380dc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.687741ms
Mar 24 03:31:55.684: INFO: Pod "pod-1845ae1c-e7d2-449d-a9b2-ff26637380dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00437729s
STEP: Saw pod success
Mar 24 03:31:55.684: INFO: Pod "pod-1845ae1c-e7d2-449d-a9b2-ff26637380dc" satisfied condition "success or failure"
Mar 24 03:31:55.686: INFO: Trying to get logs from node kind-worker2 pod pod-1845ae1c-e7d2-449d-a9b2-ff26637380dc container test-container: <nil>
STEP: delete the pod
Mar 24 03:31:55.701: INFO: Waiting for pod pod-1845ae1c-e7d2-449d-a9b2-ff26637380dc to disappear
Mar 24 03:31:55.703: INFO: Pod pod-1845ae1c-e7d2-449d-a9b2-ff26637380dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:31:55.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2876" for this suite.
Mar 24 03:32:01.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:32:01.772: INFO: namespace emptydir-2876 deletion completed in 6.067368938s

• [SLOW TEST:8.222 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:32:01.773: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-p946c in namespace proxy-2140
I0324 03:32:01.908782      18 runners.go:180] Created replication controller with name: proxy-service-p946c, namespace: proxy-2140, replica count: 1
I0324 03:32:02.959325      18 runners.go:180] proxy-service-p946c Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0324 03:32:03.959578      18 runners.go:180] proxy-service-p946c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 03:32:04.959789      18 runners.go:180] proxy-service-p946c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 03:32:05.959976      18 runners.go:180] proxy-service-p946c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 03:32:06.960198      18 runners.go:180] proxy-service-p946c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 03:32:07.960409      18 runners.go:180] proxy-service-p946c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 03:32:08.960590      18 runners.go:180] proxy-service-p946c Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 03:32:09.960800      18 runners.go:180] proxy-service-p946c Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 03:32:09.963: INFO: setup took 8.064864582s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 24 03:32:09.966: INFO: (0) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 3.582093ms)
Mar 24 03:32:09.972: INFO: (0) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 8.962664ms)
Mar 24 03:32:09.972: INFO: (0) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 8.968213ms)
Mar 24 03:32:09.973: INFO: (0) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 10.10622ms)
Mar 24 03:32:09.973: INFO: (0) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 10.299478ms)
Mar 24 03:32:09.973: INFO: (0) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 10.239296ms)
Mar 24 03:32:09.973: INFO: (0) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 10.34458ms)
Mar 24 03:32:09.974: INFO: (0) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 10.937974ms)
Mar 24 03:32:09.974: INFO: (0) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 11.274812ms)
Mar 24 03:32:09.974: INFO: (0) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 11.418404ms)
Mar 24 03:32:09.976: INFO: (0) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 13.254934ms)
Mar 24 03:32:09.977: INFO: (0) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 14.366125ms)
Mar 24 03:32:09.977: INFO: (0) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 14.503642ms)
Mar 24 03:32:09.978: INFO: (0) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 14.774196ms)
Mar 24 03:32:09.979: INFO: (0) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 16.518043ms)
Mar 24 03:32:09.982: INFO: (0) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 19.237359ms)
Mar 24 03:32:09.987: INFO: (1) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 4.628906ms)
Mar 24 03:32:09.987: INFO: (1) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 4.849537ms)
Mar 24 03:32:09.988: INFO: (1) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 5.345018ms)
Mar 24 03:32:09.988: INFO: (1) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 5.329387ms)
Mar 24 03:32:09.988: INFO: (1) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 5.302473ms)
Mar 24 03:32:09.988: INFO: (1) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 5.453059ms)
Mar 24 03:32:09.988: INFO: (1) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 5.403929ms)
Mar 24 03:32:09.988: INFO: (1) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 5.914891ms)
Mar 24 03:32:09.988: INFO: (1) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 6.021667ms)
Mar 24 03:32:09.988: INFO: (1) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 6.078902ms)
Mar 24 03:32:09.989: INFO: (1) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 6.057494ms)
Mar 24 03:32:09.989: INFO: (1) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 6.170363ms)
Mar 24 03:32:09.989: INFO: (1) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 6.151737ms)
Mar 24 03:32:09.989: INFO: (1) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 6.207138ms)
Mar 24 03:32:09.989: INFO: (1) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 6.170111ms)
Mar 24 03:32:09.989: INFO: (1) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 6.298179ms)
Mar 24 03:32:09.995: INFO: (2) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 5.616626ms)
Mar 24 03:32:09.995: INFO: (2) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 5.60982ms)
Mar 24 03:32:09.995: INFO: (2) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 5.808263ms)
Mar 24 03:32:09.995: INFO: (2) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 5.656336ms)
Mar 24 03:32:09.995: INFO: (2) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 5.820288ms)
Mar 24 03:32:09.995: INFO: (2) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 5.700805ms)
Mar 24 03:32:09.995: INFO: (2) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 5.896023ms)
Mar 24 03:32:09.995: INFO: (2) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 5.887284ms)
Mar 24 03:32:09.995: INFO: (2) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 6.378215ms)
Mar 24 03:32:09.995: INFO: (2) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 6.454319ms)
Mar 24 03:32:09.995: INFO: (2) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 6.555956ms)
Mar 24 03:32:09.995: INFO: (2) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 6.430617ms)
Mar 24 03:32:09.996: INFO: (2) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 6.813556ms)
Mar 24 03:32:09.996: INFO: (2) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 6.800917ms)
Mar 24 03:32:09.997: INFO: (2) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 8.096902ms)
Mar 24 03:32:09.997: INFO: (2) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 8.121003ms)
Mar 24 03:32:10.001: INFO: (3) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 3.132051ms)
Mar 24 03:32:10.001: INFO: (3) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 3.085961ms)
Mar 24 03:32:10.001: INFO: (3) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 3.062801ms)
Mar 24 03:32:10.001: INFO: (3) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 3.347654ms)
Mar 24 03:32:10.001: INFO: (3) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 3.14543ms)
Mar 24 03:32:10.001: INFO: (3) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 3.072519ms)
Mar 24 03:32:10.002: INFO: (3) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 4.41828ms)
Mar 24 03:32:10.002: INFO: (3) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 4.373686ms)
Mar 24 03:32:10.002: INFO: (3) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 4.070441ms)
Mar 24 03:32:10.002: INFO: (3) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 4.52855ms)
Mar 24 03:32:10.002: INFO: (3) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 4.613483ms)
Mar 24 03:32:10.002: INFO: (3) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 4.077006ms)
Mar 24 03:32:10.002: INFO: (3) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 4.27654ms)
Mar 24 03:32:10.002: INFO: (3) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 4.730523ms)
Mar 24 03:32:10.002: INFO: (3) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 4.383098ms)
Mar 24 03:32:10.002: INFO: (3) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 4.368233ms)
Mar 24 03:32:10.007: INFO: (4) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 4.8282ms)
Mar 24 03:32:10.007: INFO: (4) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 5.353733ms)
Mar 24 03:32:10.007: INFO: (4) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 5.395012ms)
Mar 24 03:32:10.007: INFO: (4) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 5.389745ms)
Mar 24 03:32:10.007: INFO: (4) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 5.501127ms)
Mar 24 03:32:10.007: INFO: (4) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 5.452203ms)
Mar 24 03:32:10.008: INFO: (4) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 5.839454ms)
Mar 24 03:32:10.008: INFO: (4) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 5.932214ms)
Mar 24 03:32:10.009: INFO: (4) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 6.760304ms)
Mar 24 03:32:10.009: INFO: (4) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 6.745872ms)
Mar 24 03:32:10.009: INFO: (4) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 6.771655ms)
Mar 24 03:32:10.009: INFO: (4) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 6.956302ms)
Mar 24 03:32:10.009: INFO: (4) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 6.967908ms)
Mar 24 03:32:10.009: INFO: (4) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 7.071537ms)
Mar 24 03:32:10.009: INFO: (4) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 7.134118ms)
Mar 24 03:32:10.009: INFO: (4) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 6.965068ms)
Mar 24 03:32:10.014: INFO: (5) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 3.627329ms)
Mar 24 03:32:10.015: INFO: (5) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 4.481294ms)
Mar 24 03:32:10.015: INFO: (5) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 4.434003ms)
Mar 24 03:32:10.015: INFO: (5) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 5.527882ms)
Mar 24 03:32:10.015: INFO: (5) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 4.656424ms)
Mar 24 03:32:10.015: INFO: (5) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 4.411264ms)
Mar 24 03:32:10.015: INFO: (5) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 5.675757ms)
Mar 24 03:32:10.015: INFO: (5) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 4.880554ms)
Mar 24 03:32:10.015: INFO: (5) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 5.408607ms)
Mar 24 03:32:10.015: INFO: (5) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 5.210566ms)
Mar 24 03:32:10.015: INFO: (5) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 5.835962ms)
Mar 24 03:32:10.015: INFO: (5) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 5.336081ms)
Mar 24 03:32:10.015: INFO: (5) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 5.139867ms)
Mar 24 03:32:10.015: INFO: (5) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 5.553675ms)
Mar 24 03:32:10.015: INFO: (5) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 6.040276ms)
Mar 24 03:32:10.015: INFO: (5) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 5.185036ms)
Mar 24 03:32:10.018: INFO: (6) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 2.820374ms)
Mar 24 03:32:10.020: INFO: (6) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 4.096538ms)
Mar 24 03:32:10.020: INFO: (6) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 4.229399ms)
Mar 24 03:32:10.021: INFO: (6) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 5.597077ms)
Mar 24 03:32:10.021: INFO: (6) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 5.596696ms)
Mar 24 03:32:10.021: INFO: (6) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 5.75038ms)
Mar 24 03:32:10.021: INFO: (6) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 5.60956ms)
Mar 24 03:32:10.021: INFO: (6) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 5.668875ms)
Mar 24 03:32:10.021: INFO: (6) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 5.606186ms)
Mar 24 03:32:10.021: INFO: (6) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 5.600194ms)
Mar 24 03:32:10.021: INFO: (6) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 5.653294ms)
Mar 24 03:32:10.021: INFO: (6) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 5.800682ms)
Mar 24 03:32:10.021: INFO: (6) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 5.640245ms)
Mar 24 03:32:10.021: INFO: (6) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 5.870825ms)
Mar 24 03:32:10.021: INFO: (6) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 5.740305ms)
Mar 24 03:32:10.021: INFO: (6) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 5.973238ms)
Mar 24 03:32:10.027: INFO: (7) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 5.872029ms)
Mar 24 03:32:10.028: INFO: (7) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 6.378568ms)
Mar 24 03:32:10.028: INFO: (7) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 6.448702ms)
Mar 24 03:32:10.028: INFO: (7) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 6.422247ms)
Mar 24 03:32:10.028: INFO: (7) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 6.53406ms)
Mar 24 03:32:10.028: INFO: (7) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 6.502766ms)
Mar 24 03:32:10.028: INFO: (7) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 6.513595ms)
Mar 24 03:32:10.028: INFO: (7) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 6.458741ms)
Mar 24 03:32:10.028: INFO: (7) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 6.568297ms)
Mar 24 03:32:10.028: INFO: (7) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 6.607528ms)
Mar 24 03:32:10.028: INFO: (7) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 6.670479ms)
Mar 24 03:32:10.029: INFO: (7) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 7.001455ms)
Mar 24 03:32:10.030: INFO: (7) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 8.032799ms)
Mar 24 03:32:10.030: INFO: (7) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 8.108837ms)
Mar 24 03:32:10.030: INFO: (7) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 8.429502ms)
Mar 24 03:32:10.030: INFO: (7) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 8.647029ms)
Mar 24 03:32:10.033: INFO: (8) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 2.466796ms)
Mar 24 03:32:10.034: INFO: (8) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 3.334876ms)
Mar 24 03:32:10.034: INFO: (8) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 3.368607ms)
Mar 24 03:32:10.034: INFO: (8) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 2.82695ms)
Mar 24 03:32:10.035: INFO: (8) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 3.791808ms)
Mar 24 03:32:10.037: INFO: (8) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 5.336831ms)
Mar 24 03:32:10.037: INFO: (8) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 5.413938ms)
Mar 24 03:32:10.037: INFO: (8) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 5.461342ms)
Mar 24 03:32:10.037: INFO: (8) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 5.54853ms)
Mar 24 03:32:10.037: INFO: (8) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 6.617343ms)
Mar 24 03:32:10.037: INFO: (8) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 6.361538ms)
Mar 24 03:32:10.037: INFO: (8) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 5.685517ms)
Mar 24 03:32:10.037: INFO: (8) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 6.965244ms)
Mar 24 03:32:10.037: INFO: (8) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 5.777268ms)
Mar 24 03:32:10.038: INFO: (8) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 6.627677ms)
Mar 24 03:32:10.038: INFO: (8) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 7.474894ms)
Mar 24 03:32:10.042: INFO: (9) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 3.353136ms)
Mar 24 03:32:10.042: INFO: (9) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 3.803549ms)
Mar 24 03:32:10.042: INFO: (9) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 3.748523ms)
Mar 24 03:32:10.042: INFO: (9) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 3.692264ms)
Mar 24 03:32:10.042: INFO: (9) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 3.704583ms)
Mar 24 03:32:10.042: INFO: (9) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 3.811246ms)
Mar 24 03:32:10.042: INFO: (9) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 3.815942ms)
Mar 24 03:32:10.042: INFO: (9) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 3.895301ms)
Mar 24 03:32:10.042: INFO: (9) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 3.844573ms)
Mar 24 03:32:10.042: INFO: (9) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 3.890559ms)
Mar 24 03:32:10.045: INFO: (9) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 6.670333ms)
Mar 24 03:32:10.045: INFO: (9) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 6.874815ms)
Mar 24 03:32:10.045: INFO: (9) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 6.719307ms)
Mar 24 03:32:10.045: INFO: (9) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 6.763209ms)
Mar 24 03:32:10.045: INFO: (9) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 6.817931ms)
Mar 24 03:32:10.045: INFO: (9) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 6.913754ms)
Mar 24 03:32:10.047: INFO: (10) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 2.208588ms)
Mar 24 03:32:10.049: INFO: (10) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 3.76162ms)
Mar 24 03:32:10.049: INFO: (10) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 4.102453ms)
Mar 24 03:32:10.049: INFO: (10) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 3.987557ms)
Mar 24 03:32:10.049: INFO: (10) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 4.134563ms)
Mar 24 03:32:10.049: INFO: (10) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 4.083815ms)
Mar 24 03:32:10.050: INFO: (10) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 4.092133ms)
Mar 24 03:32:10.050: INFO: (10) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 4.041864ms)
Mar 24 03:32:10.050: INFO: (10) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 4.728035ms)
Mar 24 03:32:10.050: INFO: (10) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 5.093714ms)
Mar 24 03:32:10.051: INFO: (10) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 5.255396ms)
Mar 24 03:32:10.051: INFO: (10) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 5.273572ms)
Mar 24 03:32:10.051: INFO: (10) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 5.21619ms)
Mar 24 03:32:10.051: INFO: (10) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 5.454861ms)
Mar 24 03:32:10.051: INFO: (10) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 5.392656ms)
Mar 24 03:32:10.051: INFO: (10) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 5.470624ms)
Mar 24 03:32:10.053: INFO: (11) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 2.378165ms)
Mar 24 03:32:10.053: INFO: (11) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 2.22638ms)
Mar 24 03:32:10.056: INFO: (11) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 3.36313ms)
Mar 24 03:32:10.056: INFO: (11) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 4.936806ms)
Mar 24 03:32:10.056: INFO: (11) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 4.771534ms)
Mar 24 03:32:10.056: INFO: (11) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 4.175688ms)
Mar 24 03:32:10.056: INFO: (11) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 5.359493ms)
Mar 24 03:32:10.056: INFO: (11) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 5.606687ms)
Mar 24 03:32:10.057: INFO: (11) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 3.737421ms)
Mar 24 03:32:10.057: INFO: (11) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 4.116685ms)
Mar 24 03:32:10.057: INFO: (11) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 3.688233ms)
Mar 24 03:32:10.057: INFO: (11) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 4.833645ms)
Mar 24 03:32:10.057: INFO: (11) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 5.93759ms)
Mar 24 03:32:10.057: INFO: (11) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 4.623226ms)
Mar 24 03:32:10.057: INFO: (11) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 5.377908ms)
Mar 24 03:32:10.057: INFO: (11) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 4.733055ms)
Mar 24 03:32:10.061: INFO: (12) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 3.439223ms)
Mar 24 03:32:10.062: INFO: (12) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 4.393336ms)
Mar 24 03:32:10.062: INFO: (12) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 4.308664ms)
Mar 24 03:32:10.062: INFO: (12) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 4.385069ms)
Mar 24 03:32:10.062: INFO: (12) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 4.369244ms)
Mar 24 03:32:10.062: INFO: (12) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 4.481868ms)
Mar 24 03:32:10.063: INFO: (12) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 4.955205ms)
Mar 24 03:32:10.063: INFO: (12) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 5.000158ms)
Mar 24 03:32:10.063: INFO: (12) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 5.082281ms)
Mar 24 03:32:10.063: INFO: (12) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 5.053114ms)
Mar 24 03:32:10.063: INFO: (12) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 5.048319ms)
Mar 24 03:32:10.063: INFO: (12) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 5.286711ms)
Mar 24 03:32:10.063: INFO: (12) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 5.360966ms)
Mar 24 03:32:10.063: INFO: (12) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 5.285429ms)
Mar 24 03:32:10.063: INFO: (12) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 5.518674ms)
Mar 24 03:32:10.063: INFO: (12) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 5.750675ms)
Mar 24 03:32:10.066: INFO: (13) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 2.793733ms)
Mar 24 03:32:10.066: INFO: (13) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 2.876745ms)
Mar 24 03:32:10.067: INFO: (13) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 3.442583ms)
Mar 24 03:32:10.068: INFO: (13) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 4.834135ms)
Mar 24 03:32:10.068: INFO: (13) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 4.699033ms)
Mar 24 03:32:10.068: INFO: (13) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 4.782082ms)
Mar 24 03:32:10.068: INFO: (13) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 4.765582ms)
Mar 24 03:32:10.068: INFO: (13) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 4.856791ms)
Mar 24 03:32:10.068: INFO: (13) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 4.786296ms)
Mar 24 03:32:10.068: INFO: (13) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 4.837285ms)
Mar 24 03:32:10.068: INFO: (13) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 4.833627ms)
Mar 24 03:32:10.068: INFO: (13) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 4.987899ms)
Mar 24 03:32:10.068: INFO: (13) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 5.028631ms)
Mar 24 03:32:10.068: INFO: (13) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 5.019345ms)
Mar 24 03:32:10.069: INFO: (13) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 5.338606ms)
Mar 24 03:32:10.069: INFO: (13) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 5.790412ms)
Mar 24 03:32:10.072: INFO: (14) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 2.36694ms)
Mar 24 03:32:10.073: INFO: (14) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 3.740211ms)
Mar 24 03:32:10.073: INFO: (14) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 3.920787ms)
Mar 24 03:32:10.075: INFO: (14) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 5.201794ms)
Mar 24 03:32:10.075: INFO: (14) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 5.300309ms)
Mar 24 03:32:10.075: INFO: (14) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 5.776073ms)
Mar 24 03:32:10.075: INFO: (14) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 5.777865ms)
Mar 24 03:32:10.075: INFO: (14) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 5.851075ms)
Mar 24 03:32:10.075: INFO: (14) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 6.070289ms)
Mar 24 03:32:10.075: INFO: (14) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 5.885253ms)
Mar 24 03:32:10.075: INFO: (14) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 5.990405ms)
Mar 24 03:32:10.075: INFO: (14) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 5.967027ms)
Mar 24 03:32:10.075: INFO: (14) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 5.94377ms)
Mar 24 03:32:10.075: INFO: (14) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 5.859131ms)
Mar 24 03:32:10.076: INFO: (14) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 6.133986ms)
Mar 24 03:32:10.077: INFO: (14) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 7.112544ms)
Mar 24 03:32:10.080: INFO: (15) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 2.78328ms)
Mar 24 03:32:10.081: INFO: (15) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 4.600477ms)
Mar 24 03:32:10.082: INFO: (15) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 4.563528ms)
Mar 24 03:32:10.082: INFO: (15) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 4.648304ms)
Mar 24 03:32:10.082: INFO: (15) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 4.934628ms)
Mar 24 03:32:10.082: INFO: (15) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 5.487855ms)
Mar 24 03:32:10.082: INFO: (15) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 5.269331ms)
Mar 24 03:32:10.082: INFO: (15) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 5.482494ms)
Mar 24 03:32:10.082: INFO: (15) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 5.365246ms)
Mar 24 03:32:10.082: INFO: (15) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 5.429089ms)
Mar 24 03:32:10.082: INFO: (15) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 5.444075ms)
Mar 24 03:32:10.082: INFO: (15) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 5.483607ms)
Mar 24 03:32:10.082: INFO: (15) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 5.462764ms)
Mar 24 03:32:10.083: INFO: (15) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 5.80818ms)
Mar 24 03:32:10.083: INFO: (15) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 5.769146ms)
Mar 24 03:32:10.083: INFO: (15) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 5.867669ms)
Mar 24 03:32:10.090: INFO: (16) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 6.936974ms)
Mar 24 03:32:10.091: INFO: (16) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 8.238823ms)
Mar 24 03:32:10.111: INFO: (16) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 28.375556ms)
Mar 24 03:32:10.112: INFO: (16) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 28.6793ms)
Mar 24 03:32:10.112: INFO: (16) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 28.523988ms)
Mar 24 03:32:10.112: INFO: (16) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 28.632071ms)
Mar 24 03:32:10.112: INFO: (16) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 28.629732ms)
Mar 24 03:32:10.112: INFO: (16) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 28.646821ms)
Mar 24 03:32:10.112: INFO: (16) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 28.70436ms)
Mar 24 03:32:10.112: INFO: (16) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 28.791334ms)
Mar 24 03:32:10.112: INFO: (16) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 28.8502ms)
Mar 24 03:32:10.112: INFO: (16) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 28.868034ms)
Mar 24 03:32:10.118: INFO: (16) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 34.90105ms)
Mar 24 03:32:10.118: INFO: (16) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 35.111776ms)
Mar 24 03:32:10.118: INFO: (16) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 34.911563ms)
Mar 24 03:32:10.118: INFO: (16) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 35.041697ms)
Mar 24 03:32:10.122: INFO: (17) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 4.137695ms)
Mar 24 03:32:10.122: INFO: (17) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 4.110398ms)
Mar 24 03:32:10.122: INFO: (17) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 4.014216ms)
Mar 24 03:32:10.122: INFO: (17) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 3.990552ms)
Mar 24 03:32:10.122: INFO: (17) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 4.093479ms)
Mar 24 03:32:10.123: INFO: (17) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 4.861906ms)
Mar 24 03:32:10.123: INFO: (17) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 4.873755ms)
Mar 24 03:32:10.123: INFO: (17) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 4.773575ms)
Mar 24 03:32:10.123: INFO: (17) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 4.98144ms)
Mar 24 03:32:10.123: INFO: (17) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 4.973502ms)
Mar 24 03:32:10.124: INFO: (17) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 5.724833ms)
Mar 24 03:32:10.124: INFO: (17) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 5.780271ms)
Mar 24 03:32:10.124: INFO: (17) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 5.817559ms)
Mar 24 03:32:10.124: INFO: (17) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 5.784208ms)
Mar 24 03:32:10.124: INFO: (17) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 6.04705ms)
Mar 24 03:32:10.124: INFO: (17) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 5.967083ms)
Mar 24 03:32:10.129: INFO: (18) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 4.698097ms)
Mar 24 03:32:10.129: INFO: (18) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 4.667835ms)
Mar 24 03:32:10.129: INFO: (18) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 4.596193ms)
Mar 24 03:32:10.129: INFO: (18) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 4.369296ms)
Mar 24 03:32:10.129: INFO: (18) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 4.992276ms)
Mar 24 03:32:10.130: INFO: (18) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 4.853365ms)
Mar 24 03:32:10.130: INFO: (18) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 5.040319ms)
Mar 24 03:32:10.130: INFO: (18) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 5.158103ms)
Mar 24 03:32:10.133: INFO: (18) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 7.326649ms)
Mar 24 03:32:10.133: INFO: (18) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 7.440793ms)
Mar 24 03:32:10.133: INFO: (18) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 7.875312ms)
Mar 24 03:32:10.133: INFO: (18) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 7.527002ms)
Mar 24 03:32:10.134: INFO: (18) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 8.497236ms)
Mar 24 03:32:10.134: INFO: (18) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 8.04457ms)
Mar 24 03:32:10.134: INFO: (18) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 8.452747ms)
Mar 24 03:32:10.134: INFO: (18) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 8.737625ms)
Mar 24 03:32:10.137: INFO: (19) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz/proxy/rewriteme">test</a> (200; 2.550528ms)
Mar 24 03:32:10.137: INFO: (19) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:460/proxy/: tls baz (200; 2.472715ms)
Mar 24 03:32:10.137: INFO: (19) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:160/proxy/: foo (200; 2.716588ms)
Mar 24 03:32:10.137: INFO: (19) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:1080/proxy/rewriteme">test<... (200; 2.666457ms)
Mar 24 03:32:10.139: INFO: (19) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:162/proxy/: bar (200; 4.727312ms)
Mar 24 03:32:10.139: INFO: (19) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:462/proxy/: tls qux (200; 4.827913ms)
Mar 24 03:32:10.139: INFO: (19) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname2/proxy/: tls qux (200; 4.880563ms)
Mar 24 03:32:10.139: INFO: (19) /api/v1/namespaces/proxy-2140/pods/proxy-service-p946c-6ttsz:162/proxy/: bar (200; 4.8016ms)
Mar 24 03:32:10.139: INFO: (19) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:160/proxy/: foo (200; 4.777703ms)
Mar 24 03:32:10.139: INFO: (19) /api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/https:proxy-service-p946c-6ttsz:443/proxy/tlsrewritem... (200; 4.71536ms)
Mar 24 03:32:10.139: INFO: (19) /api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-2140/pods/http:proxy-service-p946c-6ttsz:1080/proxy/rewriteme">... (200; 4.742798ms)
Mar 24 03:32:10.140: INFO: (19) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname1/proxy/: foo (200; 5.198061ms)
Mar 24 03:32:10.142: INFO: (19) /api/v1/namespaces/proxy-2140/services/http:proxy-service-p946c:portname2/proxy/: bar (200; 6.8942ms)
Mar 24 03:32:10.142: INFO: (19) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname2/proxy/: bar (200; 6.995416ms)
Mar 24 03:32:10.142: INFO: (19) /api/v1/namespaces/proxy-2140/services/proxy-service-p946c:portname1/proxy/: foo (200; 7.28759ms)
Mar 24 03:32:10.142: INFO: (19) /api/v1/namespaces/proxy-2140/services/https:proxy-service-p946c:tlsportname1/proxy/: tls baz (200; 7.361401ms)
STEP: deleting ReplicationController proxy-service-p946c in namespace proxy-2140, will wait for the garbage collector to delete the pods
Mar 24 03:32:10.200: INFO: Deleting ReplicationController proxy-service-p946c took: 6.584103ms
Mar 24 03:32:10.501: INFO: Terminating ReplicationController proxy-service-p946c pods took: 300.254313ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:32:21.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2140" for this suite.
Mar 24 03:32:27.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:32:27.866: INFO: namespace proxy-2140 deletion completed in 6.061199032s

• [SLOW TEST:26.093 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:32:27.866: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7783
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Mar 24 03:32:32.004: INFO: Pod pod-hostip-ef037005-a641-48ef-9791-c99222b1b07b has hostIP: 172.17.0.2
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:32:32.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7783" for this suite.
Mar 24 03:32:54.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:32:54.068: INFO: namespace pods-7783 deletion completed in 22.06163813s

• [SLOW TEST:26.202 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:32:54.069: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4370
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:32:54.198: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ccdbaaeb-5946-40cf-90c9-594841560fbe" in namespace "downward-api-4370" to be "success or failure"
Mar 24 03:32:54.200: INFO: Pod "downwardapi-volume-ccdbaaeb-5946-40cf-90c9-594841560fbe": Phase="Pending", Reason="", readiness=false. Elapsed: 1.986136ms
Mar 24 03:32:56.203: INFO: Pod "downwardapi-volume-ccdbaaeb-5946-40cf-90c9-594841560fbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004944733s
STEP: Saw pod success
Mar 24 03:32:56.203: INFO: Pod "downwardapi-volume-ccdbaaeb-5946-40cf-90c9-594841560fbe" satisfied condition "success or failure"
Mar 24 03:32:56.205: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-ccdbaaeb-5946-40cf-90c9-594841560fbe container client-container: <nil>
STEP: delete the pod
Mar 24 03:32:56.218: INFO: Waiting for pod downwardapi-volume-ccdbaaeb-5946-40cf-90c9-594841560fbe to disappear
Mar 24 03:32:56.220: INFO: Pod downwardapi-volume-ccdbaaeb-5946-40cf-90c9-594841560fbe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:32:56.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4370" for this suite.
Mar 24 03:33:02.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:33:02.284: INFO: namespace downward-api-4370 deletion completed in 6.061661442s

• [SLOW TEST:8.215 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:33:02.284: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6960
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-45627fa6-846b-4484-9d79-2ae2dd411770
STEP: Creating a pod to test consume secrets
Mar 24 03:33:02.415: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ea57dbea-f4d3-4bab-9a40-62c3dfb7ece9" in namespace "projected-6960" to be "success or failure"
Mar 24 03:33:02.417: INFO: Pod "pod-projected-secrets-ea57dbea-f4d3-4bab-9a40-62c3dfb7ece9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.802446ms
Mar 24 03:33:04.420: INFO: Pod "pod-projected-secrets-ea57dbea-f4d3-4bab-9a40-62c3dfb7ece9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005134851s
STEP: Saw pod success
Mar 24 03:33:04.420: INFO: Pod "pod-projected-secrets-ea57dbea-f4d3-4bab-9a40-62c3dfb7ece9" satisfied condition "success or failure"
Mar 24 03:33:04.422: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-ea57dbea-f4d3-4bab-9a40-62c3dfb7ece9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:33:04.433: INFO: Waiting for pod pod-projected-secrets-ea57dbea-f4d3-4bab-9a40-62c3dfb7ece9 to disappear
Mar 24 03:33:04.435: INFO: Pod pod-projected-secrets-ea57dbea-f4d3-4bab-9a40-62c3dfb7ece9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:33:04.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6960" for this suite.
Mar 24 03:33:10.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:33:10.500: INFO: namespace projected-6960 deletion completed in 6.062856866s

• [SLOW TEST:8.215 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:33:10.500: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Mar 24 03:33:10.623: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-811275467 proxy --unix-socket=/tmp/kubectl-proxy-unix848720066/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:33:10.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7735" for this suite.
Mar 24 03:33:16.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:33:16.750: INFO: namespace kubectl-7735 deletion completed in 6.063420184s

• [SLOW TEST:6.250 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:33:16.750: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-533
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-70b44b80-52ab-4c7a-8798-8d4237ea0a8d in namespace container-probe-533
Mar 24 03:33:18.883: INFO: Started pod busybox-70b44b80-52ab-4c7a-8798-8d4237ea0a8d in namespace container-probe-533
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 03:33:18.885: INFO: Initial restart count of pod busybox-70b44b80-52ab-4c7a-8798-8d4237ea0a8d is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:37:19.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-533" for this suite.
Mar 24 03:37:25.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:37:25.309: INFO: namespace container-probe-533 deletion completed in 6.06519004s

• [SLOW TEST:248.559 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:37:25.309: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7716
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Mar 24 03:37:25.439: INFO: Waiting up to 5m0s for pod "var-expansion-a007afd9-4e1e-49fb-88b1-54dccba0e7be" in namespace "var-expansion-7716" to be "success or failure"
Mar 24 03:37:25.441: INFO: Pod "var-expansion-a007afd9-4e1e-49fb-88b1-54dccba0e7be": Phase="Pending", Reason="", readiness=false. Elapsed: 1.920385ms
Mar 24 03:37:27.443: INFO: Pod "var-expansion-a007afd9-4e1e-49fb-88b1-54dccba0e7be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00442988s
STEP: Saw pod success
Mar 24 03:37:27.444: INFO: Pod "var-expansion-a007afd9-4e1e-49fb-88b1-54dccba0e7be" satisfied condition "success or failure"
Mar 24 03:37:27.446: INFO: Trying to get logs from node kind-worker2 pod var-expansion-a007afd9-4e1e-49fb-88b1-54dccba0e7be container dapi-container: <nil>
STEP: delete the pod
Mar 24 03:37:27.456: INFO: Waiting for pod var-expansion-a007afd9-4e1e-49fb-88b1-54dccba0e7be to disappear
Mar 24 03:37:27.458: INFO: Pod var-expansion-a007afd9-4e1e-49fb-88b1-54dccba0e7be no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:37:27.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7716" for this suite.
Mar 24 03:37:33.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:37:33.524: INFO: namespace var-expansion-7716 deletion completed in 6.064028127s

• [SLOW TEST:8.215 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:37:33.525: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 24 03:37:33.654: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 24 03:37:38.657: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:37:38.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5145" for this suite.
Mar 24 03:37:44.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:37:44.735: INFO: namespace replication-controller-5145 deletion completed in 6.064281968s

• [SLOW TEST:11.210 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:37:44.736: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7037
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar 24 03:37:44.864: INFO: Waiting up to 5m0s for pod "downward-api-5ff32d55-2728-41bb-8889-8251d2ba76fd" in namespace "downward-api-7037" to be "success or failure"
Mar 24 03:37:44.865: INFO: Pod "downward-api-5ff32d55-2728-41bb-8889-8251d2ba76fd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.690942ms
Mar 24 03:37:46.868: INFO: Pod "downward-api-5ff32d55-2728-41bb-8889-8251d2ba76fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004033703s
Mar 24 03:37:48.871: INFO: Pod "downward-api-5ff32d55-2728-41bb-8889-8251d2ba76fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007000616s
STEP: Saw pod success
Mar 24 03:37:48.871: INFO: Pod "downward-api-5ff32d55-2728-41bb-8889-8251d2ba76fd" satisfied condition "success or failure"
Mar 24 03:37:48.873: INFO: Trying to get logs from node kind-worker pod downward-api-5ff32d55-2728-41bb-8889-8251d2ba76fd container dapi-container: <nil>
STEP: delete the pod
Mar 24 03:37:48.886: INFO: Waiting for pod downward-api-5ff32d55-2728-41bb-8889-8251d2ba76fd to disappear
Mar 24 03:37:48.887: INFO: Pod downward-api-5ff32d55-2728-41bb-8889-8251d2ba76fd no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:37:48.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7037" for this suite.
Mar 24 03:37:54.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:37:54.952: INFO: namespace downward-api-7037 deletion completed in 6.062254628s

• [SLOW TEST:10.216 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:37:54.952: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Mar 24 03:37:55.076: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 24 03:37:55.081: INFO: Waiting for terminating namespaces to be deleted...
Mar 24 03:37:55.084: INFO: 
Logging pods the kubelet thinks is on node kind-worker before test
Mar 24 03:37:55.088: INFO: kindnet-gsvnh from kube-system started at 2020-03-24 02:40:14 +0000 UTC (1 container statuses recorded)
Mar 24 03:37:55.088: INFO: 	Container kindnet-cni ready: true, restart count 0
Mar 24 03:37:55.088: INFO: kube-proxy-kgzdn from kube-system started at 2020-03-24 02:40:14 +0000 UTC (1 container statuses recorded)
Mar 24 03:37:55.088: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 24 03:37:55.088: INFO: 
Logging pods the kubelet thinks is on node kind-worker2 before test
Mar 24 03:37:55.092: INFO: kindnet-d95hs from kube-system started at 2020-03-24 02:40:13 +0000 UTC (1 container statuses recorded)
Mar 24 03:37:55.092: INFO: 	Container kindnet-cni ready: true, restart count 0
Mar 24 03:37:55.092: INFO: e2e-conformance-test from conformance started at 2020-03-24 02:44:46 +0000 UTC (1 container statuses recorded)
Mar 24 03:37:55.092: INFO: 	Container conformance-container ready: true, restart count 0
Mar 24 03:37:55.092: INFO: kube-proxy-2vhfg from kube-system started at 2020-03-24 02:40:13 +0000 UTC (1 container statuses recorded)
Mar 24 03:37:55.092: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node kind-worker
STEP: verifying the node has the label node kind-worker2
Mar 24 03:37:55.113: INFO: Pod e2e-conformance-test requesting resource cpu=0m on Node kind-worker2
Mar 24 03:37:55.113: INFO: Pod kindnet-d95hs requesting resource cpu=100m on Node kind-worker2
Mar 24 03:37:55.113: INFO: Pod kindnet-gsvnh requesting resource cpu=100m on Node kind-worker
Mar 24 03:37:55.113: INFO: Pod kube-proxy-2vhfg requesting resource cpu=0m on Node kind-worker2
Mar 24 03:37:55.113: INFO: Pod kube-proxy-kgzdn requesting resource cpu=0m on Node kind-worker
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2028f5e6-c64d-42d6-9aba-38bf496af0bb.15ff204520341f13], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3969/filler-pod-2028f5e6-c64d-42d6-9aba-38bf496af0bb to kind-worker2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2028f5e6-c64d-42d6-9aba-38bf496af0bb.15ff204559e9a918], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2028f5e6-c64d-42d6-9aba-38bf496af0bb.15ff20455f8c9d39], Reason = [Created], Message = [Created container filler-pod-2028f5e6-c64d-42d6-9aba-38bf496af0bb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2028f5e6-c64d-42d6-9aba-38bf496af0bb.15ff20456ca288e6], Reason = [Started], Message = [Started container filler-pod-2028f5e6-c64d-42d6-9aba-38bf496af0bb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f33d40c1-69fd-474f-8279-343822269016.15ff2045201e5059], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3969/filler-pod-f33d40c1-69fd-474f-8279-343822269016 to kind-worker]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f33d40c1-69fd-474f-8279-343822269016.15ff204559e9b160], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f33d40c1-69fd-474f-8279-343822269016.15ff20455f7a7b34], Reason = [Created], Message = [Created container filler-pod-f33d40c1-69fd-474f-8279-343822269016]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f33d40c1-69fd-474f-8279-343822269016.15ff20456c0cee36], Reason = [Started], Message = [Started container filler-pod-f33d40c1-69fd-474f-8279-343822269016]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15ff204598315fc6], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node kind-worker
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node kind-worker2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:37:58.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3969" for this suite.
Mar 24 03:38:04.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:38:04.219: INFO: namespace sched-pred-3969 deletion completed in 6.064419637s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:9.268 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:38:04.220: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5359
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-pv6x
STEP: Creating a pod to test atomic-volume-subpath
Mar 24 03:38:04.355: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-pv6x" in namespace "subpath-5359" to be "success or failure"
Mar 24 03:38:04.356: INFO: Pod "pod-subpath-test-secret-pv6x": Phase="Pending", Reason="", readiness=false. Elapsed: 1.708889ms
Mar 24 03:38:06.359: INFO: Pod "pod-subpath-test-secret-pv6x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004660627s
Mar 24 03:38:08.362: INFO: Pod "pod-subpath-test-secret-pv6x": Phase="Running", Reason="", readiness=true. Elapsed: 4.007756419s
Mar 24 03:38:10.366: INFO: Pod "pod-subpath-test-secret-pv6x": Phase="Running", Reason="", readiness=true. Elapsed: 6.01102805s
Mar 24 03:38:12.369: INFO: Pod "pod-subpath-test-secret-pv6x": Phase="Running", Reason="", readiness=true. Elapsed: 8.014085778s
Mar 24 03:38:14.371: INFO: Pod "pod-subpath-test-secret-pv6x": Phase="Running", Reason="", readiness=true. Elapsed: 10.016773013s
Mar 24 03:38:16.374: INFO: Pod "pod-subpath-test-secret-pv6x": Phase="Running", Reason="", readiness=true. Elapsed: 12.019401847s
Mar 24 03:38:18.377: INFO: Pod "pod-subpath-test-secret-pv6x": Phase="Running", Reason="", readiness=true. Elapsed: 14.022192756s
Mar 24 03:38:20.380: INFO: Pod "pod-subpath-test-secret-pv6x": Phase="Running", Reason="", readiness=true. Elapsed: 16.025389817s
Mar 24 03:38:22.383: INFO: Pod "pod-subpath-test-secret-pv6x": Phase="Running", Reason="", readiness=true. Elapsed: 18.028187587s
Mar 24 03:38:24.386: INFO: Pod "pod-subpath-test-secret-pv6x": Phase="Running", Reason="", readiness=true. Elapsed: 20.031132775s
Mar 24 03:38:26.389: INFO: Pod "pod-subpath-test-secret-pv6x": Phase="Running", Reason="", readiness=true. Elapsed: 22.033964251s
Mar 24 03:38:28.391: INFO: Pod "pod-subpath-test-secret-pv6x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.036615369s
STEP: Saw pod success
Mar 24 03:38:28.391: INFO: Pod "pod-subpath-test-secret-pv6x" satisfied condition "success or failure"
Mar 24 03:38:28.393: INFO: Trying to get logs from node kind-worker2 pod pod-subpath-test-secret-pv6x container test-container-subpath-secret-pv6x: <nil>
STEP: delete the pod
Mar 24 03:38:28.408: INFO: Waiting for pod pod-subpath-test-secret-pv6x to disappear
Mar 24 03:38:28.411: INFO: Pod pod-subpath-test-secret-pv6x no longer exists
STEP: Deleting pod pod-subpath-test-secret-pv6x
Mar 24 03:38:28.411: INFO: Deleting pod "pod-subpath-test-secret-pv6x" in namespace "subpath-5359"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:38:28.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5359" for this suite.
Mar 24 03:38:34.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:38:34.481: INFO: namespace subpath-5359 deletion completed in 6.066103839s

• [SLOW TEST:30.261 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:38:34.481: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2502
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-7h5j
STEP: Creating a pod to test atomic-volume-subpath
Mar 24 03:38:34.616: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7h5j" in namespace "subpath-2502" to be "success or failure"
Mar 24 03:38:34.618: INFO: Pod "pod-subpath-test-configmap-7h5j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184707ms
Mar 24 03:38:36.621: INFO: Pod "pod-subpath-test-configmap-7h5j": Phase="Running", Reason="", readiness=true. Elapsed: 2.004987724s
Mar 24 03:38:38.623: INFO: Pod "pod-subpath-test-configmap-7h5j": Phase="Running", Reason="", readiness=true. Elapsed: 4.007727335s
Mar 24 03:38:40.627: INFO: Pod "pod-subpath-test-configmap-7h5j": Phase="Running", Reason="", readiness=true. Elapsed: 6.011013127s
Mar 24 03:38:42.630: INFO: Pod "pod-subpath-test-configmap-7h5j": Phase="Running", Reason="", readiness=true. Elapsed: 8.014013625s
Mar 24 03:38:44.633: INFO: Pod "pod-subpath-test-configmap-7h5j": Phase="Running", Reason="", readiness=true. Elapsed: 10.017035943s
Mar 24 03:38:46.636: INFO: Pod "pod-subpath-test-configmap-7h5j": Phase="Running", Reason="", readiness=true. Elapsed: 12.019926982s
Mar 24 03:38:48.638: INFO: Pod "pod-subpath-test-configmap-7h5j": Phase="Running", Reason="", readiness=true. Elapsed: 14.022713722s
Mar 24 03:38:50.641: INFO: Pod "pod-subpath-test-configmap-7h5j": Phase="Running", Reason="", readiness=true. Elapsed: 16.025682428s
Mar 24 03:38:52.644: INFO: Pod "pod-subpath-test-configmap-7h5j": Phase="Running", Reason="", readiness=true. Elapsed: 18.028615366s
Mar 24 03:38:54.647: INFO: Pod "pod-subpath-test-configmap-7h5j": Phase="Running", Reason="", readiness=true. Elapsed: 20.031582668s
Mar 24 03:38:56.650: INFO: Pod "pod-subpath-test-configmap-7h5j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.034207979s
STEP: Saw pod success
Mar 24 03:38:56.650: INFO: Pod "pod-subpath-test-configmap-7h5j" satisfied condition "success or failure"
Mar 24 03:38:56.652: INFO: Trying to get logs from node kind-worker pod pod-subpath-test-configmap-7h5j container test-container-subpath-configmap-7h5j: <nil>
STEP: delete the pod
Mar 24 03:38:56.666: INFO: Waiting for pod pod-subpath-test-configmap-7h5j to disappear
Mar 24 03:38:56.669: INFO: Pod pod-subpath-test-configmap-7h5j no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7h5j
Mar 24 03:38:56.669: INFO: Deleting pod "pod-subpath-test-configmap-7h5j" in namespace "subpath-2502"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:38:56.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2502" for this suite.
Mar 24 03:39:02.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:39:02.735: INFO: namespace subpath-2502 deletion completed in 6.059993467s

• [SLOW TEST:28.254 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:39:02.736: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4229
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-d4bbe027-4b3b-46ff-a1b9-de6130a3d175
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:39:04.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4229" for this suite.
Mar 24 03:39:26.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:39:26.949: INFO: namespace configmap-4229 deletion completed in 22.063958448s

• [SLOW TEST:24.213 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:39:26.949: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2587
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:39:27.098: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e5e75107-4a47-4aa3-86dd-2639445bd8b9", Controller:(*bool)(0xc003081446), BlockOwnerDeletion:(*bool)(0xc003081447)}}
Mar 24 03:39:27.103: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8393a639-7e35-4a31-b016-08bc2e87e540", Controller:(*bool)(0xc002959fda), BlockOwnerDeletion:(*bool)(0xc002959fdb)}}
Mar 24 03:39:27.108: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"43d7cfff-23f5-4a06-9972-45e9a872ab10", Controller:(*bool)(0xc0003533f6), BlockOwnerDeletion:(*bool)(0xc0003533f7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:39:32.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2587" for this suite.
Mar 24 03:39:38.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:39:38.177: INFO: namespace gc-2587 deletion completed in 6.060118203s

• [SLOW TEST:11.228 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:39:38.177: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:39:38.306: INFO: Waiting up to 5m0s for pod "downwardapi-volume-638fa731-7446-4bf9-8030-8fe9689c6e0b" in namespace "downward-api-9694" to be "success or failure"
Mar 24 03:39:38.308: INFO: Pod "downwardapi-volume-638fa731-7446-4bf9-8030-8fe9689c6e0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013778ms
Mar 24 03:39:40.311: INFO: Pod "downwardapi-volume-638fa731-7446-4bf9-8030-8fe9689c6e0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004984988s
STEP: Saw pod success
Mar 24 03:39:40.311: INFO: Pod "downwardapi-volume-638fa731-7446-4bf9-8030-8fe9689c6e0b" satisfied condition "success or failure"
Mar 24 03:39:40.313: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-638fa731-7446-4bf9-8030-8fe9689c6e0b container client-container: <nil>
STEP: delete the pod
Mar 24 03:39:40.324: INFO: Waiting for pod downwardapi-volume-638fa731-7446-4bf9-8030-8fe9689c6e0b to disappear
Mar 24 03:39:40.326: INFO: Pod downwardapi-volume-638fa731-7446-4bf9-8030-8fe9689c6e0b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:39:40.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9694" for this suite.
Mar 24 03:39:46.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:39:46.389: INFO: namespace downward-api-9694 deletion completed in 6.060306319s

• [SLOW TEST:8.212 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:39:46.389: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6956
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9b52300a-af15-4d47-a0f7-b377b9ec28b6
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-9b52300a-af15-4d47-a0f7-b377b9ec28b6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:39:50.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6956" for this suite.
Mar 24 03:40:04.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:40:04.609: INFO: namespace projected-6956 deletion completed in 14.062321513s

• [SLOW TEST:18.219 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:40:04.609: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3926
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:40:04.738: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c64147bc-1f2f-4d72-b24e-eaef6bad5f15" in namespace "projected-3926" to be "success or failure"
Mar 24 03:40:04.740: INFO: Pod "downwardapi-volume-c64147bc-1f2f-4d72-b24e-eaef6bad5f15": Phase="Pending", Reason="", readiness=false. Elapsed: 1.916355ms
Mar 24 03:40:06.743: INFO: Pod "downwardapi-volume-c64147bc-1f2f-4d72-b24e-eaef6bad5f15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004669151s
STEP: Saw pod success
Mar 24 03:40:06.743: INFO: Pod "downwardapi-volume-c64147bc-1f2f-4d72-b24e-eaef6bad5f15" satisfied condition "success or failure"
Mar 24 03:40:06.745: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-c64147bc-1f2f-4d72-b24e-eaef6bad5f15 container client-container: <nil>
STEP: delete the pod
Mar 24 03:40:06.758: INFO: Waiting for pod downwardapi-volume-c64147bc-1f2f-4d72-b24e-eaef6bad5f15 to disappear
Mar 24 03:40:06.760: INFO: Pod downwardapi-volume-c64147bc-1f2f-4d72-b24e-eaef6bad5f15 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:40:06.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3926" for this suite.
Mar 24 03:40:12.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:40:12.825: INFO: namespace projected-3926 deletion completed in 6.063208934s

• [SLOW TEST:8.216 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:40:12.826: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8087
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Mar 24 03:40:52.970: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:40:52.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0324 03:40:52.970384      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-8087" for this suite.
Mar 24 03:40:58.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:40:59.073: INFO: namespace gc-8087 deletion completed in 6.100680914s

• [SLOW TEST:46.247 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:40:59.074: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3190
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-402ef127-6ad2-4c50-8ecb-cb4a6ef3fc48
STEP: Creating a pod to test consume configMaps
Mar 24 03:40:59.220: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1b7ed16d-c5b9-4b40-a4fe-8d0169b78bfe" in namespace "projected-3190" to be "success or failure"
Mar 24 03:40:59.222: INFO: Pod "pod-projected-configmaps-1b7ed16d-c5b9-4b40-a4fe-8d0169b78bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 1.745147ms
Mar 24 03:41:01.225: INFO: Pod "pod-projected-configmaps-1b7ed16d-c5b9-4b40-a4fe-8d0169b78bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004399655s
Mar 24 03:41:03.228: INFO: Pod "pod-projected-configmaps-1b7ed16d-c5b9-4b40-a4fe-8d0169b78bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007485861s
STEP: Saw pod success
Mar 24 03:41:03.228: INFO: Pod "pod-projected-configmaps-1b7ed16d-c5b9-4b40-a4fe-8d0169b78bfe" satisfied condition "success or failure"
Mar 24 03:41:03.230: INFO: Trying to get logs from node kind-worker pod pod-projected-configmaps-1b7ed16d-c5b9-4b40-a4fe-8d0169b78bfe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 03:41:03.241: INFO: Waiting for pod pod-projected-configmaps-1b7ed16d-c5b9-4b40-a4fe-8d0169b78bfe to disappear
Mar 24 03:41:03.243: INFO: Pod pod-projected-configmaps-1b7ed16d-c5b9-4b40-a4fe-8d0169b78bfe no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:41:03.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3190" for this suite.
Mar 24 03:41:09.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:41:09.309: INFO: namespace projected-3190 deletion completed in 6.063934582s

• [SLOW TEST:10.235 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:41:09.310: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8505
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-747cd83a-044b-4987-9de5-b05b10a4d2e7
STEP: Creating a pod to test consume configMaps
Mar 24 03:41:09.441: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-24a44cf6-09df-4ead-9c64-d0afa40bed56" in namespace "projected-8505" to be "success or failure"
Mar 24 03:41:09.444: INFO: Pod "pod-projected-configmaps-24a44cf6-09df-4ead-9c64-d0afa40bed56": Phase="Pending", Reason="", readiness=false. Elapsed: 3.465895ms
Mar 24 03:41:11.447: INFO: Pod "pod-projected-configmaps-24a44cf6-09df-4ead-9c64-d0afa40bed56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006371553s
STEP: Saw pod success
Mar 24 03:41:11.447: INFO: Pod "pod-projected-configmaps-24a44cf6-09df-4ead-9c64-d0afa40bed56" satisfied condition "success or failure"
Mar 24 03:41:11.450: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-24a44cf6-09df-4ead-9c64-d0afa40bed56 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 03:41:11.462: INFO: Waiting for pod pod-projected-configmaps-24a44cf6-09df-4ead-9c64-d0afa40bed56 to disappear
Mar 24 03:41:11.465: INFO: Pod pod-projected-configmaps-24a44cf6-09df-4ead-9c64-d0afa40bed56 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:41:11.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8505" for this suite.
Mar 24 03:41:17.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:41:17.530: INFO: namespace projected-8505 deletion completed in 6.063181775s

• [SLOW TEST:8.220 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:41:17.530: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6872
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-5cedd155-71f9-420f-bc0e-5bd3622fdd79
STEP: Creating a pod to test consume secrets
Mar 24 03:41:17.660: INFO: Waiting up to 5m0s for pod "pod-secrets-a650859a-08ac-4bdd-a3b8-fd2dda4575e7" in namespace "secrets-6872" to be "success or failure"
Mar 24 03:41:17.662: INFO: Pod "pod-secrets-a650859a-08ac-4bdd-a3b8-fd2dda4575e7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.78706ms
Mar 24 03:41:19.664: INFO: Pod "pod-secrets-a650859a-08ac-4bdd-a3b8-fd2dda4575e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004434917s
STEP: Saw pod success
Mar 24 03:41:19.665: INFO: Pod "pod-secrets-a650859a-08ac-4bdd-a3b8-fd2dda4575e7" satisfied condition "success or failure"
Mar 24 03:41:19.666: INFO: Trying to get logs from node kind-worker pod pod-secrets-a650859a-08ac-4bdd-a3b8-fd2dda4575e7 container secret-env-test: <nil>
STEP: delete the pod
Mar 24 03:41:19.677: INFO: Waiting for pod pod-secrets-a650859a-08ac-4bdd-a3b8-fd2dda4575e7 to disappear
Mar 24 03:41:19.678: INFO: Pod pod-secrets-a650859a-08ac-4bdd-a3b8-fd2dda4575e7 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:41:19.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6872" for this suite.
Mar 24 03:41:25.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:41:25.744: INFO: namespace secrets-6872 deletion completed in 6.063741701s

• [SLOW TEST:8.214 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:41:25.744: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7415
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar 24 03:41:25.872: INFO: Waiting up to 5m0s for pod "downward-api-b50f54ce-5f3e-417d-8db7-fee01d09981a" in namespace "downward-api-7415" to be "success or failure"
Mar 24 03:41:25.874: INFO: Pod "downward-api-b50f54ce-5f3e-417d-8db7-fee01d09981a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.881015ms
Mar 24 03:41:27.877: INFO: Pod "downward-api-b50f54ce-5f3e-417d-8db7-fee01d09981a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005085715s
STEP: Saw pod success
Mar 24 03:41:27.877: INFO: Pod "downward-api-b50f54ce-5f3e-417d-8db7-fee01d09981a" satisfied condition "success or failure"
Mar 24 03:41:27.879: INFO: Trying to get logs from node kind-worker2 pod downward-api-b50f54ce-5f3e-417d-8db7-fee01d09981a container dapi-container: <nil>
STEP: delete the pod
Mar 24 03:41:27.892: INFO: Waiting for pod downward-api-b50f54ce-5f3e-417d-8db7-fee01d09981a to disappear
Mar 24 03:41:27.894: INFO: Pod downward-api-b50f54ce-5f3e-417d-8db7-fee01d09981a no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:41:27.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7415" for this suite.
Mar 24 03:41:33.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:41:33.959: INFO: namespace downward-api-7415 deletion completed in 6.062122334s

• [SLOW TEST:8.214 seconds]
[sig-node] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:41:33.959: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:41:34.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-244" for this suite.
Mar 24 03:41:56.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:41:56.159: INFO: namespace pods-244 deletion completed in 22.065305072s

• [SLOW TEST:22.200 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:41:56.159: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9085
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 24 03:41:56.292: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9085,SelfLink:/api/v1/namespaces/watch-9085/configmaps/e2e-watch-test-watch-closed,UID:597ac0e5-853f-49ba-a351-d8ab2e40d008,ResourceVersion:14343,Generation:0,CreationTimestamp:2020-03-24 03:41:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 24 03:41:56.292: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9085,SelfLink:/api/v1/namespaces/watch-9085/configmaps/e2e-watch-test-watch-closed,UID:597ac0e5-853f-49ba-a351-d8ab2e40d008,ResourceVersion:14344,Generation:0,CreationTimestamp:2020-03-24 03:41:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 24 03:41:56.300: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9085,SelfLink:/api/v1/namespaces/watch-9085/configmaps/e2e-watch-test-watch-closed,UID:597ac0e5-853f-49ba-a351-d8ab2e40d008,ResourceVersion:14345,Generation:0,CreationTimestamp:2020-03-24 03:41:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 24 03:41:56.300: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9085,SelfLink:/api/v1/namespaces/watch-9085/configmaps/e2e-watch-test-watch-closed,UID:597ac0e5-853f-49ba-a351-d8ab2e40d008,ResourceVersion:14346,Generation:0,CreationTimestamp:2020-03-24 03:41:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:41:56.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9085" for this suite.
Mar 24 03:42:02.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:42:02.364: INFO: namespace watch-9085 deletion completed in 6.062090108s

• [SLOW TEST:6.205 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:42:02.365: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1748
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-90ab7d9b-e491-4c3c-be7b-18cef922f881
STEP: Creating a pod to test consume secrets
Mar 24 03:42:02.495: INFO: Waiting up to 5m0s for pod "pod-secrets-f7ded60c-bb67-4838-94fc-30ed05a3155e" in namespace "secrets-1748" to be "success or failure"
Mar 24 03:42:02.497: INFO: Pod "pod-secrets-f7ded60c-bb67-4838-94fc-30ed05a3155e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.900324ms
Mar 24 03:42:04.500: INFO: Pod "pod-secrets-f7ded60c-bb67-4838-94fc-30ed05a3155e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004508738s
STEP: Saw pod success
Mar 24 03:42:04.500: INFO: Pod "pod-secrets-f7ded60c-bb67-4838-94fc-30ed05a3155e" satisfied condition "success or failure"
Mar 24 03:42:04.528: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-f7ded60c-bb67-4838-94fc-30ed05a3155e container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:42:04.541: INFO: Waiting for pod pod-secrets-f7ded60c-bb67-4838-94fc-30ed05a3155e to disappear
Mar 24 03:42:04.542: INFO: Pod pod-secrets-f7ded60c-bb67-4838-94fc-30ed05a3155e no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:42:04.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1748" for this suite.
Mar 24 03:42:10.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:42:10.606: INFO: namespace secrets-1748 deletion completed in 6.061338568s

• [SLOW TEST:8.241 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:42:10.606: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8841
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-2a79c9be-4b78-474a-9362-19b5c22a8d1e
STEP: Creating a pod to test consume secrets
Mar 24 03:42:10.739: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5602e6a3-01de-4ba6-9a7d-b7e9559c4e28" in namespace "projected-8841" to be "success or failure"
Mar 24 03:42:10.741: INFO: Pod "pod-projected-secrets-5602e6a3-01de-4ba6-9a7d-b7e9559c4e28": Phase="Pending", Reason="", readiness=false. Elapsed: 1.963504ms
Mar 24 03:42:12.743: INFO: Pod "pod-projected-secrets-5602e6a3-01de-4ba6-9a7d-b7e9559c4e28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004736775s
STEP: Saw pod success
Mar 24 03:42:12.743: INFO: Pod "pod-projected-secrets-5602e6a3-01de-4ba6-9a7d-b7e9559c4e28" satisfied condition "success or failure"
Mar 24 03:42:12.746: INFO: Trying to get logs from node kind-worker pod pod-projected-secrets-5602e6a3-01de-4ba6-9a7d-b7e9559c4e28 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:42:12.758: INFO: Waiting for pod pod-projected-secrets-5602e6a3-01de-4ba6-9a7d-b7e9559c4e28 to disappear
Mar 24 03:42:12.760: INFO: Pod pod-projected-secrets-5602e6a3-01de-4ba6-9a7d-b7e9559c4e28 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:42:12.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8841" for this suite.
Mar 24 03:42:18.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:42:18.828: INFO: namespace projected-8841 deletion completed in 6.065353322s

• [SLOW TEST:8.221 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:42:18.828: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5626
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-77b9a005-f2b6-4e7a-b6a7-ae2c83da0d34
STEP: Creating a pod to test consume secrets
Mar 24 03:42:18.959: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b3905460-1473-4304-8037-04500d66f5b0" in namespace "projected-5626" to be "success or failure"
Mar 24 03:42:18.962: INFO: Pod "pod-projected-secrets-b3905460-1473-4304-8037-04500d66f5b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.365248ms
Mar 24 03:42:20.965: INFO: Pod "pod-projected-secrets-b3905460-1473-4304-8037-04500d66f5b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005567943s
STEP: Saw pod success
Mar 24 03:42:20.965: INFO: Pod "pod-projected-secrets-b3905460-1473-4304-8037-04500d66f5b0" satisfied condition "success or failure"
Mar 24 03:42:20.967: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-b3905460-1473-4304-8037-04500d66f5b0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:42:20.984: INFO: Waiting for pod pod-projected-secrets-b3905460-1473-4304-8037-04500d66f5b0 to disappear
Mar 24 03:42:20.986: INFO: Pod pod-projected-secrets-b3905460-1473-4304-8037-04500d66f5b0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:42:20.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5626" for this suite.
Mar 24 03:42:26.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:42:27.049: INFO: namespace projected-5626 deletion completed in 6.060486099s

• [SLOW TEST:8.221 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:42:27.049: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4930
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-4930/configmap-test-9bde1264-8103-4b4d-9c60-f6e4723872d2
STEP: Creating a pod to test consume configMaps
Mar 24 03:42:27.179: INFO: Waiting up to 5m0s for pod "pod-configmaps-8388e51b-047d-4746-9413-890ec886bc10" in namespace "configmap-4930" to be "success or failure"
Mar 24 03:42:27.181: INFO: Pod "pod-configmaps-8388e51b-047d-4746-9413-890ec886bc10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049815ms
Mar 24 03:42:29.184: INFO: Pod "pod-configmaps-8388e51b-047d-4746-9413-890ec886bc10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004818939s
STEP: Saw pod success
Mar 24 03:42:29.184: INFO: Pod "pod-configmaps-8388e51b-047d-4746-9413-890ec886bc10" satisfied condition "success or failure"
Mar 24 03:42:29.186: INFO: Trying to get logs from node kind-worker pod pod-configmaps-8388e51b-047d-4746-9413-890ec886bc10 container env-test: <nil>
STEP: delete the pod
Mar 24 03:42:29.197: INFO: Waiting for pod pod-configmaps-8388e51b-047d-4746-9413-890ec886bc10 to disappear
Mar 24 03:42:29.198: INFO: Pod pod-configmaps-8388e51b-047d-4746-9413-890ec886bc10 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:42:29.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4930" for this suite.
Mar 24 03:42:35.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:42:35.264: INFO: namespace configmap-4930 deletion completed in 6.063823884s

• [SLOW TEST:8.216 seconds]
[sig-node] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:42:35.265: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1975
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-cfb5a32d-ca11-4cc2-83c6-6ee2498d5d02 in namespace container-probe-1975
Mar 24 03:42:39.399: INFO: Started pod liveness-cfb5a32d-ca11-4cc2-83c6-6ee2498d5d02 in namespace container-probe-1975
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 03:42:39.401: INFO: Initial restart count of pod liveness-cfb5a32d-ca11-4cc2-83c6-6ee2498d5d02 is 0
Mar 24 03:43:01.439: INFO: Restart count of pod container-probe-1975/liveness-cfb5a32d-ca11-4cc2-83c6-6ee2498d5d02 is now 1 (22.037594247s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:43:01.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1975" for this suite.
Mar 24 03:43:07.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:43:07.510: INFO: namespace container-probe-1975 deletion completed in 6.060894519s

• [SLOW TEST:32.245 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:43:07.510: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4857
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-c2332fd1-4fa3-4cda-b82d-ff9242d7a893
Mar 24 03:43:07.642: INFO: Pod name my-hostname-basic-c2332fd1-4fa3-4cda-b82d-ff9242d7a893: Found 0 pods out of 1
Mar 24 03:43:12.645: INFO: Pod name my-hostname-basic-c2332fd1-4fa3-4cda-b82d-ff9242d7a893: Found 1 pods out of 1
Mar 24 03:43:12.645: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c2332fd1-4fa3-4cda-b82d-ff9242d7a893" are running
Mar 24 03:43:12.648: INFO: Pod "my-hostname-basic-c2332fd1-4fa3-4cda-b82d-ff9242d7a893-jgnmf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-24 03:43:07 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-24 03:43:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-24 03:43:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-24 03:43:07 +0000 UTC Reason: Message:}])
Mar 24 03:43:12.648: INFO: Trying to dial the pod
Mar 24 03:43:17.656: INFO: Controller my-hostname-basic-c2332fd1-4fa3-4cda-b82d-ff9242d7a893: Got expected result from replica 1 [my-hostname-basic-c2332fd1-4fa3-4cda-b82d-ff9242d7a893-jgnmf]: "my-hostname-basic-c2332fd1-4fa3-4cda-b82d-ff9242d7a893-jgnmf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:43:17.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4857" for this suite.
Mar 24 03:43:23.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:43:23.720: INFO: namespace replication-controller-4857 deletion completed in 6.061889739s

• [SLOW TEST:16.210 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:43:23.721: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 24 03:43:28.364: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2a99450f-4cb5-4e83-8255-7c371d989a59"
Mar 24 03:43:28.364: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2a99450f-4cb5-4e83-8255-7c371d989a59" in namespace "pods-1391" to be "terminated due to deadline exceeded"
Mar 24 03:43:28.367: INFO: Pod "pod-update-activedeadlineseconds-2a99450f-4cb5-4e83-8255-7c371d989a59": Phase="Running", Reason="", readiness=true. Elapsed: 2.361045ms
Mar 24 03:43:30.369: INFO: Pod "pod-update-activedeadlineseconds-2a99450f-4cb5-4e83-8255-7c371d989a59": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.005059256s
Mar 24 03:43:30.369: INFO: Pod "pod-update-activedeadlineseconds-2a99450f-4cb5-4e83-8255-7c371d989a59" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:43:30.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1391" for this suite.
Mar 24 03:43:36.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:43:36.434: INFO: namespace pods-1391 deletion completed in 6.061331632s

• [SLOW TEST:12.713 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:43:36.434: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 24 03:43:38.572: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:43:38.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2895" for this suite.
Mar 24 03:43:44.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:43:44.647: INFO: namespace container-runtime-2895 deletion completed in 6.060546543s

• [SLOW TEST:8.213 seconds]
[k8s.io] Container Runtime
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:43:44.647: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5588
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Mar 24 03:43:44.769: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar 24 03:43:44.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 create -f - --namespace=kubectl-5588'
Mar 24 03:43:45.122: INFO: stderr: ""
Mar 24 03:43:45.122: INFO: stdout: "service/redis-slave created\n"
Mar 24 03:43:45.122: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar 24 03:43:45.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 create -f - --namespace=kubectl-5588'
Mar 24 03:43:45.365: INFO: stderr: ""
Mar 24 03:43:45.365: INFO: stdout: "service/redis-master created\n"
Mar 24 03:43:45.365: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 24 03:43:45.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 create -f - --namespace=kubectl-5588'
Mar 24 03:43:45.623: INFO: stderr: ""
Mar 24 03:43:45.623: INFO: stdout: "service/frontend created\n"
Mar 24 03:43:45.623: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar 24 03:43:45.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 create -f - --namespace=kubectl-5588'
Mar 24 03:43:45.864: INFO: stderr: ""
Mar 24 03:43:45.864: INFO: stdout: "deployment.apps/frontend created\n"
Mar 24 03:43:45.864: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 24 03:43:45.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 create -f - --namespace=kubectl-5588'
Mar 24 03:43:46.089: INFO: stderr: ""
Mar 24 03:43:46.089: INFO: stdout: "deployment.apps/redis-master created\n"
Mar 24 03:43:46.089: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar 24 03:43:46.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 create -f - --namespace=kubectl-5588'
Mar 24 03:43:46.314: INFO: stderr: ""
Mar 24 03:43:46.314: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Mar 24 03:43:46.314: INFO: Waiting for all frontend pods to be Running.
Mar 24 03:44:01.365: INFO: Waiting for frontend to serve content.
Mar 24 03:44:02.402: INFO: Trying to add a new entry to the guestbook.
Mar 24 03:44:02.417: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar 24 03:44:02.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete --grace-period=0 --force -f - --namespace=kubectl-5588'
Mar 24 03:44:02.520: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:44:02.520: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar 24 03:44:02.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete --grace-period=0 --force -f - --namespace=kubectl-5588'
Mar 24 03:44:02.650: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:44:02.650: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 24 03:44:02.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete --grace-period=0 --force -f - --namespace=kubectl-5588'
Mar 24 03:44:02.756: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:44:02.756: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 24 03:44:02.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete --grace-period=0 --force -f - --namespace=kubectl-5588'
Mar 24 03:44:02.860: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:44:02.860: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 24 03:44:02.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete --grace-period=0 --force -f - --namespace=kubectl-5588'
Mar 24 03:44:02.938: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:44:02.938: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 24 03:44:02.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete --grace-period=0 --force -f - --namespace=kubectl-5588'
Mar 24 03:44:03.019: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:44:03.019: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:44:03.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5588" for this suite.
Mar 24 03:44:43.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:44:43.089: INFO: namespace kubectl-5588 deletion completed in 40.067246113s

• [SLOW TEST:58.442 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:44:43.089: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2848
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 24 03:44:43.220: INFO: Waiting up to 5m0s for pod "pod-cbcbb8ff-b32c-4b3c-a58c-7a5ff53f8d23" in namespace "emptydir-2848" to be "success or failure"
Mar 24 03:44:43.222: INFO: Pod "pod-cbcbb8ff-b32c-4b3c-a58c-7a5ff53f8d23": Phase="Pending", Reason="", readiness=false. Elapsed: 1.995083ms
Mar 24 03:44:45.225: INFO: Pod "pod-cbcbb8ff-b32c-4b3c-a58c-7a5ff53f8d23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004718131s
STEP: Saw pod success
Mar 24 03:44:45.225: INFO: Pod "pod-cbcbb8ff-b32c-4b3c-a58c-7a5ff53f8d23" satisfied condition "success or failure"
Mar 24 03:44:45.227: INFO: Trying to get logs from node kind-worker2 pod pod-cbcbb8ff-b32c-4b3c-a58c-7a5ff53f8d23 container test-container: <nil>
STEP: delete the pod
Mar 24 03:44:45.241: INFO: Waiting for pod pod-cbcbb8ff-b32c-4b3c-a58c-7a5ff53f8d23 to disappear
Mar 24 03:44:45.243: INFO: Pod pod-cbcbb8ff-b32c-4b3c-a58c-7a5ff53f8d23 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:44:45.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2848" for this suite.
Mar 24 03:44:51.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:44:51.306: INFO: namespace emptydir-2848 deletion completed in 6.060899748s

• [SLOW TEST:8.217 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:44:51.306: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7841
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 24 03:44:51.433: INFO: Waiting up to 5m0s for pod "pod-bf1c9fd6-976b-4775-96bd-cefee17ae2b7" in namespace "emptydir-7841" to be "success or failure"
Mar 24 03:44:51.435: INFO: Pod "pod-bf1c9fd6-976b-4775-96bd-cefee17ae2b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.774649ms
Mar 24 03:44:53.437: INFO: Pod "pod-bf1c9fd6-976b-4775-96bd-cefee17ae2b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004352922s
STEP: Saw pod success
Mar 24 03:44:53.437: INFO: Pod "pod-bf1c9fd6-976b-4775-96bd-cefee17ae2b7" satisfied condition "success or failure"
Mar 24 03:44:53.439: INFO: Trying to get logs from node kind-worker pod pod-bf1c9fd6-976b-4775-96bd-cefee17ae2b7 container test-container: <nil>
STEP: delete the pod
Mar 24 03:44:53.450: INFO: Waiting for pod pod-bf1c9fd6-976b-4775-96bd-cefee17ae2b7 to disappear
Mar 24 03:44:53.452: INFO: Pod pod-bf1c9fd6-976b-4775-96bd-cefee17ae2b7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:44:53.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7841" for this suite.
Mar 24 03:44:59.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:44:59.515: INFO: namespace emptydir-7841 deletion completed in 6.060263508s

• [SLOW TEST:8.209 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:44:59.515: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4943
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:44:59.638: INFO: Creating ReplicaSet my-hostname-basic-14a9be65-8af1-43c7-b068-27a87c4db83f
Mar 24 03:44:59.644: INFO: Pod name my-hostname-basic-14a9be65-8af1-43c7-b068-27a87c4db83f: Found 0 pods out of 1
Mar 24 03:45:04.647: INFO: Pod name my-hostname-basic-14a9be65-8af1-43c7-b068-27a87c4db83f: Found 1 pods out of 1
Mar 24 03:45:04.647: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-14a9be65-8af1-43c7-b068-27a87c4db83f" is running
Mar 24 03:45:04.649: INFO: Pod "my-hostname-basic-14a9be65-8af1-43c7-b068-27a87c4db83f-k9zbp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-24 03:44:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-24 03:45:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-24 03:45:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-24 03:44:59 +0000 UTC Reason: Message:}])
Mar 24 03:45:04.649: INFO: Trying to dial the pod
Mar 24 03:45:09.657: INFO: Controller my-hostname-basic-14a9be65-8af1-43c7-b068-27a87c4db83f: Got expected result from replica 1 [my-hostname-basic-14a9be65-8af1-43c7-b068-27a87c4db83f-k9zbp]: "my-hostname-basic-14a9be65-8af1-43c7-b068-27a87c4db83f-k9zbp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:45:09.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4943" for this suite.
Mar 24 03:45:15.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:45:15.728: INFO: namespace replicaset-4943 deletion completed in 6.067384335s

• [SLOW TEST:16.213 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:45:15.728: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8548
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:45:15.853: INFO: Creating deployment "test-recreate-deployment"
Mar 24 03:45:15.856: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 24 03:45:15.860: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 24 03:45:17.865: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 24 03:45:17.867: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 24 03:45:17.871: INFO: Updating deployment test-recreate-deployment
Mar 24 03:45:17.871: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar 24 03:45:17.908: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-8548,SelfLink:/apis/apps/v1/namespaces/deployment-8548/deployments/test-recreate-deployment,UID:a1b451ab-061a-4043-b26f-1fbc12c83f05,ResourceVersion:15210,Generation:2,CreationTimestamp:2020-03-24 03:45:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2020-03-24 03:45:17 +0000 UTC 2020-03-24 03:45:17 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2020-03-24 03:45:17 +0000 UTC 2020-03-24 03:45:15 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Mar 24 03:45:17.911: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-8548,SelfLink:/apis/apps/v1/namespaces/deployment-8548/replicasets/test-recreate-deployment-5c8c9cc69d,UID:6de918b4-1d83-4b86-8cd5-20fbb648df14,ResourceVersion:15207,Generation:1,CreationTimestamp:2020-03-24 03:45:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment a1b451ab-061a-4043-b26f-1fbc12c83f05 0xc002c6c5a7 0xc002c6c5a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 24 03:45:17.911: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 24 03:45:17.911: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-8548,SelfLink:/apis/apps/v1/namespaces/deployment-8548/replicasets/test-recreate-deployment-6df85df6b9,UID:9beeedd1-3b82-4e8c-a630-109d06c5c797,ResourceVersion:15199,Generation:2,CreationTimestamp:2020-03-24 03:45:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment a1b451ab-061a-4043-b26f-1fbc12c83f05 0xc002c6c677 0xc002c6c678}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 24 03:45:17.913: INFO: Pod "test-recreate-deployment-5c8c9cc69d-dpqst" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-dpqst,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-8548,SelfLink:/api/v1/namespaces/deployment-8548/pods/test-recreate-deployment-5c8c9cc69d-dpqst,UID:a9777bd9-f2bf-40d1-afc4-c7becc63636d,ResourceVersion:15211,Generation:0,CreationTimestamp:2020-03-24 03:45:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 6de918b4-1d83-4b86-8cd5-20fbb648df14 0xc0027610b7 0xc0027610b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tthkd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tthkd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tthkd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002761140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002761160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:45:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:45:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:45:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:45:17 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.2,PodIP:,StartTime:2020-03-24 03:45:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:45:17.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8548" for this suite.
Mar 24 03:45:23.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:45:23.979: INFO: namespace deployment-8548 deletion completed in 6.063366316s

• [SLOW TEST:8.251 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:45:23.980: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1475
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0324 03:45:25.134956      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 24 03:45:25.135: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:45:25.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1475" for this suite.
Mar 24 03:45:31.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:45:31.200: INFO: namespace gc-1475 deletion completed in 6.062956835s

• [SLOW TEST:7.220 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:45:31.200: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3918
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 24 03:45:31.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3918'
Mar 24 03:45:31.407: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 24 03:45:31.407: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1426
Mar 24 03:45:31.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3918'
Mar 24 03:45:31.498: INFO: stderr: ""
Mar 24 03:45:31.498: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:45:31.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3918" for this suite.
Mar 24 03:45:53.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:45:53.563: INFO: namespace kubectl-3918 deletion completed in 22.062717397s

• [SLOW TEST:22.363 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:45:53.564: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9122
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:45:53.692: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 24 03:45:58.695: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 24 03:45:58.695: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar 24 03:45:58.708: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-9122,SelfLink:/apis/apps/v1/namespaces/deployment-9122/deployments/test-cleanup-deployment,UID:9e7271b7-2111-412b-a3dc-1c554561135c,ResourceVersion:15400,Generation:1,CreationTimestamp:2020-03-24 03:45:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Mar 24 03:45:58.711: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-9122,SelfLink:/apis/apps/v1/namespaces/deployment-9122/replicasets/test-cleanup-deployment-55bbcbc84c,UID:51df1c15-7fdf-4d8d-9ef9-eacb9fc6f455,ResourceVersion:15402,Generation:1,CreationTimestamp:2020-03-24 03:45:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 9e7271b7-2111-412b-a3dc-1c554561135c 0xc002e76fe7 0xc002e76fe8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 24 03:45:58.711: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar 24 03:45:58.711: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-9122,SelfLink:/apis/apps/v1/namespaces/deployment-9122/replicasets/test-cleanup-controller,UID:c48713d5-241a-4414-af83-865aa4e23c15,ResourceVersion:15401,Generation:1,CreationTimestamp:2020-03-24 03:45:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 9e7271b7-2111-412b-a3dc-1c554561135c 0xc002e76f17 0xc002e76f18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar 24 03:45:58.715: INFO: Pod "test-cleanup-controller-przpp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-przpp,GenerateName:test-cleanup-controller-,Namespace:deployment-9122,SelfLink:/api/v1/namespaces/deployment-9122/pods/test-cleanup-controller-przpp,UID:c22767b2-87c4-40de-bf0e-2439ca8a109a,ResourceVersion:15392,Generation:0,CreationTimestamp:2020-03-24 03:45:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller c48713d5-241a-4414-af83-865aa4e23c15 0xc002d50967 0xc002d50968}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lxbxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lxbxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lxbxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d509e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d50a00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:45:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:45:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:45:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:45:53 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.2,PodIP:10.244.1.125,StartTime:2020-03-24 03:45:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-24 03:45:54 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://672c10896f65863dcd7a80d7f50706ffe66b4d5a0a1de3dca2b36f003f448619}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 24 03:45:58.716: INFO: Pod "test-cleanup-deployment-55bbcbc84c-27g6h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-27g6h,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-9122,SelfLink:/api/v1/namespaces/deployment-9122/pods/test-cleanup-deployment-55bbcbc84c-27g6h,UID:5f503224-bcdf-49aa-aad1-836c833dde5f,ResourceVersion:15403,Generation:0,CreationTimestamp:2020-03-24 03:45:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 51df1c15-7fdf-4d8d-9ef9-eacb9fc6f455 0xc002d50ad7 0xc002d50ad8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lxbxq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lxbxq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-lxbxq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d50b40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d50b60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:45:58.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9122" for this suite.
Mar 24 03:46:04.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:46:04.797: INFO: namespace deployment-9122 deletion completed in 6.078164231s

• [SLOW TEST:11.233 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:46:04.797: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4227
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar 24 03:46:07.448: INFO: Successfully updated pod "annotationupdate8b121b5f-713d-4044-a2e3-132073aaf90c"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:46:11.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4227" for this suite.
Mar 24 03:46:33.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:46:33.529: INFO: namespace projected-4227 deletion completed in 22.061871595s

• [SLOW TEST:28.732 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:46:33.529: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-324f2092-e78d-4e9b-825b-4476841b9782
STEP: Creating a pod to test consume configMaps
Mar 24 03:46:33.659: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c31fce8b-ee3f-4865-870c-c9613a5101ea" in namespace "projected-3795" to be "success or failure"
Mar 24 03:46:33.661: INFO: Pod "pod-projected-configmaps-c31fce8b-ee3f-4865-870c-c9613a5101ea": Phase="Pending", Reason="", readiness=false. Elapsed: 1.771768ms
Mar 24 03:46:35.664: INFO: Pod "pod-projected-configmaps-c31fce8b-ee3f-4865-870c-c9613a5101ea": Phase="Running", Reason="", readiness=true. Elapsed: 2.004674764s
Mar 24 03:46:37.667: INFO: Pod "pod-projected-configmaps-c31fce8b-ee3f-4865-870c-c9613a5101ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007633931s
STEP: Saw pod success
Mar 24 03:46:37.667: INFO: Pod "pod-projected-configmaps-c31fce8b-ee3f-4865-870c-c9613a5101ea" satisfied condition "success or failure"
Mar 24 03:46:37.669: INFO: Trying to get logs from node kind-worker pod pod-projected-configmaps-c31fce8b-ee3f-4865-870c-c9613a5101ea container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 03:46:37.680: INFO: Waiting for pod pod-projected-configmaps-c31fce8b-ee3f-4865-870c-c9613a5101ea to disappear
Mar 24 03:46:37.682: INFO: Pod pod-projected-configmaps-c31fce8b-ee3f-4865-870c-c9613a5101ea no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:46:37.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3795" for this suite.
Mar 24 03:46:43.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:46:43.745: INFO: namespace projected-3795 deletion completed in 6.060175522s

• [SLOW TEST:10.216 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:46:43.745: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1026
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:46:45.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1026" for this suite.
Mar 24 03:47:35.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:47:35.950: INFO: namespace kubelet-test-1026 deletion completed in 50.065087562s

• [SLOW TEST:52.205 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:47:35.951: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:47:36.082: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 24 03:47:41.085: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 24 03:47:41.085: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 24 03:47:43.088: INFO: Creating deployment "test-rollover-deployment"
Mar 24 03:47:43.093: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 24 03:47:45.098: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 24 03:47:45.103: INFO: Ensure that both replica sets have 1 created replica
Mar 24 03:47:45.107: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 24 03:47:45.111: INFO: Updating deployment test-rollover-deployment
Mar 24 03:47:45.111: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 24 03:47:47.117: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 24 03:47:47.121: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 24 03:47:47.125: INFO: all replica sets need to contain the pod-template-hash label
Mar 24 03:47:47.125: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618463, loc:(*time.Location)(0x7eafa20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618463, loc:(*time.Location)(0x7eafa20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618466, loc:(*time.Location)(0x7eafa20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618463, loc:(*time.Location)(0x7eafa20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:47:49.130: INFO: all replica sets need to contain the pod-template-hash label
Mar 24 03:47:49.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618463, loc:(*time.Location)(0x7eafa20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618463, loc:(*time.Location)(0x7eafa20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618466, loc:(*time.Location)(0x7eafa20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618463, loc:(*time.Location)(0x7eafa20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:47:51.130: INFO: all replica sets need to contain the pod-template-hash label
Mar 24 03:47:51.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618463, loc:(*time.Location)(0x7eafa20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618463, loc:(*time.Location)(0x7eafa20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618466, loc:(*time.Location)(0x7eafa20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618463, loc:(*time.Location)(0x7eafa20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:47:53.130: INFO: all replica sets need to contain the pod-template-hash label
Mar 24 03:47:53.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618463, loc:(*time.Location)(0x7eafa20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618463, loc:(*time.Location)(0x7eafa20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618466, loc:(*time.Location)(0x7eafa20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618463, loc:(*time.Location)(0x7eafa20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:47:55.130: INFO: all replica sets need to contain the pod-template-hash label
Mar 24 03:47:55.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618463, loc:(*time.Location)(0x7eafa20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618463, loc:(*time.Location)(0x7eafa20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618466, loc:(*time.Location)(0x7eafa20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720618463, loc:(*time.Location)(0x7eafa20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:47:57.130: INFO: 
Mar 24 03:47:57.130: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar 24 03:47:57.136: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-2917,SelfLink:/apis/apps/v1/namespaces/deployment-2917/deployments/test-rollover-deployment,UID:fb5e1578-c41d-4742-9d50-329474bbe4f8,ResourceVersion:15772,Generation:2,CreationTimestamp:2020-03-24 03:47:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-03-24 03:47:43 +0000 UTC 2020-03-24 03:47:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-03-24 03:47:56 +0000 UTC 2020-03-24 03:47:43 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar 24 03:47:57.138: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-2917,SelfLink:/apis/apps/v1/namespaces/deployment-2917/replicasets/test-rollover-deployment-854595fc44,UID:7905967f-b104-442a-b0c4-dda8a1bda54c,ResourceVersion:15761,Generation:2,CreationTimestamp:2020-03-24 03:47:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment fb5e1578-c41d-4742-9d50-329474bbe4f8 0xc002ca5e77 0xc002ca5e78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar 24 03:47:57.138: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 24 03:47:57.138: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-2917,SelfLink:/apis/apps/v1/namespaces/deployment-2917/replicasets/test-rollover-controller,UID:a5763567-6273-4f2a-9a75-e8e31785b205,ResourceVersion:15770,Generation:2,CreationTimestamp:2020-03-24 03:47:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment fb5e1578-c41d-4742-9d50-329474bbe4f8 0xc002ca5da7 0xc002ca5da8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 24 03:47:57.138: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-2917,SelfLink:/apis/apps/v1/namespaces/deployment-2917/replicasets/test-rollover-deployment-9b8b997cf,UID:1bdd9200-76fd-4cf0-9f9b-9f79f9ccb122,ResourceVersion:15731,Generation:2,CreationTimestamp:2020-03-24 03:47:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment fb5e1578-c41d-4742-9d50-329474bbe4f8 0xc002ca5f40 0xc002ca5f41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 24 03:47:57.141: INFO: Pod "test-rollover-deployment-854595fc44-98pml" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-98pml,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-2917,SelfLink:/api/v1/namespaces/deployment-2917/pods/test-rollover-deployment-854595fc44-98pml,UID:517374b4-e650-4457-b721-d56426a713a5,ResourceVersion:15744,Generation:0,CreationTimestamp:2020-03-24 03:47:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 7905967f-b104-442a-b0c4-dda8a1bda54c 0xc003306b07 0xc003306b08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wz296 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wz296,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-wz296 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003306b80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003306ba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:47:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:47:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:47:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-24 03:47:45 +0000 UTC  }],Message:,Reason:,HostIP:172.17.0.4,PodIP:10.244.2.148,StartTime:2020-03-24 03:47:45 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-03-24 03:47:46 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://5a71ea8eeb10e95628a8b3200c1d8a1a7a906bf75e9364ee892af9bab604348f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:47:57.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2917" for this suite.
Mar 24 03:48:03.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:48:03.217: INFO: namespace deployment-2917 deletion completed in 6.073868449s

• [SLOW TEST:27.266 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:48:03.217: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3056
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-3056/secret-test-c5c14847-eca8-4b4b-80c8-ded9f4c81e85
STEP: Creating a pod to test consume secrets
Mar 24 03:48:03.348: INFO: Waiting up to 5m0s for pod "pod-configmaps-48532bd8-5ed8-46b2-a2f9-9bd749eff096" in namespace "secrets-3056" to be "success or failure"
Mar 24 03:48:03.350: INFO: Pod "pod-configmaps-48532bd8-5ed8-46b2-a2f9-9bd749eff096": Phase="Pending", Reason="", readiness=false. Elapsed: 1.816582ms
Mar 24 03:48:05.353: INFO: Pod "pod-configmaps-48532bd8-5ed8-46b2-a2f9-9bd749eff096": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005187701s
Mar 24 03:48:07.356: INFO: Pod "pod-configmaps-48532bd8-5ed8-46b2-a2f9-9bd749eff096": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008488094s
STEP: Saw pod success
Mar 24 03:48:07.357: INFO: Pod "pod-configmaps-48532bd8-5ed8-46b2-a2f9-9bd749eff096" satisfied condition "success or failure"
Mar 24 03:48:07.359: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-48532bd8-5ed8-46b2-a2f9-9bd749eff096 container env-test: <nil>
STEP: delete the pod
Mar 24 03:48:07.370: INFO: Waiting for pod pod-configmaps-48532bd8-5ed8-46b2-a2f9-9bd749eff096 to disappear
Mar 24 03:48:07.372: INFO: Pod pod-configmaps-48532bd8-5ed8-46b2-a2f9-9bd749eff096 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:48:07.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3056" for this suite.
Mar 24 03:48:13.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:48:13.436: INFO: namespace secrets-3056 deletion completed in 6.06194615s

• [SLOW TEST:10.219 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:48:13.436: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5375
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5375
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5375
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5375
Mar 24 03:48:13.569: INFO: Found 0 stateful pods, waiting for 1
Mar 24 03:48:23.572: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 24 03:48:23.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-5375 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 24 03:48:23.760: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 24 03:48:23.760: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 24 03:48:23.760: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 24 03:48:23.763: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 24 03:48:33.766: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 03:48:33.766: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 03:48:33.775: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999737s
Mar 24 03:48:34.778: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996983526s
Mar 24 03:48:35.781: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994063214s
Mar 24 03:48:36.784: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990970149s
Mar 24 03:48:37.787: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988180307s
Mar 24 03:48:38.790: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.985037433s
Mar 24 03:48:39.794: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.982043315s
Mar 24 03:48:40.797: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.978722229s
Mar 24 03:48:41.800: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.975689459s
Mar 24 03:48:42.804: INFO: Verifying statefulset ss doesn't scale past 1 for another 972.283079ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5375
Mar 24 03:48:43.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-5375 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 24 03:48:43.987: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar 24 03:48:43.987: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 24 03:48:43.987: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 24 03:48:43.990: INFO: Found 1 stateful pods, waiting for 3
Mar 24 03:48:53.994: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 03:48:53.994: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 03:48:53.994: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 24 03:48:53.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-5375 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 24 03:48:54.193: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 24 03:48:54.193: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 24 03:48:54.193: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 24 03:48:54.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-5375 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 24 03:48:54.381: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 24 03:48:54.381: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 24 03:48:54.381: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 24 03:48:54.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-5375 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 24 03:48:54.566: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 24 03:48:54.567: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 24 03:48:54.567: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 24 03:48:54.567: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 03:48:54.569: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 24 03:49:04.574: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 03:49:04.574: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 03:49:04.574: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 03:49:04.581: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999704s
Mar 24 03:49:05.585: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997341645s
Mar 24 03:49:06.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993955001s
Mar 24 03:49:07.591: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990911168s
Mar 24 03:49:08.594: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.987534546s
Mar 24 03:49:09.598: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.984338941s
Mar 24 03:49:10.601: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980978573s
Mar 24 03:49:11.604: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97755319s
Mar 24 03:49:12.607: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.974549844s
Mar 24 03:49:13.610: INFO: Verifying statefulset ss doesn't scale past 3 for another 971.398406ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5375
Mar 24 03:49:14.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-5375 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 24 03:49:14.795: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar 24 03:49:14.795: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 24 03:49:14.795: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 24 03:49:14.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-5375 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 24 03:49:14.969: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar 24 03:49:14.969: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 24 03:49:14.969: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 24 03:49:14.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 exec --namespace=statefulset-5375 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 24 03:49:15.137: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar 24 03:49:15.137: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 24 03:49:15.137: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 24 03:49:15.137: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar 24 03:49:35.150: INFO: Deleting all statefulset in ns statefulset-5375
Mar 24 03:49:35.153: INFO: Scaling statefulset ss to 0
Mar 24 03:49:35.159: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 03:49:35.161: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:49:35.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5375" for this suite.
Mar 24 03:49:41.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:49:41.232: INFO: namespace statefulset-5375 deletion completed in 6.061827414s

• [SLOW TEST:87.796 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:49:41.232: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6540
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 24 03:49:41.361: INFO: Waiting up to 5m0s for pod "pod-d89020af-4b8a-4865-a407-653a01f298bd" in namespace "emptydir-6540" to be "success or failure"
Mar 24 03:49:41.363: INFO: Pod "pod-d89020af-4b8a-4865-a407-653a01f298bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110654ms
Mar 24 03:49:43.365: INFO: Pod "pod-d89020af-4b8a-4865-a407-653a01f298bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004695079s
Mar 24 03:49:45.369: INFO: Pod "pod-d89020af-4b8a-4865-a407-653a01f298bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008084346s
STEP: Saw pod success
Mar 24 03:49:45.369: INFO: Pod "pod-d89020af-4b8a-4865-a407-653a01f298bd" satisfied condition "success or failure"
Mar 24 03:49:45.371: INFO: Trying to get logs from node kind-worker2 pod pod-d89020af-4b8a-4865-a407-653a01f298bd container test-container: <nil>
STEP: delete the pod
Mar 24 03:49:45.383: INFO: Waiting for pod pod-d89020af-4b8a-4865-a407-653a01f298bd to disappear
Mar 24 03:49:45.386: INFO: Pod pod-d89020af-4b8a-4865-a407-653a01f298bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:49:45.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6540" for this suite.
Mar 24 03:49:51.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:49:51.449: INFO: namespace emptydir-6540 deletion completed in 6.060861666s

• [SLOW TEST:10.216 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:49:51.450: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4823
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Mar 24 03:50:22.100: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:50:22.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0324 03:50:22.100521      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-4823" for this suite.
Mar 24 03:50:28.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:50:28.164: INFO: namespace gc-4823 deletion completed in 6.061465479s

• [SLOW TEST:36.714 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:50:28.164: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9326
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:50:28.288: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar 24 03:50:30.308: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:50:31.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9326" for this suite.
Mar 24 03:50:37.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:50:37.379: INFO: namespace replication-controller-9326 deletion completed in 6.062422111s

• [SLOW TEST:9.214 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:50:37.379: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-3425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 24 03:50:41.523: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3425 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 03:50:41.523: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 03:50:41.624: INFO: Exec stderr: ""
Mar 24 03:50:41.624: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3425 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 03:50:41.624: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 03:50:41.712: INFO: Exec stderr: ""
Mar 24 03:50:41.712: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3425 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 03:50:41.712: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 03:50:41.806: INFO: Exec stderr: ""
Mar 24 03:50:41.806: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3425 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 03:50:41.806: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 03:50:41.899: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 24 03:50:41.899: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3425 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 03:50:41.899: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 03:50:41.991: INFO: Exec stderr: ""
Mar 24 03:50:41.991: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3425 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 03:50:41.991: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 03:50:42.078: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 24 03:50:42.078: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3425 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 03:50:42.078: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 03:50:42.161: INFO: Exec stderr: ""
Mar 24 03:50:42.161: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3425 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 03:50:42.161: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 03:50:42.248: INFO: Exec stderr: ""
Mar 24 03:50:42.248: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3425 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 03:50:42.248: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 03:50:42.347: INFO: Exec stderr: ""
Mar 24 03:50:42.347: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3425 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 24 03:50:42.347: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
Mar 24 03:50:42.429: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:50:42.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3425" for this suite.
Mar 24 03:51:22.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:51:22.494: INFO: namespace e2e-kubelet-etc-hosts-3425 deletion completed in 40.06246736s

• [SLOW TEST:45.115 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:51:22.494: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3058
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Mar 24 03:51:32.635: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:51:32.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0324 03:51:32.635590      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3058" for this suite.
Mar 24 03:51:38.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:51:38.702: INFO: namespace gc-3058 deletion completed in 6.064835561s

• [SLOW TEST:16.208 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:51:38.703: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5955
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-1434285d-4a29-4849-a431-7864b38e414e
STEP: Creating configMap with name cm-test-opt-upd-d9ef2b49-66bd-419d-8de8-ec8e0a3bf3d9
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1434285d-4a29-4849-a431-7864b38e414e
STEP: Updating configmap cm-test-opt-upd-d9ef2b49-66bd-419d-8de8-ec8e0a3bf3d9
STEP: Creating configMap with name cm-test-opt-create-2c55e6a3-329a-4b69-b700-568338c7c291
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:52:45.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5955" for this suite.
Mar 24 03:53:07.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:53:07.124: INFO: namespace configmap-5955 deletion completed in 22.060756923s

• [SLOW TEST:88.422 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:53:07.125: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8584
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:53:07.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1582f8e-fff3-4538-9cf9-04a23ca99937" in namespace "downward-api-8584" to be "success or failure"
Mar 24 03:53:07.255: INFO: Pod "downwardapi-volume-d1582f8e-fff3-4538-9cf9-04a23ca99937": Phase="Pending", Reason="", readiness=false. Elapsed: 1.782658ms
Mar 24 03:53:09.258: INFO: Pod "downwardapi-volume-d1582f8e-fff3-4538-9cf9-04a23ca99937": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004462993s
STEP: Saw pod success
Mar 24 03:53:09.258: INFO: Pod "downwardapi-volume-d1582f8e-fff3-4538-9cf9-04a23ca99937" satisfied condition "success or failure"
Mar 24 03:53:09.260: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-d1582f8e-fff3-4538-9cf9-04a23ca99937 container client-container: <nil>
STEP: delete the pod
Mar 24 03:53:09.271: INFO: Waiting for pod downwardapi-volume-d1582f8e-fff3-4538-9cf9-04a23ca99937 to disappear
Mar 24 03:53:09.273: INFO: Pod downwardapi-volume-d1582f8e-fff3-4538-9cf9-04a23ca99937 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:53:09.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8584" for this suite.
Mar 24 03:53:15.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:53:15.335: INFO: namespace downward-api-8584 deletion completed in 6.059455106s

• [SLOW TEST:8.210 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:53:15.336: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9487
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar 24 03:53:25.501: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0324 03:53:25.501387      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 24 03:53:25.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9487" for this suite.
Mar 24 03:53:31.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:53:31.567: INFO: namespace gc-9487 deletion completed in 6.063744107s

• [SLOW TEST:16.231 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:53:31.568: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-6a264a3d-6b75-4ac7-b6b2-aa646fce6bcf
STEP: Creating a pod to test consume configMaps
Mar 24 03:53:31.698: INFO: Waiting up to 5m0s for pod "pod-configmaps-64dcba66-775e-4ca5-a5c0-e68914a97ee5" in namespace "configmap-8141" to be "success or failure"
Mar 24 03:53:31.700: INFO: Pod "pod-configmaps-64dcba66-775e-4ca5-a5c0-e68914a97ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.815223ms
Mar 24 03:53:33.703: INFO: Pod "pod-configmaps-64dcba66-775e-4ca5-a5c0-e68914a97ee5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004696158s
STEP: Saw pod success
Mar 24 03:53:33.703: INFO: Pod "pod-configmaps-64dcba66-775e-4ca5-a5c0-e68914a97ee5" satisfied condition "success or failure"
Mar 24 03:53:33.705: INFO: Trying to get logs from node kind-worker pod pod-configmaps-64dcba66-775e-4ca5-a5c0-e68914a97ee5 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 03:53:33.715: INFO: Waiting for pod pod-configmaps-64dcba66-775e-4ca5-a5c0-e68914a97ee5 to disappear
Mar 24 03:53:33.716: INFO: Pod pod-configmaps-64dcba66-775e-4ca5-a5c0-e68914a97ee5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:53:33.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8141" for this suite.
Mar 24 03:53:39.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:53:39.781: INFO: namespace configmap-8141 deletion completed in 6.062821725s

• [SLOW TEST:8.214 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:53:39.782: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9136
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Mar 24 03:53:39.905: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 24 03:53:39.910: INFO: Waiting for terminating namespaces to be deleted...
Mar 24 03:53:39.912: INFO: 
Logging pods the kubelet thinks is on node kind-worker before test
Mar 24 03:53:39.915: INFO: kindnet-gsvnh from kube-system started at 2020-03-24 02:40:14 +0000 UTC (1 container statuses recorded)
Mar 24 03:53:39.915: INFO: 	Container kindnet-cni ready: true, restart count 0
Mar 24 03:53:39.915: INFO: kube-proxy-kgzdn from kube-system started at 2020-03-24 02:40:14 +0000 UTC (1 container statuses recorded)
Mar 24 03:53:39.915: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 24 03:53:39.915: INFO: 
Logging pods the kubelet thinks is on node kind-worker2 before test
Mar 24 03:53:39.919: INFO: e2e-conformance-test from conformance started at 2020-03-24 02:44:46 +0000 UTC (1 container statuses recorded)
Mar 24 03:53:39.919: INFO: 	Container conformance-container ready: true, restart count 0
Mar 24 03:53:39.919: INFO: kube-proxy-2vhfg from kube-system started at 2020-03-24 02:40:13 +0000 UTC (1 container statuses recorded)
Mar 24 03:53:39.919: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 24 03:53:39.919: INFO: kindnet-d95hs from kube-system started at 2020-03-24 02:40:13 +0000 UTC (1 container statuses recorded)
Mar 24 03:53:39.919: INFO: 	Container kindnet-cni ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-282a9b87-32df-470f-a979-69e47e2ba095 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-282a9b87-32df-470f-a979-69e47e2ba095 off the node kind-worker2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-282a9b87-32df-470f-a979-69e47e2ba095
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:53:43.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9136" for this suite.
Mar 24 03:54:11.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:54:12.020: INFO: namespace sched-pred-9136 deletion completed in 28.060081974s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:32.239 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:54:12.021: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4371
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:54:12.146: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:54:13.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4371" for this suite.
Mar 24 03:54:19.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:54:19.256: INFO: namespace custom-resource-definition-4371 deletion completed in 6.062495886s

• [SLOW TEST:7.236 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:54:19.257: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:54:19.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 create -f - --namespace=kubectl-5158'
Mar 24 03:54:19.818: INFO: stderr: ""
Mar 24 03:54:19.818: INFO: stdout: "replicationcontroller/redis-master created\n"
Mar 24 03:54:19.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 create -f - --namespace=kubectl-5158'
Mar 24 03:54:20.050: INFO: stderr: ""
Mar 24 03:54:20.050: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 24 03:54:21.053: INFO: Selector matched 1 pods for map[app:redis]
Mar 24 03:54:21.053: INFO: Found 0 / 1
Mar 24 03:54:22.053: INFO: Selector matched 1 pods for map[app:redis]
Mar 24 03:54:22.053: INFO: Found 1 / 1
Mar 24 03:54:22.053: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 24 03:54:22.055: INFO: Selector matched 1 pods for map[app:redis]
Mar 24 03:54:22.055: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 24 03:54:22.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 describe pod redis-master-bs2hw --namespace=kubectl-5158'
Mar 24 03:54:22.144: INFO: stderr: ""
Mar 24 03:54:22.144: INFO: stdout: "Name:           redis-master-bs2hw\nNamespace:      kubectl-5158\nPriority:       0\nNode:           kind-worker/172.17.0.4\nStart Time:     Tue, 24 Mar 2020 03:54:19 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             10.244.2.162\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://f5f4730778b3987f5f9147c84d9c9242794717f541777d7ae70ddf2e8eb012ae\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 24 Mar 2020 03:54:20 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-kl7qz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-kl7qz:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-kl7qz\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                  Message\n  ----    ------     ----  ----                  -------\n  Normal  Scheduled  3s    default-scheduler     Successfully assigned kubectl-5158/redis-master-bs2hw to kind-worker\n  Normal  Pulled     2s    kubelet, kind-worker  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, kind-worker  Created container redis-master\n  Normal  Started    2s    kubelet, kind-worker  Started container redis-master\n"
Mar 24 03:54:22.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 describe rc redis-master --namespace=kubectl-5158'
Mar 24 03:54:22.233: INFO: stderr: ""
Mar 24 03:54:22.233: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-5158\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-bs2hw\n"
Mar 24 03:54:22.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 describe service redis-master --namespace=kubectl-5158'
Mar 24 03:54:22.319: INFO: stderr: ""
Mar 24 03:54:22.319: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-5158\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.110.0.145\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.2.162:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 24 03:54:22.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 describe node kind-control-plane'
Mar 24 03:54:22.414: INFO: stderr: ""
Mar 24 03:54:22.414: INFO: stdout: "Name:               kind-control-plane\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=kind-control-plane\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 24 Mar 2020 02:39:39 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 24 Mar 2020 03:54:11 +0000   Tue, 24 Mar 2020 02:39:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 24 Mar 2020 03:54:11 +0000   Tue, 24 Mar 2020 02:39:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 24 Mar 2020 03:54:11 +0000   Tue, 24 Mar 2020 02:39:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 24 Mar 2020 03:54:11 +0000   Tue, 24 Mar 2020 02:40:09 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.17.0.3\n  Hostname:    kind-control-plane\nCapacity:\n cpu:                8\n ephemeral-storage:  253882800Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             53582972Ki\n pods:               110\nAllocatable:\n cpu:                8\n ephemeral-storage:  253882800Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             53582972Ki\n pods:               110\nSystem Info:\n Machine ID:                 c90aa421a6dc49c19f00ed013e01d10f\n System UUID:                ec722af0-9391-4982-a403-36cd565ac134\n Boot ID:                    a6b0f4fe-8e2c-439a-89c7-d31c7d031858\n Kernel Version:             4.15.0-1044-gke\n OS Image:                   Ubuntu Eoan Ermine (development branch)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.3.0-27-g54658b88\n Kubelet Version:            v1.15.12-beta.0.7+7f18f85e0e8bcc\n Kube-Proxy Version:         v1.15.12-beta.0.7+7f18f85e0e8bcc\nPodCIDR:                     10.244.0.0/24\nNon-terminated Pods:         (8 in total)\n  Namespace                  Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                          ------------  ----------  ---------------  -------------  ---\n  kube-system                coredns-5d4dd4b4db-l24ng                      100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     74m\n  kube-system                coredns-5d4dd4b4db-thjzq                      100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     74m\n  kube-system                etcd-kind-control-plane                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         73m\n  kube-system                kindnet-xz6k5                                 100m (1%)     100m (1%)   50Mi (0%)        50Mi (0%)      74m\n  kube-system                kube-apiserver-kind-control-plane             250m (3%)     0 (0%)      0 (0%)           0 (0%)         73m\n  kube-system                kube-controller-manager-kind-control-plane    200m (2%)     0 (0%)      0 (0%)           0 (0%)         73m\n  kube-system                kube-proxy-nrwjv                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         74m\n  kube-system                kube-scheduler-kind-control-plane             100m (1%)     0 (0%)      0 (0%)           0 (0%)         73m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                850m (10%)  100m (1%)\n  memory             190Mi (0%)  390Mi (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Mar 24 03:54:22.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 describe namespace kubectl-5158'
Mar 24 03:54:22.499: INFO: stderr: ""
Mar 24 03:54:22.499: INFO: stdout: "Name:         kubectl-5158\nLabels:       e2e-framework=kubectl\n              e2e-run=4cef418f-46f9-4d79-89d9-c27b3098160c\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:54:22.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5158" for this suite.
Mar 24 03:54:44.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:54:44.560: INFO: namespace kubectl-5158 deletion completed in 22.058931325s

• [SLOW TEST:25.304 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:54:44.561: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:55:07.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4493" for this suite.
Mar 24 03:55:13.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:55:13.882: INFO: namespace container-runtime-4493 deletion completed in 6.063806126s

• [SLOW TEST:29.321 seconds]
[k8s.io] Container Runtime
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:55:13.884: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-8873a81a-a107-4a37-9740-3ecf82770e0b
STEP: Creating a pod to test consume secrets
Mar 24 03:55:14.014: INFO: Waiting up to 5m0s for pod "pod-secrets-8f69580f-f15b-421d-a39c-960021a90f6c" in namespace "secrets-5339" to be "success or failure"
Mar 24 03:55:14.016: INFO: Pod "pod-secrets-8f69580f-f15b-421d-a39c-960021a90f6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04012ms
Mar 24 03:55:16.019: INFO: Pod "pod-secrets-8f69580f-f15b-421d-a39c-960021a90f6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0048079s
STEP: Saw pod success
Mar 24 03:55:16.019: INFO: Pod "pod-secrets-8f69580f-f15b-421d-a39c-960021a90f6c" satisfied condition "success or failure"
Mar 24 03:55:16.021: INFO: Trying to get logs from node kind-worker pod pod-secrets-8f69580f-f15b-421d-a39c-960021a90f6c container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:55:16.033: INFO: Waiting for pod pod-secrets-8f69580f-f15b-421d-a39c-960021a90f6c to disappear
Mar 24 03:55:16.036: INFO: Pod pod-secrets-8f69580f-f15b-421d-a39c-960021a90f6c no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:55:16.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5339" for this suite.
Mar 24 03:55:22.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:55:22.102: INFO: namespace secrets-5339 deletion completed in 6.064183315s

• [SLOW TEST:8.219 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:55:22.102: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-762
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:55:26.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-762" for this suite.
Mar 24 03:55:32.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:55:32.300: INFO: namespace kubelet-test-762 deletion completed in 6.062221688s

• [SLOW TEST:10.198 seconds]
[k8s.io] Kubelet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:55:32.301: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-7fb44e83-52db-47a9-95ae-bede7b51c63e
STEP: Creating secret with name secret-projected-all-test-volume-bc0c4cc6-f989-4e14-ba27-4f3f190138c2
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 24 03:55:32.435: INFO: Waiting up to 5m0s for pod "projected-volume-9928f4f6-1be6-40bf-b1c4-44e88dc60357" in namespace "projected-2584" to be "success or failure"
Mar 24 03:55:32.437: INFO: Pod "projected-volume-9928f4f6-1be6-40bf-b1c4-44e88dc60357": Phase="Pending", Reason="", readiness=false. Elapsed: 1.833278ms
Mar 24 03:55:34.440: INFO: Pod "projected-volume-9928f4f6-1be6-40bf-b1c4-44e88dc60357": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004703656s
STEP: Saw pod success
Mar 24 03:55:34.440: INFO: Pod "projected-volume-9928f4f6-1be6-40bf-b1c4-44e88dc60357" satisfied condition "success or failure"
Mar 24 03:55:34.442: INFO: Trying to get logs from node kind-worker pod projected-volume-9928f4f6-1be6-40bf-b1c4-44e88dc60357 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 24 03:55:34.452: INFO: Waiting for pod projected-volume-9928f4f6-1be6-40bf-b1c4-44e88dc60357 to disappear
Mar 24 03:55:34.454: INFO: Pod projected-volume-9928f4f6-1be6-40bf-b1c4-44e88dc60357 no longer exists
[AfterEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:55:34.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2584" for this suite.
Mar 24 03:55:40.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:55:40.519: INFO: namespace projected-2584 deletion completed in 6.062758371s

• [SLOW TEST:8.218 seconds]
[sig-storage] Projected combined
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:55:40.520: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6448
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:55:40.644: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:55:42.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6448" for this suite.
Mar 24 03:56:32.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:56:32.817: INFO: namespace pods-6448 deletion completed in 50.061172099s

• [SLOW TEST:52.297 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:56:32.817: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:56:32.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 version'
Mar 24 03:56:33.016: INFO: stderr: ""
Mar 24 03:56:33.016: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15+\", GitVersion:\"v1.15.12-beta.0.7+7f18f85e0e8bcc\", GitCommit:\"7f18f85e0e8bcc202301245af383e18585e2b5b4\", GitTreeState:\"clean\", BuildDate:\"2020-03-24T00:28:47Z\", GoVersion:\"go1.12.17\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15+\", GitVersion:\"v1.15.12-beta.0.7+7f18f85e0e8bcc\", GitCommit:\"7f18f85e0e8bcc202301245af383e18585e2b5b4\", GitTreeState:\"clean\", BuildDate:\"2020-03-24T00:28:47Z\", GoVersion:\"go1.12.17\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:56:33.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4373" for this suite.
Mar 24 03:56:39.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:56:39.081: INFO: namespace kubectl-4373 deletion completed in 6.061745482s

• [SLOW TEST:6.264 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:56:39.081: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8975
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8975, will wait for the garbage collector to delete the pods
Mar 24 03:56:41.272: INFO: Deleting Job.batch foo took: 4.107703ms
Mar 24 03:56:41.572: INFO: Terminating Job.batch foo pods took: 300.24976ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:57:21.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8975" for this suite.
Mar 24 03:57:27.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:57:27.838: INFO: namespace job-8975 deletion completed in 6.060404092s

• [SLOW TEST:48.757 seconds]
[sig-apps] Job
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:57:27.839: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:57:33.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2769" for this suite.
Mar 24 03:57:39.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:57:39.649: INFO: namespace watch-2769 deletion completed in 6.155992507s

• [SLOW TEST:11.811 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:57:39.650: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3821
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-cead2336-5a9a-44a2-8ba0-da69799a22cc
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:57:39.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3821" for this suite.
Mar 24 03:57:45.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:57:45.841: INFO: namespace configmap-3821 deletion completed in 6.063864534s

• [SLOW TEST:6.192 seconds]
[sig-node] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:57:45.841: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Mar 24 03:57:45.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 create -f - --namespace=kubectl-4927'
Mar 24 03:57:46.195: INFO: stderr: ""
Mar 24 03:57:46.195: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 24 03:57:46.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4927'
Mar 24 03:57:46.286: INFO: stderr: ""
Mar 24 03:57:46.286: INFO: stdout: "update-demo-nautilus-4cb5h update-demo-nautilus-vjbrv "
Mar 24 03:57:46.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-4cb5h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4927'
Mar 24 03:57:46.370: INFO: stderr: ""
Mar 24 03:57:46.370: INFO: stdout: ""
Mar 24 03:57:46.370: INFO: update-demo-nautilus-4cb5h is created but not running
Mar 24 03:57:51.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4927'
Mar 24 03:57:51.455: INFO: stderr: ""
Mar 24 03:57:51.455: INFO: stdout: "update-demo-nautilus-4cb5h update-demo-nautilus-vjbrv "
Mar 24 03:57:51.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-4cb5h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4927'
Mar 24 03:57:51.565: INFO: stderr: ""
Mar 24 03:57:51.565: INFO: stdout: "true"
Mar 24 03:57:51.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-4cb5h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4927'
Mar 24 03:57:51.650: INFO: stderr: ""
Mar 24 03:57:51.650: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 03:57:51.650: INFO: validating pod update-demo-nautilus-4cb5h
Mar 24 03:57:51.654: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 03:57:51.654: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 03:57:51.654: INFO: update-demo-nautilus-4cb5h is verified up and running
Mar 24 03:57:51.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-vjbrv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4927'
Mar 24 03:57:51.740: INFO: stderr: ""
Mar 24 03:57:51.740: INFO: stdout: "true"
Mar 24 03:57:51.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods update-demo-nautilus-vjbrv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4927'
Mar 24 03:57:51.821: INFO: stderr: ""
Mar 24 03:57:51.821: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 03:57:51.821: INFO: validating pod update-demo-nautilus-vjbrv
Mar 24 03:57:51.825: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 03:57:51.825: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 03:57:51.825: INFO: update-demo-nautilus-vjbrv is verified up and running
STEP: using delete to clean up resources
Mar 24 03:57:51.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete --grace-period=0 --force -f - --namespace=kubectl-4927'
Mar 24 03:57:51.911: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:57:51.911: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 24 03:57:51.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4927'
Mar 24 03:57:52.006: INFO: stderr: "No resources found.\n"
Mar 24 03:57:52.006: INFO: stdout: ""
Mar 24 03:57:52.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -l name=update-demo --namespace=kubectl-4927 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 24 03:57:52.098: INFO: stderr: ""
Mar 24 03:57:52.098: INFO: stdout: "update-demo-nautilus-4cb5h\nupdate-demo-nautilus-vjbrv\n"
Mar 24 03:57:52.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4927'
Mar 24 03:57:52.685: INFO: stderr: "No resources found.\n"
Mar 24 03:57:52.685: INFO: stdout: ""
Mar 24 03:57:52.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pods -l name=update-demo --namespace=kubectl-4927 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 24 03:57:52.766: INFO: stderr: ""
Mar 24 03:57:52.766: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:57:52.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4927" for this suite.
Mar 24 03:58:14.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:58:14.828: INFO: namespace kubectl-4927 deletion completed in 22.059595587s

• [SLOW TEST:28.987 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:58:14.829: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2692
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 24 03:58:14.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2692'
Mar 24 03:58:15.048: INFO: stderr: ""
Mar 24 03:58:15.048: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Mar 24 03:58:15.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete pods e2e-test-nginx-pod --namespace=kubectl-2692'
Mar 24 03:58:31.716: INFO: stderr: ""
Mar 24 03:58:31.716: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:58:31.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2692" for this suite.
Mar 24 03:58:37.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:58:37.778: INFO: namespace kubectl-2692 deletion completed in 6.059690143s

• [SLOW TEST:22.950 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:58:37.778: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4272
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Mar 24 03:58:37.902: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 24 03:58:37.907: INFO: Waiting for terminating namespaces to be deleted...
Mar 24 03:58:37.909: INFO: 
Logging pods the kubelet thinks is on node kind-worker before test
Mar 24 03:58:37.914: INFO: kindnet-gsvnh from kube-system started at 2020-03-24 02:40:14 +0000 UTC (1 container statuses recorded)
Mar 24 03:58:37.914: INFO: 	Container kindnet-cni ready: true, restart count 0
Mar 24 03:58:37.914: INFO: kube-proxy-kgzdn from kube-system started at 2020-03-24 02:40:14 +0000 UTC (1 container statuses recorded)
Mar 24 03:58:37.914: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 24 03:58:37.914: INFO: 
Logging pods the kubelet thinks is on node kind-worker2 before test
Mar 24 03:58:37.917: INFO: kindnet-d95hs from kube-system started at 2020-03-24 02:40:13 +0000 UTC (1 container statuses recorded)
Mar 24 03:58:37.917: INFO: 	Container kindnet-cni ready: true, restart count 0
Mar 24 03:58:37.917: INFO: e2e-conformance-test from conformance started at 2020-03-24 02:44:46 +0000 UTC (1 container statuses recorded)
Mar 24 03:58:37.917: INFO: 	Container conformance-container ready: true, restart count 0
Mar 24 03:58:37.917: INFO: kube-proxy-2vhfg from kube-system started at 2020-03-24 02:40:13 +0000 UTC (1 container statuses recorded)
Mar 24 03:58:37.917: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15ff21667d27e782], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:58:38.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4272" for this suite.
Mar 24 03:58:44.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:58:44.995: INFO: namespace sched-pred-4272 deletion completed in 6.061962138s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.216 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:58:44.995: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5608
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 03:58:45.125: INFO: (0) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.795464ms)
Mar 24 03:58:45.127: INFO: (1) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.400207ms)
Mar 24 03:58:45.129: INFO: (2) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.166611ms)
Mar 24 03:58:45.132: INFO: (3) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.349347ms)
Mar 24 03:58:45.134: INFO: (4) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.247143ms)
Mar 24 03:58:45.136: INFO: (5) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.280627ms)
Mar 24 03:58:45.138: INFO: (6) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.230825ms)
Mar 24 03:58:45.141: INFO: (7) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.261185ms)
Mar 24 03:58:45.143: INFO: (8) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.985356ms)
Mar 24 03:58:45.145: INFO: (9) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.171282ms)
Mar 24 03:58:45.147: INFO: (10) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.307623ms)
Mar 24 03:58:45.149: INFO: (11) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.107905ms)
Mar 24 03:58:45.152: INFO: (12) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.190726ms)
Mar 24 03:58:45.155: INFO: (13) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.666011ms)
Mar 24 03:58:45.158: INFO: (14) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.471351ms)
Mar 24 03:58:45.160: INFO: (15) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.41153ms)
Mar 24 03:58:45.162: INFO: (16) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.127822ms)
Mar 24 03:58:45.165: INFO: (17) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.056298ms)
Mar 24 03:58:45.167: INFO: (18) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.025327ms)
Mar 24 03:58:45.169: INFO: (19) /api/v1/nodes/kind-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.009338ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:58:45.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5608" for this suite.
Mar 24 03:58:51.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:58:51.238: INFO: namespace proxy-5608 deletion completed in 6.067051903s

• [SLOW TEST:6.243 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:58:51.238: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4949
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-f7d87a02-eaea-4280-9474-036bc6ffa5ec
STEP: Creating a pod to test consume secrets
Mar 24 03:58:51.369: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ba840b0c-d31e-46f8-82dc-5475e3fa5ac4" in namespace "projected-4949" to be "success or failure"
Mar 24 03:58:51.371: INFO: Pod "pod-projected-secrets-ba840b0c-d31e-46f8-82dc-5475e3fa5ac4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.740304ms
Mar 24 03:58:53.374: INFO: Pod "pod-projected-secrets-ba840b0c-d31e-46f8-82dc-5475e3fa5ac4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004749525s
STEP: Saw pod success
Mar 24 03:58:53.374: INFO: Pod "pod-projected-secrets-ba840b0c-d31e-46f8-82dc-5475e3fa5ac4" satisfied condition "success or failure"
Mar 24 03:58:53.376: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-ba840b0c-d31e-46f8-82dc-5475e3fa5ac4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:58:53.387: INFO: Waiting for pod pod-projected-secrets-ba840b0c-d31e-46f8-82dc-5475e3fa5ac4 to disappear
Mar 24 03:58:53.389: INFO: Pod pod-projected-secrets-ba840b0c-d31e-46f8-82dc-5475e3fa5ac4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:58:53.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4949" for this suite.
Mar 24 03:58:59.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:58:59.453: INFO: namespace projected-4949 deletion completed in 6.061204629s

• [SLOW TEST:8.215 seconds]
[sig-storage] Projected secret
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:58:59.454: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4913
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-a51a67b8-4214-495b-8250-f4605ea9be8c
STEP: Creating a pod to test consume configMaps
Mar 24 03:58:59.584: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-21e95726-34fa-460f-9b14-f28f711639bd" in namespace "projected-4913" to be "success or failure"
Mar 24 03:58:59.585: INFO: Pod "pod-projected-configmaps-21e95726-34fa-460f-9b14-f28f711639bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.773117ms
Mar 24 03:59:01.588: INFO: Pod "pod-projected-configmaps-21e95726-34fa-460f-9b14-f28f711639bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004552059s
STEP: Saw pod success
Mar 24 03:59:01.588: INFO: Pod "pod-projected-configmaps-21e95726-34fa-460f-9b14-f28f711639bd" satisfied condition "success or failure"
Mar 24 03:59:01.590: INFO: Trying to get logs from node kind-worker pod pod-projected-configmaps-21e95726-34fa-460f-9b14-f28f711639bd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 03:59:01.600: INFO: Waiting for pod pod-projected-configmaps-21e95726-34fa-460f-9b14-f28f711639bd to disappear
Mar 24 03:59:01.602: INFO: Pod pod-projected-configmaps-21e95726-34fa-460f-9b14-f28f711639bd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:59:01.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4913" for this suite.
Mar 24 03:59:07.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:59:07.667: INFO: namespace projected-4913 deletion completed in 6.062278648s

• [SLOW TEST:8.213 seconds]
[sig-storage] Projected configMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:59:07.667: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2337
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Mar 24 03:59:07.795: INFO: Waiting up to 5m0s for pod "client-containers-da87e0f2-c815-4ff8-9c4f-5cc944ebe578" in namespace "containers-2337" to be "success or failure"
Mar 24 03:59:07.797: INFO: Pod "client-containers-da87e0f2-c815-4ff8-9c4f-5cc944ebe578": Phase="Pending", Reason="", readiness=false. Elapsed: 1.653373ms
Mar 24 03:59:09.800: INFO: Pod "client-containers-da87e0f2-c815-4ff8-9c4f-5cc944ebe578": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004280965s
STEP: Saw pod success
Mar 24 03:59:09.800: INFO: Pod "client-containers-da87e0f2-c815-4ff8-9c4f-5cc944ebe578" satisfied condition "success or failure"
Mar 24 03:59:09.802: INFO: Trying to get logs from node kind-worker2 pod client-containers-da87e0f2-c815-4ff8-9c4f-5cc944ebe578 container test-container: <nil>
STEP: delete the pod
Mar 24 03:59:09.816: INFO: Waiting for pod client-containers-da87e0f2-c815-4ff8-9c4f-5cc944ebe578 to disappear
Mar 24 03:59:09.818: INFO: Pod client-containers-da87e0f2-c815-4ff8-9c4f-5cc944ebe578 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:59:09.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2337" for this suite.
Mar 24 03:59:15.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:59:15.880: INFO: namespace containers-2337 deletion completed in 6.059790356s

• [SLOW TEST:8.213 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:59:15.881: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9546
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:59:16.012: INFO: Waiting up to 5m0s for pod "downwardapi-volume-09881ebe-4420-4d44-b740-3cc5e5427a21" in namespace "projected-9546" to be "success or failure"
Mar 24 03:59:16.014: INFO: Pod "downwardapi-volume-09881ebe-4420-4d44-b740-3cc5e5427a21": Phase="Pending", Reason="", readiness=false. Elapsed: 1.957952ms
Mar 24 03:59:18.017: INFO: Pod "downwardapi-volume-09881ebe-4420-4d44-b740-3cc5e5427a21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004812277s
STEP: Saw pod success
Mar 24 03:59:18.017: INFO: Pod "downwardapi-volume-09881ebe-4420-4d44-b740-3cc5e5427a21" satisfied condition "success or failure"
Mar 24 03:59:18.019: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-09881ebe-4420-4d44-b740-3cc5e5427a21 container client-container: <nil>
STEP: delete the pod
Mar 24 03:59:18.029: INFO: Waiting for pod downwardapi-volume-09881ebe-4420-4d44-b740-3cc5e5427a21 to disappear
Mar 24 03:59:18.035: INFO: Pod downwardapi-volume-09881ebe-4420-4d44-b740-3cc5e5427a21 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:59:18.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9546" for this suite.
Mar 24 03:59:24.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:59:24.100: INFO: namespace projected-9546 deletion completed in 6.062566915s

• [SLOW TEST:8.219 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:59:24.100: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2245
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:59:24.230: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0904f38f-82f9-4a5e-8130-b1c750a1f519" in namespace "downward-api-2245" to be "success or failure"
Mar 24 03:59:24.232: INFO: Pod "downwardapi-volume-0904f38f-82f9-4a5e-8130-b1c750a1f519": Phase="Pending", Reason="", readiness=false. Elapsed: 2.196162ms
Mar 24 03:59:26.235: INFO: Pod "downwardapi-volume-0904f38f-82f9-4a5e-8130-b1c750a1f519": Phase="Running", Reason="", readiness=true. Elapsed: 2.005447172s
Mar 24 03:59:28.238: INFO: Pod "downwardapi-volume-0904f38f-82f9-4a5e-8130-b1c750a1f519": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008154128s
STEP: Saw pod success
Mar 24 03:59:28.238: INFO: Pod "downwardapi-volume-0904f38f-82f9-4a5e-8130-b1c750a1f519" satisfied condition "success or failure"
Mar 24 03:59:28.240: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-0904f38f-82f9-4a5e-8130-b1c750a1f519 container client-container: <nil>
STEP: delete the pod
Mar 24 03:59:28.250: INFO: Waiting for pod downwardapi-volume-0904f38f-82f9-4a5e-8130-b1c750a1f519 to disappear
Mar 24 03:59:28.252: INFO: Pod downwardapi-volume-0904f38f-82f9-4a5e-8130-b1c750a1f519 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:59:28.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2245" for this suite.
Mar 24 03:59:34.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:59:34.319: INFO: namespace downward-api-2245 deletion completed in 6.064464825s

• [SLOW TEST:10.219 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:59:34.319: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-8018
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:59:36.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8018" for this suite.
Mar 24 03:59:42.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:59:42.542: INFO: namespace emptydir-wrapper-8018 deletion completed in 6.065260295s

• [SLOW TEST:8.223 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:59:42.542: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-918
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-5b4c1b85-ab55-406c-9b1c-f004265c2561
STEP: Creating a pod to test consume configMaps
Mar 24 03:59:42.673: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ba7f019-b19a-4a91-8290-6f41b8e4b914" in namespace "configmap-918" to be "success or failure"
Mar 24 03:59:42.676: INFO: Pod "pod-configmaps-0ba7f019-b19a-4a91-8290-6f41b8e4b914": Phase="Pending", Reason="", readiness=false. Elapsed: 2.771127ms
Mar 24 03:59:44.679: INFO: Pod "pod-configmaps-0ba7f019-b19a-4a91-8290-6f41b8e4b914": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005839153s
STEP: Saw pod success
Mar 24 03:59:44.679: INFO: Pod "pod-configmaps-0ba7f019-b19a-4a91-8290-6f41b8e4b914" satisfied condition "success or failure"
Mar 24 03:59:44.681: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-0ba7f019-b19a-4a91-8290-6f41b8e4b914 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 03:59:44.694: INFO: Waiting for pod pod-configmaps-0ba7f019-b19a-4a91-8290-6f41b8e4b914 to disappear
Mar 24 03:59:44.696: INFO: Pod pod-configmaps-0ba7f019-b19a-4a91-8290-6f41b8e4b914 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:59:44.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-918" for this suite.
Mar 24 03:59:50.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 03:59:50.765: INFO: namespace configmap-918 deletion completed in 6.067006566s

• [SLOW TEST:8.224 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 03:59:50.766: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5422
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-04e6c86c-2901-4883-96a8-17f18016eae2
STEP: Creating a pod to test consume secrets
Mar 24 03:59:50.898: INFO: Waiting up to 5m0s for pod "pod-secrets-d465d91e-63da-4252-9641-fdc7d078bf61" in namespace "secrets-5422" to be "success or failure"
Mar 24 03:59:50.901: INFO: Pod "pod-secrets-d465d91e-63da-4252-9641-fdc7d078bf61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.592648ms
Mar 24 03:59:52.904: INFO: Pod "pod-secrets-d465d91e-63da-4252-9641-fdc7d078bf61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005371148s
Mar 24 03:59:54.907: INFO: Pod "pod-secrets-d465d91e-63da-4252-9641-fdc7d078bf61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008567447s
STEP: Saw pod success
Mar 24 03:59:54.907: INFO: Pod "pod-secrets-d465d91e-63da-4252-9641-fdc7d078bf61" satisfied condition "success or failure"
Mar 24 03:59:54.909: INFO: Trying to get logs from node kind-worker pod pod-secrets-d465d91e-63da-4252-9641-fdc7d078bf61 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:59:54.921: INFO: Waiting for pod pod-secrets-d465d91e-63da-4252-9641-fdc7d078bf61 to disappear
Mar 24 03:59:54.923: INFO: Pod pod-secrets-d465d91e-63da-4252-9641-fdc7d078bf61 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 03:59:54.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5422" for this suite.
Mar 24 04:00:00.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:00:00.986: INFO: namespace secrets-5422 deletion completed in 6.06048616s

• [SLOW TEST:10.220 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:00:00.986: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3644
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-6b06e80c-3fee-4492-98a7-085f12c2da35 in namespace container-probe-3644
Mar 24 04:00:03.123: INFO: Started pod liveness-6b06e80c-3fee-4492-98a7-085f12c2da35 in namespace container-probe-3644
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 04:00:03.125: INFO: Initial restart count of pod liveness-6b06e80c-3fee-4492-98a7-085f12c2da35 is 0
Mar 24 04:00:21.156: INFO: Restart count of pod container-probe-3644/liveness-6b06e80c-3fee-4492-98a7-085f12c2da35 is now 1 (18.031128132s elapsed)
Mar 24 04:00:41.184: INFO: Restart count of pod container-probe-3644/liveness-6b06e80c-3fee-4492-98a7-085f12c2da35 is now 2 (38.059352238s elapsed)
Mar 24 04:01:01.220: INFO: Restart count of pod container-probe-3644/liveness-6b06e80c-3fee-4492-98a7-085f12c2da35 is now 3 (58.094679638s elapsed)
Mar 24 04:01:21.248: INFO: Restart count of pod container-probe-3644/liveness-6b06e80c-3fee-4492-98a7-085f12c2da35 is now 4 (1m18.123487373s elapsed)
Mar 24 04:02:31.347: INFO: Restart count of pod container-probe-3644/liveness-6b06e80c-3fee-4492-98a7-085f12c2da35 is now 5 (2m28.222344167s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:02:31.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3644" for this suite.
Mar 24 04:02:37.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:02:37.419: INFO: namespace container-probe-3644 deletion completed in 6.061074536s

• [SLOW TEST:156.433 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:02:37.419: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2555
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 24 04:02:40.564: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:02:41.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2555" for this suite.
Mar 24 04:03:03.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:03:03.643: INFO: namespace replicaset-2555 deletion completed in 22.06525075s

• [SLOW TEST:26.223 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:03:03.643: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8323
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 04:03:03.781: INFO: Create a RollingUpdate DaemonSet
Mar 24 04:03:03.784: INFO: Check that daemon pods launch on every node of the cluster
Mar 24 04:03:03.787: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 04:03:03.789: INFO: Number of nodes with available pods: 0
Mar 24 04:03:03.789: INFO: Node kind-worker is running more than one daemon pod
Mar 24 04:03:04.793: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 04:03:04.797: INFO: Number of nodes with available pods: 0
Mar 24 04:03:04.797: INFO: Node kind-worker is running more than one daemon pod
Mar 24 04:03:05.792: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 04:03:05.795: INFO: Number of nodes with available pods: 2
Mar 24 04:03:05.795: INFO: Number of running nodes: 2, number of available pods: 2
Mar 24 04:03:05.795: INFO: Update the DaemonSet to trigger a rollout
Mar 24 04:03:05.800: INFO: Updating DaemonSet daemon-set
Mar 24 04:03:09.811: INFO: Roll back the DaemonSet before rollout is complete
Mar 24 04:03:09.816: INFO: Updating DaemonSet daemon-set
Mar 24 04:03:09.816: INFO: Make sure DaemonSet rollback is complete
Mar 24 04:03:09.818: INFO: Wrong image for pod: daemon-set-96c9h. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Mar 24 04:03:09.818: INFO: Pod daemon-set-96c9h is not available
Mar 24 04:03:09.820: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 04:03:10.823: INFO: Wrong image for pod: daemon-set-96c9h. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Mar 24 04:03:10.823: INFO: Pod daemon-set-96c9h is not available
Mar 24 04:03:10.825: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 04:03:11.824: INFO: Pod daemon-set-vr6xt is not available
Mar 24 04:03:11.827: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8323, will wait for the garbage collector to delete the pods
Mar 24 04:03:11.887: INFO: Deleting DaemonSet.extensions daemon-set took: 4.132059ms
Mar 24 04:03:11.987: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.218204ms
Mar 24 04:03:21.790: INFO: Number of nodes with available pods: 0
Mar 24 04:03:21.790: INFO: Number of running nodes: 0, number of available pods: 0
Mar 24 04:03:21.792: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8323/daemonsets","resourceVersion":"18971"},"items":null}

Mar 24 04:03:21.794: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8323/pods","resourceVersion":"18971"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:03:21.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8323" for this suite.
Mar 24 04:03:27.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:03:27.863: INFO: namespace daemonsets-8323 deletion completed in 6.061411479s

• [SLOW TEST:24.220 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:03:27.864: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5951
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-504bd726-06b1-46fe-b80c-6097fb32eb12
STEP: Creating secret with name s-test-opt-upd-5c72c177-422c-481a-b130-18edb1147992
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-504bd726-06b1-46fe-b80c-6097fb32eb12
STEP: Updating secret s-test-opt-upd-5c72c177-422c-481a-b130-18edb1147992
STEP: Creating secret with name s-test-opt-create-cd25d85c-1922-468c-89af-77174c0ea02c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:04:52.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5951" for this suite.
Mar 24 04:05:04.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:05:04.346: INFO: namespace secrets-5951 deletion completed in 12.062778898s

• [SLOW TEST:96.482 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:05:04.346: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar 24 04:05:08.993: INFO: Successfully updated pod "labelsupdated586dc37-d829-4aed-8354-34c7f42b723f"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:05:11.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9628" for this suite.
Mar 24 04:05:33.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:05:33.067: INFO: namespace downward-api-9628 deletion completed in 22.061884282s

• [SLOW TEST:28.721 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:05:33.068: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-4415
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-4415
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-4415
STEP: Deleting pre-stop pod
Mar 24 04:05:44.217: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:05:44.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4415" for this suite.
Mar 24 04:06:22.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:06:22.287: INFO: namespace prestop-4415 deletion completed in 38.062633711s

• [SLOW TEST:49.220 seconds]
[k8s.io] [sig-node] PreStop
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:06:22.289: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-116
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 24 04:06:22.417: INFO: Waiting up to 5m0s for pod "pod-bcd4cfeb-bcc6-4280-ae0c-58dff4c76be7" in namespace "emptydir-116" to be "success or failure"
Mar 24 04:06:22.419: INFO: Pod "pod-bcd4cfeb-bcc6-4280-ae0c-58dff4c76be7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.996073ms
Mar 24 04:06:24.422: INFO: Pod "pod-bcd4cfeb-bcc6-4280-ae0c-58dff4c76be7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004998288s
Mar 24 04:06:26.425: INFO: Pod "pod-bcd4cfeb-bcc6-4280-ae0c-58dff4c76be7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007686349s
STEP: Saw pod success
Mar 24 04:06:26.425: INFO: Pod "pod-bcd4cfeb-bcc6-4280-ae0c-58dff4c76be7" satisfied condition "success or failure"
Mar 24 04:06:26.427: INFO: Trying to get logs from node kind-worker pod pod-bcd4cfeb-bcc6-4280-ae0c-58dff4c76be7 container test-container: <nil>
STEP: delete the pod
Mar 24 04:06:26.438: INFO: Waiting for pod pod-bcd4cfeb-bcc6-4280-ae0c-58dff4c76be7 to disappear
Mar 24 04:06:26.440: INFO: Pod pod-bcd4cfeb-bcc6-4280-ae0c-58dff4c76be7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:06:26.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-116" for this suite.
Mar 24 04:06:32.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:06:32.507: INFO: namespace emptydir-116 deletion completed in 6.064191767s

• [SLOW TEST:10.218 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:06:32.507: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Mar 24 04:06:32.635: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-781" to be "success or failure"
Mar 24 04:06:32.636: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 1.840766ms
Mar 24 04:06:34.639: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004437236s
Mar 24 04:06:36.642: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007201455s
STEP: Saw pod success
Mar 24 04:06:36.642: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar 24 04:06:36.644: INFO: Trying to get logs from node kind-worker2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar 24 04:06:36.654: INFO: Waiting for pod pod-host-path-test to disappear
Mar 24 04:06:36.656: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:06:36.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-781" for this suite.
Mar 24 04:06:42.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:06:42.723: INFO: namespace hostpath-781 deletion completed in 6.064813983s

• [SLOW TEST:10.216 seconds]
[sig-storage] HostPath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:06:42.723: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4494
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 24 04:06:42.853: INFO: Waiting up to 5m0s for pod "pod-c1a6b175-447e-49b0-a7b1-f87873568828" in namespace "emptydir-4494" to be "success or failure"
Mar 24 04:06:42.855: INFO: Pod "pod-c1a6b175-447e-49b0-a7b1-f87873568828": Phase="Pending", Reason="", readiness=false. Elapsed: 1.959953ms
Mar 24 04:06:44.857: INFO: Pod "pod-c1a6b175-447e-49b0-a7b1-f87873568828": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004483351s
STEP: Saw pod success
Mar 24 04:06:44.857: INFO: Pod "pod-c1a6b175-447e-49b0-a7b1-f87873568828" satisfied condition "success or failure"
Mar 24 04:06:44.859: INFO: Trying to get logs from node kind-worker pod pod-c1a6b175-447e-49b0-a7b1-f87873568828 container test-container: <nil>
STEP: delete the pod
Mar 24 04:06:44.871: INFO: Waiting for pod pod-c1a6b175-447e-49b0-a7b1-f87873568828 to disappear
Mar 24 04:06:44.873: INFO: Pod pod-c1a6b175-447e-49b0-a7b1-f87873568828 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:06:44.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4494" for this suite.
Mar 24 04:06:50.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:06:50.935: INFO: namespace emptydir-4494 deletion completed in 6.060329941s

• [SLOW TEST:8.212 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:06:50.936: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8185
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar 24 04:06:53.584: INFO: Successfully updated pod "labelsupdate7e2a1d61-76bc-4e6f-a7d9-9ce267ed0cde"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:06:55.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8185" for this suite.
Mar 24 04:07:17.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:07:17.660: INFO: namespace projected-8185 deletion completed in 22.062231749s

• [SLOW TEST:26.724 seconds]
[sig-storage] Projected downwardAPI
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:07:17.660: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1897
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 24 04:07:17.788: INFO: Waiting up to 5m0s for pod "pod-6737ebc3-5948-4242-8a53-530c9402bfe5" in namespace "emptydir-1897" to be "success or failure"
Mar 24 04:07:17.790: INFO: Pod "pod-6737ebc3-5948-4242-8a53-530c9402bfe5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.866884ms
Mar 24 04:07:19.793: INFO: Pod "pod-6737ebc3-5948-4242-8a53-530c9402bfe5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004545655s
STEP: Saw pod success
Mar 24 04:07:19.793: INFO: Pod "pod-6737ebc3-5948-4242-8a53-530c9402bfe5" satisfied condition "success or failure"
Mar 24 04:07:19.795: INFO: Trying to get logs from node kind-worker pod pod-6737ebc3-5948-4242-8a53-530c9402bfe5 container test-container: <nil>
STEP: delete the pod
Mar 24 04:07:19.804: INFO: Waiting for pod pod-6737ebc3-5948-4242-8a53-530c9402bfe5 to disappear
Mar 24 04:07:19.806: INFO: Pod pod-6737ebc3-5948-4242-8a53-530c9402bfe5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:07:19.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1897" for this suite.
Mar 24 04:07:25.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:07:25.871: INFO: namespace emptydir-1897 deletion completed in 6.062167274s

• [SLOW TEST:8.210 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:07:25.872: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4401
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1721
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 24 04:07:25.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-4401'
Mar 24 04:07:26.246: INFO: stderr: ""
Mar 24 04:07:26.246: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Mar 24 04:07:31.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 get pod e2e-test-nginx-pod --namespace=kubectl-4401 -o json'
Mar 24 04:07:31.373: INFO: stderr: ""
Mar 24 04:07:31.373: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-03-24T04:07:26Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-4401\",\n        \"resourceVersion\": \"19630\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-4401/pods/e2e-test-nginx-pod\",\n        \"uid\": \"330a1315-2e53-4815-afe4-0816bf139d77\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-tz8gx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kind-worker2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-tz8gx\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-tz8gx\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-24T04:07:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-24T04:07:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-24T04:07:28Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-24T04:07:26Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://a2dafc51a0932b71a2345b2faa83bcf82513124268928c3e261353e01c1f335a\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-03-24T04:07:27Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.17.0.2\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.163\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-03-24T04:07:26Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 24 04:07:31.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 replace -f - --namespace=kubectl-4401'
Mar 24 04:07:31.620: INFO: stderr: ""
Mar 24 04:07:31.620: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1726
Mar 24 04:07:31.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete pods e2e-test-nginx-pod --namespace=kubectl-4401'
Mar 24 04:07:41.250: INFO: stderr: ""
Mar 24 04:07:41.250: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:07:41.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4401" for this suite.
Mar 24 04:07:47.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:07:47.315: INFO: namespace kubectl-4401 deletion completed in 6.061575277s

• [SLOW TEST:21.443 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:07:47.316: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6615
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 24 04:07:47.444: INFO: Waiting up to 5m0s for pod "pod-94d9ada8-c529-4e38-bcea-d5331e98929e" in namespace "emptydir-6615" to be "success or failure"
Mar 24 04:07:47.446: INFO: Pod "pod-94d9ada8-c529-4e38-bcea-d5331e98929e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.89964ms
Mar 24 04:07:49.449: INFO: Pod "pod-94d9ada8-c529-4e38-bcea-d5331e98929e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004904674s
Mar 24 04:07:51.453: INFO: Pod "pod-94d9ada8-c529-4e38-bcea-d5331e98929e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008416926s
STEP: Saw pod success
Mar 24 04:07:51.453: INFO: Pod "pod-94d9ada8-c529-4e38-bcea-d5331e98929e" satisfied condition "success or failure"
Mar 24 04:07:51.455: INFO: Trying to get logs from node kind-worker pod pod-94d9ada8-c529-4e38-bcea-d5331e98929e container test-container: <nil>
STEP: delete the pod
Mar 24 04:07:51.466: INFO: Waiting for pod pod-94d9ada8-c529-4e38-bcea-d5331e98929e to disappear
Mar 24 04:07:51.468: INFO: Pod pod-94d9ada8-c529-4e38-bcea-d5331e98929e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:07:51.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6615" for this suite.
Mar 24 04:07:57.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:07:57.534: INFO: namespace emptydir-6615 deletion completed in 6.062994789s

• [SLOW TEST:10.218 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:07:57.534: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-767
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Mar 24 04:07:57.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-767'
Mar 24 04:07:57.743: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 24 04:07:57.743: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Mar 24 04:07:57.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-811275467 delete jobs e2e-test-nginx-job --namespace=kubectl-767'
Mar 24 04:07:57.831: INFO: stderr: ""
Mar 24 04:07:57.831: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:07:57.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-767" for this suite.
Mar 24 04:08:03.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:08:03.893: INFO: namespace kubectl-767 deletion completed in 6.059331309s

• [SLOW TEST:6.359 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:08:03.894: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7078
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar 24 04:08:04.027: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 24 04:08:04.031: INFO: Number of nodes with available pods: 0
Mar 24 04:08:04.031: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar 24 04:08:04.042: INFO: Number of nodes with available pods: 0
Mar 24 04:08:04.042: INFO: Node kind-worker is running more than one daemon pod
Mar 24 04:08:05.045: INFO: Number of nodes with available pods: 0
Mar 24 04:08:05.045: INFO: Node kind-worker is running more than one daemon pod
Mar 24 04:08:06.045: INFO: Number of nodes with available pods: 1
Mar 24 04:08:06.045: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 24 04:08:06.054: INFO: Number of nodes with available pods: 1
Mar 24 04:08:06.054: INFO: Number of running nodes: 0, number of available pods: 1
Mar 24 04:08:07.057: INFO: Number of nodes with available pods: 0
Mar 24 04:08:07.057: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 24 04:08:07.063: INFO: Number of nodes with available pods: 0
Mar 24 04:08:07.063: INFO: Node kind-worker is running more than one daemon pod
Mar 24 04:08:08.066: INFO: Number of nodes with available pods: 0
Mar 24 04:08:08.066: INFO: Node kind-worker is running more than one daemon pod
Mar 24 04:08:09.066: INFO: Number of nodes with available pods: 0
Mar 24 04:08:09.066: INFO: Node kind-worker is running more than one daemon pod
Mar 24 04:08:10.066: INFO: Number of nodes with available pods: 0
Mar 24 04:08:10.066: INFO: Node kind-worker is running more than one daemon pod
Mar 24 04:08:11.066: INFO: Number of nodes with available pods: 0
Mar 24 04:08:11.066: INFO: Node kind-worker is running more than one daemon pod
Mar 24 04:08:12.066: INFO: Number of nodes with available pods: 0
Mar 24 04:08:12.066: INFO: Node kind-worker is running more than one daemon pod
Mar 24 04:08:13.066: INFO: Number of nodes with available pods: 0
Mar 24 04:08:13.066: INFO: Node kind-worker is running more than one daemon pod
Mar 24 04:08:14.066: INFO: Number of nodes with available pods: 1
Mar 24 04:08:14.066: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7078, will wait for the garbage collector to delete the pods
Mar 24 04:08:14.127: INFO: Deleting DaemonSet.extensions daemon-set took: 4.657955ms
Mar 24 04:08:14.427: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.336156ms
Mar 24 04:08:21.729: INFO: Number of nodes with available pods: 0
Mar 24 04:08:21.729: INFO: Number of running nodes: 0, number of available pods: 0
Mar 24 04:08:21.731: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7078/daemonsets","resourceVersion":"19834"},"items":null}

Mar 24 04:08:21.733: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7078/pods","resourceVersion":"19834"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:08:21.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7078" for this suite.
Mar 24 04:08:27.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:08:27.805: INFO: namespace daemonsets-7078 deletion completed in 6.060222775s

• [SLOW TEST:23.911 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:08:27.805: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5978
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-eea3534a-03e2-4036-940a-dd3fbf371286 in namespace container-probe-5978
Mar 24 04:08:29.940: INFO: Started pod test-webserver-eea3534a-03e2-4036-940a-dd3fbf371286 in namespace container-probe-5978
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 04:08:29.943: INFO: Initial restart count of pod test-webserver-eea3534a-03e2-4036-940a-dd3fbf371286 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:12:30.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5978" for this suite.
Mar 24 04:12:36.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:12:36.355: INFO: namespace container-probe-5978 deletion completed in 6.062795077s

• [SLOW TEST:248.550 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:12:36.356: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1863
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1863
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1863
STEP: Creating statefulset with conflicting port in namespace statefulset-1863
STEP: Waiting until pod test-pod will start running in namespace statefulset-1863
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1863
Mar 24 04:12:38.501: INFO: Observed stateful pod in namespace: statefulset-1863, name: ss-0, uid: 604dc931-1be1-4699-92ff-833033263121, status phase: Pending. Waiting for statefulset controller to delete.
Mar 24 04:12:39.095: INFO: Observed stateful pod in namespace: statefulset-1863, name: ss-0, uid: 604dc931-1be1-4699-92ff-833033263121, status phase: Failed. Waiting for statefulset controller to delete.
Mar 24 04:12:39.100: INFO: Observed stateful pod in namespace: statefulset-1863, name: ss-0, uid: 604dc931-1be1-4699-92ff-833033263121, status phase: Failed. Waiting for statefulset controller to delete.
Mar 24 04:12:39.102: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1863
STEP: Removing pod with conflicting port in namespace statefulset-1863
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1863 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar 24 04:12:43.118: INFO: Deleting all statefulset in ns statefulset-1863
Mar 24 04:12:43.120: INFO: Scaling statefulset ss to 0
Mar 24 04:12:53.129: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 04:12:53.131: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:12:53.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1863" for this suite.
Mar 24 04:12:59.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:12:59.202: INFO: namespace statefulset-1863 deletion completed in 6.061017409s

• [SLOW TEST:22.846 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:12:59.202: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5501
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Mar 24 04:12:59.838: INFO: created pod pod-service-account-defaultsa
Mar 24 04:12:59.839: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 24 04:12:59.843: INFO: created pod pod-service-account-mountsa
Mar 24 04:12:59.843: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 24 04:12:59.846: INFO: created pod pod-service-account-nomountsa
Mar 24 04:12:59.846: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 24 04:12:59.853: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 24 04:12:59.853: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 24 04:12:59.860: INFO: created pod pod-service-account-mountsa-mountspec
Mar 24 04:12:59.861: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 24 04:12:59.865: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 24 04:12:59.865: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 24 04:12:59.869: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 24 04:12:59.869: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 24 04:12:59.876: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 24 04:12:59.876: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 24 04:12:59.886: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 24 04:12:59.886: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:12:59.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5501" for this suite.
Mar 24 04:13:05.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:13:05.954: INFO: namespace svcaccounts-5501 deletion completed in 6.063535183s

• [SLOW TEST:6.752 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar 24 04:13:05.955: INFO: >>> kubeConfig: /tmp/kubeconfig-811275467
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1104
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-2dea7c5b-ef98-4597-8fa1-4886b6f86539
STEP: Creating a pod to test consume configMaps
Mar 24 04:13:06.088: INFO: Waiting up to 5m0s for pod "pod-configmaps-56057474-3ba0-4785-a6c3-33d111de0a0c" in namespace "configmap-1104" to be "success or failure"
Mar 24 04:13:06.090: INFO: Pod "pod-configmaps-56057474-3ba0-4785-a6c3-33d111de0a0c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.866778ms
Mar 24 04:13:08.093: INFO: Pod "pod-configmaps-56057474-3ba0-4785-a6c3-33d111de0a0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00445645s
STEP: Saw pod success
Mar 24 04:13:08.093: INFO: Pod "pod-configmaps-56057474-3ba0-4785-a6c3-33d111de0a0c" satisfied condition "success or failure"
Mar 24 04:13:08.095: INFO: Trying to get logs from node kind-worker pod pod-configmaps-56057474-3ba0-4785-a6c3-33d111de0a0c container configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 04:13:08.106: INFO: Waiting for pod pod-configmaps-56057474-3ba0-4785-a6c3-33d111de0a0c to disappear
Mar 24 04:13:08.107: INFO: Pod pod-configmaps-56057474-3ba0-4785-a6c3-33d111de0a0c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar 24 04:13:08.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1104" for this suite.
Mar 24 04:13:14.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 24 04:13:14.170: INFO: namespace configmap-1104 deletion completed in 6.060932389s

• [SLOW TEST:8.216 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSMar 24 04:13:14.171: INFO: Running AfterSuite actions on all nodes
Mar 24 04:13:14.171: INFO: Running AfterSuite actions on node 1
Mar 24 04:13:14.171: INFO: Skipping dumping logs from cluster

Ran 215 of 4412 Specs in 5298.825 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4197 Skipped
PASS

Ginkgo ran 1 suite in 1h28m19.920511208s
Test Suite Passed
