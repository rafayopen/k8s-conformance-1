**************************************************************************
bootstrap.py is deprecated!
test-infra oncall does not support any job still using bootstrap.py.
Please migrate your job to podutils!
https://github.com/kubernetes/test-infra/blob/master/prow/pod-utilities.md
**************************************************************************
Args: --job=ci-kubernetes-gce-conformance-latest-1-15 --service-account=/etc/service-account/service-account.json --upload=gs://kubernetes-jenkins/logs --timeout=220 --bare --scenario=kubernetes_e2e -- --extract=ci/latest-1.15 --gcp-master-image=gci --gcp-node-image=gci --gcp-zone=us-west1-b --provider=gce '--test_args=--ginkgo.focus=\[Conformance\]' --timeout=200m
Bootstrap ci-kubernetes-gce-conformance-latest-1-15...
Builder: 1f71c92b-5a40-11ea-b9e9-322e34f19f2f
Image: gcr.io/k8s-testimages/kubekins-e2e:v20200227-9385733-1.15
Gubernator results at https://gubernator.k8s.io/build/kubernetes-jenkins/logs/ci-kubernetes-gce-conformance-latest-1-15/1233415853953781761
Call:  gcloud auth activate-service-account --key-file=/etc/service-account/service-account.json
Activated service account credentials for: [pr-kubekins@kubernetes-jenkins-pull.iam.gserviceaccount.com]
process 35 exited with code 0 after 0.0m
Call:  gcloud config get-value account
process 49 exited with code 0 after 0.0m
Will upload results to gs://kubernetes-jenkins/logs using pr-kubekins@kubernetes-jenkins-pull.iam.gserviceaccount.com
Root: /workspace
cd to /workspace
Configure environment...
Call:  git show -s --format=format:%ct HEAD
fatal: not a git repository (or any of the parent directories): .git
process 63 exited with code 128 after 0.0m
Unable to print commit date for HEAD
Call:  gcloud auth activate-service-account --key-file=/etc/service-account/service-account.json
Activated service account credentials for: [pr-kubekins@kubernetes-jenkins-pull.iam.gserviceaccount.com]
process 64 exited with code 0 after 0.0m
Call:  gcloud config get-value account
process 78 exited with code 0 after 0.0m
Will upload results to gs://kubernetes-jenkins/logs using pr-kubekins@kubernetes-jenkins-pull.iam.gserviceaccount.com
Start 1233415853953781761 at ...
Call:  gsutil -q -h Content-Type:application/json cp /tmp/gsutil_2Vq3XS gs://kubernetes-jenkins/logs/ci-kubernetes-gce-conformance-latest-1-15/1233415853953781761/started.json
process 92 exited with code 0 after 0.0m
Call:  /workspace/./test-infra/jenkins/../scenarios/kubernetes_e2e.py --extract=ci/latest-1.15 --gcp-master-image=gci --gcp-node-image=gci --gcp-zone=us-west1-b --provider=gce '--test_args=--ginkgo.focus=\[Conformance\]' --timeout=200m
starts with local mode
Environment:
ARTIFACTS=/workspace/_artifacts
AWS_SSH_PRIVATE_KEY_FILE=/root/.ssh/kube_aws_rsa
AWS_SSH_PUBLIC_KEY_FILE=/root/.ssh/kube_aws_rsa.pub
BAZEL_REMOTE_CACHE_ENABLED=false
BAZEL_VERSION=0.23.2
BOOTSTRAP_MIGRATION=yes
BOSKOS_METRICS_PORT=tcp://10.63.252.110:9090
BOSKOS_METRICS_PORT_9090_TCP=tcp://10.63.252.110:9090
BOSKOS_METRICS_PORT_9090_TCP_ADDR=10.63.252.110
BOSKOS_METRICS_PORT_9090_TCP_PORT=9090
BOSKOS_METRICS_PORT_9090_TCP_PROTO=tcp
BOSKOS_METRICS_SERVICE_HOST=10.63.252.110
BOSKOS_METRICS_SERVICE_PORT=9090
BOSKOS_METRICS_SERVICE_PORT_METRICS=9090
BOSKOS_PORT=tcp://10.63.250.132:80
BOSKOS_PORT_80_TCP=tcp://10.63.250.132:80
BOSKOS_PORT_80_TCP_ADDR=10.63.250.132
BOSKOS_PORT_80_TCP_PORT=80
BOSKOS_PORT_80_TCP_PROTO=tcp
BOSKOS_SERVICE_HOST=10.63.250.132
BOSKOS_SERVICE_PORT=80
BOSKOS_SERVICE_PORT_DEFAULT=80
BUILD_ID=1233415853953781761
BUILD_NUMBER=1233415853953781761
CI=true
CLOUDSDK_COMPONENT_MANAGER_DISABLE_UPDATE_CHECK=true
CLOUDSDK_CONFIG=/workspace/.config/gcloud
CLOUDSDK_CORE_DISABLE_PROMPTS=1
CLOUDSDK_EXPERIMENTAL_FAST_COMPONENT_UPDATE=false
DOCKER_IN_DOCKER_ENABLED=false
DOCKER_IN_DOCKER_IPV6_ENABLED=false
E2E_GOOGLE_APPLICATION_CREDENTIALS=/etc/service-account/service-account.json
GCS_ARTIFACTS_DIR=gs://kubernetes-jenkins/logs/ci-kubernetes-gce-conformance-latest-1-15/1233415853953781761/artifacts
GOOGLE_APPLICATION_CREDENTIALS=/etc/service-account/service-account.json
GOPATH=/go
GOPROXY=https://proxy.golang.org
GO_TARBALL=go1.12.12.linux-amd64.tar.gz
HOME=/workspace
HOSTNAME=1f71c92b-5a40-11ea-b9e9-322e34f19f2f
IMAGE=gcr.io/k8s-testimages/kubekins-e2e:v20200227-9385733-1.15
INSTANCE_PREFIX=bootstrap-e2e
JENKINS_GCE_SSH_PRIVATE_KEY_FILE=/workspace/.ssh/google_compute_engine
JENKINS_GCE_SSH_PUBLIC_KEY_FILE=/workspace/.ssh/google_compute_engine.pub
JOB_NAME=ci-kubernetes-gce-conformance-latest-1-15
JOB_SPEC={"type":"periodic","job":"ci-kubernetes-gce-conformance-latest-1-15","buildid":"1233415853953781761","prowjobid":"1f71c92b-5a40-11ea-b9e9-322e34f19f2f"}
JOB_TYPE=periodic
KUBERNETES_PORT=tcp://10.63.240.1:443
KUBERNETES_PORT_443_TCP=tcp://10.63.240.1:443
KUBERNETES_PORT_443_TCP_ADDR=10.63.240.1
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_SERVICE_HOST=10.63.240.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBETEST_IN_DOCKER=true
KUBETEST_MANUAL_DUMP=y
KUBE_AWS_INSTANCE_PREFIX=bootstrap-e2e
KUBE_GCE_INSTANCE_PREFIX=bootstrap-e2e
NODE_NAME=1f71c92b-5a40-11ea-b9e9-322e34f19f2f
PATH=/go/bin:/go/bin:/usr/local/go/bin:/google-cloud-sdk/bin:/workspace:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PROW_JOB_ID=1f71c92b-5a40-11ea-b9e9-322e34f19f2f
PWD=/workspace
SHLVL=2
SOURCE_DATE_EPOCH=
TERM=xterm
USER=prow
WORKSPACE=/workspace
_=./test-infra/jenkins/bootstrap.py
Run: ('kubetest', '--dump=/workspace/_artifacts', '--gcp-service-account=/etc/service-account/service-account.json', '--up', '--down', '--test', '--provider=gce', '--cluster=bootstrap-e2e', '--gcp-network=bootstrap-e2e', '--extract=ci/latest-1.15', '--gcp-master-image=gci', '--gcp-node-image=gci', '--gcp-zone=us-west1-b', '--test_args=--ginkgo.focus=\\[Conformance\\]', '--timeout=200m')
2020/02/28 15:37:30 main.go:329: Limiting testing to 3h20m0s
2020/02/28 15:37:30 process.go:153: Running: gcloud auth activate-service-account --key-file=/etc/service-account/service-account.json
Activated service account credentials for: [pr-kubekins@kubernetes-jenkins-pull.iam.gserviceaccount.com]
2020/02/28 15:37:31 process.go:155: Step 'gcloud auth activate-service-account --key-file=/etc/service-account/service-account.json' finished in 674.609293ms
2020/02/28 15:37:31 main.go:731: --gcp-project is missing, trying to fetch a project from boskos.
(for local runs please set --gcp-project to your dev project)
2020/02/28 15:37:31 main.go:743: provider gce, will acquire project type gce-project from boskos
2020/02/28 15:37:31 process.go:153: Running: gcloud config set project gce-gci-upg-1-3-1-4-ctl-skew
Updated property [core/project].
2020/02/28 15:37:32 process.go:155: Step 'gcloud config set project gce-gci-upg-1-3-1-4-ctl-skew' finished in 705.142707ms
2020/02/28 15:37:32 main.go:782: Checking existing of GCP ssh keys...
2020/02/28 15:37:32 main.go:792: Checking presence of public key in gce-gci-upg-1-3-1-4-ctl-skew
2020/02/28 15:37:32 process.go:153: Running: gcloud compute --project=gce-gci-upg-1-3-1-4-ctl-skew project-info describe
2020/02/28 15:37:33 process.go:155: Step 'gcloud compute --project=gce-gci-upg-1-3-1-4-ctl-skew project-info describe' finished in 1.282926337s
2020/02/28 15:37:33 extract_k8s.go:136: rm kubernetes
2020/02/28 15:37:33 extract_k8s.go:288: U=https://storage.googleapis.com/kubernetes-release-dev/ci R=v1.15.11-beta.0.18+89a7468341d588 get-kube.sh
2020/02/28 15:37:33 process.go:153: Running: ./get-kube.sh
Downloading kubernetes release v1.15.11-beta.0.18+89a7468341d588
  from https://storage.googleapis.com/kubernetes-release-dev/ci/v1.15.11-beta.0.18+89a7468341d588/kubernetes.tar.gz
  to /workspace/kubernetes.tar.gz
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  437k  100  437k    0     0  15.8M      0 --:--:-- --:--:-- --:--:-- 15.8M
Unpacking kubernetes release v1.15.11-beta.0.18+89a7468341d588
Kubernetes release: v1.15.11-beta.0.18+89a7468341d588
Server: linux/amd64  (to override, set KUBERNETES_SERVER_ARCH)
Client: linux/amd64  (autodetected)  (to override, set KUBERNETES_CLIENT_OS and/or KUBERNETES_CLIENT_ARCH)

Will download kubernetes-server-linux-amd64.tar.gz from https://storage.googleapis.com/kubernetes-release-dev/ci/v1.15.11-beta.0.18+89a7468341d588
Will download and extract kubernetes-client-linux-amd64.tar.gz from https://storage.googleapis.com/kubernetes-release-dev/ci/v1.15.11-beta.0.18+89a7468341d588
Will download and extract kubernetes-test tarball(s) from https://storage.googleapis.com/kubernetes-release-dev/ci/v1.15.11-beta.0.18+89a7468341d588
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0 18  645M   18  119M    0     0   146M      0  0:00:04 --:--:--  0:00:04  146M 50  645M   50  325M    0     0   179M      0  0:00:03  0:00:01  0:00:02  178M 75  645M   75  488M    0     0   172M      0  0:00:03  0:00:02  0:00:01  172M100  645M  100  645M    0     0   176M      0  0:00:03  0:00:03 --:--:--  176M

md5sum(kubernetes-server-linux-amd64.tar.gz)=8909b9090a7f4aff755d709981a31104
sha1sum(kubernetes-server-linux-amd64.tar.gz)=4b0c161a57e661c7ec785d71ca03afd4291293e5

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 21.7M  100 21.7M    0     0  39.5M      0 --:--:-- --:--:-- --:--:-- 39.5M

md5sum(kubernetes-client-linux-amd64.tar.gz)=16824ea5fe4bce5b0cda1b04f1868c7b
sha1sum(kubernetes-client-linux-amd64.tar.gz)=c24cfd0079044f4b7bf4c9789b0a5955929e0e3f

Extracting /workspace/kubernetes/client/kubernetes-client-linux-amd64.tar.gz into /workspace/kubernetes/platforms/linux/amd64
Add '/workspace/kubernetes/client/bin' to your PATH to use newly-installed binaries.
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  195k  100  195k    0     0  6319k      0 --:--:-- --:--:-- --:--:-- 6319k

md5sum(kubernetes-test-portable.tar.gz)=20e80daea785360214baf99348cdd1da
sha1sum(kubernetes-test-portable.tar.gz)=75a3efd4ecaa1b7791e9f7201dacd08c5d4d4be9

Extracting kubernetes-test-portable.tar.gz into /workspace/kubernetes
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0  342M    0  132k    0     0   964k      0  0:06:04 --:--:--  0:06:04  957k 59  342M   59  205M    0     0   190M      0  0:00:01  0:00:01 --:--:--  190M100  342M  100  342M    0     0   191M      0  0:00:01  0:00:01 --:--:--  191M

md5sum(kubernetes-test-linux-amd64.tar.gz)=11eaaa8e0920269787072ee90ca05397
sha1sum(kubernetes-test-linux-amd64.tar.gz)=623a198fe3491fbad4f3e485c55ab1a4348edef6

Extracting /workspace/kubernetes/test/kubernetes-test-linux-amd64.tar.gz into /workspace/kubernetes/platforms/linux/amd64
2020/02/28 15:38:00 process.go:155: Step './get-kube.sh' finished in 26.443863977s
2020/02/28 15:38:00 process.go:153: Running: ./hack/e2e-internal/e2e-down.sh
Project: gce-gci-upg-1-3-1-4-ctl-skew
Network Project: gce-gci-upg-1-3-1-4-ctl-skew
Zone: us-west1-b
Shutting down test cluster in background.
Bringing down cluster using provider: gce
... calling verify-prereqs
... calling verify-kube-binaries
... calling kube-down
Project: gce-gci-upg-1-3-1-4-ctl-skew
Network Project: gce-gci-upg-1-3-1-4-ctl-skew
Zone: us-west1-b
INSTANCE_GROUPS=
NODE_NAMES=
Bringing down cluster
Deleting firewall rules remaining in network bootstrap-e2e: 
Property "clusters.gce-gci-upg-1-3-1-4-ctl-skew_bootstrap-e2e" unset.
Property "users.gce-gci-upg-1-3-1-4-ctl-skew_bootstrap-e2e" unset.
Property "users.gce-gci-upg-1-3-1-4-ctl-skew_bootstrap-e2e-basic-auth" unset.
Property "contexts.gce-gci-upg-1-3-1-4-ctl-skew_bootstrap-e2e" unset.
Cleared config for gce-gci-upg-1-3-1-4-ctl-skew_bootstrap-e2e from /workspace/.kube/config
Done
2020/02/28 15:38:28 process.go:155: Step './hack/e2e-internal/e2e-down.sh' finished in 28.583835897s
2020/02/28 15:38:28 process.go:153: Running: ./hack/e2e-internal/e2e-up.sh
Project: gce-gci-upg-1-3-1-4-ctl-skew
Network Project: gce-gci-upg-1-3-1-4-ctl-skew
Zone: us-west1-b
... Starting cluster in us-west1-b using provider gce
... calling verify-prereqs
... calling verify-kube-binaries
... calling verify-release-tars
... calling kube-up
Project: gce-gci-upg-1-3-1-4-ctl-skew
Network Project: gce-gci-upg-1-3-1-4-ctl-skew
Zone: us-west1-b
+++ Staging tars to Google Storage: gs://kubernetes-staging-8ac5d6d59f/bootstrap-e2e-devel
+++ kubernetes-server-linux-amd64.tar.gz uploaded (sha1 = 4b0c161a57e661c7ec785d71ca03afd4291293e5)
+++ kubernetes-manifests.tar.gz uploaded (sha1 = fe2dfe03cf44e7a2b7c0fb68c9a1797e9d3460a0)
Creating new auto network: bootstrap-e2e
Created [https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/global/networks/bootstrap-e2e].
NAME           SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4
bootstrap-e2e  AUTO         REGIONAL

Instances on this network will not be reachable until firewall rules
are created. As an example, you can allow all internal traffic between
instances as well as SSH, RDP, and ICMP by running:

$ gcloud compute firewall-rules create <FIREWALL_NAME> --network bootstrap-e2e --allow tcp,udp,icmp --source-ranges <IP_RANGE>
$ gcloud compute firewall-rules create <FIREWALL_NAME> --network bootstrap-e2e --allow tcp:22,tcp:3389,icmp

Creating firewall...
.Creating firewall...
IP aliases are disabled.
.Creating firewall...
..Creating firewall...
Found subnet for region us-west1 in network bootstrap-e2e: bootstrap-e2e
Starting master and configuring firewalls
.....Created [https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/zones/us-west1-b/disks/bootstrap-e2e-master-pd].
.
New disks are unformatted. You must format and mount a disk before it
can be used. You can find instructions on how to do this at:

https://cloud.google.com/compute/docs/disks/add-persistent-disk#formatting

....Created [https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/global/firewalls/bootstrap-e2e-default-internal-master].
NAME                     ZONE        SIZE_GB  TYPE    STATUS
bootstrap-e2e-master-pd  us-west1-b  20       pd-ssd  READY
NAME                                   NETWORK        DIRECTION  PRIORITY  ALLOW                                       DENY  DISABLED
bootstrap-e2e-default-internal-master  bootstrap-e2e  INGRESS    1000      tcp:1-2379,tcp:2382-65535,udp:1-65535,icmp        False
Creating firewall...
done.
...Created [https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/global/firewalls/bootstrap-e2e-default-internal-node].
.done.
.Created [https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/global/firewalls/bootstrap-e2e-default-ssh].
NAME                                 NETWORK        DIRECTION  PRIORITY  ALLOW                         DENY  DISABLED
bootstrap-e2e-default-internal-node  bootstrap-e2e  INGRESS    1000      tcp:1-65535,udp:1-65535,icmp        False
NAME                       NETWORK        DIRECTION  PRIORITY  ALLOW   DENY  DISABLED
bootstrap-e2e-default-ssh  bootstrap-e2e  INGRESS    1000      tcp:22        False
done.
Created [https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/regions/us-west1/addresses/bootstrap-e2e-master-ip].
...Created [https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/global/firewalls/bootstrap-e2e-master-https].
done.
NAME                        NETWORK        DIRECTION  PRIORITY  ALLOW    DENY  DISABLED
bootstrap-e2e-master-https  bootstrap-e2e  INGRESS    1000      tcp:443        False
.2020/02/28 15:39:46 [INFO] generating a new CA key and certificate from CSR
2020/02/28 15:39:46 [INFO] generate received request
2020/02/28 15:39:46 [INFO] received CSR
2020/02/28 15:39:46 [INFO] generating key: ecdsa-256
2020/02/28 15:39:46 [INFO] encoded CSR
2020/02/28 15:39:46 [INFO] signed certificate with serial number 694178907044919168806586949000927435556734571512
2020/02/28 15:39:46 [INFO] generate received request
2020/02/28 15:39:46 [INFO] received CSR
2020/02/28 15:39:46 [INFO] generating key: ecdsa-256
2020/02/28 15:39:46 [INFO] encoded CSR
2020/02/28 15:39:46 [INFO] signed certificate with serial number 566190035246882236623952097762339711245897010879
2020/02/28 15:39:46 [INFO] generate received request
2020/02/28 15:39:46 [INFO] received CSR
2020/02/28 15:39:46 [INFO] generating key: ecdsa-256
2020/02/28 15:39:46 [INFO] encoded CSR
2020/02/28 15:39:46 [INFO] signed certificate with serial number 577766622730602591555711190995811057038253702616
2020/02/28 15:39:47 [INFO] generate received request
2020/02/28 15:39:47 [INFO] received CSR
2020/02/28 15:39:47 [INFO] generating key: ecdsa-256
2020/02/28 15:39:47 [INFO] encoded CSR
2020/02/28 15:39:47 [INFO] signed certificate with serial number 173638067099640974193959765107204724658819304818
2020/02/28 15:39:47 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").
Generating certs for alternate-names: IP:34.82.216.130,IP:10.0.0.1,DNS:kubernetes,DNS:kubernetes.default,DNS:kubernetes.default.svc,DNS:kubernetes.default.svc.cluster.local,DNS:bootstrap-e2e-master
Generate peer certificates...
Generate server certificates...
Generate client certificates...
+++ Logging using Fluentd to gcp
Creating firewall...
..Created [https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/global/firewalls/bootstrap-e2e-master-etcd].
done.
....Created [https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/global/firewalls/bootstrap-e2e-minion-all].
NAME                       NETWORK        DIRECTION  PRIORITY  ALLOW              DENY  DISABLED
bootstrap-e2e-master-etcd  bootstrap-e2e  INGRESS    1000      tcp:2380,tcp:2381        False
done.
NAME                      NETWORK        DIRECTION  PRIORITY  ALLOW                     DENY  DISABLED
bootstrap-e2e-minion-all  bootstrap-e2e  INGRESS    1000      tcp,udp,icmp,esp,ah,sctp        False
WARNING: You have selected a disk size of under [200GB]. This may result in poor I/O performance. For more information, see: https://developers.google.com/compute/docs/disks#performance.
Created [https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/zones/us-west1-b/instances/bootstrap-e2e-master].
WARNING: Some requests generated warnings:
 - Disk size: '20 GB' is larger than image size: '10 GB'. You might need to resize the root repartition manually if the operating system does not support automatic resizing. See https://cloud.google.com/compute/docs/disks/add-persistent-disk#resize_pd for details.
 - The resource 'projects/cos-cloud/global/images/cos-73-11647-163-0' is deprecated. A suggested replacement is 'projects/cos-cloud/global/images/cos-73-11647-182-0'.

NAME                  ZONE        MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP    STATUS
bootstrap-e2e-master  us-west1-b  n1-standard-1               10.138.0.2   34.82.216.130  RUNNING
Creating nodes.
Using subnet bootstrap-e2e
Attempt 1 to create bootstrap-e2e-minion-template
WARNING: You have selected a disk size of under [200GB]. This may result in poor I/O performance. For more information, see: https://developers.google.com/compute/docs/disks#performance.
Created [https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/global/instanceTemplates/bootstrap-e2e-minion-template].
NAME                           MACHINE_TYPE   PREEMPTIBLE  CREATION_TIMESTAMP
bootstrap-e2e-minion-template  n1-standard-2               2020-02-28T07:40:01.771-08:00
Using subnet bootstrap-e2e
Attempt 1 to create bootstrap-e2e-windows-node-template
WARNING: You have selected a disk size of under [200GB]. This may result in poor I/O performance. For more information, see: https://developers.google.com/compute/docs/disks#performance.
Created [https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/global/instanceTemplates/bootstrap-e2e-windows-node-template].
NAME                                 MACHINE_TYPE   PREEMPTIBLE  CREATION_TIMESTAMP
bootstrap-e2e-windows-node-template  n1-standard-2               2020-02-28T07:40:06.134-08:00
Created [https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/zones/us-west1-b/instanceGroupManagers/bootstrap-e2e-minion-group].
NAME                        LOCATION    SCOPE  BASE_INSTANCE_NAME          SIZE  TARGET_SIZE  INSTANCE_TEMPLATE              AUTOSCALED
bootstrap-e2e-minion-group  us-west1-b  zone   bootstrap-e2e-minion-group  0     3            bootstrap-e2e-minion-template  no
WARNING: `gcloud compute instance-groups managed wait-until-stable` is deprecated. Please use `gcloud compute instance-groups managed wait-until --stable` instead.
Waiting for group to become stable, current operations: creating: 3
Waiting for group to become stable, current operations: creating: 1
Group is stable
INSTANCE_GROUPS=bootstrap-e2e-minion-group
NODE_NAMES=bootstrap-e2e-minion-group-q1l7 bootstrap-e2e-minion-group-s43f bootstrap-e2e-minion-group-t863
Trying to find master named 'bootstrap-e2e-master'
Looking for address 'bootstrap-e2e-master-ip'
Using master: bootstrap-e2e-master (external IP: 34.82.216.130)
Waiting up to 300 seconds for cluster initialization.

  This will continually check to see if the API for kubernetes is reachable.
  This may time out if there was some uncaught error during start up.

..............Kubernetes cluster created.
Cluster "gce-gci-upg-1-3-1-4-ctl-skew_bootstrap-e2e" set.
User "gce-gci-upg-1-3-1-4-ctl-skew_bootstrap-e2e" set.
Context "gce-gci-upg-1-3-1-4-ctl-skew_bootstrap-e2e" created.
Switched to context "gce-gci-upg-1-3-1-4-ctl-skew_bootstrap-e2e".
User "gce-gci-upg-1-3-1-4-ctl-skew_bootstrap-e2e-basic-auth" set.
Wrote config for gce-gci-upg-1-3-1-4-ctl-skew_bootstrap-e2e to /workspace/.kube/config
Updated [https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/zones/us-west1-b/instances/bootstrap-e2e-master].

Kubernetes cluster is running.  The master is running at:

  https://34.82.216.130

The user name and password to use is located in /workspace/.kube/config.

Validating gce cluster, MULTIZONE=
... calling validate-cluster
Project: gce-gci-upg-1-3-1-4-ctl-skew
Network Project: gce-gci-upg-1-3-1-4-ctl-skew
Zone: us-west1-b
No resources found.
Waiting for 4 ready nodes. 0 ready nodes, 0 registered. Retrying.
Found 4 node(s).
NAME                              STATUS                     ROLES    AGE   VERSION
bootstrap-e2e-master              Ready,SchedulingDisabled   <none>   8s    v1.15.11-beta.0.18+89a7468341d588
bootstrap-e2e-minion-group-q1l7   Ready                      <none>   4s    v1.15.11-beta.0.18+89a7468341d588
bootstrap-e2e-minion-group-s43f   Ready                      <none>   5s    v1.15.11-beta.0.18+89a7468341d588
bootstrap-e2e-minion-group-t863   Ready                      <none>   6s    v1.15.11-beta.0.18+89a7468341d588
Validate output:
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   
etcd-1               Healthy   {"health":"true"}   
Cluster validation succeeded
Done, listing cluster services:

Kubernetes master is running at https://34.82.216.130
GLBCDefaultBackend is running at https://34.82.216.130/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy
Heapster is running at https://34.82.216.130/api/v1/namespaces/kube-system/services/heapster/proxy
CoreDNS is running at https://34.82.216.130/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
kubernetes-dashboard is running at https://34.82.216.130/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy
Metrics-server is running at https://34.82.216.130/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

NAME                                         NETWORK        DIRECTION  PRIORITY  ALLOW            DENY  DISABLED
bootstrap-e2e-minion-bootstrap-e2e-http-alt  bootstrap-e2e  INGRESS    1000      tcp:80,tcp:8080        False
allowed:
- IPProtocol: tcp
  ports:
  - '80'
- IPProtocol: tcp
  ports:
  - '8080'
creationTimestamp: '2020-02-28T07:42:25.053-08:00'
description: ''
direction: INGRESS
disabled: false
id: '4308608694116190734'
kind: compute#firewall
logConfig:
  enable: false
name: bootstrap-e2e-minion-bootstrap-e2e-http-alt
network: https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/global/networks/bootstrap-e2e
priority: 1000
selfLink: https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/global/firewalls/bootstrap-e2e-minion-bootstrap-e2e-http-alt
sourceRanges:
- 0.0.0.0/0
targetTags:
- bootstrap-e2e-minion
NAME                                          NETWORK        DIRECTION  PRIORITY  ALLOW                            DENY  DISABLED
bootstrap-e2e-minion-bootstrap-e2e-nodeports  bootstrap-e2e  INGRESS    1000      tcp:30000-32767,udp:30000-32767        False
allowed:
- IPProtocol: tcp
  ports:
  - 30000-32767
- IPProtocol: udp
  ports:
  - 30000-32767
creationTimestamp: '2020-02-28T07:42:34.468-08:00'
description: ''
direction: INGRESS
disabled: false
id: '1541488130047379973'
kind: compute#firewall
logConfig:
  enable: false
name: bootstrap-e2e-minion-bootstrap-e2e-nodeports
network: https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/global/networks/bootstrap-e2e
priority: 1000
selfLink: https://www.googleapis.com/compute/v1/projects/gce-gci-upg-1-3-1-4-ctl-skew/global/firewalls/bootstrap-e2e-minion-bootstrap-e2e-nodeports
sourceRanges:
- 0.0.0.0/0
targetTags:
- bootstrap-e2e-minion
2020/02/28 15:42:42 process.go:155: Step './hack/e2e-internal/e2e-up.sh' finished in 4m14.202378232s
2020/02/28 15:42:42 process.go:153: Running: ./cluster/kubectl.sh --match-server-version=false version
2020/02/28 15:42:43 process.go:155: Step './cluster/kubectl.sh --match-server-version=false version' finished in 296.520929ms
2020/02/28 15:42:43 process.go:153: Running: ./cluster/kubectl.sh --match-server-version=false get nodes -oyaml
2020/02/28 15:42:43 process.go:155: Step './cluster/kubectl.sh --match-server-version=false get nodes -oyaml' finished in 348.090156ms
2020/02/28 15:42:43 process.go:153: Running: ./hack/e2e-internal/e2e-status.sh
Project: gce-gci-upg-1-3-1-4-ctl-skew
Network Project: gce-gci-upg-1-3-1-4-ctl-skew
Zone: us-west1-b
Client Version: version.Info{Major:"1", Minor:"15+", GitVersion:"v1.15.11-beta.0.18+89a7468341d588", GitCommit:"89a7468341d588301c6c8be360ed2abb871009e1", GitTreeState:"clean", BuildDate:"2020-02-28T00:28:38Z", GoVersion:"go1.12.17", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"15+", GitVersion:"v1.15.11-beta.0.18+89a7468341d588", GitCommit:"89a7468341d588301c6c8be360ed2abb871009e1", GitTreeState:"clean", BuildDate:"2020-02-28T00:28:38Z", GoVersion:"go1.12.17", Compiler:"gc", Platform:"linux/amd64"}
2020/02/28 15:42:43 process.go:155: Step './hack/e2e-internal/e2e-status.sh' finished in 395.726182ms
2020/02/28 15:42:43 process.go:153: Running: ./cluster/kubectl.sh --match-server-version=false version
2020/02/28 15:42:44 process.go:155: Step './cluster/kubectl.sh --match-server-version=false version' finished in 335.000818ms
2020/02/28 15:42:44 process.go:153: Running: ./hack/ginkgo-e2e.sh --ginkgo.focus=\[Conformance\] --report-dir=/workspace/_artifacts --disable-log-dump=true
Setting up for KUBERNETES_PROVIDER="gce".
Project: gce-gci-upg-1-3-1-4-ctl-skew
Network Project: gce-gci-upg-1-3-1-4-ctl-skew
Zone: us-west1-b
Trying to find master named 'bootstrap-e2e-master'
Looking for address 'bootstrap-e2e-master-ip'
Using master: bootstrap-e2e-master (external IP: 34.82.216.130)
Feb 28 15:42:46.374: INFO: Fetching cloud provider for "gce"
I0228 15:42:46.375121    8260 gce.go:863] Using DefaultTokenSource &oauth2.reuseTokenSource{new:jwt.jwtSource{ctx:(*context.emptyCtx)(0xc0000da018), conf:(*jwt.Config)(0xc0026354d0)}, mu:sync.Mutex{state:0, sema:0x0}, t:(*oauth2.Token)(nil)}
I0228 15:42:46.451814    8260 gce.go:863] Using DefaultTokenSource &oauth2.reuseTokenSource{new:jwt.jwtSource{ctx:(*context.emptyCtx)(0xc0000da018), conf:(*jwt.Config)(0xc0023d2480)}, mu:sync.Mutex{state:0, sema:0x0}, t:(*oauth2.Token)(nil)}
I0228 15:42:46.463414    8260 gce.go:863] Using DefaultTokenSource &oauth2.reuseTokenSource{new:jwt.jwtSource{ctx:(*context.emptyCtx)(0xc0000da018), conf:(*jwt.Config)(0xc00240d680)}, mu:sync.Mutex{state:0, sema:0x0}, t:(*oauth2.Token)(nil)}
W0228 15:42:46.476609    8260 gce.go:461] No network name or URL specified.
I0228 15:42:46.476754    8260 e2e.go:243] Starting e2e run "f2d2476f-1135-406d-947a-9a4fd4eebadc" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1582904565 - Will randomize all specs
Will run 215 of 4412 specs

Feb 28 15:42:51.304: INFO: cluster-master-image: cos-73-11647-163-0
Feb 28 15:42:51.304: INFO: cluster-node-image: cos-73-11647-163-0
Feb 28 15:42:51.304: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 15:42:51.308: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 28 15:42:51.497: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 28 15:42:51.668: INFO: The status of Pod fluentd-gcp-v3.2.0-2d22v is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:42:51.668: INFO: The status of Pod metrics-server-v0.3.3-dfb87d66b-82962 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:42:51.668: INFO: 22 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 28 15:42:51.668: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 28 15:42:51.668: INFO: POD                                    NODE                             PHASE    GRACE  CONDITIONS
Feb 28 15:42:51.668: INFO: fluentd-gcp-v3.2.0-2d22v               bootstrap-e2e-minion-group-q1l7  Pending  60s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:50 +0000 UTC ContainersNotReady containers with unready status: [fluentd-gcp prometheus-to-sd-exporter]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:50 +0000 UTC ContainersNotReady containers with unready status: [fluentd-gcp prometheus-to-sd-exporter]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:18 +0000 UTC  }]
Feb 28 15:42:51.668: INFO: metrics-server-v0.3.3-dfb87d66b-82962  bootstrap-e2e-minion-group-t863  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:31 +0000 UTC ContainersNotReady containers with unready status: [metrics-server metrics-server-nanny]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:31 +0000 UTC ContainersNotReady containers with unready status: [metrics-server metrics-server-nanny]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:31 +0000 UTC  }]
Feb 28 15:42:51.668: INFO: 
Feb 28 15:42:53.796: INFO: The status of Pod etcd-server-bootstrap-e2e-master is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:42:53.796: INFO: The status of Pod fluentd-gcp-v3.2.0-2d22v is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:42:53.796: INFO: The status of Pod metrics-server-v0.3.3-dfb87d66b-82962 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:42:53.796: INFO: 22 / 25 pods in namespace 'kube-system' are running and ready (2 seconds elapsed)
Feb 28 15:42:53.796: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 28 15:42:53.796: INFO: POD                                    NODE                             PHASE    GRACE  CONDITIONS
Feb 28 15:42:53.796: INFO: etcd-server-bootstrap-e2e-master       bootstrap-e2e-master             Pending         []
Feb 28 15:42:53.796: INFO: fluentd-gcp-v3.2.0-2d22v               bootstrap-e2e-minion-group-q1l7  Pending  60s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:50 +0000 UTC ContainersNotReady containers with unready status: [fluentd-gcp prometheus-to-sd-exporter]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:50 +0000 UTC ContainersNotReady containers with unready status: [fluentd-gcp prometheus-to-sd-exporter]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:18 +0000 UTC  }]
Feb 28 15:42:53.796: INFO: metrics-server-v0.3.3-dfb87d66b-82962  bootstrap-e2e-minion-group-t863  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:31 +0000 UTC ContainersNotReady containers with unready status: [metrics-server metrics-server-nanny]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:31 +0000 UTC ContainersNotReady containers with unready status: [metrics-server metrics-server-nanny]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:31 +0000 UTC  }]
Feb 28 15:42:53.796: INFO: 
Feb 28 15:42:55.795: INFO: The status of Pod fluentd-gcp-v3.2.0-2d22v is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:42:55.795: INFO: The status of Pod metrics-server-v0.3.3-dfb87d66b-82962 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:42:55.795: INFO: 23 / 25 pods in namespace 'kube-system' are running and ready (4 seconds elapsed)
Feb 28 15:42:55.795: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 28 15:42:55.795: INFO: POD                                    NODE                             PHASE    GRACE  CONDITIONS
Feb 28 15:42:55.795: INFO: fluentd-gcp-v3.2.0-2d22v               bootstrap-e2e-minion-group-q1l7  Pending  60s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:50 +0000 UTC ContainersNotReady containers with unready status: [fluentd-gcp prometheus-to-sd-exporter]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:50 +0000 UTC ContainersNotReady containers with unready status: [fluentd-gcp prometheus-to-sd-exporter]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:18 +0000 UTC  }]
Feb 28 15:42:55.795: INFO: metrics-server-v0.3.3-dfb87d66b-82962  bootstrap-e2e-minion-group-t863  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:31 +0000 UTC ContainersNotReady containers with unready status: [metrics-server metrics-server-nanny]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:31 +0000 UTC ContainersNotReady containers with unready status: [metrics-server metrics-server-nanny]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:31 +0000 UTC  }]
Feb 28 15:42:55.795: INFO: 
Feb 28 15:42:57.793: INFO: The status of Pod etcd-server-events-bootstrap-e2e-master is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:42:57.793: INFO: The status of Pod fluentd-gcp-v3.2.0-2d22v is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:42:57.793: INFO: 23 / 25 pods in namespace 'kube-system' are running and ready (6 seconds elapsed)
Feb 28 15:42:57.793: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 28 15:42:57.793: INFO: POD                                      NODE                             PHASE    GRACE  CONDITIONS
Feb 28 15:42:57.793: INFO: etcd-server-events-bootstrap-e2e-master  bootstrap-e2e-master             Pending         []
Feb 28 15:42:57.793: INFO: fluentd-gcp-v3.2.0-2d22v                 bootstrap-e2e-minion-group-q1l7  Pending  60s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:50 +0000 UTC ContainersNotReady containers with unready status: [fluentd-gcp prometheus-to-sd-exporter]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:50 +0000 UTC ContainersNotReady containers with unready status: [fluentd-gcp prometheus-to-sd-exporter]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:18 +0000 UTC  }]
Feb 28 15:42:57.793: INFO: 
Feb 28 15:42:59.796: INFO: The status of Pod etcd-server-events-bootstrap-e2e-master is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:42:59.796: INFO: The status of Pod fluentd-gcp-v3.2.0-9q7z2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:42:59.796: INFO: 23 / 25 pods in namespace 'kube-system' are running and ready (8 seconds elapsed)
Feb 28 15:42:59.796: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 28 15:42:59.796: INFO: POD                                      NODE                             PHASE    GRACE  CONDITIONS
Feb 28 15:42:59.796: INFO: etcd-server-events-bootstrap-e2e-master  bootstrap-e2e-master             Pending         []
Feb 28 15:42:59.796: INFO: fluentd-gcp-v3.2.0-9q7z2                 bootstrap-e2e-minion-group-q1l7  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:58 +0000 UTC ContainersNotReady containers with unready status: [fluentd-gcp prometheus-to-sd-exporter]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:58 +0000 UTC ContainersNotReady containers with unready status: [fluentd-gcp prometheus-to-sd-exporter]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:42:58 +0000 UTC  }]
Feb 28 15:42:59.796: INFO: 
Feb 28 15:43:01.795: INFO: The status of Pod etcd-server-events-bootstrap-e2e-master is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:43:01.795: INFO: 24 / 25 pods in namespace 'kube-system' are running and ready (10 seconds elapsed)
Feb 28 15:43:01.795: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 28 15:43:01.795: INFO: POD                                      NODE                  PHASE    GRACE  CONDITIONS
Feb 28 15:43:01.795: INFO: etcd-server-events-bootstrap-e2e-master  bootstrap-e2e-master  Pending         []
Feb 28 15:43:01.795: INFO: 
Feb 28 15:43:03.796: INFO: The status of Pod etcd-server-events-bootstrap-e2e-master is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:43:03.796: INFO: 24 / 25 pods in namespace 'kube-system' are running and ready (12 seconds elapsed)
Feb 28 15:43:03.796: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 28 15:43:03.796: INFO: POD                                      NODE                  PHASE    GRACE  CONDITIONS
Feb 28 15:43:03.796: INFO: etcd-server-events-bootstrap-e2e-master  bootstrap-e2e-master  Pending         []
Feb 28 15:43:03.796: INFO: 
Feb 28 15:43:05.795: INFO: The status of Pod etcd-empty-dir-cleanup-bootstrap-e2e-master is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:43:05.795: INFO: 25 / 26 pods in namespace 'kube-system' are running and ready (14 seconds elapsed)
Feb 28 15:43:05.796: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 28 15:43:05.796: INFO: POD                                          NODE                  PHASE    GRACE  CONDITIONS
Feb 28 15:43:05.796: INFO: etcd-empty-dir-cleanup-bootstrap-e2e-master  bootstrap-e2e-master  Pending         []
Feb 28 15:43:05.796: INFO: 
Feb 28 15:43:07.795: INFO: The status of Pod etcd-empty-dir-cleanup-bootstrap-e2e-master is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:43:07.795: INFO: 25 / 26 pods in namespace 'kube-system' are running and ready (16 seconds elapsed)
Feb 28 15:43:07.795: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 28 15:43:07.795: INFO: POD                                          NODE                  PHASE    GRACE  CONDITIONS
Feb 28 15:43:07.795: INFO: etcd-empty-dir-cleanup-bootstrap-e2e-master  bootstrap-e2e-master  Pending         []
Feb 28 15:43:07.795: INFO: 
Feb 28 15:43:09.802: INFO: The status of Pod etcd-empty-dir-cleanup-bootstrap-e2e-master is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:43:09.802: INFO: 25 / 26 pods in namespace 'kube-system' are running and ready (18 seconds elapsed)
Feb 28 15:43:09.802: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 28 15:43:09.802: INFO: POD                                          NODE                  PHASE    GRACE  CONDITIONS
Feb 28 15:43:09.802: INFO: etcd-empty-dir-cleanup-bootstrap-e2e-master  bootstrap-e2e-master  Pending         []
Feb 28 15:43:09.802: INFO: 
Feb 28 15:43:11.800: INFO: The status of Pod etcd-empty-dir-cleanup-bootstrap-e2e-master is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:43:11.800: INFO: 25 / 26 pods in namespace 'kube-system' are running and ready (20 seconds elapsed)
Feb 28 15:43:11.800: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 28 15:43:11.800: INFO: POD                                          NODE                  PHASE    GRACE  CONDITIONS
Feb 28 15:43:11.800: INFO: etcd-empty-dir-cleanup-bootstrap-e2e-master  bootstrap-e2e-master  Pending         []
Feb 28 15:43:11.800: INFO: 
Feb 28 15:43:13.798: INFO: The status of Pod etcd-empty-dir-cleanup-bootstrap-e2e-master is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 28 15:43:13.798: INFO: 25 / 26 pods in namespace 'kube-system' are running and ready (22 seconds elapsed)
Feb 28 15:43:13.798: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 28 15:43:13.798: INFO: POD                                          NODE                  PHASE    GRACE  CONDITIONS
Feb 28 15:43:13.798: INFO: etcd-empty-dir-cleanup-bootstrap-e2e-master  bootstrap-e2e-master  Pending         []
Feb 28 15:43:13.798: INFO: 
Feb 28 15:43:15.794: INFO: 26 / 26 pods in namespace 'kube-system' are running and ready (24 seconds elapsed)
Feb 28 15:43:15.794: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 28 15:43:15.794: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 28 15:43:15.840: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'fluentd-gcp-v3.2.0' (0 seconds elapsed)
Feb 28 15:43:15.840: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'metadata-proxy-v0.1' (0 seconds elapsed)
Feb 28 15:43:15.840: INFO: e2e test version: v1.15.11-beta.0.18+89a7468341d588
Feb 28 15:43:15.876: INFO: kube-apiserver version: v1.15.11-beta.0.18+89a7468341d588
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:43:15.877: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename var-expansion
Feb 28 15:43:16.066: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Feb 28 15:43:16.148: INFO: Waiting up to 5m0s for pod "var-expansion-cc89d782-1f0d-4d8b-b825-32482c951a4c" in namespace "var-expansion-1797" to be "success or failure"
Feb 28 15:43:16.186: INFO: Pod "var-expansion-cc89d782-1f0d-4d8b-b825-32482c951a4c": Phase="Pending", Reason="", readiness=false. Elapsed: 37.354113ms
Feb 28 15:43:18.224: INFO: Pod "var-expansion-cc89d782-1f0d-4d8b-b825-32482c951a4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075558539s
STEP: Saw pod success
Feb 28 15:43:18.224: INFO: Pod "var-expansion-cc89d782-1f0d-4d8b-b825-32482c951a4c" satisfied condition "success or failure"
Feb 28 15:43:18.262: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod var-expansion-cc89d782-1f0d-4d8b-b825-32482c951a4c container dapi-container: <nil>
STEP: delete the pod
Feb 28 15:43:18.364: INFO: Waiting for pod var-expansion-cc89d782-1f0d-4d8b-b825-32482c951a4c to disappear
Feb 28 15:43:18.402: INFO: Pod var-expansion-cc89d782-1f0d-4d8b-b825-32482c951a4c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:43:18.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1797" for this suite.
Feb 28 15:43:24.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:43:26.033: INFO: namespace var-expansion-1797 deletion completed in 7.590947386s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:43:26.033: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-b6c581de-09ec-4b8a-8287-bdfb62c13bd2
STEP: Creating a pod to test consume secrets
Feb 28 15:43:26.465: INFO: Waiting up to 5m0s for pod "pod-secrets-e4f5ed9d-9fac-414e-a303-6fc710dd9fd6" in namespace "secrets-9483" to be "success or failure"
Feb 28 15:43:26.503: INFO: Pod "pod-secrets-e4f5ed9d-9fac-414e-a303-6fc710dd9fd6": Phase="Pending", Reason="", readiness=false. Elapsed: 37.501175ms
Feb 28 15:43:28.540: INFO: Pod "pod-secrets-e4f5ed9d-9fac-414e-a303-6fc710dd9fd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075329258s
Feb 28 15:43:30.578: INFO: Pod "pod-secrets-e4f5ed9d-9fac-414e-a303-6fc710dd9fd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.113307255s
STEP: Saw pod success
Feb 28 15:43:30.578: INFO: Pod "pod-secrets-e4f5ed9d-9fac-414e-a303-6fc710dd9fd6" satisfied condition "success or failure"
Feb 28 15:43:30.616: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-secrets-e4f5ed9d-9fac-414e-a303-6fc710dd9fd6 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 15:43:30.719: INFO: Waiting for pod pod-secrets-e4f5ed9d-9fac-414e-a303-6fc710dd9fd6 to disappear
Feb 28 15:43:30.757: INFO: Pod pod-secrets-e4f5ed9d-9fac-414e-a303-6fc710dd9fd6 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:43:30.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9483" for this suite.
Feb 28 15:43:36.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:43:38.363: INFO: namespace secrets-9483 deletion completed in 7.56821429s
STEP: Destroying namespace "secret-namespace-5739" for this suite.
Feb 28 15:43:44.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:43:45.939: INFO: namespace secret-namespace-5739 deletion completed in 7.576190196s
•SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:43:45.940: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-fd7481d2-40c8-4b51-a272-6eb917d2349e
STEP: Creating configMap with name cm-test-opt-upd-08f1dfca-9174-40e9-9364-0c71b66069b1
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-fd7481d2-40c8-4b51-a272-6eb917d2349e
STEP: Updating configmap cm-test-opt-upd-08f1dfca-9174-40e9-9364-0c71b66069b1
STEP: Creating configMap with name cm-test-opt-create-d96f6025-0cb3-4ada-b928-e5c2ed014ce9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:45:18.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-444" for this suite.
Feb 28 15:45:40.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:45:42.323: INFO: namespace configmap-444 deletion completed in 23.563008115s
•SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:45:42.324: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-b6c09fd7-3d9f-45fa-9167-fe00aaee5ed9
STEP: Creating a pod to test consume configMaps
Feb 28 15:45:42.567: INFO: Waiting up to 5m0s for pod "pod-configmaps-6c07b4b5-9a5f-485a-a654-3eec48336176" in namespace "configmap-1709" to be "success or failure"
Feb 28 15:45:42.605: INFO: Pod "pod-configmaps-6c07b4b5-9a5f-485a-a654-3eec48336176": Phase="Pending", Reason="", readiness=false. Elapsed: 38.164131ms
Feb 28 15:45:44.645: INFO: Pod "pod-configmaps-6c07b4b5-9a5f-485a-a654-3eec48336176": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077933777s
STEP: Saw pod success
Feb 28 15:45:44.645: INFO: Pod "pod-configmaps-6c07b4b5-9a5f-485a-a654-3eec48336176" satisfied condition "success or failure"
Feb 28 15:45:44.683: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-configmaps-6c07b4b5-9a5f-485a-a654-3eec48336176 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 15:45:44.782: INFO: Waiting for pod pod-configmaps-6c07b4b5-9a5f-485a-a654-3eec48336176 to disappear
Feb 28 15:45:44.820: INFO: Pod pod-configmaps-6c07b4b5-9a5f-485a-a654-3eec48336176 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:45:44.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1709" for this suite.
Feb 28 15:45:50.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:45:52.425: INFO: namespace configmap-1709 deletion completed in 7.565783553s
•SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:45:52.425: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-598
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-598
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-598
Feb 28 15:45:52.797: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb 28 15:46:02.837: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 28 15:46:02.875: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 15:46:03.543: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 28 15:46:03.543: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 15:46:03.543: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 15:46:03.581: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 28 15:46:13.620: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 15:46:13.620: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 15:46:13.778: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999586s
Feb 28 15:46:14.816: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.962099894s
Feb 28 15:46:15.857: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.923962904s
Feb 28 15:46:16.895: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.883482267s
Feb 28 15:46:17.936: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.84481223s
Feb 28 15:46:18.974: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.804588541s
Feb 28 15:46:20.012: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.766240343s
Feb 28 15:46:21.051: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.728026584s
Feb 28 15:46:22.088: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.689609823s
Feb 28 15:46:23.127: INFO: Verifying statefulset ss doesn't scale past 1 for another 651.876557ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-598
Feb 28 15:46:24.165: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:46:24.792: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 28 15:46:24.792: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 15:46:24.792: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 15:46:24.842: INFO: Found 1 stateful pods, waiting for 3
Feb 28 15:46:34.882: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 15:46:34.882: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 15:46:34.882: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 28 15:46:34.958: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 15:46:35.611: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 28 15:46:35.611: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 15:46:35.611: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 15:46:35.611: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 15:46:36.253: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 28 15:46:36.253: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 15:46:36.253: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 15:46:36.253: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 15:46:36.873: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 28 15:46:36.873: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 15:46:36.873: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 15:46:36.873: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 15:46:36.911: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 28 15:46:46.989: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 15:46:46.989: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 15:46:46.989: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 15:46:47.111: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999622s
Feb 28 15:46:48.150: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.96147269s
Feb 28 15:46:49.189: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.922073879s
Feb 28 15:46:50.227: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.883536603s
Feb 28 15:46:51.266: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.844719016s
Feb 28 15:46:52.305: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.806209446s
Feb 28 15:46:53.343: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.767535923s
Feb 28 15:46:54.381: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.729505981s
Feb 28 15:46:55.420: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.691249464s
Feb 28 15:46:56.458: INFO: Verifying statefulset ss doesn't scale past 3 for another 652.604098ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-598
Feb 28 15:46:57.497: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:46:58.132: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 28 15:46:58.132: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 15:46:58.132: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 15:46:58.132: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:46:58.763: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 28 15:46:58.763: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 15:46:58.763: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 15:46:58.763: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:46:59.452: INFO: rc: 126
Feb 28 15:46:59.452: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil> OCI runtime exec failed: exec failed: cannot exec a container that has stopped: unknown
 command terminated with exit code 126
 [] <nil> 0xc0020c9e60 exit status 126 <nil> <nil> true [0xc001b9fc68 0xc001b9fce0 0xc001b9fd48] [0xc001b9fc68 0xc001b9fce0 0xc001b9fd48] [0xc001b9fcd0 0xc001b9fd18] [0xba7080 0xba7080] 0xc001795bc0 <nil>}:
Command stdout:
OCI runtime exec failed: exec failed: cannot exec a container that has stopped: unknown

stderr:
command terminated with exit code 126

error:
exit status 126
Feb 28 15:47:09.452: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:47:09.737: INFO: rc: 1
Feb 28 15:47:09.737: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00263dd40 exit status 1 <nil> <nil> true [0xc002214a28 0xc002214a40 0xc002214a58] [0xc002214a28 0xc002214a40 0xc002214a58] [0xc002214a38 0xc002214a50] [0xba7080 0xba7080] 0xc0015f8480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:47:19.737: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:47:20.047: INFO: rc: 1
Feb 28 15:47:20.047: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0025fc6c0 exit status 1 <nil> <nil> true [0xc000010218 0xc000010488 0xc000010668] [0xc000010218 0xc000010488 0xc000010668] [0xc000010440 0xc0000104c8] [0xba7080 0xba7080] 0xc001660de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:47:30.047: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:47:30.329: INFO: rc: 1
Feb 28 15:47:30.329: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002608720 exit status 1 <nil> <nil> true [0xc000356420 0xc000357718 0xc000357760] [0xc000356420 0xc000357718 0xc000357760] [0xc000356560 0xc000357750] [0xba7080 0xba7080] 0xc0024be4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:47:40.329: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:47:40.589: INFO: rc: 1
Feb 28 15:47:40.589: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0025fce40 exit status 1 <nil> <nil> true [0xc000010b80 0xc000010e30 0xc000010eb0] [0xc000010b80 0xc000010e30 0xc000010eb0] [0xc000010db0 0xc000010e98] [0xba7080 0xba7080] 0xc001661140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:47:50.589: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:47:50.890: INFO: rc: 1
Feb 28 15:47:50.890: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f6c9c0 exit status 1 <nil> <nil> true [0xc0000de050 0xc000693f10 0xc000693fe0] [0xc0000de050 0xc000693f10 0xc000693fe0] [0xc000693e80 0xc000693fc8] [0xba7080 0xba7080] 0xc001cfd980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:48:00.891: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:48:01.180: INFO: rc: 1
Feb 28 15:48:01.180: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002608e40 exit status 1 <nil> <nil> true [0xc0003577a0 0xc000357858 0xc0003578f0] [0xc0003577a0 0xc000357858 0xc0003578f0] [0xc0003577b0 0xc000357890] [0xba7080 0xba7080] 0xc0024bea20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:48:11.180: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:48:11.486: INFO: rc: 1
Feb 28 15:48:11.486: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002609560 exit status 1 <nil> <nil> true [0xc000357900 0xc000357a10 0xc000357ac8] [0xc000357900 0xc000357a10 0xc000357ac8] [0xc000357990 0xc000357aa8] [0xba7080 0xba7080] 0xc0024befc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:48:21.487: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:48:21.802: INFO: rc: 1
Feb 28 15:48:21.802: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002609cb0 exit status 1 <nil> <nil> true [0xc000357ae0 0xc000357ba8 0xc000357c20] [0xc000357ae0 0xc000357ba8 0xc000357c20] [0xc000357b90 0xc000357bd0] [0xba7080 0xba7080] 0xc0024bf5c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:48:31.802: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:48:32.079: INFO: rc: 1
Feb 28 15:48:32.079: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f6d0e0 exit status 1 <nil> <nil> true [0xc00032a0d0 0xc00032ac98 0xc00032adc8] [0xc00032a0d0 0xc00032ac98 0xc00032adc8] [0xc00032a1e0 0xc00032ad20] [0xba7080 0xba7080] 0xc00213e3c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:48:42.079: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:48:42.349: INFO: rc: 1
Feb 28 15:48:42.349: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f6d7d0 exit status 1 <nil> <nil> true [0xc00032ae00 0xc00032aed0 0xc00032afa8] [0xc00032ae00 0xc00032aed0 0xc00032afa8] [0xc00032ae78 0xc00032af68] [0xba7080 0xba7080] 0xc00201e3c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:48:52.350: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:48:52.754: INFO: rc: 1
Feb 28 15:48:52.754: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f6def0 exit status 1 <nil> <nil> true [0xc00032b018 0xc00032b168 0xc00032b2a0] [0xc00032b018 0xc00032b168 0xc00032b2a0] [0xc00032b110 0xc00032b240] [0xba7080 0xba7080] 0xc00201fda0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:49:02.758: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:49:03.118: INFO: rc: 1
Feb 28 15:49:03.118: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001aaa630 exit status 1 <nil> <nil> true [0xc00032b320 0xc00032b3e0 0xc00032b488] [0xc00032b320 0xc00032b3e0 0xc00032b488] [0xc00032b3a8 0xc00032b460] [0xba7080 0xba7080] 0xc001f51a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:49:13.121: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:49:13.558: INFO: rc: 1
Feb 28 15:49:13.558: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00082d350 exit status 1 <nil> <nil> true [0xc000357c70 0xc000357cf8 0xc000357d18] [0xc000357c70 0xc000357cf8 0xc000357d18] [0xc000357ce0 0xc000357d10] [0xba7080 0xba7080] 0xc0024bfb00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:49:23.558: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:49:23.840: INFO: rc: 1
Feb 28 15:49:23.841: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f6c990 exit status 1 <nil> <nil> true [0xc000693f10 0xc000693fe0 0xc00032a0d0] [0xc000693f10 0xc000693fe0 0xc00032a0d0] [0xc000693fc8 0xc0000de088] [0xba7080 0xba7080] 0xc001f51380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:49:33.841: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:49:34.134: INFO: rc: 1
Feb 28 15:49:34.134: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f6d110 exit status 1 <nil> <nil> true [0xc00032a108 0xc00032ad00 0xc00032ae00] [0xc00032a108 0xc00032ad00 0xc00032ae00] [0xc00032ac98 0xc00032adc8] [0xba7080 0xba7080] 0xc00201efc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:49:44.134: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:49:44.427: INFO: rc: 1
Feb 28 15:49:44.427: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f6d860 exit status 1 <nil> <nil> true [0xc00032ae18 0xc00032af40 0xc00032b018] [0xc00032ae18 0xc00032af40 0xc00032b018] [0xc00032aed0 0xc00032afa8] [0xba7080 0xba7080] 0xc00213e540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:49:54.427: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:49:54.708: INFO: rc: 1
Feb 28 15:49:54.708: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002608780 exit status 1 <nil> <nil> true [0xc000010010 0xc000010440 0xc0000104c8] [0xc000010010 0xc000010440 0xc0000104c8] [0xc000010290 0xc000010498] [0xba7080 0xba7080] 0xc001cfd980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:50:04.708: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:50:05.009: INFO: rc: 1
Feb 28 15:50:05.009: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002608f00 exit status 1 <nil> <nil> true [0xc000010668 0xc000010db0 0xc000010e98] [0xc000010668 0xc000010db0 0xc000010e98] [0xc000010cc0 0xc000010e68] [0xba7080 0xba7080] 0xc001c70e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:50:15.009: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:50:15.294: INFO: rc: 1
Feb 28 15:50:15.294: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001708720 exit status 1 <nil> <nil> true [0xc000356420 0xc000357718 0xc000357760] [0xc000356420 0xc000357718 0xc000357760] [0xc000356560 0xc000357750] [0xba7080 0xba7080] 0xc001660de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:50:25.296: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:50:25.845: INFO: rc: 1
Feb 28 15:50:25.845: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f6dfb0 exit status 1 <nil> <nil> true [0xc00032b090 0xc00032b1a8 0xc00032b320] [0xc00032b090 0xc00032b1a8 0xc00032b320] [0xc00032b168 0xc00032b2a0] [0xba7080 0xba7080] 0xc0024be180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:50:35.845: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:50:36.250: INFO: rc: 1
Feb 28 15:50:36.250: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0026096b0 exit status 1 <nil> <nil> true [0xc000010eb0 0xc000010f58 0xc0000110d0] [0xc000010eb0 0xc000010f58 0xc0000110d0] [0xc000010f40 0xc000011038] [0xba7080 0xba7080] 0xc001e064e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:50:46.254: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:50:46.680: INFO: rc: 1
Feb 28 15:50:46.680: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001aaa720 exit status 1 <nil> <nil> true [0xc00032b358 0xc00032b3f8 0xc00032b4d0] [0xc00032b358 0xc00032b3f8 0xc00032b4d0] [0xc00032b3e0 0xc00032b488] [0xba7080 0xba7080] 0xc0024be840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:50:56.682: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:50:57.114: INFO: rc: 1
Feb 28 15:50:57.114: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001aaae40 exit status 1 <nil> <nil> true [0xc00032b528 0xc00032b570 0xc00032b620] [0xc00032b528 0xc00032b570 0xc00032b620] [0xc00032b558 0xc00032b5f0] [0xba7080 0xba7080] 0xc0024bede0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:51:07.114: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:51:07.600: INFO: rc: 1
Feb 28 15:51:07.600: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001708e70 exit status 1 <nil> <nil> true [0xc0003577a0 0xc000357858 0xc0003578f0] [0xc0003577a0 0xc000357858 0xc0003578f0] [0xc0003577b0 0xc000357890] [0xba7080 0xba7080] 0xc001661140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:51:17.606: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:51:18.086: INFO: rc: 1
Feb 28 15:51:18.086: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002608720 exit status 1 <nil> <nil> true [0xc000010218 0xc000010488 0xc000010668] [0xc000010218 0xc000010488 0xc000010668] [0xc000010440 0xc0000104c8] [0xba7080 0xba7080] 0xc001c70120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:51:28.087: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:51:28.450: INFO: rc: 1
Feb 28 15:51:28.450: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002608e70 exit status 1 <nil> <nil> true [0xc000010b80 0xc000010e30 0xc000010eb0] [0xc000010b80 0xc000010e30 0xc000010eb0] [0xc000010db0 0xc000010e98] [0xba7080 0xba7080] 0xc001c71ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:51:38.450: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:51:38.756: INFO: rc: 1
Feb 28 15:51:38.756: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0026095c0 exit status 1 <nil> <nil> true [0xc000010ee8 0xc000010fd0 0xc000011110] [0xc000010ee8 0xc000010fd0 0xc000011110] [0xc000010f58 0xc0000110d0] [0xba7080 0xba7080] 0xc001cfd980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:51:48.756: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:51:49.046: INFO: rc: 1
Feb 28 15:51:49.046: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001708780 exit status 1 <nil> <nil> true [0xc0000de050 0xc000693f10 0xc000693fe0] [0xc0000de050 0xc000693f10 0xc000693fe0] [0xc000693e80 0xc000693fc8] [0xba7080 0xba7080] 0xc00213ed80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb 28 15:51:59.050: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 15:51:59.478: INFO: rc: 1
Feb 28 15:51:59.478: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Feb 28 15:51:59.478: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 28 15:51:59.597: INFO: Deleting all statefulset in ns statefulset-598
Feb 28 15:51:59.637: INFO: Scaling statefulset ss to 0
Feb 28 15:51:59.752: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 15:51:59.790: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:51:59.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-598" for this suite.
Feb 28 15:52:06.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:52:07.590: INFO: namespace statefulset-598 deletion completed in 7.63856672s

• [SLOW TEST:375.166 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:52:07.591: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4667
I0228 15:52:07.830840    8260 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4667, replica count: 1
I0228 15:52:08.934500    8260 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 15:52:09.938478    8260 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 15:52:10.942458    8260 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 28 15:52:11.099: INFO: Created: latency-svc-n6jp9
Feb 28 15:52:11.099: INFO: Got endpoints: latency-svc-n6jp9 [52.76966ms]
Feb 28 15:52:11.150: INFO: Created: latency-svc-9hrcz
Feb 28 15:52:11.156: INFO: Created: latency-svc-nn8n8
Feb 28 15:52:11.159: INFO: Got endpoints: latency-svc-9hrcz [59.550011ms]
Feb 28 15:52:11.166: INFO: Got endpoints: latency-svc-nn8n8 [66.731944ms]
Feb 28 15:52:11.167: INFO: Created: latency-svc-g27ks
Feb 28 15:52:11.182: INFO: Got endpoints: latency-svc-g27ks [82.377077ms]
Feb 28 15:52:11.190: INFO: Created: latency-svc-mczz7
Feb 28 15:52:11.198: INFO: Got endpoints: latency-svc-mczz7 [98.188677ms]
Feb 28 15:52:11.202: INFO: Created: latency-svc-4plkl
Feb 28 15:52:11.206: INFO: Got endpoints: latency-svc-4plkl [104.760676ms]
Feb 28 15:52:11.213: INFO: Created: latency-svc-sc8xd
Feb 28 15:52:11.220: INFO: Created: latency-svc-m72xk
Feb 28 15:52:11.226: INFO: Got endpoints: latency-svc-sc8xd [126.026709ms]
Feb 28 15:52:11.229: INFO: Got endpoints: latency-svc-m72xk [128.253571ms]
Feb 28 15:52:11.234: INFO: Created: latency-svc-nb4rc
Feb 28 15:52:11.242: INFO: Got endpoints: latency-svc-nb4rc [141.45438ms]
Feb 28 15:52:11.250: INFO: Created: latency-svc-5hzmv
Feb 28 15:52:11.262: INFO: Got endpoints: latency-svc-5hzmv [161.665114ms]
Feb 28 15:52:11.278: INFO: Created: latency-svc-kjtp6
Feb 28 15:52:11.282: INFO: Got endpoints: latency-svc-kjtp6 [181.591543ms]
Feb 28 15:52:11.298: INFO: Created: latency-svc-5h448
Feb 28 15:52:11.301: INFO: Got endpoints: latency-svc-5h448 [199.766958ms]
Feb 28 15:52:11.310: INFO: Created: latency-svc-s4zzz
Feb 28 15:52:11.318: INFO: Got endpoints: latency-svc-s4zzz [217.013311ms]
Feb 28 15:52:11.323: INFO: Created: latency-svc-677vz
Feb 28 15:52:11.338: INFO: Created: latency-svc-mxqmb
Feb 28 15:52:11.346: INFO: Created: latency-svc-zvpv7
Feb 28 15:52:11.346: INFO: Got endpoints: latency-svc-mxqmb [244.910965ms]
Feb 28 15:52:11.347: INFO: Got endpoints: latency-svc-677vz [245.352806ms]
Feb 28 15:52:11.358: INFO: Created: latency-svc-cjhcj
Feb 28 15:52:11.363: INFO: Got endpoints: latency-svc-zvpv7 [262.080331ms]
Feb 28 15:52:11.363: INFO: Got endpoints: latency-svc-cjhcj [204.857192ms]
Feb 28 15:52:11.368: INFO: Created: latency-svc-22vz2
Feb 28 15:52:11.378: INFO: Got endpoints: latency-svc-22vz2 [211.92776ms]
Feb 28 15:52:11.382: INFO: Created: latency-svc-57kvz
Feb 28 15:52:11.388: INFO: Got endpoints: latency-svc-57kvz [205.932214ms]
Feb 28 15:52:11.398: INFO: Created: latency-svc-ztkph
Feb 28 15:52:11.410: INFO: Created: latency-svc-dqp46
Feb 28 15:52:11.415: INFO: Got endpoints: latency-svc-ztkph [217.157341ms]
Feb 28 15:52:11.422: INFO: Got endpoints: latency-svc-dqp46 [215.33438ms]
Feb 28 15:52:11.430: INFO: Created: latency-svc-hlzf5
Feb 28 15:52:11.438: INFO: Created: latency-svc-w7bk7
Feb 28 15:52:11.438: INFO: Got endpoints: latency-svc-hlzf5 [212.025614ms]
Feb 28 15:52:11.446: INFO: Created: latency-svc-8ts2r
Feb 28 15:52:11.454: INFO: Got endpoints: latency-svc-w7bk7 [225.356193ms]
Feb 28 15:52:11.455: INFO: Got endpoints: latency-svc-8ts2r [212.440948ms]
Feb 28 15:52:11.461: INFO: Created: latency-svc-gdwpt
Feb 28 15:52:11.467: INFO: Got endpoints: latency-svc-gdwpt [204.389466ms]
Feb 28 15:52:11.470: INFO: Created: latency-svc-r2dx9
Feb 28 15:52:11.482: INFO: Created: latency-svc-mtqtw
Feb 28 15:52:11.485: INFO: Got endpoints: latency-svc-r2dx9 [202.703928ms]
Feb 28 15:52:11.494: INFO: Created: latency-svc-pnck2
Feb 28 15:52:11.494: INFO: Got endpoints: latency-svc-mtqtw [193.470788ms]
Feb 28 15:52:11.506: INFO: Created: latency-svc-ptrz2
Feb 28 15:52:11.510: INFO: Got endpoints: latency-svc-pnck2 [192.046098ms]
Feb 28 15:52:11.516: INFO: Got endpoints: latency-svc-ptrz2 [170.156431ms]
Feb 28 15:52:11.518: INFO: Created: latency-svc-p76nf
Feb 28 15:52:11.526: INFO: Created: latency-svc-8t444
Feb 28 15:52:11.538: INFO: Got endpoints: latency-svc-p76nf [191.6658ms]
Feb 28 15:52:11.542: INFO: Got endpoints: latency-svc-8t444 [178.928896ms]
Feb 28 15:52:11.542: INFO: Created: latency-svc-wrrqd
Feb 28 15:52:11.554: INFO: Created: latency-svc-p6tg9
Feb 28 15:52:11.558: INFO: Got endpoints: latency-svc-wrrqd [194.556149ms]
Feb 28 15:52:11.562: INFO: Got endpoints: latency-svc-p6tg9 [67.845659ms]
Feb 28 15:52:11.570: INFO: Created: latency-svc-tfsx6
Feb 28 15:52:11.574: INFO: Created: latency-svc-6l9tk
Feb 28 15:52:11.581: INFO: Created: latency-svc-tc2pb
Feb 28 15:52:11.586: INFO: Got endpoints: latency-svc-tfsx6 [207.92932ms]
Feb 28 15:52:11.587: INFO: Got endpoints: latency-svc-6l9tk [198.456504ms]
Feb 28 15:52:11.596: INFO: Created: latency-svc-8wf5x
Feb 28 15:52:11.599: INFO: Got endpoints: latency-svc-tc2pb [183.536394ms]
Feb 28 15:52:11.610: INFO: Got endpoints: latency-svc-8wf5x [188.821022ms]
Feb 28 15:52:11.614: INFO: Created: latency-svc-4sndk
Feb 28 15:52:11.624: INFO: Created: latency-svc-z66q6
Feb 28 15:52:11.634: INFO: Got endpoints: latency-svc-4sndk [195.22981ms]
Feb 28 15:52:11.638: INFO: Got endpoints: latency-svc-z66q6 [183.753506ms]
Feb 28 15:52:11.642: INFO: Created: latency-svc-mhkgt
Feb 28 15:52:11.646: INFO: Got endpoints: latency-svc-mhkgt [191.4452ms]
Feb 28 15:52:11.658: INFO: Created: latency-svc-xj4hc
Feb 28 15:52:11.666: INFO: Got endpoints: latency-svc-xj4hc [199.390877ms]
Feb 28 15:52:11.666: INFO: Created: latency-svc-dsbgl
Feb 28 15:52:11.674: INFO: Created: latency-svc-tlqks
Feb 28 15:52:11.682: INFO: Created: latency-svc-6xts4
Feb 28 15:52:11.690: INFO: Created: latency-svc-fl8hg
Feb 28 15:52:11.697: INFO: Created: latency-svc-q2fhr
Feb 28 15:52:11.704: INFO: Got endpoints: latency-svc-dsbgl [218.439398ms]
Feb 28 15:52:11.710: INFO: Created: latency-svc-sbjdr
Feb 28 15:52:11.718: INFO: Created: latency-svc-tg2zk
Feb 28 15:52:11.718: INFO: Created: latency-svc-fp7jb
Feb 28 15:52:11.730: INFO: Created: latency-svc-6zd6d
Feb 28 15:52:11.734: INFO: Created: latency-svc-92nbm
Feb 28 15:52:11.746: INFO: Created: latency-svc-lsz5v
Feb 28 15:52:11.758: INFO: Created: latency-svc-52pch
Feb 28 15:52:11.758: INFO: Created: latency-svc-q5t5v
Feb 28 15:52:11.762: INFO: Got endpoints: latency-svc-tlqks [251.835478ms]
Feb 28 15:52:11.774: INFO: Created: latency-svc-wgvhv
Feb 28 15:52:11.789: INFO: Created: latency-svc-2s8fc
Feb 28 15:52:11.814: INFO: Created: latency-svc-wx2r2
Feb 28 15:52:11.818: INFO: Got endpoints: latency-svc-6xts4 [301.562831ms]
Feb 28 15:52:11.838: INFO: Created: latency-svc-sqkhj
Feb 28 15:52:11.854: INFO: Got endpoints: latency-svc-fl8hg [315.884897ms]
Feb 28 15:52:11.870: INFO: Created: latency-svc-kv9jg
Feb 28 15:52:11.902: INFO: Created: latency-svc-g9mwq
Feb 28 15:52:11.906: INFO: Got endpoints: latency-svc-q2fhr [363.455159ms]
Feb 28 15:52:11.950: INFO: Created: latency-svc-dcqmb
Feb 28 15:52:11.953: INFO: Got endpoints: latency-svc-sbjdr [395.3897ms]
Feb 28 15:52:12.002: INFO: Created: latency-svc-ksp5h
Feb 28 15:52:12.006: INFO: Got endpoints: latency-svc-fp7jb [443.865734ms]
Feb 28 15:52:12.054: INFO: Created: latency-svc-tdths
Feb 28 15:52:12.055: INFO: Got endpoints: latency-svc-tg2zk [468.536045ms]
Feb 28 15:52:12.099: INFO: Created: latency-svc-l6qr5
Feb 28 15:52:12.104: INFO: Got endpoints: latency-svc-6zd6d [517.344084ms]
Feb 28 15:52:12.151: INFO: Created: latency-svc-b2mkr
Feb 28 15:52:12.156: INFO: Got endpoints: latency-svc-92nbm [556.75893ms]
Feb 28 15:52:12.202: INFO: Created: latency-svc-nzbsw
Feb 28 15:52:12.205: INFO: Got endpoints: latency-svc-lsz5v [594.283447ms]
Feb 28 15:52:12.250: INFO: Created: latency-svc-8qc5n
Feb 28 15:52:12.258: INFO: Got endpoints: latency-svc-q5t5v [624.505822ms]
Feb 28 15:52:12.305: INFO: Created: latency-svc-zpnll
Feb 28 15:52:12.306: INFO: Got endpoints: latency-svc-52pch [667.492129ms]
Feb 28 15:52:12.350: INFO: Created: latency-svc-9mrs9
Feb 28 15:52:12.358: INFO: Got endpoints: latency-svc-wgvhv [712.03135ms]
Feb 28 15:52:12.406: INFO: Created: latency-svc-6c8px
Feb 28 15:52:12.410: INFO: Got endpoints: latency-svc-2s8fc [744.057468ms]
Feb 28 15:52:12.456: INFO: Got endpoints: latency-svc-wx2r2 [752.761154ms]
Feb 28 15:52:12.461: INFO: Created: latency-svc-8pfxh
Feb 28 15:52:12.506: INFO: Created: latency-svc-5jbbx
Feb 28 15:52:12.508: INFO: Got endpoints: latency-svc-sqkhj [746.115844ms]
Feb 28 15:52:12.557: INFO: Created: latency-svc-7wtrn
Feb 28 15:52:12.562: INFO: Got endpoints: latency-svc-kv9jg [744.112329ms]
Feb 28 15:52:12.606: INFO: Got endpoints: latency-svc-g9mwq [752.074606ms]
Feb 28 15:52:12.615: INFO: Created: latency-svc-l7gqt
Feb 28 15:52:12.654: INFO: Created: latency-svc-np4h8
Feb 28 15:52:12.658: INFO: Got endpoints: latency-svc-dcqmb [752.479674ms]
Feb 28 15:52:12.706: INFO: Got endpoints: latency-svc-ksp5h [752.607375ms]
Feb 28 15:52:12.707: INFO: Created: latency-svc-rc6kb
Feb 28 15:52:12.754: INFO: Created: latency-svc-r4s58
Feb 28 15:52:12.755: INFO: Got endpoints: latency-svc-tdths [749.235518ms]
Feb 28 15:52:12.806: INFO: Got endpoints: latency-svc-l6qr5 [751.394321ms]
Feb 28 15:52:12.807: INFO: Created: latency-svc-w9bl6
Feb 28 15:52:12.854: INFO: Created: latency-svc-8k5xw
Feb 28 15:52:12.858: INFO: Got endpoints: latency-svc-b2mkr [754.357819ms]
Feb 28 15:52:12.906: INFO: Created: latency-svc-bksph
Feb 28 15:52:12.910: INFO: Got endpoints: latency-svc-nzbsw [754.406195ms]
Feb 28 15:52:12.955: INFO: Got endpoints: latency-svc-8qc5n [750.48676ms]
Feb 28 15:52:12.962: INFO: Created: latency-svc-9k2lb
Feb 28 15:52:13.000: INFO: Created: latency-svc-xnt2f
Feb 28 15:52:13.006: INFO: Got endpoints: latency-svc-zpnll [748.038933ms]
Feb 28 15:52:13.054: INFO: Created: latency-svc-6rgjc
Feb 28 15:52:13.057: INFO: Got endpoints: latency-svc-9mrs9 [751.691448ms]
Feb 28 15:52:13.104: INFO: Created: latency-svc-2lfj4
Feb 28 15:52:13.109: INFO: Got endpoints: latency-svc-6c8px [750.941753ms]
Feb 28 15:52:13.155: INFO: Created: latency-svc-g9pcl
Feb 28 15:52:13.156: INFO: Got endpoints: latency-svc-8pfxh [746.235437ms]
Feb 28 15:52:13.204: INFO: Created: latency-svc-tfh4w
Feb 28 15:52:13.205: INFO: Got endpoints: latency-svc-5jbbx [748.945106ms]
Feb 28 15:52:13.254: INFO: Created: latency-svc-9lqsg
Feb 28 15:52:13.256: INFO: Got endpoints: latency-svc-7wtrn [747.764293ms]
Feb 28 15:52:13.308: INFO: Created: latency-svc-9mzdq
Feb 28 15:52:13.311: INFO: Got endpoints: latency-svc-l7gqt [749.003828ms]
Feb 28 15:52:13.354: INFO: Got endpoints: latency-svc-np4h8 [747.758356ms]
Feb 28 15:52:13.362: INFO: Created: latency-svc-djms7
Feb 28 15:52:13.399: INFO: Created: latency-svc-9dvrs
Feb 28 15:52:13.403: INFO: Got endpoints: latency-svc-rc6kb [745.280557ms]
Feb 28 15:52:13.450: INFO: Created: latency-svc-9xcrn
Feb 28 15:52:13.455: INFO: Got endpoints: latency-svc-r4s58 [749.232425ms]
Feb 28 15:52:13.501: INFO: Created: latency-svc-bfhvh
Feb 28 15:52:13.509: INFO: Got endpoints: latency-svc-w9bl6 [753.789151ms]
Feb 28 15:52:13.554: INFO: Got endpoints: latency-svc-8k5xw [747.933335ms]
Feb 28 15:52:13.559: INFO: Created: latency-svc-7pfrm
Feb 28 15:52:13.618: INFO: Got endpoints: latency-svc-bksph [759.846169ms]
Feb 28 15:52:13.623: INFO: Created: latency-svc-jh8s4
Feb 28 15:52:13.654: INFO: Got endpoints: latency-svc-9k2lb [744.039289ms]
Feb 28 15:52:13.664: INFO: Created: latency-svc-9dxzf
Feb 28 15:52:13.709: INFO: Created: latency-svc-7zprf
Feb 28 15:52:13.709: INFO: Got endpoints: latency-svc-xnt2f [753.475718ms]
Feb 28 15:52:13.757: INFO: Got endpoints: latency-svc-6rgjc [750.771893ms]
Feb 28 15:52:13.766: INFO: Created: latency-svc-6twjg
Feb 28 15:52:13.822: INFO: Got endpoints: latency-svc-2lfj4 [764.841719ms]
Feb 28 15:52:13.834: INFO: Created: latency-svc-4zg4s
Feb 28 15:52:13.855: INFO: Got endpoints: latency-svc-g9pcl [745.43926ms]
Feb 28 15:52:13.870: INFO: Created: latency-svc-pzkvx
Feb 28 15:52:13.901: INFO: Created: latency-svc-j9gxb
Feb 28 15:52:13.906: INFO: Got endpoints: latency-svc-tfh4w [749.649332ms]
Feb 28 15:52:13.954: INFO: Created: latency-svc-swr9q
Feb 28 15:52:13.954: INFO: Got endpoints: latency-svc-9lqsg [748.923611ms]
Feb 28 15:52:14.002: INFO: Created: latency-svc-8wqct
Feb 28 15:52:14.005: INFO: Got endpoints: latency-svc-9mzdq [748.640271ms]
Feb 28 15:52:14.054: INFO: Created: latency-svc-92bs7
Feb 28 15:52:14.056: INFO: Got endpoints: latency-svc-djms7 [744.98597ms]
Feb 28 15:52:14.104: INFO: Created: latency-svc-54rbb
Feb 28 15:52:14.104: INFO: Got endpoints: latency-svc-9dvrs [750.071052ms]
Feb 28 15:52:14.149: INFO: Created: latency-svc-pclc5
Feb 28 15:52:14.153: INFO: Got endpoints: latency-svc-9xcrn [749.591542ms]
Feb 28 15:52:14.198: INFO: Created: latency-svc-cw4bl
Feb 28 15:52:14.202: INFO: Got endpoints: latency-svc-bfhvh [746.75946ms]
Feb 28 15:52:14.250: INFO: Created: latency-svc-qp8x9
Feb 28 15:52:14.254: INFO: Got endpoints: latency-svc-7pfrm [744.908226ms]
Feb 28 15:52:14.301: INFO: Created: latency-svc-2zszp
Feb 28 15:52:14.314: INFO: Got endpoints: latency-svc-jh8s4 [760.082981ms]
Feb 28 15:52:14.354: INFO: Got endpoints: latency-svc-9dxzf [735.989061ms]
Feb 28 15:52:14.366: INFO: Created: latency-svc-z42t6
Feb 28 15:52:14.400: INFO: Created: latency-svc-4lhdw
Feb 28 15:52:14.405: INFO: Got endpoints: latency-svc-7zprf [750.442164ms]
Feb 28 15:52:14.450: INFO: Created: latency-svc-4vzr5
Feb 28 15:52:14.455: INFO: Got endpoints: latency-svc-6twjg [746.054948ms]
Feb 28 15:52:14.500: INFO: Created: latency-svc-shg6t
Feb 28 15:52:14.506: INFO: Got endpoints: latency-svc-4zg4s [748.667187ms]
Feb 28 15:52:14.552: INFO: Created: latency-svc-ztfnt
Feb 28 15:52:14.555: INFO: Got endpoints: latency-svc-pzkvx [732.51097ms]
Feb 28 15:52:14.602: INFO: Created: latency-svc-pfwl7
Feb 28 15:52:14.606: INFO: Got endpoints: latency-svc-j9gxb [751.30349ms]
Feb 28 15:52:14.654: INFO: Got endpoints: latency-svc-swr9q [747.651648ms]
Feb 28 15:52:14.658: INFO: Created: latency-svc-j79xb
Feb 28 15:52:14.698: INFO: Created: latency-svc-7krhc
Feb 28 15:52:14.703: INFO: Got endpoints: latency-svc-8wqct [748.459199ms]
Feb 28 15:52:14.748: INFO: Created: latency-svc-cr6j8
Feb 28 15:52:14.751: INFO: Got endpoints: latency-svc-92bs7 [746.417331ms]
Feb 28 15:52:14.798: INFO: Created: latency-svc-xlqrk
Feb 28 15:52:14.803: INFO: Got endpoints: latency-svc-54rbb [746.299947ms]
Feb 28 15:52:14.850: INFO: Created: latency-svc-pgxbr
Feb 28 15:52:14.857: INFO: Got endpoints: latency-svc-pclc5 [752.511257ms]
Feb 28 15:52:14.903: INFO: Got endpoints: latency-svc-cw4bl [749.585888ms]
Feb 28 15:52:14.907: INFO: Created: latency-svc-g6znk
Feb 28 15:52:14.950: INFO: Created: latency-svc-sjfzc
Feb 28 15:52:14.954: INFO: Got endpoints: latency-svc-qp8x9 [751.894043ms]
Feb 28 15:52:15.002: INFO: Created: latency-svc-mgjx5
Feb 28 15:52:15.006: INFO: Got endpoints: latency-svc-2zszp [751.973133ms]
Feb 28 15:52:15.053: INFO: Created: latency-svc-mfm8c
Feb 28 15:52:15.054: INFO: Got endpoints: latency-svc-z42t6 [739.252436ms]
Feb 28 15:52:15.098: INFO: Created: latency-svc-gfc5w
Feb 28 15:52:15.106: INFO: Got endpoints: latency-svc-4lhdw [751.915228ms]
Feb 28 15:52:15.154: INFO: Got endpoints: latency-svc-4vzr5 [749.512066ms]
Feb 28 15:52:15.155: INFO: Created: latency-svc-rqsfm
Feb 28 15:52:15.201: INFO: Created: latency-svc-nfbns
Feb 28 15:52:15.204: INFO: Got endpoints: latency-svc-shg6t [749.024084ms]
Feb 28 15:52:15.255: INFO: Got endpoints: latency-svc-ztfnt [748.94042ms]
Feb 28 15:52:15.265: INFO: Created: latency-svc-tz6jq
Feb 28 15:52:15.305: INFO: Created: latency-svc-8lwh7
Feb 28 15:52:15.305: INFO: Got endpoints: latency-svc-pfwl7 [750.27662ms]
Feb 28 15:52:15.355: INFO: Created: latency-svc-nqshd
Feb 28 15:52:15.356: INFO: Got endpoints: latency-svc-j79xb [749.606463ms]
Feb 28 15:52:15.400: INFO: Created: latency-svc-m2d7l
Feb 28 15:52:15.406: INFO: Got endpoints: latency-svc-7krhc [752.431858ms]
Feb 28 15:52:15.458: INFO: Got endpoints: latency-svc-cr6j8 [755.226032ms]
Feb 28 15:52:15.459: INFO: Created: latency-svc-qztvs
Feb 28 15:52:15.510: INFO: Created: latency-svc-8grr6
Feb 28 15:52:15.511: INFO: Got endpoints: latency-svc-xlqrk [760.129042ms]
Feb 28 15:52:15.561: INFO: Got endpoints: latency-svc-pgxbr [758.233351ms]
Feb 28 15:52:15.570: INFO: Created: latency-svc-2svfk
Feb 28 15:52:15.606: INFO: Got endpoints: latency-svc-g6znk [749.393046ms]
Feb 28 15:52:15.614: INFO: Created: latency-svc-ctbhj
Feb 28 15:52:15.658: INFO: Created: latency-svc-6hv2r
Feb 28 15:52:15.661: INFO: Got endpoints: latency-svc-sjfzc [758.418889ms]
Feb 28 15:52:15.709: INFO: Got endpoints: latency-svc-mgjx5 [754.535079ms]
Feb 28 15:52:15.718: INFO: Created: latency-svc-6vz65
Feb 28 15:52:15.754: INFO: Got endpoints: latency-svc-mfm8c [747.970761ms]
Feb 28 15:52:15.758: INFO: Created: latency-svc-p7qgm
Feb 28 15:52:15.814: INFO: Got endpoints: latency-svc-gfc5w [760.640542ms]
Feb 28 15:52:15.824: INFO: Created: latency-svc-t9x4l
Feb 28 15:52:15.854: INFO: Got endpoints: latency-svc-rqsfm [747.919129ms]
Feb 28 15:52:15.862: INFO: Created: latency-svc-c85l7
Feb 28 15:52:15.899: INFO: Created: latency-svc-l7bn8
Feb 28 15:52:15.903: INFO: Got endpoints: latency-svc-nfbns [748.580339ms]
Feb 28 15:52:15.948: INFO: Created: latency-svc-pqzf6
Feb 28 15:52:15.954: INFO: Got endpoints: latency-svc-tz6jq [750.130382ms]
Feb 28 15:52:16.002: INFO: Created: latency-svc-m8g82
Feb 28 15:52:16.010: INFO: Got endpoints: latency-svc-8lwh7 [755.143592ms]
Feb 28 15:52:16.054: INFO: Got endpoints: latency-svc-nqshd [749.036048ms]
Feb 28 15:52:16.059: INFO: Created: latency-svc-nrlsw
Feb 28 15:52:16.098: INFO: Created: latency-svc-6zght
Feb 28 15:52:16.102: INFO: Got endpoints: latency-svc-m2d7l [746.551744ms]
Feb 28 15:52:16.146: INFO: Created: latency-svc-lcbk5
Feb 28 15:52:16.152: INFO: Got endpoints: latency-svc-qztvs [745.477248ms]
Feb 28 15:52:16.197: INFO: Created: latency-svc-cfg46
Feb 28 15:52:16.202: INFO: Got endpoints: latency-svc-8grr6 [743.843857ms]
Feb 28 15:52:16.246: INFO: Created: latency-svc-f7t7t
Feb 28 15:52:16.253: INFO: Got endpoints: latency-svc-2svfk [741.770307ms]
Feb 28 15:52:16.313: INFO: Created: latency-svc-xb42g
Feb 28 15:52:16.317: INFO: Got endpoints: latency-svc-ctbhj [755.791854ms]
Feb 28 15:52:16.353: INFO: Got endpoints: latency-svc-6hv2r [746.913897ms]
Feb 28 15:52:16.364: INFO: Created: latency-svc-v566d
Feb 28 15:52:16.398: INFO: Created: latency-svc-ztv4v
Feb 28 15:52:16.404: INFO: Got endpoints: latency-svc-6vz65 [742.627961ms]
Feb 28 15:52:16.450: INFO: Created: latency-svc-ftplj
Feb 28 15:52:16.454: INFO: Got endpoints: latency-svc-p7qgm [745.070494ms]
Feb 28 15:52:16.499: INFO: Created: latency-svc-s2gv4
Feb 28 15:52:16.503: INFO: Got endpoints: latency-svc-t9x4l [748.642778ms]
Feb 28 15:52:16.550: INFO: Created: latency-svc-qfp65
Feb 28 15:52:16.556: INFO: Got endpoints: latency-svc-c85l7 [742.14527ms]
Feb 28 15:52:16.604: INFO: Created: latency-svc-f4mh9
Feb 28 15:52:16.606: INFO: Got endpoints: latency-svc-l7bn8 [752.111731ms]
Feb 28 15:52:16.652: INFO: Created: latency-svc-z8rkb
Feb 28 15:52:16.654: INFO: Got endpoints: latency-svc-pqzf6 [751.223122ms]
Feb 28 15:52:16.701: INFO: Created: latency-svc-rsdq4
Feb 28 15:52:16.706: INFO: Got endpoints: latency-svc-m8g82 [752.03753ms]
Feb 28 15:52:16.755: INFO: Got endpoints: latency-svc-nrlsw [745.0071ms]
Feb 28 15:52:16.756: INFO: Created: latency-svc-xtjzt
Feb 28 15:52:16.801: INFO: Created: latency-svc-kv5n2
Feb 28 15:52:16.805: INFO: Got endpoints: latency-svc-6zght [750.697576ms]
Feb 28 15:52:16.854: INFO: Got endpoints: latency-svc-lcbk5 [751.803159ms]
Feb 28 15:52:16.855: INFO: Created: latency-svc-fdpcg
Feb 28 15:52:16.898: INFO: Created: latency-svc-bs62l
Feb 28 15:52:16.906: INFO: Got endpoints: latency-svc-cfg46 [754.270898ms]
Feb 28 15:52:16.954: INFO: Created: latency-svc-87mrg
Feb 28 15:52:16.958: INFO: Got endpoints: latency-svc-f7t7t [756.021211ms]
Feb 28 15:52:17.006: INFO: Got endpoints: latency-svc-xb42g [752.984892ms]
Feb 28 15:52:17.007: INFO: Created: latency-svc-ltkpv
Feb 28 15:52:17.051: INFO: Created: latency-svc-rbzws
Feb 28 15:52:17.054: INFO: Got endpoints: latency-svc-v566d [737.505597ms]
Feb 28 15:52:17.102: INFO: Created: latency-svc-dkz7h
Feb 28 15:52:17.110: INFO: Got endpoints: latency-svc-ztv4v [756.856278ms]
Feb 28 15:52:17.158: INFO: Got endpoints: latency-svc-ftplj [754.153762ms]
Feb 28 15:52:17.160: INFO: Created: latency-svc-ffvb2
Feb 28 15:52:17.204: INFO: Created: latency-svc-tp8ph
Feb 28 15:52:17.206: INFO: Got endpoints: latency-svc-s2gv4 [752.243298ms]
Feb 28 15:52:17.254: INFO: Created: latency-svc-gpzs6
Feb 28 15:52:17.254: INFO: Got endpoints: latency-svc-qfp65 [751.507633ms]
Feb 28 15:52:17.301: INFO: Created: latency-svc-6lfpr
Feb 28 15:52:17.310: INFO: Got endpoints: latency-svc-f4mh9 [753.787446ms]
Feb 28 15:52:17.362: INFO: Got endpoints: latency-svc-z8rkb [755.803587ms]
Feb 28 15:52:17.366: INFO: Created: latency-svc-w228v
Feb 28 15:52:17.405: INFO: Got endpoints: latency-svc-rsdq4 [750.600307ms]
Feb 28 15:52:17.414: INFO: Created: latency-svc-qlw6r
Feb 28 15:52:17.449: INFO: Created: latency-svc-s9zc9
Feb 28 15:52:17.453: INFO: Got endpoints: latency-svc-xtjzt [747.112262ms]
Feb 28 15:52:17.501: INFO: Created: latency-svc-897w4
Feb 28 15:52:17.505: INFO: Got endpoints: latency-svc-kv5n2 [750.110284ms]
Feb 28 15:52:17.550: INFO: Created: latency-svc-ctb8r
Feb 28 15:52:17.558: INFO: Got endpoints: latency-svc-fdpcg [753.208848ms]
Feb 28 15:52:17.606: INFO: Created: latency-svc-kgm7m
Feb 28 15:52:17.610: INFO: Got endpoints: latency-svc-bs62l [755.960541ms]
Feb 28 15:52:17.658: INFO: Got endpoints: latency-svc-87mrg [752.042385ms]
Feb 28 15:52:17.662: INFO: Created: latency-svc-rnwfl
Feb 28 15:52:17.706: INFO: Created: latency-svc-6t46x
Feb 28 15:52:17.710: INFO: Got endpoints: latency-svc-ltkpv [751.898479ms]
Feb 28 15:52:17.758: INFO: Got endpoints: latency-svc-rbzws [751.914189ms]
Feb 28 15:52:17.762: INFO: Created: latency-svc-fx9h6
Feb 28 15:52:17.821: INFO: Got endpoints: latency-svc-dkz7h [766.183988ms]
Feb 28 15:52:17.834: INFO: Created: latency-svc-8v27b
Feb 28 15:52:17.850: INFO: Got endpoints: latency-svc-ffvb2 [740.067764ms]
Feb 28 15:52:17.866: INFO: Created: latency-svc-2hhpg
Feb 28 15:52:17.898: INFO: Created: latency-svc-nb9z7
Feb 28 15:52:17.902: INFO: Got endpoints: latency-svc-tp8ph [743.929764ms]
Feb 28 15:52:17.947: INFO: Created: latency-svc-wmfn4
Feb 28 15:52:17.954: INFO: Got endpoints: latency-svc-gpzs6 [747.888555ms]
Feb 28 15:52:17.998: INFO: Created: latency-svc-7rcck
Feb 28 15:52:18.003: INFO: Got endpoints: latency-svc-6lfpr [748.193497ms]
Feb 28 15:52:18.050: INFO: Created: latency-svc-7prft
Feb 28 15:52:18.054: INFO: Got endpoints: latency-svc-w228v [743.786861ms]
Feb 28 15:52:18.102: INFO: Created: latency-svc-cvwkm
Feb 28 15:52:18.105: INFO: Got endpoints: latency-svc-qlw6r [742.810151ms]
Feb 28 15:52:18.152: INFO: Created: latency-svc-vwxkl
Feb 28 15:52:18.156: INFO: Got endpoints: latency-svc-s9zc9 [751.41753ms]
Feb 28 15:52:18.206: INFO: Got endpoints: latency-svc-897w4 [752.757017ms]
Feb 28 15:52:18.207: INFO: Created: latency-svc-4cbpx
Feb 28 15:52:18.258: INFO: Got endpoints: latency-svc-ctb8r [752.722518ms]
Feb 28 15:52:18.259: INFO: Created: latency-svc-t7qj4
Feb 28 15:52:18.306: INFO: Created: latency-svc-th798
Feb 28 15:52:18.310: INFO: Got endpoints: latency-svc-kgm7m [751.877155ms]
Feb 28 15:52:18.360: INFO: Got endpoints: latency-svc-rnwfl [749.507551ms]
Feb 28 15:52:18.373: INFO: Created: latency-svc-t8qg4
Feb 28 15:52:18.407: INFO: Got endpoints: latency-svc-6t46x [748.142881ms]
Feb 28 15:52:18.410: INFO: Created: latency-svc-jtll9
Feb 28 15:52:18.454: INFO: Created: latency-svc-j6sds
Feb 28 15:52:18.458: INFO: Got endpoints: latency-svc-fx9h6 [747.957769ms]
Feb 28 15:52:18.506: INFO: Created: latency-svc-mh6mc
Feb 28 15:52:18.510: INFO: Got endpoints: latency-svc-8v27b [751.92807ms]
Feb 28 15:52:18.554: INFO: Got endpoints: latency-svc-2hhpg [733.594933ms]
Feb 28 15:52:18.562: INFO: Created: latency-svc-bnx9s
Feb 28 15:52:18.602: INFO: Created: latency-svc-qx82h
Feb 28 15:52:18.606: INFO: Got endpoints: latency-svc-nb9z7 [755.890706ms]
Feb 28 15:52:18.654: INFO: Created: latency-svc-z8nbr
Feb 28 15:52:18.658: INFO: Got endpoints: latency-svc-wmfn4 [755.980085ms]
Feb 28 15:52:18.702: INFO: Created: latency-svc-4flq5
Feb 28 15:52:18.710: INFO: Got endpoints: latency-svc-7rcck [755.998224ms]
Feb 28 15:52:18.754: INFO: Got endpoints: latency-svc-7prft [751.515343ms]
Feb 28 15:52:18.759: INFO: Created: latency-svc-csfsd
Feb 28 15:52:18.801: INFO: Created: latency-svc-l6rxj
Feb 28 15:52:18.806: INFO: Got endpoints: latency-svc-cvwkm [752.006369ms]
Feb 28 15:52:18.864: INFO: Got endpoints: latency-svc-vwxkl [759.121275ms]
Feb 28 15:52:18.870: INFO: Created: latency-svc-n4gwk
Feb 28 15:52:18.914: INFO: Created: latency-svc-fc745
Feb 28 15:52:18.918: INFO: Got endpoints: latency-svc-4cbpx [761.81941ms]
Feb 28 15:52:18.951: INFO: Got endpoints: latency-svc-t7qj4 [744.63346ms]
Feb 28 15:52:18.966: INFO: Created: latency-svc-tx6qf
Feb 28 15:52:19.002: INFO: Got endpoints: latency-svc-th798 [743.90058ms]
Feb 28 15:52:19.054: INFO: Got endpoints: latency-svc-t8qg4 [744.08151ms]
Feb 28 15:52:19.102: INFO: Got endpoints: latency-svc-jtll9 [742.433863ms]
Feb 28 15:52:19.154: INFO: Got endpoints: latency-svc-j6sds [747.809708ms]
Feb 28 15:52:19.202: INFO: Got endpoints: latency-svc-mh6mc [743.688279ms]
Feb 28 15:52:19.252: INFO: Got endpoints: latency-svc-bnx9s [741.464547ms]
Feb 28 15:52:19.306: INFO: Got endpoints: latency-svc-qx82h [751.920887ms]
Feb 28 15:52:19.354: INFO: Got endpoints: latency-svc-z8nbr [747.973686ms]
Feb 28 15:52:19.401: INFO: Got endpoints: latency-svc-4flq5 [742.429198ms]
Feb 28 15:52:19.454: INFO: Got endpoints: latency-svc-csfsd [744.005315ms]
Feb 28 15:52:19.502: INFO: Got endpoints: latency-svc-l6rxj [747.89978ms]
Feb 28 15:52:19.553: INFO: Got endpoints: latency-svc-n4gwk [746.317098ms]
Feb 28 15:52:19.602: INFO: Got endpoints: latency-svc-fc745 [738.034943ms]
Feb 28 15:52:19.654: INFO: Got endpoints: latency-svc-tx6qf [735.975004ms]
Feb 28 15:52:19.654: INFO: Latencies: [59.550011ms 66.731944ms 67.845659ms 82.377077ms 98.188677ms 104.760676ms 126.026709ms 128.253571ms 141.45438ms 161.665114ms 170.156431ms 178.928896ms 181.591543ms 183.536394ms 183.753506ms 188.821022ms 191.4452ms 191.6658ms 192.046098ms 193.470788ms 194.556149ms 195.22981ms 198.456504ms 199.390877ms 199.766958ms 202.703928ms 204.389466ms 204.857192ms 205.932214ms 207.92932ms 211.92776ms 212.025614ms 212.440948ms 215.33438ms 217.013311ms 217.157341ms 218.439398ms 225.356193ms 244.910965ms 245.352806ms 251.835478ms 262.080331ms 301.562831ms 315.884897ms 363.455159ms 395.3897ms 443.865734ms 468.536045ms 517.344084ms 556.75893ms 594.283447ms 624.505822ms 667.492129ms 712.03135ms 732.51097ms 733.594933ms 735.975004ms 735.989061ms 737.505597ms 738.034943ms 739.252436ms 740.067764ms 741.464547ms 741.770307ms 742.14527ms 742.429198ms 742.433863ms 742.627961ms 742.810151ms 743.688279ms 743.786861ms 743.843857ms 743.90058ms 743.929764ms 744.005315ms 744.039289ms 744.057468ms 744.08151ms 744.112329ms 744.63346ms 744.908226ms 744.98597ms 745.0071ms 745.070494ms 745.280557ms 745.43926ms 745.477248ms 746.054948ms 746.115844ms 746.235437ms 746.299947ms 746.317098ms 746.417331ms 746.551744ms 746.75946ms 746.913897ms 747.112262ms 747.651648ms 747.758356ms 747.764293ms 747.809708ms 747.888555ms 747.89978ms 747.919129ms 747.933335ms 747.957769ms 747.970761ms 747.973686ms 748.038933ms 748.142881ms 748.193497ms 748.459199ms 748.580339ms 748.640271ms 748.642778ms 748.667187ms 748.923611ms 748.94042ms 748.945106ms 749.003828ms 749.024084ms 749.036048ms 749.232425ms 749.235518ms 749.393046ms 749.507551ms 749.512066ms 749.585888ms 749.591542ms 749.606463ms 749.649332ms 750.071052ms 750.110284ms 750.130382ms 750.27662ms 750.442164ms 750.48676ms 750.600307ms 750.697576ms 750.771893ms 750.941753ms 751.223122ms 751.30349ms 751.394321ms 751.41753ms 751.507633ms 751.515343ms 751.691448ms 751.803159ms 751.877155ms 751.894043ms 751.898479ms 751.914189ms 751.915228ms 751.920887ms 751.92807ms 751.973133ms 752.006369ms 752.03753ms 752.042385ms 752.074606ms 752.111731ms 752.243298ms 752.431858ms 752.479674ms 752.511257ms 752.607375ms 752.722518ms 752.757017ms 752.761154ms 752.984892ms 753.208848ms 753.475718ms 753.787446ms 753.789151ms 754.153762ms 754.270898ms 754.357819ms 754.406195ms 754.535079ms 755.143592ms 755.226032ms 755.791854ms 755.803587ms 755.890706ms 755.960541ms 755.980085ms 755.998224ms 756.021211ms 756.856278ms 758.233351ms 758.418889ms 759.121275ms 759.846169ms 760.082981ms 760.129042ms 760.640542ms 761.81941ms 764.841719ms 766.183988ms]
Feb 28 15:52:19.654: INFO: 50 %ile: 747.809708ms
Feb 28 15:52:19.654: INFO: 90 %ile: 755.143592ms
Feb 28 15:52:19.654: INFO: 99 %ile: 764.841719ms
Feb 28 15:52:19.654: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:52:19.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4667" for this suite.
Feb 28 15:52:43.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:52:45.357: INFO: namespace svc-latency-4667 deletion completed in 25.662738978s
•SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:52:45.358: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 28 15:52:45.555: INFO: PodSpec: initContainers in spec.initContainers
Feb 28 15:53:29.307: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-df1e3042-a8ab-4723-833e-68412641ed16", GenerateName:"", Namespace:"init-container-478", SelfLink:"/api/v1/namespaces/init-container-478/pods/pod-init-df1e3042-a8ab-4723-833e-68412641ed16", UID:"0eb8dcb1-4ccf-4e8c-a480-99b513bea1b6", ResourceVersion:"3570", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63718501965, loc:(*time.Location)(0x7eb0a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"554933165"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-w5hmk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0025f2000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-w5hmk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-w5hmk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-w5hmk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000c68098), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"bootstrap-e2e-minion-group-q1l7", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0024be000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c68140)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c68170)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000c68178), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000c6817c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718501965, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718501965, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718501965, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718501965, loc:(*time.Location)(0x7eb0a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.138.0.4", PodIP:"10.64.3.6", StartTime:(*v1.Time)(0xc001a9e180), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0025b6070)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0025b60e0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:e004c2cc521c95383aebb1fb5893719aa7a8eae2e7a71f316a4410784edb00a9", ContainerID:"docker://d47dd760099160db541e88d0c7becacd8ae17322cc5b8da47f289624a780df55"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001a9e260), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001a9e1c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:53:29.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-478" for this suite.
Feb 28 15:53:51.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:53:52.930: INFO: namespace init-container-478 deletion completed in 23.58285583s
•
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:53:52.930: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 28 15:53:55.955: INFO: Successfully updated pod "annotationupdatee49fb38f-3cd1-4b88-b94c-340756b3d6ae"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:53:58.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2992" for this suite.
Feb 28 15:54:20.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:54:21.685: INFO: namespace downward-api-2992 deletion completed in 23.596156462s
•SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:54:21.685: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 15:54:21.894: INFO: Creating deployment "test-recreate-deployment"
Feb 28 15:54:21.939: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 28 15:54:22.069: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 28 15:54:22.106: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718502061, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718502061, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718502062, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718502061, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 15:54:24.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718502061, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718502061, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718502062, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718502061, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 15:54:26.144: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 28 15:54:26.221: INFO: Updating deployment test-recreate-deployment
Feb 28 15:54:26.221: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 28 15:54:26.397: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-2893,SelfLink:/apis/apps/v1/namespaces/deployment-2893/deployments/test-recreate-deployment,UID:b8ca1b16-0ef6-46b3-b081-792eda620636,ResourceVersion:3752,Generation:2,CreationTimestamp:2020-02-28 15:54:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2020-02-28 15:54:26 +0000 UTC 2020-02-28 15:54:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2020-02-28 15:54:26 +0000 UTC 2020-02-28 15:54:21 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 28 15:54:26.435: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-2893,SelfLink:/apis/apps/v1/namespaces/deployment-2893/replicasets/test-recreate-deployment-5c8c9cc69d,UID:5949c2a4-69f5-4570-b101-7fd278f6c06b,ResourceVersion:3749,Generation:1,CreationTimestamp:2020-02-28 15:54:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment b8ca1b16-0ef6-46b3-b081-792eda620636 0xc000b401b7 0xc000b401b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 15:54:26.435: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 28 15:54:26.436: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-2893,SelfLink:/apis/apps/v1/namespaces/deployment-2893/replicasets/test-recreate-deployment-6df85df6b9,UID:cafc0e61-0178-4652-a2aa-27d77896d76e,ResourceVersion:3743,Generation:2,CreationTimestamp:2020-02-28 15:54:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment b8ca1b16-0ef6-46b3-b081-792eda620636 0xc000b40287 0xc000b40288}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 15:54:26.474: INFO: Pod "test-recreate-deployment-5c8c9cc69d-tc6wh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-tc6wh,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-2893,SelfLink:/api/v1/namespaces/deployment-2893/pods/test-recreate-deployment-5c8c9cc69d-tc6wh,UID:43e9585d-9792-4fcd-acce-c3854bfaf496,ResourceVersion:3751,Generation:0,CreationTimestamp:2020-02-28 15:54:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 5949c2a4-69f5-4570-b101-7fd278f6c06b 0xc000bc9137 0xc000bc9138}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kx4kj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kx4kj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kx4kj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000bc91b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000bc91d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:54:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:54:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:54:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 15:54:26 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2020-02-28 15:54:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:54:26.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2893" for this suite.
Feb 28 15:54:32.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:54:34.158: INFO: namespace deployment-2893 deletion completed in 7.644644957s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:54:34.158: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4781
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Feb 28 15:54:34.473: INFO: Found 1 stateful pods, waiting for 3
Feb 28 15:54:44.514: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 15:54:44.514: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 15:54:44.514: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 28 15:54:44.718: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 28 15:54:44.885: INFO: Updating stateful set ss2
Feb 28 15:54:44.963: INFO: Waiting for Pod statefulset-4781/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Feb 28 15:54:55.040: INFO: Waiting for Pod statefulset-4781/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Feb 28 15:55:05.250: INFO: Found 2 stateful pods, waiting for 3
Feb 28 15:55:15.289: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 15:55:15.289: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 15:55:15.289: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 28 15:55:15.458: INFO: Updating stateful set ss2
Feb 28 15:55:15.534: INFO: Waiting for Pod statefulset-4781/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Feb 28 15:55:25.613: INFO: Waiting for Pod statefulset-4781/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Feb 28 15:55:35.704: INFO: Updating stateful set ss2
Feb 28 15:55:35.782: INFO: Waiting for StatefulSet statefulset-4781/ss2 to complete update
Feb 28 15:55:35.782: INFO: Waiting for Pod statefulset-4781/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Feb 28 15:55:45.859: INFO: Waiting for StatefulSet statefulset-4781/ss2 to complete update
Feb 28 15:55:45.859: INFO: Waiting for Pod statefulset-4781/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 28 15:55:55.860: INFO: Deleting all statefulset in ns statefulset-4781
Feb 28 15:55:55.898: INFO: Scaling statefulset ss2 to 0
Feb 28 15:56:26.060: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 15:56:26.097: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:56:26.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4781" for this suite.
Feb 28 15:56:32.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:56:33.823: INFO: namespace statefulset-4781 deletion completed in 7.562991808s
•SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:56:33.823: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Feb 28 15:56:34.058: INFO: Waiting up to 5m0s for pod "client-containers-77cb8fb5-0304-458a-974e-f12f57250f6f" in namespace "containers-3562" to be "success or failure"
Feb 28 15:56:34.096: INFO: Pod "client-containers-77cb8fb5-0304-458a-974e-f12f57250f6f": Phase="Pending", Reason="", readiness=false. Elapsed: 37.477106ms
Feb 28 15:56:36.133: INFO: Pod "client-containers-77cb8fb5-0304-458a-974e-f12f57250f6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075354407s
Feb 28 15:56:38.172: INFO: Pod "client-containers-77cb8fb5-0304-458a-974e-f12f57250f6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.113637729s
STEP: Saw pod success
Feb 28 15:56:38.172: INFO: Pod "client-containers-77cb8fb5-0304-458a-974e-f12f57250f6f" satisfied condition "success or failure"
Feb 28 15:56:38.211: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod client-containers-77cb8fb5-0304-458a-974e-f12f57250f6f container test-container: <nil>
STEP: delete the pod
Feb 28 15:56:38.305: INFO: Waiting for pod client-containers-77cb8fb5-0304-458a-974e-f12f57250f6f to disappear
Feb 28 15:56:38.343: INFO: Pod client-containers-77cb8fb5-0304-458a-974e-f12f57250f6f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:56:38.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3562" for this suite.
Feb 28 15:56:44.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:56:46.055: INFO: namespace containers-3562 deletion completed in 7.673535263s
•
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:56:46.055: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 28 15:56:46.485: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3030,SelfLink:/api/v1/namespaces/watch-3030/configmaps/e2e-watch-test-label-changed,UID:b2d6e3a0-f619-4564-a796-57c5806a1585,ResourceVersion:4233,Generation:0,CreationTimestamp:2020-02-28 15:56:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 15:56:46.485: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3030,SelfLink:/api/v1/namespaces/watch-3030/configmaps/e2e-watch-test-label-changed,UID:b2d6e3a0-f619-4564-a796-57c5806a1585,ResourceVersion:4234,Generation:0,CreationTimestamp:2020-02-28 15:56:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 28 15:56:46.485: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3030,SelfLink:/api/v1/namespaces/watch-3030/configmaps/e2e-watch-test-label-changed,UID:b2d6e3a0-f619-4564-a796-57c5806a1585,ResourceVersion:4235,Generation:0,CreationTimestamp:2020-02-28 15:56:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 28 15:56:56.760: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3030,SelfLink:/api/v1/namespaces/watch-3030/configmaps/e2e-watch-test-label-changed,UID:b2d6e3a0-f619-4564-a796-57c5806a1585,ResourceVersion:4260,Generation:0,CreationTimestamp:2020-02-28 15:56:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 15:56:56.760: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3030,SelfLink:/api/v1/namespaces/watch-3030/configmaps/e2e-watch-test-label-changed,UID:b2d6e3a0-f619-4564-a796-57c5806a1585,ResourceVersion:4261,Generation:0,CreationTimestamp:2020-02-28 15:56:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 28 15:56:56.760: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3030,SelfLink:/api/v1/namespaces/watch-3030/configmaps/e2e-watch-test-label-changed,UID:b2d6e3a0-f619-4564-a796-57c5806a1585,ResourceVersion:4262,Generation:0,CreationTimestamp:2020-02-28 15:56:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:56:56.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3030" for this suite.
Feb 28 15:57:02.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:57:04.452: INFO: namespace watch-3030 deletion completed in 7.653036174s
•SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:57:04.452: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 15:57:04.875: INFO: Create a RollingUpdate DaemonSet
Feb 28 15:57:04.917: INFO: Check that daemon pods launch on every node of the cluster
Feb 28 15:57:04.975: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 15:57:05.036: INFO: Number of nodes with available pods: 0
Feb 28 15:57:05.036: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 15:57:06.075: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 15:57:06.114: INFO: Number of nodes with available pods: 0
Feb 28 15:57:06.114: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 15:57:07.075: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 15:57:07.119: INFO: Number of nodes with available pods: 2
Feb 28 15:57:07.119: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 15:57:08.075: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 15:57:08.113: INFO: Number of nodes with available pods: 3
Feb 28 15:57:08.113: INFO: Number of running nodes: 3, number of available pods: 3
Feb 28 15:57:08.113: INFO: Update the DaemonSet to trigger a rollout
Feb 28 15:57:08.195: INFO: Updating DaemonSet daemon-set
Feb 28 15:57:18.349: INFO: Roll back the DaemonSet before rollout is complete
Feb 28 15:57:18.429: INFO: Updating DaemonSet daemon-set
Feb 28 15:57:18.429: INFO: Make sure DaemonSet rollback is complete
Feb 28 15:57:18.468: INFO: Wrong image for pod: daemon-set-78n48. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Feb 28 15:57:18.468: INFO: Pod daemon-set-78n48 is not available
Feb 28 15:57:18.507: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 15:57:19.545: INFO: Wrong image for pod: daemon-set-78n48. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Feb 28 15:57:19.545: INFO: Pod daemon-set-78n48 is not available
Feb 28 15:57:19.586: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 15:57:20.545: INFO: Pod daemon-set-2nqqz is not available
Feb 28 15:57:20.583: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1870, will wait for the garbage collector to delete the pods
Feb 28 15:57:20.788: INFO: Deleting DaemonSet.extensions daemon-set took: 42.071667ms
Feb 28 15:57:21.389: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.305484ms
Feb 28 15:57:37.428: INFO: Number of nodes with available pods: 0
Feb 28 15:57:37.428: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 15:57:37.467: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1870/daemonsets","resourceVersion":"4416"},"items":null}

Feb 28 15:57:37.505: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1870/pods","resourceVersion":"4416"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:57:37.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1870" for this suite.
Feb 28 15:57:43.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:57:45.273: INFO: namespace daemonsets-1870 deletion completed in 7.579525492s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:57:45.274: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 28 15:57:45.516: INFO: Waiting up to 5m0s for pod "pod-7f12b61b-d88d-443c-9141-990147128788" in namespace "emptydir-9289" to be "success or failure"
Feb 28 15:57:45.553: INFO: Pod "pod-7f12b61b-d88d-443c-9141-990147128788": Phase="Pending", Reason="", readiness=false. Elapsed: 37.617858ms
Feb 28 15:57:47.592: INFO: Pod "pod-7f12b61b-d88d-443c-9141-990147128788": Phase="Running", Reason="", readiness=true. Elapsed: 2.076060693s
Feb 28 15:57:49.630: INFO: Pod "pod-7f12b61b-d88d-443c-9141-990147128788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.114297553s
STEP: Saw pod success
Feb 28 15:57:49.630: INFO: Pod "pod-7f12b61b-d88d-443c-9141-990147128788" satisfied condition "success or failure"
Feb 28 15:57:49.668: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-7f12b61b-d88d-443c-9141-990147128788 container test-container: <nil>
STEP: delete the pod
Feb 28 15:57:49.761: INFO: Waiting for pod pod-7f12b61b-d88d-443c-9141-990147128788 to disappear
Feb 28 15:57:49.802: INFO: Pod pod-7f12b61b-d88d-443c-9141-990147128788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:57:49.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9289" for this suite.
Feb 28 15:57:55.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:57:57.400: INFO: namespace emptydir-9289 deletion completed in 7.559374599s
•SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:57:57.400: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1721
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 15:57:57.555: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-1105'
Feb 28 15:57:58.082: INFO: stderr: ""
Feb 28 15:57:58.082: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 28 15:58:03.133: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pod e2e-test-nginx-pod --namespace=kubectl-1105 -o json'
Feb 28 15:58:03.370: INFO: stderr: ""
Feb 28 15:58:03.370: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-02-28T15:57:58Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-1105\",\n        \"resourceVersion\": \"4494\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1105/pods/e2e-test-nginx-pod\",\n        \"uid\": \"5b36fa9b-6a9d-4640-8f1e-d7896abe527f\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-zztx2\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"bootstrap-e2e-minion-group-q1l7\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-zztx2\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-zztx2\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-28T15:57:58Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-28T15:58:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-28T15:58:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-28T15:57:58Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://9afe490268f249d22b7c0c5d78cdc904ba2ce37a7d0bc6f9c8d5ef2a7920ee46\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-02-28T15:57:59Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.138.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.64.3.13\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-02-28T15:57:58Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 28 15:58:03.370: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config replace -f - --namespace=kubectl-1105'
Feb 28 15:58:03.927: INFO: stderr: ""
Feb 28 15:58:03.927: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1726
Feb 28 15:58:03.965: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete pods e2e-test-nginx-pod --namespace=kubectl-1105'
Feb 28 15:58:08.688: INFO: stderr: ""
Feb 28 15:58:08.688: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:58:08.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1105" for this suite.
Feb 28 15:58:14.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:58:16.348: INFO: namespace kubectl-1105 deletion completed in 7.621211132s
•SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:58:16.348: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 28 15:58:16.884: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 15:58:16.938: INFO: Number of nodes with available pods: 0
Feb 28 15:58:16.938: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 15:58:17.976: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 15:58:18.015: INFO: Number of nodes with available pods: 0
Feb 28 15:58:18.015: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 15:58:18.977: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 15:58:19.014: INFO: Number of nodes with available pods: 3
Feb 28 15:58:19.015: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 28 15:58:19.190: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 15:58:19.232: INFO: Number of nodes with available pods: 2
Feb 28 15:58:19.232: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 15:58:20.273: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 15:58:20.311: INFO: Number of nodes with available pods: 2
Feb 28 15:58:20.311: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 15:58:21.270: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 15:58:21.308: INFO: Number of nodes with available pods: 2
Feb 28 15:58:21.308: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 15:58:22.271: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 15:58:22.309: INFO: Number of nodes with available pods: 3
Feb 28 15:58:22.309: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9834, will wait for the garbage collector to delete the pods
Feb 28 15:58:22.513: INFO: Deleting DaemonSet.extensions daemon-set took: 41.529763ms
Feb 28 15:58:23.014: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.308916ms
Feb 28 15:58:28.753: INFO: Number of nodes with available pods: 0
Feb 28 15:58:28.753: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 15:58:28.790: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9834/daemonsets","resourceVersion":"4609"},"items":null}

Feb 28 15:58:28.828: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9834/pods","resourceVersion":"4609"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:58:28.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9834" for this suite.
Feb 28 15:58:35.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:58:36.574: INFO: namespace daemonsets-9834 deletion completed in 7.55733176s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:58:36.575: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-58b772b6-d6a2-49f1-9410-e38e9eea82aa
STEP: Creating a pod to test consume secrets
Feb 28 15:58:36.851: INFO: Waiting up to 5m0s for pod "pod-secrets-05ea7b8f-26c9-484f-8b1e-d72c5d158891" in namespace "secrets-3318" to be "success or failure"
Feb 28 15:58:36.889: INFO: Pod "pod-secrets-05ea7b8f-26c9-484f-8b1e-d72c5d158891": Phase="Pending", Reason="", readiness=false. Elapsed: 37.713045ms
Feb 28 15:58:38.927: INFO: Pod "pod-secrets-05ea7b8f-26c9-484f-8b1e-d72c5d158891": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075780933s
STEP: Saw pod success
Feb 28 15:58:38.927: INFO: Pod "pod-secrets-05ea7b8f-26c9-484f-8b1e-d72c5d158891" satisfied condition "success or failure"
Feb 28 15:58:38.965: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-secrets-05ea7b8f-26c9-484f-8b1e-d72c5d158891 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 15:58:39.060: INFO: Waiting for pod pod-secrets-05ea7b8f-26c9-484f-8b1e-d72c5d158891 to disappear
Feb 28 15:58:39.097: INFO: Pod pod-secrets-05ea7b8f-26c9-484f-8b1e-d72c5d158891 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:58:39.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3318" for this suite.
Feb 28 15:58:45.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:58:46.704: INFO: namespace secrets-3318 deletion completed in 7.568151688s
•
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:58:46.704: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:58:49.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8358" for this suite.
Feb 28 15:59:31.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:59:32.670: INFO: namespace kubelet-test-8358 deletion completed in 43.565682013s
•SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:59:32.670: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-780ac719-39a0-4ddc-9a54-8a87743194c3
STEP: Creating a pod to test consume configMaps
Feb 28 15:59:32.915: INFO: Waiting up to 5m0s for pod "pod-configmaps-1b23a41b-024b-493f-baf8-8ae01c33ba74" in namespace "configmap-5240" to be "success or failure"
Feb 28 15:59:32.952: INFO: Pod "pod-configmaps-1b23a41b-024b-493f-baf8-8ae01c33ba74": Phase="Pending", Reason="", readiness=false. Elapsed: 37.898163ms
Feb 28 15:59:34.990: INFO: Pod "pod-configmaps-1b23a41b-024b-493f-baf8-8ae01c33ba74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075766575s
STEP: Saw pod success
Feb 28 15:59:34.990: INFO: Pod "pod-configmaps-1b23a41b-024b-493f-baf8-8ae01c33ba74" satisfied condition "success or failure"
Feb 28 15:59:35.029: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-configmaps-1b23a41b-024b-493f-baf8-8ae01c33ba74 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 15:59:35.127: INFO: Waiting for pod pod-configmaps-1b23a41b-024b-493f-baf8-8ae01c33ba74 to disappear
Feb 28 15:59:35.165: INFO: Pod pod-configmaps-1b23a41b-024b-493f-baf8-8ae01c33ba74 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 15:59:35.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5240" for this suite.
Feb 28 15:59:41.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 15:59:42.767: INFO: namespace configmap-5240 deletion completed in 7.563913552s
•SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 15:59:42.767: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-lt8f
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 15:59:43.082: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lt8f" in namespace "subpath-6023" to be "success or failure"
Feb 28 15:59:43.127: INFO: Pod "pod-subpath-test-configmap-lt8f": Phase="Pending", Reason="", readiness=false. Elapsed: 44.371205ms
Feb 28 15:59:45.165: INFO: Pod "pod-subpath-test-configmap-lt8f": Phase="Running", Reason="", readiness=true. Elapsed: 2.082195947s
Feb 28 15:59:47.203: INFO: Pod "pod-subpath-test-configmap-lt8f": Phase="Running", Reason="", readiness=true. Elapsed: 4.120200039s
Feb 28 15:59:49.241: INFO: Pod "pod-subpath-test-configmap-lt8f": Phase="Running", Reason="", readiness=true. Elapsed: 6.158557256s
Feb 28 15:59:51.279: INFO: Pod "pod-subpath-test-configmap-lt8f": Phase="Running", Reason="", readiness=true. Elapsed: 8.196583928s
Feb 28 15:59:53.317: INFO: Pod "pod-subpath-test-configmap-lt8f": Phase="Running", Reason="", readiness=true. Elapsed: 10.234754236s
Feb 28 15:59:55.355: INFO: Pod "pod-subpath-test-configmap-lt8f": Phase="Running", Reason="", readiness=true. Elapsed: 12.273008002s
Feb 28 15:59:57.394: INFO: Pod "pod-subpath-test-configmap-lt8f": Phase="Running", Reason="", readiness=true. Elapsed: 14.311645897s
Feb 28 15:59:59.432: INFO: Pod "pod-subpath-test-configmap-lt8f": Phase="Running", Reason="", readiness=true. Elapsed: 16.350034058s
Feb 28 16:00:01.471: INFO: Pod "pod-subpath-test-configmap-lt8f": Phase="Running", Reason="", readiness=true. Elapsed: 18.388087887s
Feb 28 16:00:03.509: INFO: Pod "pod-subpath-test-configmap-lt8f": Phase="Running", Reason="", readiness=true. Elapsed: 20.426177473s
Feb 28 16:00:05.547: INFO: Pod "pod-subpath-test-configmap-lt8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.464434126s
STEP: Saw pod success
Feb 28 16:00:05.547: INFO: Pod "pod-subpath-test-configmap-lt8f" satisfied condition "success or failure"
Feb 28 16:00:05.585: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-subpath-test-configmap-lt8f container test-container-subpath-configmap-lt8f: <nil>
STEP: delete the pod
Feb 28 16:00:05.683: INFO: Waiting for pod pod-subpath-test-configmap-lt8f to disappear
Feb 28 16:00:05.720: INFO: Pod pod-subpath-test-configmap-lt8f no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lt8f
Feb 28 16:00:05.721: INFO: Deleting pod "pod-subpath-test-configmap-lt8f" in namespace "subpath-6023"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:00:05.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6023" for this suite.
Feb 28 16:00:11.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:00:13.377: INFO: namespace subpath-6023 deletion completed in 7.575075991s
•SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:00:13.377: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Feb 28 16:00:13.582: INFO: Waiting up to 5m0s for pod "client-containers-d3fd2afe-278f-4062-9d89-19f13f796f93" in namespace "containers-3809" to be "success or failure"
Feb 28 16:00:13.620: INFO: Pod "client-containers-d3fd2afe-278f-4062-9d89-19f13f796f93": Phase="Pending", Reason="", readiness=false. Elapsed: 37.899131ms
Feb 28 16:00:15.658: INFO: Pod "client-containers-d3fd2afe-278f-4062-9d89-19f13f796f93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076292685s
Feb 28 16:00:17.697: INFO: Pod "client-containers-d3fd2afe-278f-4062-9d89-19f13f796f93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.114656719s
STEP: Saw pod success
Feb 28 16:00:17.697: INFO: Pod "client-containers-d3fd2afe-278f-4062-9d89-19f13f796f93" satisfied condition "success or failure"
Feb 28 16:00:17.734: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod client-containers-d3fd2afe-278f-4062-9d89-19f13f796f93 container test-container: <nil>
STEP: delete the pod
Feb 28 16:00:17.835: INFO: Waiting for pod client-containers-d3fd2afe-278f-4062-9d89-19f13f796f93 to disappear
Feb 28 16:00:17.873: INFO: Pod client-containers-d3fd2afe-278f-4062-9d89-19f13f796f93 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:00:17.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3809" for this suite.
Feb 28 16:00:24.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:00:25.499: INFO: namespace containers-3809 deletion completed in 7.587482733s
•
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:00:25.499: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-351
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 16:00:25.692: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 16:00:50.514: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.3.20:8080/dial?request=hostName&protocol=http&host=10.64.2.22&port=8080&tries=1'] Namespace:pod-network-test-351 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:00:50.514: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:00:50.958: INFO: Waiting for endpoints: map[]
Feb 28 16:00:50.996: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.3.20:8080/dial?request=hostName&protocol=http&host=10.64.3.19&port=8080&tries=1'] Namespace:pod-network-test-351 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:00:50.996: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:00:51.451: INFO: Waiting for endpoints: map[]
Feb 28 16:00:51.490: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.3.20:8080/dial?request=hostName&protocol=http&host=10.64.1.14&port=8080&tries=1'] Namespace:pod-network-test-351 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:00:51.490: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:00:51.891: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:00:51.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-351" for this suite.
Feb 28 16:01:14.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:01:15.493: INFO: namespace pod-network-test-351 deletion completed in 23.564052353s
•SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:01:15.494: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-ef05a391-7830-4a41-aa8e-27634c017042 in namespace container-probe-5530
Feb 28 16:01:17.809: INFO: Started pod busybox-ef05a391-7830-4a41-aa8e-27634c017042 in namespace container-probe-5530
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 16:01:17.846: INFO: Initial restart count of pod busybox-ef05a391-7830-4a41-aa8e-27634c017042 is 0
Feb 28 16:02:12.921: INFO: Restart count of pod container-probe-5530/busybox-ef05a391-7830-4a41-aa8e-27634c017042 is now 1 (55.074921199s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:02:12.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5530" for this suite.
Feb 28 16:02:19.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:02:20.569: INFO: namespace container-probe-5530 deletion completed in 7.56056291s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:02:20.569: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-a7545830-6a86-4327-9293-4094e2a18b9e
STEP: Creating a pod to test consume configMaps
Feb 28 16:02:20.845: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3e68e422-53ba-486b-a9a4-afc696a6e9d6" in namespace "projected-3156" to be "success or failure"
Feb 28 16:02:20.883: INFO: Pod "pod-projected-configmaps-3e68e422-53ba-486b-a9a4-afc696a6e9d6": Phase="Pending", Reason="", readiness=false. Elapsed: 37.672066ms
Feb 28 16:02:22.921: INFO: Pod "pod-projected-configmaps-3e68e422-53ba-486b-a9a4-afc696a6e9d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075787269s
STEP: Saw pod success
Feb 28 16:02:22.921: INFO: Pod "pod-projected-configmaps-3e68e422-53ba-486b-a9a4-afc696a6e9d6" satisfied condition "success or failure"
Feb 28 16:02:22.958: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-projected-configmaps-3e68e422-53ba-486b-a9a4-afc696a6e9d6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 16:02:23.051: INFO: Waiting for pod pod-projected-configmaps-3e68e422-53ba-486b-a9a4-afc696a6e9d6 to disappear
Feb 28 16:02:23.088: INFO: Pod pod-projected-configmaps-3e68e422-53ba-486b-a9a4-afc696a6e9d6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:02:23.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3156" for this suite.
Feb 28 16:02:29.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:02:30.688: INFO: namespace projected-3156 deletion completed in 7.561257612s
•SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:02:30.688: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Feb 28 16:02:30.923: INFO: Waiting up to 5m0s for pod "client-containers-ddb32414-11d7-4afd-82c7-f763e80d601c" in namespace "containers-9350" to be "success or failure"
Feb 28 16:02:30.960: INFO: Pod "client-containers-ddb32414-11d7-4afd-82c7-f763e80d601c": Phase="Pending", Reason="", readiness=false. Elapsed: 37.44263ms
Feb 28 16:02:32.999: INFO: Pod "client-containers-ddb32414-11d7-4afd-82c7-f763e80d601c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076094874s
STEP: Saw pod success
Feb 28 16:02:32.999: INFO: Pod "client-containers-ddb32414-11d7-4afd-82c7-f763e80d601c" satisfied condition "success or failure"
Feb 28 16:02:33.038: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod client-containers-ddb32414-11d7-4afd-82c7-f763e80d601c container test-container: <nil>
STEP: delete the pod
Feb 28 16:02:33.134: INFO: Waiting for pod client-containers-ddb32414-11d7-4afd-82c7-f763e80d601c to disappear
Feb 28 16:02:33.171: INFO: Pod client-containers-ddb32414-11d7-4afd-82c7-f763e80d601c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:02:33.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9350" for this suite.
Feb 28 16:02:39.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:02:40.781: INFO: namespace containers-9350 deletion completed in 7.571696867s
•SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:02:40.782: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-7ec9e749-1cba-42d8-bfb0-4957f44140d6 in namespace container-probe-884
Feb 28 16:02:45.054: INFO: Started pod liveness-7ec9e749-1cba-42d8-bfb0-4957f44140d6 in namespace container-probe-884
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 16:02:45.092: INFO: Initial restart count of pod liveness-7ec9e749-1cba-42d8-bfb0-4957f44140d6 is 0
Feb 28 16:02:59.396: INFO: Restart count of pod container-probe-884/liveness-7ec9e749-1cba-42d8-bfb0-4957f44140d6 is now 1 (14.303897922s elapsed)
Feb 28 16:03:19.781: INFO: Restart count of pod container-probe-884/liveness-7ec9e749-1cba-42d8-bfb0-4957f44140d6 is now 2 (34.688774038s elapsed)
Feb 28 16:03:40.164: INFO: Restart count of pod container-probe-884/liveness-7ec9e749-1cba-42d8-bfb0-4957f44140d6 is now 3 (55.071543122s elapsed)
Feb 28 16:04:00.547: INFO: Restart count of pod container-probe-884/liveness-7ec9e749-1cba-42d8-bfb0-4957f44140d6 is now 4 (1m15.454433395s elapsed)
Feb 28 16:05:13.933: INFO: Restart count of pod container-probe-884/liveness-7ec9e749-1cba-42d8-bfb0-4957f44140d6 is now 5 (2m28.840929252s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:05:13.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-884" for this suite.
Feb 28 16:05:20.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:05:21.590: INFO: namespace container-probe-884 deletion completed in 7.570526974s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:05:21.591: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 16:05:21.838: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f96f8829-83af-4bd9-a9e9-c8c245038fd4" in namespace "projected-5138" to be "success or failure"
Feb 28 16:05:21.891: INFO: Pod "downwardapi-volume-f96f8829-83af-4bd9-a9e9-c8c245038fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 53.278435ms
Feb 28 16:05:23.930: INFO: Pod "downwardapi-volume-f96f8829-83af-4bd9-a9e9-c8c245038fd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.091760909s
STEP: Saw pod success
Feb 28 16:05:23.930: INFO: Pod "downwardapi-volume-f96f8829-83af-4bd9-a9e9-c8c245038fd4" satisfied condition "success or failure"
Feb 28 16:05:23.968: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod downwardapi-volume-f96f8829-83af-4bd9-a9e9-c8c245038fd4 container client-container: <nil>
STEP: delete the pod
Feb 28 16:05:24.075: INFO: Waiting for pod downwardapi-volume-f96f8829-83af-4bd9-a9e9-c8c245038fd4 to disappear
Feb 28 16:05:24.113: INFO: Pod downwardapi-volume-f96f8829-83af-4bd9-a9e9-c8c245038fd4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:05:24.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5138" for this suite.
Feb 28 16:05:30.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:05:31.728: INFO: namespace projected-5138 deletion completed in 7.574962392s
•SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:05:31.728: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-0c931cbd-11bc-4512-ac81-6761898c37b6
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-0c931cbd-11bc-4512-ac81-6761898c37b6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:05:36.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1453" for this suite.
Feb 28 16:05:58.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:05:59.958: INFO: namespace projected-1453 deletion completed in 23.582857177s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:05:59.959: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Feb 28 16:06:00.201: INFO: Waiting up to 5m0s for pod "client-containers-4465c2ce-6934-47da-a78a-eb2b70cd646a" in namespace "containers-4587" to be "success or failure"
Feb 28 16:06:00.238: INFO: Pod "client-containers-4465c2ce-6934-47da-a78a-eb2b70cd646a": Phase="Pending", Reason="", readiness=false. Elapsed: 37.682651ms
Feb 28 16:06:02.276: INFO: Pod "client-containers-4465c2ce-6934-47da-a78a-eb2b70cd646a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075636456s
STEP: Saw pod success
Feb 28 16:06:02.277: INFO: Pod "client-containers-4465c2ce-6934-47da-a78a-eb2b70cd646a" satisfied condition "success or failure"
Feb 28 16:06:02.319: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod client-containers-4465c2ce-6934-47da-a78a-eb2b70cd646a container test-container: <nil>
STEP: delete the pod
Feb 28 16:06:02.410: INFO: Waiting for pod client-containers-4465c2ce-6934-47da-a78a-eb2b70cd646a to disappear
Feb 28 16:06:02.449: INFO: Pod client-containers-4465c2ce-6934-47da-a78a-eb2b70cd646a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:06:02.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4587" for this suite.
Feb 28 16:06:08.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:06:10.070: INFO: namespace containers-4587 deletion completed in 7.58228154s
•SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:06:10.070: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-be420a97-9e0e-48d0-8260-325e475d49e9
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:06:14.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7403" for this suite.
Feb 28 16:06:36.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:06:38.204: INFO: namespace configmap-7403 deletion completed in 23.561376538s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:06:38.204: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 28 16:06:38.441: INFO: Waiting up to 5m0s for pod "pod-0d6e41e7-d9a8-4550-ae92-e68b509d3934" in namespace "emptydir-2434" to be "success or failure"
Feb 28 16:06:38.479: INFO: Pod "pod-0d6e41e7-d9a8-4550-ae92-e68b509d3934": Phase="Pending", Reason="", readiness=false. Elapsed: 38.172145ms
Feb 28 16:06:40.517: INFO: Pod "pod-0d6e41e7-d9a8-4550-ae92-e68b509d3934": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076104777s
STEP: Saw pod success
Feb 28 16:06:40.517: INFO: Pod "pod-0d6e41e7-d9a8-4550-ae92-e68b509d3934" satisfied condition "success or failure"
Feb 28 16:06:40.554: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-0d6e41e7-d9a8-4550-ae92-e68b509d3934 container test-container: <nil>
STEP: delete the pod
Feb 28 16:06:40.652: INFO: Waiting for pod pod-0d6e41e7-d9a8-4550-ae92-e68b509d3934 to disappear
Feb 28 16:06:40.689: INFO: Pod pod-0d6e41e7-d9a8-4550-ae92-e68b509d3934 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:06:40.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2434" for this suite.
Feb 28 16:06:46.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:06:48.351: INFO: namespace emptydir-2434 deletion completed in 7.623674394s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:06:48.351: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Feb 28 16:06:48.540: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 28 16:06:48.623: INFO: Waiting for terminating namespaces to be deleted...
Feb 28 16:06:48.660: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-q1l7 before test
Feb 28 16:06:48.702: INFO: kube-proxy-bootstrap-e2e-minion-group-q1l7 from kube-system started at 2020-02-28 15:42:18 +0000 UTC (1 container statuses recorded)
Feb 28 16:06:48.702: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 16:06:48.702: INFO: fluentd-gcp-v3.2.0-9q7z2 from kube-system started at 2020-02-28 15:42:58 +0000 UTC (2 container statuses recorded)
Feb 28 16:06:48.702: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 28 16:06:48.702: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:06:48.702: INFO: metadata-proxy-v0.1-kbkll from kube-system started at 2020-02-28 15:42:19 +0000 UTC (2 container statuses recorded)
Feb 28 16:06:48.702: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 28 16:06:48.702: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:06:48.702: INFO: metrics-server-v0.3.3-7947ffd59d-rfjr4 from kube-system started at 2020-02-28 15:42:40 +0000 UTC (2 container statuses recorded)
Feb 28 16:06:48.702: INFO: 	Container metrics-server ready: true, restart count 0
Feb 28 16:06:48.702: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 28 16:06:48.702: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-s43f before test
Feb 28 16:06:48.743: INFO: kube-proxy-bootstrap-e2e-minion-group-s43f from kube-system started at 2020-02-28 15:42:17 +0000 UTC (1 container statuses recorded)
Feb 28 16:06:48.743: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 16:06:48.743: INFO: kube-dns-autoscaler-6d6bc99fd8-t6kwd from kube-system started at 2020-02-28 15:42:18 +0000 UTC (1 container statuses recorded)
Feb 28 16:06:48.743: INFO: 	Container autoscaler ready: true, restart count 0
Feb 28 16:06:48.743: INFO: metadata-proxy-v0.1-tnmbw from kube-system started at 2020-02-28 15:42:18 +0000 UTC (2 container statuses recorded)
Feb 28 16:06:48.743: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 28 16:06:48.743: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:06:48.743: INFO: fluentd-gcp-scaler-6848d689fb-8slw6 from kube-system started at 2020-02-28 15:42:18 +0000 UTC (1 container statuses recorded)
Feb 28 16:06:48.743: INFO: 	Container fluentd-gcp-scaler ready: true, restart count 0
Feb 28 16:06:48.743: INFO: fluentd-gcp-v3.2.0-k64bw from kube-system started at 2020-02-28 15:42:35 +0000 UTC (2 container statuses recorded)
Feb 28 16:06:48.743: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 28 16:06:48.743: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:06:48.743: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-t863 before test
Feb 28 16:06:48.809: INFO: coredns-557dcdc9f5-9hvg8 from kube-system started at 2020-02-28 15:42:29 +0000 UTC (1 container statuses recorded)
Feb 28 16:06:48.809: INFO: 	Container coredns ready: true, restart count 0
Feb 28 16:06:48.809: INFO: kubernetes-dashboard-66b96fb8d7-cr7kc from kube-system started at 2020-02-28 15:42:16 +0000 UTC (1 container statuses recorded)
Feb 28 16:06:48.809: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 28 16:06:48.809: INFO: metadata-proxy-v0.1-ft86w from kube-system started at 2020-02-28 15:42:16 +0000 UTC (2 container statuses recorded)
Feb 28 16:06:48.809: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 28 16:06:48.809: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:06:48.809: INFO: l7-default-backend-84c9fcfbb-jzxhq from kube-system started at 2020-02-28 15:42:29 +0000 UTC (1 container statuses recorded)
Feb 28 16:06:48.809: INFO: 	Container default-http-backend ready: true, restart count 0
Feb 28 16:06:48.809: INFO: coredns-557dcdc9f5-dg8nb from kube-system started at 2020-02-28 15:42:29 +0000 UTC (1 container statuses recorded)
Feb 28 16:06:48.809: INFO: 	Container coredns ready: true, restart count 0
Feb 28 16:06:48.809: INFO: event-exporter-v0.2.5-5fd6f794f7-5kbmp from kube-system started at 2020-02-28 15:42:30 +0000 UTC (2 container statuses recorded)
Feb 28 16:06:48.809: INFO: 	Container event-exporter ready: true, restart count 0
Feb 28 16:06:48.809: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:06:48.809: INFO: heapster-v1.6.0-beta.1-6b89b777b5-7mnqk from kube-system started at 2020-02-28 15:42:31 +0000 UTC (2 container statuses recorded)
Feb 28 16:06:48.809: INFO: 	Container heapster ready: true, restart count 0
Feb 28 16:06:48.809: INFO: 	Container heapster-nanny ready: true, restart count 0
Feb 28 16:06:48.809: INFO: kube-proxy-bootstrap-e2e-minion-group-t863 from kube-system started at 2020-02-28 15:42:16 +0000 UTC (1 container statuses recorded)
Feb 28 16:06:48.809: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 16:06:48.809: INFO: fluentd-gcp-v3.2.0-j8frl from kube-system started at 2020-02-28 15:42:36 +0000 UTC (2 container statuses recorded)
Feb 28 16:06:48.809: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 28 16:06:48.809: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-53814715-77c5-4456-8796-df5fe9a4aaea 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-53814715-77c5-4456-8796-df5fe9a4aaea off the node bootstrap-e2e-minion-group-s43f
STEP: verifying the node doesn't have the label kubernetes.io/e2e-53814715-77c5-4456-8796-df5fe9a4aaea
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:06:55.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8620" for this suite.
Feb 28 16:07:09.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:07:11.023: INFO: namespace sched-pred-8620 deletion completed in 15.608550582s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:07:11.023: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8694
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 16:07:11.179: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 16:07:34.033: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.64.1.15 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8694 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:07:34.033: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:07:35.425: INFO: Found all expected endpoints: [netserver-0]
Feb 28 16:07:35.463: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.64.2.29 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8694 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:07:35.463: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:07:36.834: INFO: Found all expected endpoints: [netserver-1]
Feb 28 16:07:36.871: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.64.3.26 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8694 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:07:36.871: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:07:38.259: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:07:38.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8694" for this suite.
Feb 28 16:08:00.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:08:01.861: INFO: namespace pod-network-test-8694 deletion completed in 23.56384632s
•S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:08:01.861: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 16:08:02.285: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 28 16:08:02.365: INFO: Number of nodes with available pods: 0
Feb 28 16:08:02.365: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 28 16:08:02.531: INFO: Number of nodes with available pods: 0
Feb 28 16:08:02.531: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:03.569: INFO: Number of nodes with available pods: 0
Feb 28 16:08:03.569: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:04.569: INFO: Number of nodes with available pods: 1
Feb 28 16:08:04.569: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 28 16:08:04.728: INFO: Number of nodes with available pods: 0
Feb 28 16:08:04.728: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 28 16:08:04.811: INFO: Number of nodes with available pods: 0
Feb 28 16:08:04.811: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:05.850: INFO: Number of nodes with available pods: 0
Feb 28 16:08:05.850: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:06.849: INFO: Number of nodes with available pods: 0
Feb 28 16:08:06.849: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:07.850: INFO: Number of nodes with available pods: 0
Feb 28 16:08:07.850: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:08.849: INFO: Number of nodes with available pods: 0
Feb 28 16:08:08.849: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:09.849: INFO: Number of nodes with available pods: 0
Feb 28 16:08:09.849: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:10.849: INFO: Number of nodes with available pods: 0
Feb 28 16:08:10.849: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:11.850: INFO: Number of nodes with available pods: 0
Feb 28 16:08:11.850: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:12.849: INFO: Number of nodes with available pods: 0
Feb 28 16:08:12.849: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:13.849: INFO: Number of nodes with available pods: 0
Feb 28 16:08:13.849: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:14.849: INFO: Number of nodes with available pods: 0
Feb 28 16:08:14.849: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:15.850: INFO: Number of nodes with available pods: 0
Feb 28 16:08:15.850: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:16.849: INFO: Number of nodes with available pods: 0
Feb 28 16:08:16.850: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:17.850: INFO: Number of nodes with available pods: 0
Feb 28 16:08:17.850: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:18.849: INFO: Number of nodes with available pods: 0
Feb 28 16:08:18.849: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:19.849: INFO: Number of nodes with available pods: 0
Feb 28 16:08:19.850: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:08:20.850: INFO: Number of nodes with available pods: 1
Feb 28 16:08:20.850: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1942, will wait for the garbage collector to delete the pods
Feb 28 16:08:21.056: INFO: Deleting DaemonSet.extensions daemon-set took: 42.827328ms
Feb 28 16:08:21.657: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.301481ms
Feb 28 16:08:28.796: INFO: Number of nodes with available pods: 0
Feb 28 16:08:28.796: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 16:08:28.834: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1942/daemonsets","resourceVersion":"6263"},"items":null}

Feb 28 16:08:28.871: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1942/pods","resourceVersion":"6263"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:08:29.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1942" for this suite.
Feb 28 16:08:35.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:08:36.722: INFO: namespace daemonsets-1942 deletion completed in 7.608588791s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:08:36.723: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4696
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Feb 28 16:08:37.054: INFO: Found 1 stateful pods, waiting for 3
Feb 28 16:08:47.094: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 16:08:47.094: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 16:08:47.094: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 16:08:47.213: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-4696 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 16:08:48.141: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 28 16:08:48.141: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 16:08:48.141: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 28 16:08:58.390: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 28 16:08:58.507: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-4696 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:08:59.162: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 28 16:08:59.162: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 16:08:59.162: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

STEP: Rolling back to a previous revision
Feb 28 16:09:19.397: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-4696 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 16:09:20.038: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 28 16:09:20.038: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 16:09:20.038: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 16:09:30.283: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 28 16:09:30.398: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-4696 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:09:31.028: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 28 16:09:31.028: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 16:09:31.028: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 16:09:51.285: INFO: Waiting for StatefulSet statefulset-4696/ss2 to complete update
Feb 28 16:09:51.285: INFO: Waiting for Pod statefulset-4696/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 28 16:10:01.363: INFO: Deleting all statefulset in ns statefulset-4696
Feb 28 16:10:01.401: INFO: Scaling statefulset ss2 to 0
Feb 28 16:10:21.567: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 16:10:21.610: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:10:21.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4696" for this suite.
Feb 28 16:10:27.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:10:29.340: INFO: namespace statefulset-4696 deletion completed in 7.573080167s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:10:29.341: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5973
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-5973
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5973
Feb 28 16:10:29.632: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb 28 16:10:39.672: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 28 16:10:39.710: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 16:10:40.328: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 28 16:10:40.328: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 16:10:40.328: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 16:10:40.366: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 28 16:10:50.407: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 16:10:50.407: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 16:10:50.603: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999498s
Feb 28 16:10:51.641: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.922943306s
Feb 28 16:10:52.680: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.884596657s
Feb 28 16:10:53.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.846204829s
Feb 28 16:10:54.757: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.807715163s
Feb 28 16:10:55.796: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.769117032s
Feb 28 16:10:56.835: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.729996396s
Feb 28 16:10:57.874: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.69054487s
Feb 28 16:10:58.913: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.652026936s
Feb 28 16:10:59.952: INFO: Verifying statefulset ss doesn't scale past 3 for another 612.880262ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5973
Feb 28 16:11:00.991: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:11:01.642: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 28 16:11:01.642: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 16:11:01.642: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 16:11:01.642: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:11:02.284: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 28 16:11:02.284: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 16:11:02.284: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 16:11:02.284: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:11:02.947: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 28 16:11:02.947: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 28 16:11:02.947: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 28 16:11:02.985: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 16:11:02.985: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 28 16:11:02.985: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 28 16:11:03.023: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 16:11:03.662: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 28 16:11:03.662: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 16:11:03.662: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 16:11:03.662: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 16:11:04.260: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 28 16:11:04.260: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 16:11:04.260: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 16:11:04.260: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 28 16:11:04.920: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 28 16:11:04.920: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 28 16:11:04.920: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 28 16:11:04.920: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 16:11:04.958: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 28 16:11:15.035: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 16:11:15.035: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 16:11:15.035: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 28 16:11:15.160: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 28 16:11:15.160: INFO: ss-0  bootstrap-e2e-minion-group-s43f  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  }]
Feb 28 16:11:15.160: INFO: ss-1  bootstrap-e2e-minion-group-q1l7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  }]
Feb 28 16:11:15.160: INFO: ss-2  bootstrap-e2e-minion-group-t863  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  }]
Feb 28 16:11:15.161: INFO: 
Feb 28 16:11:15.161: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 16:11:16.199: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 28 16:11:16.199: INFO: ss-0  bootstrap-e2e-minion-group-s43f  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  }]
Feb 28 16:11:16.199: INFO: ss-1  bootstrap-e2e-minion-group-q1l7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  }]
Feb 28 16:11:16.199: INFO: ss-2  bootstrap-e2e-minion-group-t863  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  }]
Feb 28 16:11:16.199: INFO: 
Feb 28 16:11:16.199: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 16:11:17.238: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 28 16:11:17.238: INFO: ss-0  bootstrap-e2e-minion-group-s43f  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  }]
Feb 28 16:11:17.238: INFO: ss-1  bootstrap-e2e-minion-group-q1l7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  }]
Feb 28 16:11:17.238: INFO: ss-2  bootstrap-e2e-minion-group-t863  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  }]
Feb 28 16:11:17.238: INFO: 
Feb 28 16:11:17.238: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 16:11:18.277: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 28 16:11:18.277: INFO: ss-0  bootstrap-e2e-minion-group-s43f  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  }]
Feb 28 16:11:18.277: INFO: ss-1  bootstrap-e2e-minion-group-q1l7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  }]
Feb 28 16:11:18.277: INFO: ss-2  bootstrap-e2e-minion-group-t863  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  }]
Feb 28 16:11:18.277: INFO: 
Feb 28 16:11:18.277: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 28 16:11:19.315: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 28 16:11:19.315: INFO: ss-0  bootstrap-e2e-minion-group-s43f  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  }]
Feb 28 16:11:19.315: INFO: ss-2  bootstrap-e2e-minion-group-t863  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  }]
Feb 28 16:11:19.315: INFO: 
Feb 28 16:11:19.315: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 28 16:11:20.354: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 28 16:11:20.354: INFO: ss-0  bootstrap-e2e-minion-group-s43f  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  }]
Feb 28 16:11:20.354: INFO: ss-2  bootstrap-e2e-minion-group-t863  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  }]
Feb 28 16:11:20.354: INFO: 
Feb 28 16:11:20.354: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 28 16:11:21.393: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 28 16:11:21.393: INFO: ss-0  bootstrap-e2e-minion-group-s43f  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  }]
Feb 28 16:11:21.393: INFO: ss-2  bootstrap-e2e-minion-group-t863  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  }]
Feb 28 16:11:21.393: INFO: 
Feb 28 16:11:21.393: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 28 16:11:22.432: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 28 16:11:22.432: INFO: ss-0  bootstrap-e2e-minion-group-s43f  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  }]
Feb 28 16:11:22.432: INFO: ss-2  bootstrap-e2e-minion-group-t863  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  }]
Feb 28 16:11:22.432: INFO: 
Feb 28 16:11:22.432: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 28 16:11:23.470: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 28 16:11:23.470: INFO: ss-0  bootstrap-e2e-minion-group-s43f  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  }]
Feb 28 16:11:23.470: INFO: ss-2  bootstrap-e2e-minion-group-t863  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  }]
Feb 28 16:11:23.471: INFO: 
Feb 28 16:11:23.471: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 28 16:11:24.509: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Feb 28 16:11:24.509: INFO: ss-0  bootstrap-e2e-minion-group-s43f  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:29 +0000 UTC  }]
Feb 28 16:11:24.510: INFO: ss-2  bootstrap-e2e-minion-group-t863  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:11:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:10:50 +0000 UTC  }]
Feb 28 16:11:24.510: INFO: 
Feb 28 16:11:24.510: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5973
Feb 28 16:11:25.549: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:11:25.953: INFO: rc: 1
Feb 28 16:11:25.953: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0021c6c90 exit status 1 <nil> <nil> true [0xc000010010 0xc000010440 0xc0000104c8] [0xc000010010 0xc000010440 0xc0000104c8] [0xc000010290 0xc000010498] [0xba7080 0xba7080] 0xc002603080 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Feb 28 16:11:35.953: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:11:36.241: INFO: rc: 1
Feb 28 16:11:36.241: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00314a6f0 exit status 1 <nil> <nil> true [0xc0000de050 0xc000693f10 0xc000693fe0] [0xc0000de050 0xc000693f10 0xc000693fe0] [0xc000693e80 0xc000693fc8] [0xba7080 0xba7080] 0xc0033d6d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:11:46.241: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:11:46.517: INFO: rc: 1
Feb 28 16:11:46.518: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00314ae10 exit status 1 <nil> <nil> true [0xc000356420 0xc000357718 0xc000357760] [0xc000356420 0xc000357718 0xc000357760] [0xc000356560 0xc000357750] [0xba7080 0xba7080] 0xc0033d7140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:11:56.518: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:11:56.791: INFO: rc: 1
Feb 28 16:11:56.791: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00341e780 exit status 1 <nil> <nil> true [0xc001c36000 0xc001c36018 0xc001c36030] [0xc001c36000 0xc001c36018 0xc001c36030] [0xc001c36010 0xc001c36028] [0xba7080 0xba7080] 0xc001c71320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:12:06.791: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:12:07.048: INFO: rc: 1
Feb 28 16:12:07.048: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00341eea0 exit status 1 <nil> <nil> true [0xc001c36038 0xc001c36050 0xc001c36068] [0xc001c36038 0xc001c36050 0xc001c36068] [0xc001c36048 0xc001c36060] [0xba7080 0xba7080] 0xc001e6c120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:12:17.048: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:12:17.309: INFO: rc: 1
Feb 28 16:12:17.309: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0021c73b0 exit status 1 <nil> <nil> true [0xc000010668 0xc000010db0 0xc000010e98] [0xc000010668 0xc000010db0 0xc000010e98] [0xc000010cc0 0xc000010e68] [0xba7080 0xba7080] 0xc002603860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:12:27.310: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:12:27.584: INFO: rc: 1
Feb 28 16:12:27.584: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00314b5c0 exit status 1 <nil> <nil> true [0xc0003577a0 0xc000357858 0xc0003578f0] [0xc0003577a0 0xc000357858 0xc0003578f0] [0xc0003577b0 0xc000357890] [0xba7080 0xba7080] 0xc0033d74a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:12:37.584: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:12:37.860: INFO: rc: 1
Feb 28 16:12:37.860: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00314bd10 exit status 1 <nil> <nil> true [0xc000357900 0xc000357a10 0xc000357ac8] [0xc000357900 0xc000357a10 0xc000357ac8] [0xc000357990 0xc000357aa8] [0xba7080 0xba7080] 0xc0033d7980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:12:47.861: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:12:48.132: INFO: rc: 1
Feb 28 16:12:48.132: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e6e720 exit status 1 <nil> <nil> true [0xc001d40000 0xc001d40018 0xc001d40030] [0xc001d40000 0xc001d40018 0xc001d40030] [0xc001d40010 0xc001d40028] [0xba7080 0xba7080] 0xc00224bb60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:12:58.132: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:12:58.392: INFO: rc: 1
Feb 28 16:12:58.392: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0021c7b90 exit status 1 <nil> <nil> true [0xc000010eb0 0xc000010f58 0xc0000110d0] [0xc000010eb0 0xc000010f58 0xc0000110d0] [0xc000010f40 0xc000011038] [0xba7080 0xba7080] 0xc002603e60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:13:08.392: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:13:08.677: INFO: rc: 1
Feb 28 16:13:08.677: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bd65d0 exit status 1 <nil> <nil> true [0xc000357ae0 0xc000357ba8 0xc000357c20] [0xc000357ae0 0xc000357ba8 0xc000357c20] [0xc000357b90 0xc000357bd0] [0xba7080 0xba7080] 0xc0033d7c80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:13:18.677: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:13:18.938: INFO: rc: 1
Feb 28 16:13:18.938: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001bd6d20 exit status 1 <nil> <nil> true [0xc000357c70 0xc000357cf8 0xc000357d18] [0xc000357c70 0xc000357cf8 0xc000357d18] [0xc000357ce0 0xc000357d10] [0xba7080 0xba7080] 0xc0033d7f80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:13:28.939: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:13:29.216: INFO: rc: 1
Feb 28 16:13:29.216: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00314a720 exit status 1 <nil> <nil> true [0xc000693f10 0xc000693fe0 0xc000010010] [0xc000693f10 0xc000693fe0 0xc000010010] [0xc000693fc8 0xc0000de088] [0xba7080 0xba7080] 0xc001c70b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:13:39.217: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:13:39.493: INFO: rc: 1
Feb 28 16:13:39.493: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00314ae70 exit status 1 <nil> <nil> true [0xc000010218 0xc000010488 0xc000010668] [0xc000010218 0xc000010488 0xc000010668] [0xc000010440 0xc0000104c8] [0xba7080 0xba7080] 0xc0033d6900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:13:49.494: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:13:49.755: INFO: rc: 1
Feb 28 16:13:49.755: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00341e750 exit status 1 <nil> <nil> true [0xc001c36000 0xc001c36018 0xc001c36030] [0xc001c36000 0xc001c36018 0xc001c36030] [0xc001c36010 0xc001c36028] [0xba7080 0xba7080] 0xc002602a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:13:59.755: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:14:00.012: INFO: rc: 1
Feb 28 16:14:00.012: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00341eed0 exit status 1 <nil> <nil> true [0xc001c36038 0xc001c36050 0xc001c36068] [0xc001c36038 0xc001c36050 0xc001c36068] [0xc001c36048 0xc001c36060] [0xba7080 0xba7080] 0xc0026037a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:14:10.013: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:14:10.290: INFO: rc: 1
Feb 28 16:14:10.290: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00341f620 exit status 1 <nil> <nil> true [0xc001c36070 0xc001c36088 0xc001c360a0] [0xc001c36070 0xc001c36088 0xc001c360a0] [0xc001c36080 0xc001c36098] [0xba7080 0xba7080] 0xc002603da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:14:20.291: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:14:20.551: INFO: rc: 1
Feb 28 16:14:20.551: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e6e6f0 exit status 1 <nil> <nil> true [0xc001d40000 0xc001d40018 0xc001d40030] [0xc001d40000 0xc001d40018 0xc001d40030] [0xc001d40010 0xc001d40028] [0xba7080 0xba7080] 0xc001e6c000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:14:30.552: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:14:30.833: INFO: rc: 1
Feb 28 16:14:30.833: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e6ee40 exit status 1 <nil> <nil> true [0xc001d40038 0xc001d40058 0xc001d40080] [0xc001d40038 0xc001d40058 0xc001d40080] [0xc001d40048 0xc001d40078] [0xba7080 0xba7080] 0xc001e6ccc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:14:40.833: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:14:41.090: INFO: rc: 1
Feb 28 16:14:41.090: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00314b5f0 exit status 1 <nil> <nil> true [0xc000010b80 0xc000010e30 0xc000010eb0] [0xc000010b80 0xc000010e30 0xc000010eb0] [0xc000010db0 0xc000010e98] [0xba7080 0xba7080] 0xc0033d6f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:14:51.090: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:14:51.352: INFO: rc: 1
Feb 28 16:14:51.352: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0021c6ae0 exit status 1 <nil> <nil> true [0xc000356420 0xc000357718 0xc000357760] [0xc000356420 0xc000357718 0xc000357760] [0xc000356560 0xc000357750] [0xba7080 0xba7080] 0xc00224bb60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:15:01.352: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:15:01.632: INFO: rc: 1
Feb 28 16:15:01.632: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e6f530 exit status 1 <nil> <nil> true [0xc001d40090 0xc001d400e0 0xc001d40108] [0xc001d40090 0xc001d400e0 0xc001d40108] [0xc001d400c8 0xc001d40100] [0xba7080 0xba7080] 0xc002031680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:15:11.632: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:15:11.895: INFO: rc: 1
Feb 28 16:15:11.896: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00314bdd0 exit status 1 <nil> <nil> true [0xc000010ee8 0xc000010fd0 0xc000011110] [0xc000010ee8 0xc000010fd0 0xc000011110] [0xc000010f58 0xc0000110d0] [0xba7080 0xba7080] 0xc0033d7320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:15:21.896: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:15:22.158: INFO: rc: 1
Feb 28 16:15:22.158: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e6fc50 exit status 1 <nil> <nil> true [0xc001d40120 0xc001d40160 0xc001d40190] [0xc001d40120 0xc001d40160 0xc001d40190] [0xc001d40158 0xc001d40170] [0xba7080 0xba7080] 0xc001e63020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:15:32.158: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:15:32.435: INFO: rc: 1
Feb 28 16:15:32.435: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e6e6c0 exit status 1 <nil> <nil> true [0xc0000de088 0xc000693fb0 0xc001d40000] [0xc0000de088 0xc000693fb0 0xc001d40000] [0xc000693f10 0xc000693fe0] [0xba7080 0xba7080] 0xc001e6c060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:15:42.435: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:15:42.701: INFO: rc: 1
Feb 28 16:15:42.701: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0021c6a80 exit status 1 <nil> <nil> true [0xc000356420 0xc000357718 0xc000357760] [0xc000356420 0xc000357718 0xc000357760] [0xc000356560 0xc000357750] [0xba7080 0xba7080] 0xc001e06d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:15:52.702: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:15:52.985: INFO: rc: 1
Feb 28 16:15:52.986: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0021c71d0 exit status 1 <nil> <nil> true [0xc0003577a0 0xc000357858 0xc0003578f0] [0xc0003577a0 0xc000357858 0xc0003578f0] [0xc0003577b0 0xc000357890] [0xba7080 0xba7080] 0xc001cfc4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:16:02.986: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:16:03.385: INFO: rc: 1
Feb 28 16:16:03.385: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00314a6f0 exit status 1 <nil> <nil> true [0xc000010010 0xc000010440 0xc0000104c8] [0xc000010010 0xc000010440 0xc0000104c8] [0xc000010290 0xc000010498] [0xba7080 0xba7080] 0xc001e91f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:16:13.386: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:16:13.639: INFO: rc: 1
Feb 28 16:16:13.639: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00314aea0 exit status 1 <nil> <nil> true [0xc000010668 0xc000010db0 0xc000010e98] [0xc000010668 0xc000010db0 0xc000010e98] [0xc000010cc0 0xc000010e68] [0xba7080 0xba7080] 0xc00224ac00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:16:23.642: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:16:24.001: INFO: rc: 1
Feb 28 16:16:24.001: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/workspace/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00314b620 exit status 1 <nil> <nil> true [0xc000010eb0 0xc000010f58 0xc0000110d0] [0xc000010eb0 0xc000010f58 0xc0000110d0] [0xc000010f40 0xc000011038] [0xba7080 0xba7080] 0xc0033d6b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 28 16:16:34.002: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-5973 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 28 16:16:34.292: INFO: rc: 1
Feb 28 16:16:34.292: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Feb 28 16:16:34.292: INFO: Scaling statefulset ss to 0
Feb 28 16:16:34.408: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 28 16:16:34.446: INFO: Deleting all statefulset in ns statefulset-5973
Feb 28 16:16:34.484: INFO: Scaling statefulset ss to 0
Feb 28 16:16:34.601: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 16:16:34.638: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:16:34.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5973" for this suite.
Feb 28 16:16:40.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:16:42.438: INFO: namespace statefulset-5973 deletion completed in 7.637105059s

• [SLOW TEST:373.098 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:16:42.439: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-413b6476-79c6-43e4-bac1-e6f5ea25bd61
STEP: Creating a pod to test consume configMaps
Feb 28 16:16:42.720: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-22f52b72-a5c0-407e-9bae-bc265be2aacf" in namespace "projected-9580" to be "success or failure"
Feb 28 16:16:42.758: INFO: Pod "pod-projected-configmaps-22f52b72-a5c0-407e-9bae-bc265be2aacf": Phase="Pending", Reason="", readiness=false. Elapsed: 38.4766ms
Feb 28 16:16:44.797: INFO: Pod "pod-projected-configmaps-22f52b72-a5c0-407e-9bae-bc265be2aacf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076830108s
Feb 28 16:16:46.841: INFO: Pod "pod-projected-configmaps-22f52b72-a5c0-407e-9bae-bc265be2aacf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.121023998s
STEP: Saw pod success
Feb 28 16:16:46.841: INFO: Pod "pod-projected-configmaps-22f52b72-a5c0-407e-9bae-bc265be2aacf" satisfied condition "success or failure"
Feb 28 16:16:46.879: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-projected-configmaps-22f52b72-a5c0-407e-9bae-bc265be2aacf container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 16:16:46.975: INFO: Waiting for pod pod-projected-configmaps-22f52b72-a5c0-407e-9bae-bc265be2aacf to disappear
Feb 28 16:16:47.014: INFO: Pod pod-projected-configmaps-22f52b72-a5c0-407e-9bae-bc265be2aacf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:16:47.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9580" for this suite.
Feb 28 16:16:53.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:16:54.697: INFO: namespace projected-9580 deletion completed in 7.64163147s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:16:54.697: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 28 16:16:57.649: INFO: Successfully updated pod "labelsupdate294c3e04-7413-42fc-b798-a2174887ced7"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:17:01.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8484" for this suite.
Feb 28 16:17:17.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:17:19.482: INFO: namespace projected-8484 deletion completed in 17.651933244s
•SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:17:19.482: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-ed4287ca-455b-4c57-83cf-c5ed4daf689d
STEP: Creating a pod to test consume configMaps
Feb 28 16:17:19.731: INFO: Waiting up to 5m0s for pod "pod-configmaps-eaea0323-5a40-4e82-a8d3-63b5527a98c3" in namespace "configmap-4820" to be "success or failure"
Feb 28 16:17:19.771: INFO: Pod "pod-configmaps-eaea0323-5a40-4e82-a8d3-63b5527a98c3": Phase="Pending", Reason="", readiness=false. Elapsed: 39.197165ms
Feb 28 16:17:21.812: INFO: Pod "pod-configmaps-eaea0323-5a40-4e82-a8d3-63b5527a98c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.081034655s
STEP: Saw pod success
Feb 28 16:17:21.813: INFO: Pod "pod-configmaps-eaea0323-5a40-4e82-a8d3-63b5527a98c3" satisfied condition "success or failure"
Feb 28 16:17:21.851: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-configmaps-eaea0323-5a40-4e82-a8d3-63b5527a98c3 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 16:17:21.945: INFO: Waiting for pod pod-configmaps-eaea0323-5a40-4e82-a8d3-63b5527a98c3 to disappear
Feb 28 16:17:21.983: INFO: Pod pod-configmaps-eaea0323-5a40-4e82-a8d3-63b5527a98c3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:17:21.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4820" for this suite.
Feb 28 16:17:28.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:17:29.587: INFO: namespace configmap-4820 deletion completed in 7.564253343s
•SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:17:29.587: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-096829b0-ff88-4240-ac98-25aaa2ca1051
STEP: Creating a pod to test consume secrets
Feb 28 16:17:29.865: INFO: Waiting up to 5m0s for pod "pod-secrets-3ce931a9-76ad-4c9e-8682-4076b3116332" in namespace "secrets-3849" to be "success or failure"
Feb 28 16:17:29.903: INFO: Pod "pod-secrets-3ce931a9-76ad-4c9e-8682-4076b3116332": Phase="Pending", Reason="", readiness=false. Elapsed: 37.593213ms
Feb 28 16:17:31.941: INFO: Pod "pod-secrets-3ce931a9-76ad-4c9e-8682-4076b3116332": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075900421s
STEP: Saw pod success
Feb 28 16:17:31.941: INFO: Pod "pod-secrets-3ce931a9-76ad-4c9e-8682-4076b3116332" satisfied condition "success or failure"
Feb 28 16:17:31.981: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-secrets-3ce931a9-76ad-4c9e-8682-4076b3116332 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 16:17:32.076: INFO: Waiting for pod pod-secrets-3ce931a9-76ad-4c9e-8682-4076b3116332 to disappear
Feb 28 16:17:32.115: INFO: Pod pod-secrets-3ce931a9-76ad-4c9e-8682-4076b3116332 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:17:32.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3849" for this suite.
Feb 28 16:17:38.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:17:39.720: INFO: namespace secrets-3849 deletion completed in 7.566418192s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:17:39.720: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1557
[It] should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 16:17:39.915: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-1943'
Feb 28 16:17:40.163: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 28 16:17:40.163: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Feb 28 16:17:42.261: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete deployment e2e-test-nginx-deployment --namespace=kubectl-1943'
Feb 28 16:17:42.567: INFO: stderr: ""
Feb 28 16:17:42.567: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:17:42.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1943" for this suite.
Feb 28 16:18:04.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:18:06.352: INFO: namespace kubectl-1943 deletion completed in 23.746349792s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:18:06.352: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 28 16:18:14.868: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 16:18:14.905: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 16:18:16.905: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 16:18:16.946: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 16:18:18.905: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 16:18:18.943: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 16:18:20.905: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 16:18:20.944: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 16:18:22.905: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 16:18:22.943: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 16:18:24.905: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 16:18:24.943: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 16:18:26.905: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 16:18:26.943: INFO: Pod pod-with-prestop-http-hook still exists
Feb 28 16:18:28.905: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 28 16:18:28.943: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:18:28.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-309" for this suite.
Feb 28 16:18:51.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:18:52.601: INFO: namespace container-lifecycle-hook-309 deletion completed in 23.572299515s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:18:52.602: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 28 16:18:52.803: INFO: Waiting up to 5m0s for pod "pod-648c56ef-7b47-4014-838c-db9eb70459b5" in namespace "emptydir-8860" to be "success or failure"
Feb 28 16:18:52.841: INFO: Pod "pod-648c56ef-7b47-4014-838c-db9eb70459b5": Phase="Pending", Reason="", readiness=false. Elapsed: 37.397909ms
Feb 28 16:18:54.879: INFO: Pod "pod-648c56ef-7b47-4014-838c-db9eb70459b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075268872s
STEP: Saw pod success
Feb 28 16:18:54.879: INFO: Pod "pod-648c56ef-7b47-4014-838c-db9eb70459b5" satisfied condition "success or failure"
Feb 28 16:18:54.916: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-648c56ef-7b47-4014-838c-db9eb70459b5 container test-container: <nil>
STEP: delete the pod
Feb 28 16:18:55.020: INFO: Waiting for pod pod-648c56ef-7b47-4014-838c-db9eb70459b5 to disappear
Feb 28 16:18:55.058: INFO: Pod pod-648c56ef-7b47-4014-838c-db9eb70459b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:18:55.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8860" for this suite.
Feb 28 16:19:01.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:19:02.673: INFO: namespace emptydir-8860 deletion completed in 7.577279189s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:19:02.674: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 16:19:02.877: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fef5f54-2563-43f5-ba46-3c0260c8e846" in namespace "downward-api-2500" to be "success or failure"
Feb 28 16:19:02.914: INFO: Pod "downwardapi-volume-0fef5f54-2563-43f5-ba46-3c0260c8e846": Phase="Pending", Reason="", readiness=false. Elapsed: 37.536709ms
Feb 28 16:19:04.952: INFO: Pod "downwardapi-volume-0fef5f54-2563-43f5-ba46-3c0260c8e846": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075672314s
Feb 28 16:19:06.992: INFO: Pod "downwardapi-volume-0fef5f54-2563-43f5-ba46-3c0260c8e846": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.114994564s
STEP: Saw pod success
Feb 28 16:19:06.992: INFO: Pod "downwardapi-volume-0fef5f54-2563-43f5-ba46-3c0260c8e846" satisfied condition "success or failure"
Feb 28 16:19:07.030: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod downwardapi-volume-0fef5f54-2563-43f5-ba46-3c0260c8e846 container client-container: <nil>
STEP: delete the pod
Feb 28 16:19:07.126: INFO: Waiting for pod downwardapi-volume-0fef5f54-2563-43f5-ba46-3c0260c8e846 to disappear
Feb 28 16:19:07.164: INFO: Pod downwardapi-volume-0fef5f54-2563-43f5-ba46-3c0260c8e846 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:19:07.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2500" for this suite.
Feb 28 16:19:13.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:19:14.787: INFO: namespace downward-api-2500 deletion completed in 7.584306906s
•SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:19:14.787: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-153.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-153.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-153.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-153.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-153.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-153.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 28 16:19:29.414: INFO: DNS probes using dns-153/dns-test-a10bccd1-7a3d-4b61-825e-70859df7b659 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:19:29.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-153" for this suite.
Feb 28 16:19:35.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:19:37.080: INFO: namespace dns-153 deletion completed in 7.572721463s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:19:37.080: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1292
STEP: creating an rc
Feb 28 16:19:37.237: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-7124'
Feb 28 16:19:37.970: INFO: stderr: ""
Feb 28 16:19:37.970: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Feb 28 16:19:39.008: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 16:19:39.009: INFO: Found 0 / 1
Feb 28 16:19:40.009: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 16:19:40.009: INFO: Found 1 / 1
Feb 28 16:19:40.009: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 28 16:19:40.047: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 16:19:40.048: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 28 16:19:40.048: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config logs redis-master-9wc2v redis-master --namespace=kubectl-7124'
Feb 28 16:19:40.316: INFO: stderr: ""
Feb 28 16:19:40.316: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Feb 16:19:38.951 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Feb 16:19:38.951 # Server started, Redis version 3.2.12\n1:M 28 Feb 16:19:38.951 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Feb 16:19:38.951 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 28 16:19:40.316: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config logs redis-master-9wc2v redis-master --namespace=kubectl-7124 --tail=1'
Feb 28 16:19:40.581: INFO: stderr: ""
Feb 28 16:19:40.581: INFO: stdout: "1:M 28 Feb 16:19:38.951 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 28 16:19:40.581: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config logs redis-master-9wc2v redis-master --namespace=kubectl-7124 --limit-bytes=1'
Feb 28 16:19:40.864: INFO: stderr: ""
Feb 28 16:19:40.864: INFO: stdout: " "
STEP: exposing timestamps
Feb 28 16:19:40.864: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config logs redis-master-9wc2v redis-master --namespace=kubectl-7124 --tail=1 --timestamps'
Feb 28 16:19:41.133: INFO: stderr: ""
Feb 28 16:19:41.133: INFO: stdout: "2020-02-28T16:19:38.952204835Z 1:M 28 Feb 16:19:38.951 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 28 16:19:43.633: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config logs redis-master-9wc2v redis-master --namespace=kubectl-7124 --since=1s'
Feb 28 16:19:43.931: INFO: stderr: ""
Feb 28 16:19:43.931: INFO: stdout: ""
Feb 28 16:19:43.931: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config logs redis-master-9wc2v redis-master --namespace=kubectl-7124 --since=24h'
Feb 28 16:19:44.227: INFO: stderr: ""
Feb 28 16:19:44.227: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Feb 16:19:38.951 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Feb 16:19:38.951 # Server started, Redis version 3.2.12\n1:M 28 Feb 16:19:38.951 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Feb 16:19:38.951 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
STEP: using delete to clean up resources
Feb 28 16:19:44.227: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-7124'
Feb 28 16:19:44.503: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 16:19:44.503: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 28 16:19:44.503: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get rc,svc -l name=nginx --no-headers --namespace=kubectl-7124'
Feb 28 16:19:44.775: INFO: stderr: "No resources found.\n"
Feb 28 16:19:44.775: INFO: stdout: ""
Feb 28 16:19:44.775: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -l name=nginx --namespace=kubectl-7124 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 16:19:45.037: INFO: stderr: ""
Feb 28 16:19:45.037: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:19:45.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7124" for this suite.
Feb 28 16:19:51.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:19:52.675: INFO: namespace kubectl-7124 deletion completed in 7.599840474s
•SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:19:52.675: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 16:19:52.948: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 28 16:19:57.025: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 28 16:20:01.356: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-1332,SelfLink:/apis/apps/v1/namespaces/deployment-1332/deployments/test-cleanup-deployment,UID:975248eb-91af-4339-bb55-b008e19712dd,ResourceVersion:8210,Generation:1,CreationTimestamp:2020-02-28 16:19:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-02-28 16:19:57 +0000 UTC 2020-02-28 16:19:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-02-28 16:20:00 +0000 UTC 2020-02-28 16:19:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 28 16:20:01.394: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-1332,SelfLink:/apis/apps/v1/namespaces/deployment-1332/replicasets/test-cleanup-deployment-55bbcbc84c,UID:92da336e-4c41-43ea-aa87-7566e9022dec,ResourceVersion:8202,Generation:1,CreationTimestamp:2020-02-28 16:19:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 975248eb-91af-4339-bb55-b008e19712dd 0xc0009d9a47 0xc0009d9a48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 28 16:20:01.433: INFO: Pod "test-cleanup-deployment-55bbcbc84c-49nhp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-49nhp,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-1332,SelfLink:/api/v1/namespaces/deployment-1332/pods/test-cleanup-deployment-55bbcbc84c-49nhp,UID:81b473c1-d4d0-4b1c-813e-ed3b9d441cd2,ResourceVersion:8201,Generation:0,CreationTimestamp:2020-02-28 16:19:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 92da336e-4c41-43ea-aa87-7566e9022dec 0xc002748117 0xc002748118}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9kzfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9kzfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-9kzfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-q1l7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002748180} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027481a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:19:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:19:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:19:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:19:57 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.4,PodIP:10.64.3.41,StartTime:2020-02-28 16:19:57 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-02-28 16:19:59 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://ed6b380aa17e686e0d724f99fa55a5eaf06111745118f3519001da4aea578cb0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:20:01.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1332" for this suite.
Feb 28 16:20:07.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:20:09.147: INFO: namespace deployment-1332 deletion completed in 7.675226401s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:20:09.147: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-f63334a6-2a0a-47ed-9bf3-36aba479baf7
STEP: Creating secret with name s-test-opt-upd-90c6df6d-62ce-407d-aa31-4090e41bc005
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f63334a6-2a0a-47ed-9bf3-36aba479baf7
STEP: Updating secret s-test-opt-upd-90c6df6d-62ce-407d-aa31-4090e41bc005
STEP: Creating secret with name s-test-opt-create-836e68ae-47f6-4c08-a80f-7704bd022dcb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:20:14.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1117" for this suite.
Feb 28 16:20:36.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:20:37.750: INFO: namespace secrets-1117 deletion completed in 23.704199127s
•SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:20:37.750: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 16:20:38.035: INFO: (0) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 44.223802ms)
Feb 28 16:20:38.074: INFO: (1) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 39.841448ms)
Feb 28 16:20:38.114: INFO: (2) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 39.131406ms)
Feb 28 16:20:38.153: INFO: (3) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 39.451041ms)
Feb 28 16:20:38.199: INFO: (4) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 45.706759ms)
Feb 28 16:20:38.245: INFO: (5) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 46.2102ms)
Feb 28 16:20:38.285: INFO: (6) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 39.987716ms)
Feb 28 16:20:38.325: INFO: (7) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 39.83026ms)
Feb 28 16:20:38.365: INFO: (8) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 39.528763ms)
Feb 28 16:20:38.404: INFO: (9) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 39.794726ms)
Feb 28 16:20:38.445: INFO: (10) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 40.096614ms)
Feb 28 16:20:38.484: INFO: (11) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 39.154725ms)
Feb 28 16:20:38.524: INFO: (12) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 40.362581ms)
Feb 28 16:20:38.564: INFO: (13) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 39.676572ms)
Feb 28 16:20:38.603: INFO: (14) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 39.193319ms)
Feb 28 16:20:38.642: INFO: (15) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 38.959797ms)
Feb 28 16:20:38.685: INFO: (16) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 42.823324ms)
Feb 28 16:20:38.725: INFO: (17) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 39.848057ms)
Feb 28 16:20:38.764: INFO: (18) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 39.231208ms)
Feb 28 16:20:38.803: INFO: (19) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 38.928506ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:20:38.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1374" for this suite.
Feb 28 16:20:44.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:20:46.405: INFO: namespace proxy-1374 deletion completed in 7.562385666s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:20:46.405: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-4c8343a8-1dfa-4008-b760-fc0f48744d2e
STEP: Creating a pod to test consume configMaps
Feb 28 16:20:46.679: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4cc2be76-36a6-4960-84df-d6f05e40100d" in namespace "projected-8622" to be "success or failure"
Feb 28 16:20:46.718: INFO: Pod "pod-projected-configmaps-4cc2be76-36a6-4960-84df-d6f05e40100d": Phase="Pending", Reason="", readiness=false. Elapsed: 38.855043ms
Feb 28 16:20:48.757: INFO: Pod "pod-projected-configmaps-4cc2be76-36a6-4960-84df-d6f05e40100d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077035776s
STEP: Saw pod success
Feb 28 16:20:48.757: INFO: Pod "pod-projected-configmaps-4cc2be76-36a6-4960-84df-d6f05e40100d" satisfied condition "success or failure"
Feb 28 16:20:48.796: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-projected-configmaps-4cc2be76-36a6-4960-84df-d6f05e40100d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 16:20:48.894: INFO: Waiting for pod pod-projected-configmaps-4cc2be76-36a6-4960-84df-d6f05e40100d to disappear
Feb 28 16:20:48.932: INFO: Pod pod-projected-configmaps-4cc2be76-36a6-4960-84df-d6f05e40100d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:20:48.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8622" for this suite.
Feb 28 16:20:55.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:20:56.557: INFO: namespace projected-8622 deletion completed in 7.586009411s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:20:56.558: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:20:56.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2558" for this suite.
Feb 28 16:21:18.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:21:20.550: INFO: namespace pods-2558 deletion completed in 23.674555083s
•S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:21:20.550: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Feb 28 16:21:21.411: INFO: created pod pod-service-account-defaultsa
Feb 28 16:21:21.411: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 28 16:21:21.452: INFO: created pod pod-service-account-mountsa
Feb 28 16:21:21.452: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 28 16:21:21.493: INFO: created pod pod-service-account-nomountsa
Feb 28 16:21:21.493: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 28 16:21:21.535: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 28 16:21:21.535: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 28 16:21:21.578: INFO: created pod pod-service-account-mountsa-mountspec
Feb 28 16:21:21.578: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 28 16:21:21.620: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 28 16:21:21.620: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 28 16:21:21.666: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 28 16:21:21.666: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 28 16:21:21.709: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 28 16:21:21.709: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 28 16:21:21.751: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 28 16:21:21.751: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:21:21.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2694" for this suite.
Feb 28 16:21:43.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:21:45.389: INFO: namespace svcaccounts-2694 deletion completed in 23.599751551s
•
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:21:45.389: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-4fde5f27-6369-4721-a1a7-8194d356445d
STEP: Creating a pod to test consume configMaps
Feb 28 16:21:45.665: INFO: Waiting up to 5m0s for pod "pod-configmaps-7817833b-c821-40a9-956d-51107117b271" in namespace "configmap-2753" to be "success or failure"
Feb 28 16:21:45.703: INFO: Pod "pod-configmaps-7817833b-c821-40a9-956d-51107117b271": Phase="Pending", Reason="", readiness=false. Elapsed: 37.378717ms
Feb 28 16:21:47.742: INFO: Pod "pod-configmaps-7817833b-c821-40a9-956d-51107117b271": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076562779s
STEP: Saw pod success
Feb 28 16:21:47.742: INFO: Pod "pod-configmaps-7817833b-c821-40a9-956d-51107117b271" satisfied condition "success or failure"
Feb 28 16:21:47.786: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-configmaps-7817833b-c821-40a9-956d-51107117b271 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 16:21:47.883: INFO: Waiting for pod pod-configmaps-7817833b-c821-40a9-956d-51107117b271 to disappear
Feb 28 16:21:47.920: INFO: Pod pod-configmaps-7817833b-c821-40a9-956d-51107117b271 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:21:47.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2753" for this suite.
Feb 28 16:21:54.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:21:55.654: INFO: namespace configmap-2753 deletion completed in 7.69487651s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:21:55.654: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 16:21:55.916: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e5ec9db-7aaf-435e-8d10-b1c439666256" in namespace "projected-7145" to be "success or failure"
Feb 28 16:21:55.954: INFO: Pod "downwardapi-volume-0e5ec9db-7aaf-435e-8d10-b1c439666256": Phase="Pending", Reason="", readiness=false. Elapsed: 37.960624ms
Feb 28 16:21:57.992: INFO: Pod "downwardapi-volume-0e5ec9db-7aaf-435e-8d10-b1c439666256": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075931352s
Feb 28 16:22:00.032: INFO: Pod "downwardapi-volume-0e5ec9db-7aaf-435e-8d10-b1c439666256": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.1151342s
STEP: Saw pod success
Feb 28 16:22:00.032: INFO: Pod "downwardapi-volume-0e5ec9db-7aaf-435e-8d10-b1c439666256" satisfied condition "success or failure"
Feb 28 16:22:00.070: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod downwardapi-volume-0e5ec9db-7aaf-435e-8d10-b1c439666256 container client-container: <nil>
STEP: delete the pod
Feb 28 16:22:00.176: INFO: Waiting for pod downwardapi-volume-0e5ec9db-7aaf-435e-8d10-b1c439666256 to disappear
Feb 28 16:22:00.214: INFO: Pod downwardapi-volume-0e5ec9db-7aaf-435e-8d10-b1c439666256 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:22:00.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7145" for this suite.
Feb 28 16:22:06.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:22:07.848: INFO: namespace projected-7145 deletion completed in 7.595194747s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:22:07.848: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 28 16:22:08.084: INFO: Waiting up to 5m0s for pod "pod-3cba8bbd-3d88-4909-b4d5-f6a032898232" in namespace "emptydir-5468" to be "success or failure"
Feb 28 16:22:08.122: INFO: Pod "pod-3cba8bbd-3d88-4909-b4d5-f6a032898232": Phase="Pending", Reason="", readiness=false. Elapsed: 37.390179ms
Feb 28 16:22:10.161: INFO: Pod "pod-3cba8bbd-3d88-4909-b4d5-f6a032898232": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076322415s
STEP: Saw pod success
Feb 28 16:22:10.161: INFO: Pod "pod-3cba8bbd-3d88-4909-b4d5-f6a032898232" satisfied condition "success or failure"
Feb 28 16:22:10.200: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-3cba8bbd-3d88-4909-b4d5-f6a032898232 container test-container: <nil>
STEP: delete the pod
Feb 28 16:22:10.299: INFO: Waiting for pod pod-3cba8bbd-3d88-4909-b4d5-f6a032898232 to disappear
Feb 28 16:22:10.336: INFO: Pod pod-3cba8bbd-3d88-4909-b4d5-f6a032898232 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:22:10.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5468" for this suite.
Feb 28 16:22:16.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:22:17.940: INFO: namespace emptydir-5468 deletion completed in 7.565605997s
•SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:22:17.940: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5876
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 16:22:18.132: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 16:22:40.968: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.64.3.50:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5876 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:22:40.968: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:22:41.398: INFO: Found all expected endpoints: [netserver-0]
Feb 28 16:22:41.436: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.64.2.46:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5876 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:22:41.436: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:22:41.853: INFO: Found all expected endpoints: [netserver-1]
Feb 28 16:22:41.890: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.64.1.20:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5876 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:22:41.890: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:22:42.268: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:22:42.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5876" for this suite.
Feb 28 16:23:04.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:23:05.886: INFO: namespace pod-network-test-5876 deletion completed in 23.579456018s
•SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:23:05.886: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-rx5j
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 16:23:06.202: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rx5j" in namespace "subpath-3136" to be "success or failure"
Feb 28 16:23:06.239: INFO: Pod "pod-subpath-test-configmap-rx5j": Phase="Pending", Reason="", readiness=false. Elapsed: 37.832615ms
Feb 28 16:23:08.278: INFO: Pod "pod-subpath-test-configmap-rx5j": Phase="Running", Reason="", readiness=true. Elapsed: 2.075920917s
Feb 28 16:23:10.316: INFO: Pod "pod-subpath-test-configmap-rx5j": Phase="Running", Reason="", readiness=true. Elapsed: 4.114131672s
Feb 28 16:23:12.355: INFO: Pod "pod-subpath-test-configmap-rx5j": Phase="Running", Reason="", readiness=true. Elapsed: 6.1531596s
Feb 28 16:23:14.394: INFO: Pod "pod-subpath-test-configmap-rx5j": Phase="Running", Reason="", readiness=true. Elapsed: 8.192139115s
Feb 28 16:23:16.432: INFO: Pod "pod-subpath-test-configmap-rx5j": Phase="Running", Reason="", readiness=true. Elapsed: 10.230517397s
Feb 28 16:23:18.470: INFO: Pod "pod-subpath-test-configmap-rx5j": Phase="Running", Reason="", readiness=true. Elapsed: 12.26858492s
Feb 28 16:23:20.509: INFO: Pod "pod-subpath-test-configmap-rx5j": Phase="Running", Reason="", readiness=true. Elapsed: 14.306934861s
Feb 28 16:23:22.547: INFO: Pod "pod-subpath-test-configmap-rx5j": Phase="Running", Reason="", readiness=true. Elapsed: 16.345226281s
Feb 28 16:23:24.585: INFO: Pod "pod-subpath-test-configmap-rx5j": Phase="Running", Reason="", readiness=true. Elapsed: 18.382950461s
Feb 28 16:23:26.623: INFO: Pod "pod-subpath-test-configmap-rx5j": Phase="Running", Reason="", readiness=true. Elapsed: 20.421325061s
Feb 28 16:23:28.661: INFO: Pod "pod-subpath-test-configmap-rx5j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.459704029s
STEP: Saw pod success
Feb 28 16:23:28.661: INFO: Pod "pod-subpath-test-configmap-rx5j" satisfied condition "success or failure"
Feb 28 16:23:28.704: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-subpath-test-configmap-rx5j container test-container-subpath-configmap-rx5j: <nil>
STEP: delete the pod
Feb 28 16:23:28.800: INFO: Waiting for pod pod-subpath-test-configmap-rx5j to disappear
Feb 28 16:23:28.838: INFO: Pod pod-subpath-test-configmap-rx5j no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rx5j
Feb 28 16:23:28.838: INFO: Deleting pod "pod-subpath-test-configmap-rx5j" in namespace "subpath-3136"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:23:28.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3136" for this suite.
Feb 28 16:23:35.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:23:36.486: INFO: namespace subpath-3136 deletion completed in 7.571236576s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:23:36.487: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 16:23:36.644: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-63'
Feb 28 16:23:36.893: INFO: stderr: ""
Feb 28 16:23:36.893: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Feb 28 16:23:36.931: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete pods e2e-test-nginx-pod --namespace=kubectl-63'
Feb 28 16:23:37.984: INFO: stderr: ""
Feb 28 16:23:37.984: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:23:37.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-63" for this suite.
Feb 28 16:23:44.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:23:45.589: INFO: namespace kubectl-63 deletion completed in 7.563043002s
•SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:23:45.589: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 16:23:45.797: INFO: Waiting up to 5m0s for pod "downwardapi-volume-93ac78a8-1d44-4116-8a70-07a03780f079" in namespace "projected-5090" to be "success or failure"
Feb 28 16:23:45.865: INFO: Pod "downwardapi-volume-93ac78a8-1d44-4116-8a70-07a03780f079": Phase="Pending", Reason="", readiness=false. Elapsed: 67.96471ms
Feb 28 16:23:47.903: INFO: Pod "downwardapi-volume-93ac78a8-1d44-4116-8a70-07a03780f079": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.106336934s
STEP: Saw pod success
Feb 28 16:23:47.903: INFO: Pod "downwardapi-volume-93ac78a8-1d44-4116-8a70-07a03780f079" satisfied condition "success or failure"
Feb 28 16:23:47.941: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod downwardapi-volume-93ac78a8-1d44-4116-8a70-07a03780f079 container client-container: <nil>
STEP: delete the pod
Feb 28 16:23:48.042: INFO: Waiting for pod downwardapi-volume-93ac78a8-1d44-4116-8a70-07a03780f079 to disappear
Feb 28 16:23:48.080: INFO: Pod downwardapi-volume-93ac78a8-1d44-4116-8a70-07a03780f079 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:23:48.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5090" for this suite.
Feb 28 16:23:54.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:23:55.848: INFO: namespace projected-5090 deletion completed in 7.729471014s
•S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:23:55.848: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
STEP: creating the pod
Feb 28 16:23:56.000: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-773'
Feb 28 16:23:56.462: INFO: stderr: ""
Feb 28 16:23:56.462: INFO: stdout: "pod/pause created\n"
Feb 28 16:23:56.462: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 28 16:23:56.462: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-773" to be "running and ready"
Feb 28 16:23:56.501: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 39.016413ms
Feb 28 16:23:58.539: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.077062022s
Feb 28 16:23:58.539: INFO: Pod "pause" satisfied condition "running and ready"
Feb 28 16:23:58.539: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 28 16:23:58.540: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config label pods pause testing-label=testing-label-value --namespace=kubectl-773'
Feb 28 16:23:58.821: INFO: stderr: ""
Feb 28 16:23:58.821: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 28 16:23:58.821: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pod pause -L testing-label --namespace=kubectl-773'
Feb 28 16:23:59.045: INFO: stderr: ""
Feb 28 16:23:59.045: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 28 16:23:59.045: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config label pods pause testing-label- --namespace=kubectl-773'
Feb 28 16:23:59.306: INFO: stderr: ""
Feb 28 16:23:59.306: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 28 16:23:59.306: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pod pause -L testing-label --namespace=kubectl-773'
Feb 28 16:23:59.555: INFO: stderr: ""
Feb 28 16:23:59.555: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1217
STEP: using delete to clean up resources
Feb 28 16:23:59.555: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-773'
Feb 28 16:23:59.836: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 16:23:59.837: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 28 16:23:59.837: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get rc,svc -l name=pause --no-headers --namespace=kubectl-773'
Feb 28 16:24:00.118: INFO: stderr: "No resources found.\n"
Feb 28 16:24:00.118: INFO: stdout: ""
Feb 28 16:24:00.118: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -l name=pause --namespace=kubectl-773 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 16:24:00.336: INFO: stderr: ""
Feb 28 16:24:00.336: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:24:00.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-773" for this suite.
Feb 28 16:24:06.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:24:07.951: INFO: namespace kubectl-773 deletion completed in 7.576727688s
•SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:24:07.952: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 28 16:24:08.193: INFO: Waiting up to 5m0s for pod "downward-api-57e27efd-2e91-4c24-8390-206e0086d03d" in namespace "downward-api-3453" to be "success or failure"
Feb 28 16:24:08.231: INFO: Pod "downward-api-57e27efd-2e91-4c24-8390-206e0086d03d": Phase="Pending", Reason="", readiness=false. Elapsed: 37.593715ms
Feb 28 16:24:10.270: INFO: Pod "downward-api-57e27efd-2e91-4c24-8390-206e0086d03d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076702807s
STEP: Saw pod success
Feb 28 16:24:10.270: INFO: Pod "downward-api-57e27efd-2e91-4c24-8390-206e0086d03d" satisfied condition "success or failure"
Feb 28 16:24:10.308: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod downward-api-57e27efd-2e91-4c24-8390-206e0086d03d container dapi-container: <nil>
STEP: delete the pod
Feb 28 16:24:10.404: INFO: Waiting for pod downward-api-57e27efd-2e91-4c24-8390-206e0086d03d to disappear
Feb 28 16:24:10.442: INFO: Pod downward-api-57e27efd-2e91-4c24-8390-206e0086d03d no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:24:10.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3453" for this suite.
Feb 28 16:24:16.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:24:18.080: INFO: namespace downward-api-3453 deletion completed in 7.598250416s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:24:18.080: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1890, will wait for the garbage collector to delete the pods
Feb 28 16:24:22.480: INFO: Deleting Job.batch foo took: 40.43196ms
Feb 28 16:24:22.980: INFO: Terminating Job.batch foo pods took: 500.307973ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:24:58.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1890" for this suite.
Feb 28 16:25:04.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:25:06.354: INFO: namespace job-1890 deletion completed in 7.597575637s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:25:06.356: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 16:25:06.513: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-9001'
Feb 28 16:25:06.783: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 28 16:25:06.783: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1426
Feb 28 16:25:06.852: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete deployment e2e-test-nginx-deployment --namespace=kubectl-9001'
Feb 28 16:25:07.131: INFO: stderr: ""
Feb 28 16:25:07.132: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:25:07.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9001" for this suite.
Feb 28 16:25:29.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:25:30.755: INFO: namespace kubectl-9001 deletion completed in 23.583345662s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:25:30.756: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-b1524649-84df-48c8-8782-349058296d6e
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:25:30.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8984" for this suite.
Feb 28 16:25:37.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:25:38.600: INFO: namespace configmap-8984 deletion completed in 7.567169187s
•SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:25:38.601: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Feb 28 16:25:38.754: INFO: Asynchronously running '/workspace/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config proxy --unix-socket=/tmp/kubectl-proxy-unix098445516/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:25:38.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5362" for this suite.
Feb 28 16:25:44.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:25:46.432: INFO: namespace kubectl-5362 deletion completed in 7.569695095s
•SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:25:46.432: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Feb 28 16:25:46.620: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 28 16:25:46.699: INFO: Waiting for terminating namespaces to be deleted...
Feb 28 16:25:46.737: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-q1l7 before test
Feb 28 16:25:46.780: INFO: kube-proxy-bootstrap-e2e-minion-group-q1l7 from kube-system started at 2020-02-28 15:42:18 +0000 UTC (1 container statuses recorded)
Feb 28 16:25:46.780: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 16:25:46.780: INFO: fluentd-gcp-v3.2.0-9q7z2 from kube-system started at 2020-02-28 15:42:58 +0000 UTC (2 container statuses recorded)
Feb 28 16:25:46.780: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 28 16:25:46.780: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:25:46.780: INFO: metadata-proxy-v0.1-kbkll from kube-system started at 2020-02-28 15:42:19 +0000 UTC (2 container statuses recorded)
Feb 28 16:25:46.780: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 28 16:25:46.780: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:25:46.780: INFO: metrics-server-v0.3.3-7947ffd59d-rfjr4 from kube-system started at 2020-02-28 15:42:40 +0000 UTC (2 container statuses recorded)
Feb 28 16:25:46.780: INFO: 	Container metrics-server ready: true, restart count 0
Feb 28 16:25:46.780: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 28 16:25:46.780: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-s43f before test
Feb 28 16:25:46.824: INFO: kube-dns-autoscaler-6d6bc99fd8-t6kwd from kube-system started at 2020-02-28 15:42:18 +0000 UTC (1 container statuses recorded)
Feb 28 16:25:46.824: INFO: 	Container autoscaler ready: true, restart count 0
Feb 28 16:25:46.824: INFO: metadata-proxy-v0.1-tnmbw from kube-system started at 2020-02-28 15:42:18 +0000 UTC (2 container statuses recorded)
Feb 28 16:25:46.824: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 28 16:25:46.824: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:25:46.824: INFO: kube-proxy-bootstrap-e2e-minion-group-s43f from kube-system started at 2020-02-28 15:42:17 +0000 UTC (1 container statuses recorded)
Feb 28 16:25:46.824: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 16:25:46.824: INFO: fluentd-gcp-scaler-6848d689fb-8slw6 from kube-system started at 2020-02-28 15:42:18 +0000 UTC (1 container statuses recorded)
Feb 28 16:25:46.824: INFO: 	Container fluentd-gcp-scaler ready: true, restart count 0
Feb 28 16:25:46.824: INFO: fluentd-gcp-v3.2.0-k64bw from kube-system started at 2020-02-28 15:42:35 +0000 UTC (2 container statuses recorded)
Feb 28 16:25:46.824: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 28 16:25:46.824: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:25:46.824: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-t863 before test
Feb 28 16:25:46.870: INFO: kubernetes-dashboard-66b96fb8d7-cr7kc from kube-system started at 2020-02-28 15:42:16 +0000 UTC (1 container statuses recorded)
Feb 28 16:25:46.870: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 28 16:25:46.870: INFO: coredns-557dcdc9f5-9hvg8 from kube-system started at 2020-02-28 15:42:29 +0000 UTC (1 container statuses recorded)
Feb 28 16:25:46.870: INFO: 	Container coredns ready: true, restart count 0
Feb 28 16:25:46.870: INFO: coredns-557dcdc9f5-dg8nb from kube-system started at 2020-02-28 15:42:29 +0000 UTC (1 container statuses recorded)
Feb 28 16:25:46.870: INFO: 	Container coredns ready: true, restart count 0
Feb 28 16:25:46.870: INFO: event-exporter-v0.2.5-5fd6f794f7-5kbmp from kube-system started at 2020-02-28 15:42:30 +0000 UTC (2 container statuses recorded)
Feb 28 16:25:46.870: INFO: 	Container event-exporter ready: true, restart count 0
Feb 28 16:25:46.870: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:25:46.870: INFO: heapster-v1.6.0-beta.1-6b89b777b5-7mnqk from kube-system started at 2020-02-28 15:42:31 +0000 UTC (2 container statuses recorded)
Feb 28 16:25:46.870: INFO: 	Container heapster ready: true, restart count 0
Feb 28 16:25:46.870: INFO: 	Container heapster-nanny ready: true, restart count 0
Feb 28 16:25:46.870: INFO: kube-proxy-bootstrap-e2e-minion-group-t863 from kube-system started at 2020-02-28 15:42:16 +0000 UTC (1 container statuses recorded)
Feb 28 16:25:46.870: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 16:25:46.870: INFO: fluentd-gcp-v3.2.0-j8frl from kube-system started at 2020-02-28 15:42:36 +0000 UTC (2 container statuses recorded)
Feb 28 16:25:46.870: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 28 16:25:46.870: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:25:46.870: INFO: metadata-proxy-v0.1-ft86w from kube-system started at 2020-02-28 15:42:16 +0000 UTC (2 container statuses recorded)
Feb 28 16:25:46.870: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 28 16:25:46.870: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:25:46.870: INFO: l7-default-backend-84c9fcfbb-jzxhq from kube-system started at 2020-02-28 15:42:29 +0000 UTC (1 container statuses recorded)
Feb 28 16:25:46.870: INFO: 	Container default-http-backend ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node bootstrap-e2e-minion-group-q1l7
STEP: verifying the node has the label node bootstrap-e2e-minion-group-s43f
STEP: verifying the node has the label node bootstrap-e2e-minion-group-t863
Feb 28 16:25:47.203: INFO: Pod coredns-557dcdc9f5-9hvg8 requesting resource cpu=100m on Node bootstrap-e2e-minion-group-t863
Feb 28 16:25:47.203: INFO: Pod coredns-557dcdc9f5-dg8nb requesting resource cpu=100m on Node bootstrap-e2e-minion-group-t863
Feb 28 16:25:47.203: INFO: Pod event-exporter-v0.2.5-5fd6f794f7-5kbmp requesting resource cpu=0m on Node bootstrap-e2e-minion-group-t863
Feb 28 16:25:47.203: INFO: Pod fluentd-gcp-scaler-6848d689fb-8slw6 requesting resource cpu=0m on Node bootstrap-e2e-minion-group-s43f
Feb 28 16:25:47.203: INFO: Pod fluentd-gcp-v3.2.0-9q7z2 requesting resource cpu=100m on Node bootstrap-e2e-minion-group-q1l7
Feb 28 16:25:47.203: INFO: Pod fluentd-gcp-v3.2.0-j8frl requesting resource cpu=100m on Node bootstrap-e2e-minion-group-t863
Feb 28 16:25:47.203: INFO: Pod fluentd-gcp-v3.2.0-k64bw requesting resource cpu=100m on Node bootstrap-e2e-minion-group-s43f
Feb 28 16:25:47.203: INFO: Pod heapster-v1.6.0-beta.1-6b89b777b5-7mnqk requesting resource cpu=138m on Node bootstrap-e2e-minion-group-t863
Feb 28 16:25:47.203: INFO: Pod kube-dns-autoscaler-6d6bc99fd8-t6kwd requesting resource cpu=20m on Node bootstrap-e2e-minion-group-s43f
Feb 28 16:25:47.203: INFO: Pod kube-proxy-bootstrap-e2e-minion-group-q1l7 requesting resource cpu=100m on Node bootstrap-e2e-minion-group-q1l7
Feb 28 16:25:47.203: INFO: Pod kube-proxy-bootstrap-e2e-minion-group-s43f requesting resource cpu=100m on Node bootstrap-e2e-minion-group-s43f
Feb 28 16:25:47.203: INFO: Pod kube-proxy-bootstrap-e2e-minion-group-t863 requesting resource cpu=100m on Node bootstrap-e2e-minion-group-t863
Feb 28 16:25:47.203: INFO: Pod kubernetes-dashboard-66b96fb8d7-cr7kc requesting resource cpu=50m on Node bootstrap-e2e-minion-group-t863
Feb 28 16:25:47.203: INFO: Pod l7-default-backend-84c9fcfbb-jzxhq requesting resource cpu=10m on Node bootstrap-e2e-minion-group-t863
Feb 28 16:25:47.203: INFO: Pod metadata-proxy-v0.1-ft86w requesting resource cpu=32m on Node bootstrap-e2e-minion-group-t863
Feb 28 16:25:47.203: INFO: Pod metadata-proxy-v0.1-kbkll requesting resource cpu=32m on Node bootstrap-e2e-minion-group-q1l7
Feb 28 16:25:47.203: INFO: Pod metadata-proxy-v0.1-tnmbw requesting resource cpu=32m on Node bootstrap-e2e-minion-group-s43f
Feb 28 16:25:47.203: INFO: Pod metrics-server-v0.3.3-7947ffd59d-rfjr4 requesting resource cpu=53m on Node bootstrap-e2e-minion-group-q1l7
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2fb8a74e-00ab-4ebb-b82b-c12b65beb636.15f79da9f2ee70a6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9213/filler-pod-2fb8a74e-00ab-4ebb-b82b-c12b65beb636 to bootstrap-e2e-minion-group-t863]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2fb8a74e-00ab-4ebb-b82b-c12b65beb636.15f79daa2b9c5998], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2fb8a74e-00ab-4ebb-b82b-c12b65beb636.15f79daa301f4415], Reason = [Created], Message = [Created container filler-pod-2fb8a74e-00ab-4ebb-b82b-c12b65beb636]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2fb8a74e-00ab-4ebb-b82b-c12b65beb636.15f79daa3d501a18], Reason = [Started], Message = [Started container filler-pod-2fb8a74e-00ab-4ebb-b82b-c12b65beb636]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3937d77b-c31b-41f1-970c-ae03b486438f.15f79da9ee298fe4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9213/filler-pod-3937d77b-c31b-41f1-970c-ae03b486438f to bootstrap-e2e-minion-group-q1l7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3937d77b-c31b-41f1-970c-ae03b486438f.15f79daa1d394b45], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3937d77b-c31b-41f1-970c-ae03b486438f.15f79daa211847dd], Reason = [Created], Message = [Created container filler-pod-3937d77b-c31b-41f1-970c-ae03b486438f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3937d77b-c31b-41f1-970c-ae03b486438f.15f79daa2aa5e3c0], Reason = [Started], Message = [Started container filler-pod-3937d77b-c31b-41f1-970c-ae03b486438f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-52a8775a-ee43-4c57-a95b-838a6051838f.15f79da9f05cccf1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9213/filler-pod-52a8775a-ee43-4c57-a95b-838a6051838f to bootstrap-e2e-minion-group-s43f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-52a8775a-ee43-4c57-a95b-838a6051838f.15f79daa57af5646], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-52a8775a-ee43-4c57-a95b-838a6051838f.15f79daa5b872040], Reason = [Created], Message = [Created container filler-pod-52a8775a-ee43-4c57-a95b-838a6051838f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-52a8775a-ee43-4c57-a95b-838a6051838f.15f79daa66d30a34], Reason = [Started], Message = [Started container filler-pod-52a8775a-ee43-4c57-a95b-838a6051838f]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f79daa79ff3c3b], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) were unschedulable, 3 Insufficient cpu.]
STEP: removing the label node off the node bootstrap-e2e-minion-group-s43f
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node bootstrap-e2e-minion-group-t863
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node bootstrap-e2e-minion-group-q1l7
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:25:51.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9213" for this suite.
Feb 28 16:25:57.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:25:58.642: INFO: namespace sched-pred-9213 deletion completed in 7.600000227s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:25:58.642: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Feb 28 16:25:58.799: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config --namespace=kubectl-9787 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 28 16:26:01.700: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 28 16:26:01.700: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:26:03.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9787" for this suite.
Feb 28 16:26:09.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:26:11.400: INFO: namespace kubectl-9787 deletion completed in 7.586080116s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:26:11.401: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:27:11.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7914" for this suite.
Feb 28 16:27:33.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:27:35.292: INFO: namespace container-probe-7914 deletion completed in 23.566974708s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:27:35.293: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 28 16:27:41.769: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7507 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:27:41.769: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:27:42.159: INFO: Exec stderr: ""
Feb 28 16:27:42.159: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7507 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:27:42.159: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:27:42.548: INFO: Exec stderr: ""
Feb 28 16:27:42.548: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7507 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:27:42.548: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:27:42.926: INFO: Exec stderr: ""
Feb 28 16:27:42.926: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7507 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:27:42.926: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:27:43.307: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 28 16:27:43.307: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7507 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:27:43.307: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:27:43.677: INFO: Exec stderr: ""
Feb 28 16:27:43.677: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7507 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:27:43.677: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:27:44.039: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 28 16:27:44.039: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7507 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:27:44.039: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:27:44.437: INFO: Exec stderr: ""
Feb 28 16:27:44.437: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7507 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:27:44.437: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:27:44.821: INFO: Exec stderr: ""
Feb 28 16:27:44.821: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7507 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:27:44.821: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:27:45.200: INFO: Exec stderr: ""
Feb 28 16:27:45.200: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7507 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:27:45.200: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:27:45.631: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:27:45.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7507" for this suite.
Feb 28 16:28:29.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:28:31.251: INFO: namespace e2e-kubelet-etc-hosts-7507 deletion completed in 45.581178917s
•SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:28:31.251: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-ba6ecfbe-da56-4815-8439-c53cc08489c8
STEP: Creating a pod to test consume configMaps
Feb 28 16:28:31.530: INFO: Waiting up to 5m0s for pod "pod-configmaps-11fac03e-3b09-42b4-9d00-b6a3fce618ca" in namespace "configmap-8376" to be "success or failure"
Feb 28 16:28:31.567: INFO: Pod "pod-configmaps-11fac03e-3b09-42b4-9d00-b6a3fce618ca": Phase="Pending", Reason="", readiness=false. Elapsed: 37.31662ms
Feb 28 16:28:33.606: INFO: Pod "pod-configmaps-11fac03e-3b09-42b4-9d00-b6a3fce618ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076067356s
Feb 28 16:28:35.644: INFO: Pod "pod-configmaps-11fac03e-3b09-42b4-9d00-b6a3fce618ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.114292735s
STEP: Saw pod success
Feb 28 16:28:35.644: INFO: Pod "pod-configmaps-11fac03e-3b09-42b4-9d00-b6a3fce618ca" satisfied condition "success or failure"
Feb 28 16:28:35.682: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-configmaps-11fac03e-3b09-42b4-9d00-b6a3fce618ca container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 16:28:35.776: INFO: Waiting for pod pod-configmaps-11fac03e-3b09-42b4-9d00-b6a3fce618ca to disappear
Feb 28 16:28:35.813: INFO: Pod pod-configmaps-11fac03e-3b09-42b4-9d00-b6a3fce618ca no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:28:35.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8376" for this suite.
Feb 28 16:28:41.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:28:43.421: INFO: namespace configmap-8376 deletion completed in 7.569562548s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:28:43.422: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 28 16:28:50.004: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:28:50.042: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 16:28:52.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:28:52.081: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 16:28:54.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:28:54.080: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 16:28:56.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:28:56.080: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 16:28:58.043: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:28:58.081: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 16:29:00.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:29:00.081: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 16:29:02.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:29:02.081: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 16:29:04.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:29:04.081: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 16:29:06.043: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:29:06.080: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 16:29:08.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:29:08.081: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 16:29:10.043: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:29:10.081: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 16:29:12.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:29:12.081: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 16:29:14.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:29:14.081: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 16:29:16.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:29:16.080: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 16:29:18.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:29:18.081: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 28 16:29:20.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 28 16:29:20.080: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:29:20.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8573" for this suite.
Feb 28 16:29:42.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:29:43.679: INFO: namespace container-lifecycle-hook-8573 deletion completed in 23.56012557s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:29:43.680: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
[It] should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 16:29:43.834: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-5566'
Feb 28 16:29:44.424: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 28 16:29:44.424: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Feb 28 16:29:44.504: INFO: scanned /workspace for discovery docs: <nil>
Feb 28 16:29:44.504: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-5566'
Feb 28 16:29:56.137: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 28 16:29:56.137: INFO: stdout: "Created e2e-test-nginx-rc-b63a5dcd93c83b9afdf8671c409240c3\nScaling up e2e-test-nginx-rc-b63a5dcd93c83b9afdf8671c409240c3 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-b63a5dcd93c83b9afdf8671c409240c3 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-b63a5dcd93c83b9afdf8671c409240c3 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 28 16:29:56.137: INFO: stdout: "Created e2e-test-nginx-rc-b63a5dcd93c83b9afdf8671c409240c3\nScaling up e2e-test-nginx-rc-b63a5dcd93c83b9afdf8671c409240c3 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-b63a5dcd93c83b9afdf8671c409240c3 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-b63a5dcd93c83b9afdf8671c409240c3 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 28 16:29:56.137: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-5566'
Feb 28 16:29:56.360: INFO: stderr: ""
Feb 28 16:29:56.360: INFO: stdout: "e2e-test-nginx-rc-6ppkt e2e-test-nginx-rc-b63a5dcd93c83b9afdf8671c409240c3-c2q5x "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb 28 16:30:01.360: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-5566'
Feb 28 16:30:01.604: INFO: stderr: ""
Feb 28 16:30:01.604: INFO: stdout: "e2e-test-nginx-rc-b63a5dcd93c83b9afdf8671c409240c3-c2q5x "
Feb 28 16:30:01.604: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods e2e-test-nginx-rc-b63a5dcd93c83b9afdf8671c409240c3-c2q5x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5566'
Feb 28 16:30:01.856: INFO: stderr: ""
Feb 28 16:30:01.856: INFO: stdout: "true"
Feb 28 16:30:01.856: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods e2e-test-nginx-rc-b63a5dcd93c83b9afdf8671c409240c3-c2q5x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5566'
Feb 28 16:30:02.080: INFO: stderr: ""
Feb 28 16:30:02.080: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Feb 28 16:30:02.080: INFO: e2e-test-nginx-rc-b63a5dcd93c83b9afdf8671c409240c3-c2q5x is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1522
Feb 28 16:30:02.080: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete rc e2e-test-nginx-rc --namespace=kubectl-5566'
Feb 28 16:30:02.361: INFO: stderr: ""
Feb 28 16:30:02.361: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:30:02.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5566" for this suite.
Feb 28 16:30:24.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:30:25.969: INFO: namespace kubectl-5566 deletion completed in 23.569682634s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:30:25.970: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 28 16:30:26.171: INFO: Waiting up to 5m0s for pod "pod-b0c506e7-f841-401a-a7a7-b6bf2f055bb9" in namespace "emptydir-8344" to be "success or failure"
Feb 28 16:30:26.209: INFO: Pod "pod-b0c506e7-f841-401a-a7a7-b6bf2f055bb9": Phase="Pending", Reason="", readiness=false. Elapsed: 37.280703ms
Feb 28 16:30:28.247: INFO: Pod "pod-b0c506e7-f841-401a-a7a7-b6bf2f055bb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075672269s
STEP: Saw pod success
Feb 28 16:30:28.247: INFO: Pod "pod-b0c506e7-f841-401a-a7a7-b6bf2f055bb9" satisfied condition "success or failure"
Feb 28 16:30:28.285: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-b0c506e7-f841-401a-a7a7-b6bf2f055bb9 container test-container: <nil>
STEP: delete the pod
Feb 28 16:30:28.382: INFO: Waiting for pod pod-b0c506e7-f841-401a-a7a7-b6bf2f055bb9 to disappear
Feb 28 16:30:28.420: INFO: Pod pod-b0c506e7-f841-401a-a7a7-b6bf2f055bb9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:30:28.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8344" for this suite.
Feb 28 16:30:34.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:30:36.021: INFO: namespace emptydir-8344 deletion completed in 7.562752214s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:30:36.021: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:30:38.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6169" for this suite.
Feb 28 16:30:44.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:30:46.182: INFO: namespace emptydir-wrapper-6169 deletion completed in 7.561554967s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:30:46.182: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-4934cf0e-a349-4128-9100-2228cb0b3d3a
STEP: Creating a pod to test consume configMaps
Feb 28 16:30:46.458: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-96d9c7b7-1719-4bd1-a184-4d4df39f6eb7" in namespace "projected-8163" to be "success or failure"
Feb 28 16:30:46.496: INFO: Pod "pod-projected-configmaps-96d9c7b7-1719-4bd1-a184-4d4df39f6eb7": Phase="Pending", Reason="", readiness=false. Elapsed: 37.671385ms
Feb 28 16:30:48.534: INFO: Pod "pod-projected-configmaps-96d9c7b7-1719-4bd1-a184-4d4df39f6eb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076025377s
STEP: Saw pod success
Feb 28 16:30:48.534: INFO: Pod "pod-projected-configmaps-96d9c7b7-1719-4bd1-a184-4d4df39f6eb7" satisfied condition "success or failure"
Feb 28 16:30:48.572: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-projected-configmaps-96d9c7b7-1719-4bd1-a184-4d4df39f6eb7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 16:30:48.666: INFO: Waiting for pod pod-projected-configmaps-96d9c7b7-1719-4bd1-a184-4d4df39f6eb7 to disappear
Feb 28 16:30:48.703: INFO: Pod pod-projected-configmaps-96d9c7b7-1719-4bd1-a184-4d4df39f6eb7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:30:48.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8163" for this suite.
Feb 28 16:30:54.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:30:56.360: INFO: namespace projected-8163 deletion completed in 7.618881881s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:30:56.361: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 28 16:30:56.561: INFO: Waiting up to 5m0s for pod "pod-0d783576-6ba3-4bf9-924f-1e7d17a1e43f" in namespace "emptydir-8258" to be "success or failure"
Feb 28 16:30:56.598: INFO: Pod "pod-0d783576-6ba3-4bf9-924f-1e7d17a1e43f": Phase="Pending", Reason="", readiness=false. Elapsed: 37.636798ms
Feb 28 16:30:58.637: INFO: Pod "pod-0d783576-6ba3-4bf9-924f-1e7d17a1e43f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076032846s
STEP: Saw pod success
Feb 28 16:30:58.637: INFO: Pod "pod-0d783576-6ba3-4bf9-924f-1e7d17a1e43f" satisfied condition "success or failure"
Feb 28 16:30:58.674: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-0d783576-6ba3-4bf9-924f-1e7d17a1e43f container test-container: <nil>
STEP: delete the pod
Feb 28 16:30:58.765: INFO: Waiting for pod pod-0d783576-6ba3-4bf9-924f-1e7d17a1e43f to disappear
Feb 28 16:30:58.803: INFO: Pod pod-0d783576-6ba3-4bf9-924f-1e7d17a1e43f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:30:58.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8258" for this suite.
Feb 28 16:31:04.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:31:06.410: INFO: namespace emptydir-8258 deletion completed in 7.568671923s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:31:06.411: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-ee7ad4c4-6744-4684-94b0-0a737f7e6576
STEP: Creating a pod to test consume configMaps
Feb 28 16:31:06.653: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d993afe7-ee92-407e-9305-ad25c886fa92" in namespace "projected-4789" to be "success or failure"
Feb 28 16:31:06.696: INFO: Pod "pod-projected-configmaps-d993afe7-ee92-407e-9305-ad25c886fa92": Phase="Pending", Reason="", readiness=false. Elapsed: 43.74595ms
Feb 28 16:31:08.735: INFO: Pod "pod-projected-configmaps-d993afe7-ee92-407e-9305-ad25c886fa92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.082111233s
STEP: Saw pod success
Feb 28 16:31:08.735: INFO: Pod "pod-projected-configmaps-d993afe7-ee92-407e-9305-ad25c886fa92" satisfied condition "success or failure"
Feb 28 16:31:08.773: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-projected-configmaps-d993afe7-ee92-407e-9305-ad25c886fa92 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 16:31:08.864: INFO: Waiting for pod pod-projected-configmaps-d993afe7-ee92-407e-9305-ad25c886fa92 to disappear
Feb 28 16:31:08.902: INFO: Pod pod-projected-configmaps-d993afe7-ee92-407e-9305-ad25c886fa92 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:31:08.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4789" for this suite.
Feb 28 16:31:15.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:31:16.515: INFO: namespace projected-4789 deletion completed in 7.574022153s
•SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:31:16.515: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 16:31:16.769: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 28 16:31:18.846: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 28 16:31:20.884: INFO: Creating deployment "test-rollover-deployment"
Feb 28 16:31:20.983: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 28 16:31:21.030: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 28 16:31:21.106: INFO: Ensure that both replica sets have 1 created replica
Feb 28 16:31:21.183: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 28 16:31:21.264: INFO: Updating deployment test-rollover-deployment
Feb 28 16:31:21.264: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 28 16:31:21.338: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 28 16:31:21.447: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 28 16:31:21.523: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 16:31:21.523: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504281, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 16:31:23.600: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 16:31:23.600: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504281, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 16:31:25.600: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 16:31:25.600: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504283, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 16:31:27.600: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 16:31:27.600: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504283, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 16:31:29.600: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 16:31:29.600: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504283, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 16:31:31.600: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 16:31:31.600: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504283, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 16:31:33.599: INFO: all replica sets need to contain the pod-template-hash label
Feb 28 16:31:33.599: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504283, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504280, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 16:31:35.601: INFO: 
Feb 28 16:31:35.601: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 28 16:31:35.715: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-9112,SelfLink:/apis/apps/v1/namespaces/deployment-9112/deployments/test-rollover-deployment,UID:4ab2e0c7-46d3-4747-898b-76e30490d7d2,ResourceVersion:10386,Generation:2,CreationTimestamp:2020-02-28 16:31:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-02-28 16:31:20 +0000 UTC 2020-02-28 16:31:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-02-28 16:31:33 +0000 UTC 2020-02-28 16:31:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 28 16:31:35.754: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-9112,SelfLink:/apis/apps/v1/namespaces/deployment-9112/replicasets/test-rollover-deployment-854595fc44,UID:078bb460-11de-4846-b48b-9e31ca93765b,ResourceVersion:10379,Generation:2,CreationTimestamp:2020-02-28 16:31:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 4ab2e0c7-46d3-4747-898b-76e30490d7d2 0xc002fd5347 0xc002fd5348}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 28 16:31:35.754: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 28 16:31:35.754: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-9112,SelfLink:/apis/apps/v1/namespaces/deployment-9112/replicasets/test-rollover-controller,UID:77722cd4-c03c-40b9-950a-0670b4d42257,ResourceVersion:10385,Generation:2,CreationTimestamp:2020-02-28 16:31:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 4ab2e0c7-46d3-4747-898b-76e30490d7d2 0xc002fd50a7 0xc002fd50a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 16:31:35.754: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-9112,SelfLink:/apis/apps/v1/namespaces/deployment-9112/replicasets/test-rollover-deployment-9b8b997cf,UID:54d235a6-6016-439c-a86c-eb00cef464ba,ResourceVersion:10345,Generation:2,CreationTimestamp:2020-02-28 16:31:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 4ab2e0c7-46d3-4747-898b-76e30490d7d2 0xc002fd5520 0xc002fd5521}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 16:31:35.794: INFO: Pod "test-rollover-deployment-854595fc44-m7wkf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-m7wkf,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-9112,SelfLink:/api/v1/namespaces/deployment-9112/pods/test-rollover-deployment-854595fc44-m7wkf,UID:bea1b783-6bc1-40ed-9541-70accf7322ce,ResourceVersion:10356,Generation:0,CreationTimestamp:2020-02-28 16:31:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 078bb460-11de-4846-b48b-9e31ca93765b 0xc000d6e357 0xc000d6e358}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f5hzm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f5hzm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-f5hzm true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-q1l7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d6e450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d6e470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:31:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:31:23 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:31:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:31:21 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.4,PodIP:10.64.3.66,StartTime:2020-02-28 16:31:21 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-02-28 16:31:22 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://3f14f37eeed6c9f37af7601da1b351d2c06a1da5dbf57d2be05b3e73a435094e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:31:35.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9112" for this suite.
Feb 28 16:31:41.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:31:43.443: INFO: namespace deployment-9112 deletion completed in 7.610832126s
•SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:31:43.443: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 28 16:31:43.632: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:31:47.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6011" for this suite.
Feb 28 16:31:53.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:31:54.690: INFO: namespace init-container-6011 deletion completed in 7.58273501s
•SSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:31:54.690: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 16:31:54.882: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 28 16:31:54.979: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 28 16:31:57.057: INFO: Creating deployment "test-rolling-update-deployment"
Feb 28 16:31:57.098: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 28 16:31:57.191: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 28 16:31:57.229: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504317, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504317, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504317, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718504317, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 16:31:59.268: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 28 16:31:59.382: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-1906,SelfLink:/apis/apps/v1/namespaces/deployment-1906/deployments/test-rolling-update-deployment,UID:0e85fda4-2ee2-4773-81e5-9ae1e2ff8670,ResourceVersion:10499,Generation:1,CreationTimestamp:2020-02-28 16:31:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-02-28 16:31:57 +0000 UTC 2020-02-28 16:31:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-02-28 16:31:59 +0000 UTC 2020-02-28 16:31:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 28 16:31:59.421: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-1906,SelfLink:/apis/apps/v1/namespaces/deployment-1906/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:3d834307-07ab-46a3-8748-3e24a1f51fb9,ResourceVersion:10492,Generation:1,CreationTimestamp:2020-02-28 16:31:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0e85fda4-2ee2-4773-81e5-9ae1e2ff8670 0xc003177087 0xc003177088}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 28 16:31:59.421: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 28 16:31:59.421: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-1906,SelfLink:/apis/apps/v1/namespaces/deployment-1906/replicasets/test-rolling-update-controller,UID:5c8dd4d5-d430-49dd-8e61-2e89907c05dd,ResourceVersion:10498,Generation:2,CreationTimestamp:2020-02-28 16:31:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0e85fda4-2ee2-4773-81e5-9ae1e2ff8670 0xc003176fb7 0xc003176fb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 16:31:59.459: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-lkhcs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-lkhcs,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-1906,SelfLink:/api/v1/namespaces/deployment-1906/pods/test-rolling-update-deployment-79f6b9d75c-lkhcs,UID:284c7f2b-d0f8-4c35-b255-e6df14d8f12b,ResourceVersion:10491,Generation:0,CreationTimestamp:2020-02-28 16:31:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 3d834307-07ab-46a3-8748-3e24a1f51fb9 0xc000479797 0xc000479798}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qfp4f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qfp4f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-qfp4f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0004798a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0004798c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:31:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:31:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:31:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:31:57 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.2.60,StartTime:2020-02-28 16:31:57 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-02-28 16:31:58 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://490d551be8d799f48db1c5cf150f16172fb5cdeba1b54828871b2142fea6380d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:31:59.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1906" for this suite.
Feb 28 16:32:05.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:32:07.070: INFO: namespace deployment-1906 deletion completed in 7.572839501s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:32:07.071: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 28 16:32:17.880: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 3357
	[quantile=0.9] = 179477
	[quantile=0.99] = 866385
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 205166
	[quantile=0.9] = 592510
	[quantile=0.99] = 874649
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = 6
	[quantile=0.9] = 7
	[quantile=0.99] = 7
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = 488974
	[quantile=0.9] = 534984
	[quantile=0.99] = 534984
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 5
	[quantile=0.9] = 7
	[quantile=0.99] = 26
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 14
	[quantile=0.9] = 28
	[quantile=0.99] = 69
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 12
	[quantile=0.9] = 14
	[quantile=0.99] = 46
For namespace_queue_latency_sum:
	[] = 1918
For namespace_queue_latency_count:
	[] = 147
For namespace_retries:
	[] = 148
For namespace_work_duration:
	[quantile=0.5] = 343317
	[quantile=0.9] = 507452
	[quantile=0.99] = 600628
For namespace_work_duration_sum:
	[] = 53627524
For namespace_work_duration_count:
	[] = 147
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:32:17.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9024" for this suite.
Feb 28 16:32:24.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:32:25.493: INFO: namespace gc-9024 deletion completed in 7.574435384s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:32:25.494: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 28 16:32:32.018: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 1319
	[quantile=0.9] = 106583
	[quantile=0.99] = 866385
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 125853
	[quantile=0.9] = 617880
	[quantile=0.99] = 874649
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = 6
	[quantile=0.9] = 7
	[quantile=0.99] = 7
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = 488974
	[quantile=0.9] = 534984
	[quantile=0.99] = 534984
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 5
	[quantile=0.9] = 7
	[quantile=0.99] = 26
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 14
	[quantile=0.9] = 28
	[quantile=0.99] = 72
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 12
	[quantile=0.9] = 14
	[quantile=0.99] = 46
For namespace_queue_latency_sum:
	[] = 1933
For namespace_queue_latency_count:
	[] = 148
For namespace_retries:
	[] = 149
For namespace_work_duration:
	[quantile=0.5] = 343317
	[quantile=0.9] = 566430
	[quantile=0.99] = 609681
For namespace_work_duration_sum:
	[] = 54237205
For namespace_work_duration_count:
	[] = 148
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:32:32.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7771" for this suite.
Feb 28 16:32:38.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:32:39.643: INFO: namespace gc-7771 deletion completed in 7.586532382s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:32:39.643: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 16:32:39.844: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:32:42.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5509" for this suite.
Feb 28 16:33:34.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:33:35.775: INFO: namespace pods-5509 deletion completed in 53.559525932s
•SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:33:35.775: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 16:33:36.022: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75435340-396e-451e-9230-3b5a105fbcde" in namespace "projected-3189" to be "success or failure"
Feb 28 16:33:36.064: INFO: Pod "downwardapi-volume-75435340-396e-451e-9230-3b5a105fbcde": Phase="Pending", Reason="", readiness=false. Elapsed: 42.207738ms
Feb 28 16:33:38.102: INFO: Pod "downwardapi-volume-75435340-396e-451e-9230-3b5a105fbcde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080179842s
Feb 28 16:33:40.141: INFO: Pod "downwardapi-volume-75435340-396e-451e-9230-3b5a105fbcde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.1183383s
STEP: Saw pod success
Feb 28 16:33:40.141: INFO: Pod "downwardapi-volume-75435340-396e-451e-9230-3b5a105fbcde" satisfied condition "success or failure"
Feb 28 16:33:40.178: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod downwardapi-volume-75435340-396e-451e-9230-3b5a105fbcde container client-container: <nil>
STEP: delete the pod
Feb 28 16:33:40.269: INFO: Waiting for pod downwardapi-volume-75435340-396e-451e-9230-3b5a105fbcde to disappear
Feb 28 16:33:40.306: INFO: Pod downwardapi-volume-75435340-396e-451e-9230-3b5a105fbcde no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:33:40.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3189" for this suite.
Feb 28 16:33:46.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:33:47.908: INFO: namespace projected-3189 deletion completed in 7.563209734s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:33:47.908: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 16:33:48.063: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:33:48.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1817" for this suite.
Feb 28 16:33:54.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:33:55.969: INFO: namespace custom-resource-definition-1817 deletion completed in 7.554521748s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:33:55.969: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-f80fa9e6-3a9f-4e65-b063-35890e67599a
STEP: Creating a pod to test consume configMaps
Feb 28 16:33:56.242: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-00b40ea2-0720-4be3-8bc0-d8fb55110343" in namespace "projected-9266" to be "success or failure"
Feb 28 16:33:56.280: INFO: Pod "pod-projected-configmaps-00b40ea2-0720-4be3-8bc0-d8fb55110343": Phase="Pending", Reason="", readiness=false. Elapsed: 37.256912ms
Feb 28 16:33:58.318: INFO: Pod "pod-projected-configmaps-00b40ea2-0720-4be3-8bc0-d8fb55110343": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.07590788s
STEP: Saw pod success
Feb 28 16:33:58.318: INFO: Pod "pod-projected-configmaps-00b40ea2-0720-4be3-8bc0-d8fb55110343" satisfied condition "success or failure"
Feb 28 16:33:58.357: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-projected-configmaps-00b40ea2-0720-4be3-8bc0-d8fb55110343 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 16:33:58.454: INFO: Waiting for pod pod-projected-configmaps-00b40ea2-0720-4be3-8bc0-d8fb55110343 to disappear
Feb 28 16:33:58.491: INFO: Pod pod-projected-configmaps-00b40ea2-0720-4be3-8bc0-d8fb55110343 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:33:58.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9266" for this suite.
Feb 28 16:34:04.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:34:06.095: INFO: namespace projected-9266 deletion completed in 7.566095623s
•SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:34:06.096: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:34:10.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1346" for this suite.
Feb 28 16:34:16.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:34:18.010: INFO: namespace kubelet-test-1346 deletion completed in 7.56036936s
•SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:34:18.010: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2844
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2844
STEP: Creating statefulset with conflicting port in namespace statefulset-2844
STEP: Waiting until pod test-pod will start running in namespace statefulset-2844
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2844
Feb 28 16:34:22.471: INFO: Observed stateful pod in namespace: statefulset-2844, name: ss-0, uid: 65730fdb-7f82-407f-ace9-0e1e157e1016, status phase: Pending. Waiting for statefulset controller to delete.
Feb 28 16:34:22.705: INFO: Observed stateful pod in namespace: statefulset-2844, name: ss-0, uid: 65730fdb-7f82-407f-ace9-0e1e157e1016, status phase: Failed. Waiting for statefulset controller to delete.
Feb 28 16:34:22.715: INFO: Observed stateful pod in namespace: statefulset-2844, name: ss-0, uid: 65730fdb-7f82-407f-ace9-0e1e157e1016, status phase: Failed. Waiting for statefulset controller to delete.
Feb 28 16:34:22.720: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2844
STEP: Removing pod with conflicting port in namespace statefulset-2844
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2844 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 28 16:34:24.845: INFO: Deleting all statefulset in ns statefulset-2844
Feb 28 16:34:24.884: INFO: Scaling statefulset ss to 0
Feb 28 16:34:45.041: INFO: Waiting for statefulset status.replicas updated to 0
Feb 28 16:34:45.079: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:34:45.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2844" for this suite.
Feb 28 16:34:51.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:34:52.692: INFO: namespace statefulset-2844 deletion completed in 7.454898662s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:34:52.692: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Feb 28 16:34:52.837: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-5007'
Feb 28 16:34:53.285: INFO: stderr: ""
Feb 28 16:34:53.285: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 16:34:53.285: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5007'
Feb 28 16:34:53.527: INFO: stderr: ""
Feb 28 16:34:53.527: INFO: stdout: "update-demo-nautilus-4g7ww update-demo-nautilus-jxbr5 "
Feb 28 16:34:53.527: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-4g7ww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5007'
Feb 28 16:34:53.772: INFO: stderr: ""
Feb 28 16:34:53.772: INFO: stdout: ""
Feb 28 16:34:53.772: INFO: update-demo-nautilus-4g7ww is created but not running
Feb 28 16:34:58.772: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5007'
Feb 28 16:34:59.023: INFO: stderr: ""
Feb 28 16:34:59.023: INFO: stdout: "update-demo-nautilus-4g7ww update-demo-nautilus-jxbr5 "
Feb 28 16:34:59.024: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-4g7ww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5007'
Feb 28 16:34:59.322: INFO: stderr: ""
Feb 28 16:34:59.322: INFO: stdout: "true"
Feb 28 16:34:59.323: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-4g7ww -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5007'
Feb 28 16:34:59.605: INFO: stderr: ""
Feb 28 16:34:59.606: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 16:34:59.606: INFO: validating pod update-demo-nautilus-4g7ww
Feb 28 16:34:59.643: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 16:34:59.644: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 16:34:59.644: INFO: update-demo-nautilus-4g7ww is verified up and running
Feb 28 16:34:59.644: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-jxbr5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5007'
Feb 28 16:34:59.929: INFO: stderr: ""
Feb 28 16:34:59.929: INFO: stdout: "true"
Feb 28 16:34:59.929: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-jxbr5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5007'
Feb 28 16:35:00.171: INFO: stderr: ""
Feb 28 16:35:00.171: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 16:35:00.171: INFO: validating pod update-demo-nautilus-jxbr5
Feb 28 16:35:00.209: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 16:35:00.209: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 16:35:00.209: INFO: update-demo-nautilus-jxbr5 is verified up and running
STEP: rolling-update to new replication controller
Feb 28 16:35:00.211: INFO: scanned /workspace for discovery docs: <nil>
Feb 28 16:35:00.211: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-5007'
Feb 28 16:35:16.175: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 28 16:35:16.175: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 16:35:16.175: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5007'
Feb 28 16:35:16.403: INFO: stderr: ""
Feb 28 16:35:16.403: INFO: stdout: "update-demo-kitten-62d8s update-demo-kitten-cnqtv "
Feb 28 16:35:16.403: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-kitten-62d8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5007'
Feb 28 16:35:16.629: INFO: stderr: ""
Feb 28 16:35:16.629: INFO: stdout: "true"
Feb 28 16:35:16.629: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-kitten-62d8s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5007'
Feb 28 16:35:16.874: INFO: stderr: ""
Feb 28 16:35:16.874: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 28 16:35:16.874: INFO: validating pod update-demo-kitten-62d8s
Feb 28 16:35:16.912: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 28 16:35:16.912: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 28 16:35:16.912: INFO: update-demo-kitten-62d8s is verified up and running
Feb 28 16:35:16.912: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-kitten-cnqtv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5007'
Feb 28 16:35:17.152: INFO: stderr: ""
Feb 28 16:35:17.152: INFO: stdout: "true"
Feb 28 16:35:17.152: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-kitten-cnqtv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5007'
Feb 28 16:35:17.398: INFO: stderr: ""
Feb 28 16:35:17.398: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 28 16:35:17.398: INFO: validating pod update-demo-kitten-cnqtv
Feb 28 16:35:17.436: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 28 16:35:17.436: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 28 16:35:17.436: INFO: update-demo-kitten-cnqtv is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:35:17.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5007" for this suite.
Feb 28 16:35:39.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:35:40.907: INFO: namespace kubectl-5007 deletion completed in 23.435137303s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:35:40.907: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Feb 28 16:35:41.092: INFO: Waiting up to 5m0s for pod "var-expansion-dd54cfa1-55a1-49cf-b175-0c3976502dbe" in namespace "var-expansion-1866" to be "success or failure"
Feb 28 16:35:41.127: INFO: Pod "var-expansion-dd54cfa1-55a1-49cf-b175-0c3976502dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 34.717487ms
Feb 28 16:35:43.162: INFO: Pod "var-expansion-dd54cfa1-55a1-49cf-b175-0c3976502dbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.070103697s
STEP: Saw pod success
Feb 28 16:35:43.162: INFO: Pod "var-expansion-dd54cfa1-55a1-49cf-b175-0c3976502dbe" satisfied condition "success or failure"
Feb 28 16:35:43.197: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod var-expansion-dd54cfa1-55a1-49cf-b175-0c3976502dbe container dapi-container: <nil>
STEP: delete the pod
Feb 28 16:35:43.281: INFO: Waiting for pod var-expansion-dd54cfa1-55a1-49cf-b175-0c3976502dbe to disappear
Feb 28 16:35:43.315: INFO: Pod var-expansion-dd54cfa1-55a1-49cf-b175-0c3976502dbe no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:35:43.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1866" for this suite.
Feb 28 16:35:49.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:35:50.850: INFO: namespace var-expansion-1866 deletion completed in 7.499867977s
•SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:35:50.851: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 28 16:35:53.213: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:35:53.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3471" for this suite.
Feb 28 16:35:59.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:36:00.782: INFO: namespace container-runtime-3471 deletion completed in 7.452210406s
•SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:36:00.783: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 28 16:36:01.009: INFO: Waiting up to 5m0s for pod "pod-591f4b48-cfb7-4b82-ba7c-26196720da5e" in namespace "emptydir-630" to be "success or failure"
Feb 28 16:36:01.044: INFO: Pod "pod-591f4b48-cfb7-4b82-ba7c-26196720da5e": Phase="Pending", Reason="", readiness=false. Elapsed: 34.656536ms
Feb 28 16:36:03.079: INFO: Pod "pod-591f4b48-cfb7-4b82-ba7c-26196720da5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069783148s
Feb 28 16:36:05.115: INFO: Pod "pod-591f4b48-cfb7-4b82-ba7c-26196720da5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.105623819s
STEP: Saw pod success
Feb 28 16:36:05.115: INFO: Pod "pod-591f4b48-cfb7-4b82-ba7c-26196720da5e" satisfied condition "success or failure"
Feb 28 16:36:05.150: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-591f4b48-cfb7-4b82-ba7c-26196720da5e container test-container: <nil>
STEP: delete the pod
Feb 28 16:36:05.235: INFO: Waiting for pod pod-591f4b48-cfb7-4b82-ba7c-26196720da5e to disappear
Feb 28 16:36:05.269: INFO: Pod pod-591f4b48-cfb7-4b82-ba7c-26196720da5e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:36:05.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-630" for this suite.
Feb 28 16:36:11.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:36:12.807: INFO: namespace emptydir-630 deletion completed in 7.502107447s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:36:12.807: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-a858fd22-0784-4dd1-9053-121f7ce68604
STEP: Creating a pod to test consume secrets
Feb 28 16:36:13.032: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f3ea7747-eccc-44f1-ab80-54a33c512b43" in namespace "projected-7235" to be "success or failure"
Feb 28 16:36:13.067: INFO: Pod "pod-projected-secrets-f3ea7747-eccc-44f1-ab80-54a33c512b43": Phase="Pending", Reason="", readiness=false. Elapsed: 34.246154ms
Feb 28 16:36:15.102: INFO: Pod "pod-projected-secrets-f3ea7747-eccc-44f1-ab80-54a33c512b43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069579084s
Feb 28 16:36:17.137: INFO: Pod "pod-projected-secrets-f3ea7747-eccc-44f1-ab80-54a33c512b43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.104515415s
STEP: Saw pod success
Feb 28 16:36:17.137: INFO: Pod "pod-projected-secrets-f3ea7747-eccc-44f1-ab80-54a33c512b43" satisfied condition "success or failure"
Feb 28 16:36:17.172: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-projected-secrets-f3ea7747-eccc-44f1-ab80-54a33c512b43 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 16:36:17.261: INFO: Waiting for pod pod-projected-secrets-f3ea7747-eccc-44f1-ab80-54a33c512b43 to disappear
Feb 28 16:36:17.295: INFO: Pod pod-projected-secrets-f3ea7747-eccc-44f1-ab80-54a33c512b43 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:36:17.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7235" for this suite.
Feb 28 16:36:23.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:36:24.774: INFO: namespace projected-7235 deletion completed in 7.442927332s
•SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:36:24.774: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 28 16:36:25.116: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3249,SelfLink:/api/v1/namespaces/watch-3249/configmaps/e2e-watch-test-watch-closed,UID:4006036e-0524-4215-810f-06d4f8e47091,ResourceVersion:11569,Generation:0,CreationTimestamp:2020-02-28 16:36:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 16:36:25.116: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3249,SelfLink:/api/v1/namespaces/watch-3249/configmaps/e2e-watch-test-watch-closed,UID:4006036e-0524-4215-810f-06d4f8e47091,ResourceVersion:11570,Generation:0,CreationTimestamp:2020-02-28 16:36:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 28 16:36:25.265: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3249,SelfLink:/api/v1/namespaces/watch-3249/configmaps/e2e-watch-test-watch-closed,UID:4006036e-0524-4215-810f-06d4f8e47091,ResourceVersion:11571,Generation:0,CreationTimestamp:2020-02-28 16:36:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 16:36:25.265: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3249,SelfLink:/api/v1/namespaces/watch-3249/configmaps/e2e-watch-test-watch-closed,UID:4006036e-0524-4215-810f-06d4f8e47091,ResourceVersion:11572,Generation:0,CreationTimestamp:2020-02-28 16:36:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:36:25.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3249" for this suite.
Feb 28 16:36:31.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:36:32.757: INFO: namespace watch-3249 deletion completed in 7.45741182s
•SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:36:32.758: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Feb 28 16:36:32.950: INFO: Waiting up to 5m0s for pod "var-expansion-1fa4a436-2c61-4120-8a34-03c1888c2b66" in namespace "var-expansion-2877" to be "success or failure"
Feb 28 16:36:32.985: INFO: Pod "var-expansion-1fa4a436-2c61-4120-8a34-03c1888c2b66": Phase="Pending", Reason="", readiness=false. Elapsed: 34.370125ms
Feb 28 16:36:35.020: INFO: Pod "var-expansion-1fa4a436-2c61-4120-8a34-03c1888c2b66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.069461658s
STEP: Saw pod success
Feb 28 16:36:35.020: INFO: Pod "var-expansion-1fa4a436-2c61-4120-8a34-03c1888c2b66" satisfied condition "success or failure"
Feb 28 16:36:35.056: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod var-expansion-1fa4a436-2c61-4120-8a34-03c1888c2b66 container dapi-container: <nil>
STEP: delete the pod
Feb 28 16:36:35.152: INFO: Waiting for pod var-expansion-1fa4a436-2c61-4120-8a34-03c1888c2b66 to disappear
Feb 28 16:36:35.187: INFO: Pod var-expansion-1fa4a436-2c61-4120-8a34-03c1888c2b66 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:36:35.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2877" for this suite.
Feb 28 16:36:41.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:36:42.659: INFO: namespace var-expansion-2877 deletion completed in 7.436422828s
•SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:36:42.659: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-7827/secret-test-2ac67809-b877-4a81-8cdd-9b2f3f362385
STEP: Creating a pod to test consume secrets
Feb 28 16:36:42.879: INFO: Waiting up to 5m0s for pod "pod-configmaps-a44fa5cb-5571-44b2-915e-b717c90ff47f" in namespace "secrets-7827" to be "success or failure"
Feb 28 16:36:42.913: INFO: Pod "pod-configmaps-a44fa5cb-5571-44b2-915e-b717c90ff47f": Phase="Pending", Reason="", readiness=false. Elapsed: 34.110072ms
Feb 28 16:36:44.948: INFO: Pod "pod-configmaps-a44fa5cb-5571-44b2-915e-b717c90ff47f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.06910326s
STEP: Saw pod success
Feb 28 16:36:44.948: INFO: Pod "pod-configmaps-a44fa5cb-5571-44b2-915e-b717c90ff47f" satisfied condition "success or failure"
Feb 28 16:36:44.983: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-configmaps-a44fa5cb-5571-44b2-915e-b717c90ff47f container env-test: <nil>
STEP: delete the pod
Feb 28 16:36:45.072: INFO: Waiting for pod pod-configmaps-a44fa5cb-5571-44b2-915e-b717c90ff47f to disappear
Feb 28 16:36:45.107: INFO: Pod pod-configmaps-a44fa5cb-5571-44b2-915e-b717c90ff47f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:36:45.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7827" for this suite.
Feb 28 16:36:51.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:36:52.588: INFO: namespace secrets-7827 deletion completed in 7.442998277s
•SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:36:52.588: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Feb 28 16:36:53.742: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 972
	[quantile=0.9] = 106583
	[quantile=0.99] = 866385
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 94064
	[quantile=0.9] = 617880
	[quantile=0.99] = 874649
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = 5
	[quantile=0.9] = 7
	[quantile=0.99] = 7
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = 534984
	[quantile=0.9] = 597880
	[quantile=0.99] = 597880
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 5
	[quantile=0.9] = 7
	[quantile=0.99] = 24
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 13
	[quantile=0.9] = 27
	[quantile=0.99] = 72
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 12
	[quantile=0.9] = 15
	[quantile=0.99] = 46
For namespace_queue_latency_sum:
	[] = 2337
For namespace_queue_latency_count:
	[] = 177
For namespace_retries:
	[] = 178
For namespace_work_duration:
	[quantile=0.5] = 284681
	[quantile=0.9] = 486902
	[quantile=0.99] = 624208
For namespace_work_duration_sum:
	[] = 63168336
For namespace_work_duration_count:
	[] = 177
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:36:53.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6966" for this suite.
Feb 28 16:36:59.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:37:01.248: INFO: namespace gc-6966 deletion completed in 7.470766363s
•SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:37:01.248: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-33aea42b-2f78-48dc-b934-56e06f2f2b52
STEP: Creating a pod to test consume secrets
Feb 28 16:37:01.506: INFO: Waiting up to 5m0s for pod "pod-secrets-53863cf5-dde2-4600-9338-618160c215d5" in namespace "secrets-4256" to be "success or failure"
Feb 28 16:37:01.549: INFO: Pod "pod-secrets-53863cf5-dde2-4600-9338-618160c215d5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.201231ms
Feb 28 16:37:03.584: INFO: Pod "pod-secrets-53863cf5-dde2-4600-9338-618160c215d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077634374s
STEP: Saw pod success
Feb 28 16:37:03.584: INFO: Pod "pod-secrets-53863cf5-dde2-4600-9338-618160c215d5" satisfied condition "success or failure"
Feb 28 16:37:03.619: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-secrets-53863cf5-dde2-4600-9338-618160c215d5 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 16:37:03.716: INFO: Waiting for pod pod-secrets-53863cf5-dde2-4600-9338-618160c215d5 to disappear
Feb 28 16:37:03.751: INFO: Pod pod-secrets-53863cf5-dde2-4600-9338-618160c215d5 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:37:03.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4256" for this suite.
Feb 28 16:37:09.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:37:11.339: INFO: namespace secrets-4256 deletion completed in 7.551004183s
•SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:37:11.339: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 28 16:37:13.664: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:37:13.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9360" for this suite.
Feb 28 16:37:19.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:37:21.232: INFO: namespace container-runtime-9360 deletion completed in 7.450980529s
•SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:37:21.232: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 28 16:37:21.551: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-101,SelfLink:/api/v1/namespaces/watch-101/configmaps/e2e-watch-test-configmap-a,UID:fbb49453-3d3d-40fe-ae18-0c93d2265dde,ResourceVersion:11789,Generation:0,CreationTimestamp:2020-02-28 16:37:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 16:37:21.551: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-101,SelfLink:/api/v1/namespaces/watch-101/configmaps/e2e-watch-test-configmap-a,UID:fbb49453-3d3d-40fe-ae18-0c93d2265dde,ResourceVersion:11789,Generation:0,CreationTimestamp:2020-02-28 16:37:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 28 16:37:31.625: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-101,SelfLink:/api/v1/namespaces/watch-101/configmaps/e2e-watch-test-configmap-a,UID:fbb49453-3d3d-40fe-ae18-0c93d2265dde,ResourceVersion:11812,Generation:0,CreationTimestamp:2020-02-28 16:37:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 28 16:37:31.625: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-101,SelfLink:/api/v1/namespaces/watch-101/configmaps/e2e-watch-test-configmap-a,UID:fbb49453-3d3d-40fe-ae18-0c93d2265dde,ResourceVersion:11812,Generation:0,CreationTimestamp:2020-02-28 16:37:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 28 16:37:41.697: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-101,SelfLink:/api/v1/namespaces/watch-101/configmaps/e2e-watch-test-configmap-a,UID:fbb49453-3d3d-40fe-ae18-0c93d2265dde,ResourceVersion:11833,Generation:0,CreationTimestamp:2020-02-28 16:37:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 16:37:41.697: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-101,SelfLink:/api/v1/namespaces/watch-101/configmaps/e2e-watch-test-configmap-a,UID:fbb49453-3d3d-40fe-ae18-0c93d2265dde,ResourceVersion:11833,Generation:0,CreationTimestamp:2020-02-28 16:37:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 28 16:37:51.736: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-101,SelfLink:/api/v1/namespaces/watch-101/configmaps/e2e-watch-test-configmap-a,UID:fbb49453-3d3d-40fe-ae18-0c93d2265dde,ResourceVersion:11855,Generation:0,CreationTimestamp:2020-02-28 16:37:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 16:37:51.736: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-101,SelfLink:/api/v1/namespaces/watch-101/configmaps/e2e-watch-test-configmap-a,UID:fbb49453-3d3d-40fe-ae18-0c93d2265dde,ResourceVersion:11855,Generation:0,CreationTimestamp:2020-02-28 16:37:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 28 16:38:01.776: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-101,SelfLink:/api/v1/namespaces/watch-101/configmaps/e2e-watch-test-configmap-b,UID:793c872d-f377-4398-9749-5c54576a3af5,ResourceVersion:11879,Generation:0,CreationTimestamp:2020-02-28 16:38:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 16:38:01.777: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-101,SelfLink:/api/v1/namespaces/watch-101/configmaps/e2e-watch-test-configmap-b,UID:793c872d-f377-4398-9749-5c54576a3af5,ResourceVersion:11879,Generation:0,CreationTimestamp:2020-02-28 16:38:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 28 16:38:11.823: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-101,SelfLink:/api/v1/namespaces/watch-101/configmaps/e2e-watch-test-configmap-b,UID:793c872d-f377-4398-9749-5c54576a3af5,ResourceVersion:11901,Generation:0,CreationTimestamp:2020-02-28 16:38:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 28 16:38:11.823: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-101,SelfLink:/api/v1/namespaces/watch-101/configmaps/e2e-watch-test-configmap-b,UID:793c872d-f377-4398-9749-5c54576a3af5,ResourceVersion:11901,Generation:0,CreationTimestamp:2020-02-28 16:38:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:38:21.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-101" for this suite.
Feb 28 16:38:27.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:38:29.295: INFO: namespace watch-101 deletion completed in 7.435006627s
•SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:38:29.295: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-1c4d53c3-2ba5-4035-b5d4-c876a783e777
STEP: Creating a pod to test consume secrets
Feb 28 16:38:29.558: INFO: Waiting up to 5m0s for pod "pod-secrets-7231148a-d02d-48c6-b466-0992aba4c880" in namespace "secrets-8534" to be "success or failure"
Feb 28 16:38:29.598: INFO: Pod "pod-secrets-7231148a-d02d-48c6-b466-0992aba4c880": Phase="Pending", Reason="", readiness=false. Elapsed: 40.372771ms
Feb 28 16:38:31.633: INFO: Pod "pod-secrets-7231148a-d02d-48c6-b466-0992aba4c880": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075452002s
Feb 28 16:38:33.668: INFO: Pod "pod-secrets-7231148a-d02d-48c6-b466-0992aba4c880": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.110772247s
STEP: Saw pod success
Feb 28 16:38:33.669: INFO: Pod "pod-secrets-7231148a-d02d-48c6-b466-0992aba4c880" satisfied condition "success or failure"
Feb 28 16:38:33.703: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-secrets-7231148a-d02d-48c6-b466-0992aba4c880 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 16:38:33.787: INFO: Waiting for pod pod-secrets-7231148a-d02d-48c6-b466-0992aba4c880 to disappear
Feb 28 16:38:33.822: INFO: Pod pod-secrets-7231148a-d02d-48c6-b466-0992aba4c880 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:38:33.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8534" for this suite.
Feb 28 16:38:39.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:38:41.438: INFO: namespace secrets-8534 deletion completed in 7.580207782s
•SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:38:41.439: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 16:38:41.625: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b79b8f9-bd10-451e-82d1-32e2c5192898" in namespace "projected-8747" to be "success or failure"
Feb 28 16:38:41.659: INFO: Pod "downwardapi-volume-6b79b8f9-bd10-451e-82d1-32e2c5192898": Phase="Pending", Reason="", readiness=false. Elapsed: 34.371861ms
Feb 28 16:38:43.695: INFO: Pod "downwardapi-volume-6b79b8f9-bd10-451e-82d1-32e2c5192898": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06991664s
Feb 28 16:38:45.730: INFO: Pod "downwardapi-volume-6b79b8f9-bd10-451e-82d1-32e2c5192898": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.10545042s
STEP: Saw pod success
Feb 28 16:38:45.730: INFO: Pod "downwardapi-volume-6b79b8f9-bd10-451e-82d1-32e2c5192898" satisfied condition "success or failure"
Feb 28 16:38:45.765: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod downwardapi-volume-6b79b8f9-bd10-451e-82d1-32e2c5192898 container client-container: <nil>
STEP: delete the pod
Feb 28 16:38:45.849: INFO: Waiting for pod downwardapi-volume-6b79b8f9-bd10-451e-82d1-32e2c5192898 to disappear
Feb 28 16:38:45.884: INFO: Pod downwardapi-volume-6b79b8f9-bd10-451e-82d1-32e2c5192898 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:38:45.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8747" for this suite.
Feb 28 16:38:52.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:38:53.355: INFO: namespace projected-8747 deletion completed in 7.435919186s
•SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:38:53.355: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-4638
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4638 to expose endpoints map[]
Feb 28 16:38:53.577: INFO: successfully validated that service multi-endpoint-test in namespace services-4638 exposes endpoints map[] (35.624409ms elapsed)
STEP: Creating pod pod1 in namespace services-4638
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4638 to expose endpoints map[pod1:[100]]
Feb 28 16:38:55.840: INFO: successfully validated that service multi-endpoint-test in namespace services-4638 exposes endpoints map[pod1:[100]] (2.22146018s elapsed)
STEP: Creating pod pod2 in namespace services-4638
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4638 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 28 16:38:58.195: INFO: successfully validated that service multi-endpoint-test in namespace services-4638 exposes endpoints map[pod1:[100] pod2:[101]] (2.315439808s elapsed)
STEP: Deleting pod pod1 in namespace services-4638
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4638 to expose endpoints map[pod2:[101]]
Feb 28 16:38:58.303: INFO: successfully validated that service multi-endpoint-test in namespace services-4638 exposes endpoints map[pod2:[101]] (69.20228ms elapsed)
STEP: Deleting pod pod2 in namespace services-4638
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4638 to expose endpoints map[]
Feb 28 16:38:58.377: INFO: successfully validated that service multi-endpoint-test in namespace services-4638 exposes endpoints map[] (34.553505ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:38:58.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4638" for this suite.
Feb 28 16:39:20.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:39:21.906: INFO: namespace services-4638 deletion completed in 23.437094146s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:39:21.906: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 16:39:22.129: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b80369c-f966-4bd3-ab4a-47c802d9c298" in namespace "downward-api-3450" to be "success or failure"
Feb 28 16:39:22.164: INFO: Pod "downwardapi-volume-7b80369c-f966-4bd3-ab4a-47c802d9c298": Phase="Pending", Reason="", readiness=false. Elapsed: 34.797356ms
Feb 28 16:39:24.199: INFO: Pod "downwardapi-volume-7b80369c-f966-4bd3-ab4a-47c802d9c298": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.070073375s
STEP: Saw pod success
Feb 28 16:39:24.199: INFO: Pod "downwardapi-volume-7b80369c-f966-4bd3-ab4a-47c802d9c298" satisfied condition "success or failure"
Feb 28 16:39:24.234: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod downwardapi-volume-7b80369c-f966-4bd3-ab4a-47c802d9c298 container client-container: <nil>
STEP: delete the pod
Feb 28 16:39:24.321: INFO: Waiting for pod downwardapi-volume-7b80369c-f966-4bd3-ab4a-47c802d9c298 to disappear
Feb 28 16:39:24.358: INFO: Pod downwardapi-volume-7b80369c-f966-4bd3-ab4a-47c802d9c298 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:39:24.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3450" for this suite.
Feb 28 16:39:30.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:39:31.891: INFO: namespace downward-api-3450 deletion completed in 7.4966607s
•SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:39:31.892: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Feb 28 16:39:32.079: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 28 16:39:32.151: INFO: Waiting for terminating namespaces to be deleted...
Feb 28 16:39:32.185: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-q1l7 before test
Feb 28 16:39:32.226: INFO: kube-proxy-bootstrap-e2e-minion-group-q1l7 from kube-system started at 2020-02-28 15:42:18 +0000 UTC (1 container statuses recorded)
Feb 28 16:39:32.226: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 16:39:32.226: INFO: fluentd-gcp-v3.2.0-9q7z2 from kube-system started at 2020-02-28 15:42:58 +0000 UTC (2 container statuses recorded)
Feb 28 16:39:32.226: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 28 16:39:32.226: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:39:32.226: INFO: metadata-proxy-v0.1-kbkll from kube-system started at 2020-02-28 15:42:19 +0000 UTC (2 container statuses recorded)
Feb 28 16:39:32.226: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 28 16:39:32.226: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:39:32.226: INFO: metrics-server-v0.3.3-7947ffd59d-rfjr4 from kube-system started at 2020-02-28 15:42:40 +0000 UTC (2 container statuses recorded)
Feb 28 16:39:32.226: INFO: 	Container metrics-server ready: true, restart count 0
Feb 28 16:39:32.226: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 28 16:39:32.226: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-s43f before test
Feb 28 16:39:32.272: INFO: kube-dns-autoscaler-6d6bc99fd8-t6kwd from kube-system started at 2020-02-28 15:42:18 +0000 UTC (1 container statuses recorded)
Feb 28 16:39:32.272: INFO: 	Container autoscaler ready: true, restart count 0
Feb 28 16:39:32.272: INFO: metadata-proxy-v0.1-tnmbw from kube-system started at 2020-02-28 15:42:18 +0000 UTC (2 container statuses recorded)
Feb 28 16:39:32.272: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 28 16:39:32.272: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:39:32.272: INFO: kube-proxy-bootstrap-e2e-minion-group-s43f from kube-system started at 2020-02-28 15:42:17 +0000 UTC (1 container statuses recorded)
Feb 28 16:39:32.272: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 16:39:32.272: INFO: fluentd-gcp-scaler-6848d689fb-8slw6 from kube-system started at 2020-02-28 15:42:18 +0000 UTC (1 container statuses recorded)
Feb 28 16:39:32.272: INFO: 	Container fluentd-gcp-scaler ready: true, restart count 0
Feb 28 16:39:32.272: INFO: fluentd-gcp-v3.2.0-k64bw from kube-system started at 2020-02-28 15:42:35 +0000 UTC (2 container statuses recorded)
Feb 28 16:39:32.272: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 28 16:39:32.272: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:39:32.272: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-t863 before test
Feb 28 16:39:32.320: INFO: kubernetes-dashboard-66b96fb8d7-cr7kc from kube-system started at 2020-02-28 15:42:16 +0000 UTC (1 container statuses recorded)
Feb 28 16:39:32.320: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 28 16:39:32.321: INFO: coredns-557dcdc9f5-9hvg8 from kube-system started at 2020-02-28 15:42:29 +0000 UTC (1 container statuses recorded)
Feb 28 16:39:32.321: INFO: 	Container coredns ready: true, restart count 0
Feb 28 16:39:32.321: INFO: l7-default-backend-84c9fcfbb-jzxhq from kube-system started at 2020-02-28 15:42:29 +0000 UTC (1 container statuses recorded)
Feb 28 16:39:32.321: INFO: 	Container default-http-backend ready: true, restart count 0
Feb 28 16:39:32.321: INFO: coredns-557dcdc9f5-dg8nb from kube-system started at 2020-02-28 15:42:29 +0000 UTC (1 container statuses recorded)
Feb 28 16:39:32.321: INFO: 	Container coredns ready: true, restart count 0
Feb 28 16:39:32.321: INFO: event-exporter-v0.2.5-5fd6f794f7-5kbmp from kube-system started at 2020-02-28 15:42:30 +0000 UTC (2 container statuses recorded)
Feb 28 16:39:32.321: INFO: 	Container event-exporter ready: true, restart count 0
Feb 28 16:39:32.321: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:39:32.321: INFO: heapster-v1.6.0-beta.1-6b89b777b5-7mnqk from kube-system started at 2020-02-28 15:42:31 +0000 UTC (2 container statuses recorded)
Feb 28 16:39:32.321: INFO: 	Container heapster ready: true, restart count 0
Feb 28 16:39:32.321: INFO: 	Container heapster-nanny ready: true, restart count 0
Feb 28 16:39:32.321: INFO: kube-proxy-bootstrap-e2e-minion-group-t863 from kube-system started at 2020-02-28 15:42:16 +0000 UTC (1 container statuses recorded)
Feb 28 16:39:32.321: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 28 16:39:32.321: INFO: fluentd-gcp-v3.2.0-j8frl from kube-system started at 2020-02-28 15:42:36 +0000 UTC (2 container statuses recorded)
Feb 28 16:39:32.321: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 28 16:39:32.321: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 28 16:39:32.321: INFO: metadata-proxy-v0.1-ft86w from kube-system started at 2020-02-28 15:42:16 +0000 UTC (2 container statuses recorded)
Feb 28 16:39:32.321: INFO: 	Container metadata-proxy ready: true, restart count 0
Feb 28 16:39:32.321: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f79e6a102bff80], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) were unschedulable, 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:39:33.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3705" for this suite.
Feb 28 16:39:39.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:39:41.090: INFO: namespace sched-pred-3705 deletion completed in 7.543592457s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:39:41.091: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-3319f33d-201f-48b3-a60d-1c3baba282e8 in namespace container-probe-8185
Feb 28 16:39:43.354: INFO: Started pod test-webserver-3319f33d-201f-48b3-a60d-1c3baba282e8 in namespace container-probe-8185
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 16:39:43.389: INFO: Initial restart count of pod test-webserver-3319f33d-201f-48b3-a60d-1c3baba282e8 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:43:43.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8185" for this suite.
Feb 28 16:43:49.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:43:51.325: INFO: namespace container-probe-8185 deletion completed in 7.436829737s
•SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:43:51.325: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Feb 28 16:43:51.511: INFO: Asynchronously running '/workspace/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:43:51.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9643" for this suite.
Feb 28 16:43:57.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:43:59.245: INFO: namespace kubectl-9643 deletion completed in 7.470454022s
•SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:43:59.245: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 16:43:59.468: INFO: (0) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 38.941895ms)
Feb 28 16:43:59.504: INFO: (1) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 36.123999ms)
Feb 28 16:43:59.542: INFO: (2) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 38.348999ms)
Feb 28 16:43:59.579: INFO: (3) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 36.591966ms)
Feb 28 16:43:59.620: INFO: (4) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 40.97331ms)
Feb 28 16:43:59.656: INFO: (5) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 36.035663ms)
Feb 28 16:43:59.693: INFO: (6) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 36.610054ms)
Feb 28 16:43:59.731: INFO: (7) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 38.034019ms)
Feb 28 16:43:59.767: INFO: (8) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 36.049428ms)
Feb 28 16:43:59.806: INFO: (9) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 39.423555ms)
Feb 28 16:43:59.844: INFO: (10) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 37.166066ms)
Feb 28 16:43:59.880: INFO: (11) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 36.449716ms)
Feb 28 16:43:59.916: INFO: (12) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 36.229186ms)
Feb 28 16:43:59.953: INFO: (13) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 36.348749ms)
Feb 28 16:43:59.989: INFO: (14) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 36.150529ms)
Feb 28 16:44:00.025: INFO: (15) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 36.438841ms)
Feb 28 16:44:00.062: INFO: (16) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 36.44769ms)
Feb 28 16:44:00.098: INFO: (17) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 36.114043ms)
Feb 28 16:44:00.135: INFO: (18) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 36.60173ms)
Feb 28 16:44:00.171: INFO: (19) /api/v1/nodes/bootstrap-e2e-minion-group-q1l7:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 36.345255ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:44:00.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5491" for this suite.
Feb 28 16:44:06.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:44:07.648: INFO: namespace proxy-5491 deletion completed in 7.441175276s
•SSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:44:07.648: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 16:44:07.803: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:44:12.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9813" for this suite.
Feb 28 16:44:50.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:44:51.900: INFO: namespace pods-9813 deletion completed in 39.638994535s
•SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:44:51.900: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:44:54.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2029" for this suite.
Feb 28 16:45:16.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:45:17.842: INFO: namespace replication-controller-2029 deletion completed in 23.492053302s
•SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:45:17.843: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 16:45:17.994: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4961'
Feb 28 16:45:18.761: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 28 16:45:18.762: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Feb 28 16:45:18.798: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete jobs e2e-test-nginx-job --namespace=kubectl-4961'
Feb 28 16:45:19.198: INFO: stderr: ""
Feb 28 16:45:19.198: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:45:19.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4961" for this suite.
Feb 28 16:45:41.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:45:42.730: INFO: namespace kubectl-4961 deletion completed in 23.49477135s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:45:42.731: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 28 16:45:42.970: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:45:43.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6898" for this suite.
Feb 28 16:45:49.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:45:50.582: INFO: namespace replication-controller-6898 deletion completed in 7.454874214s
•S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:45:50.582: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1456
[It] should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 28 16:45:50.767: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7594'
Feb 28 16:45:51.014: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 28 16:45:51.014: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 28 16:45:51.089: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-n8nhf]
Feb 28 16:45:51.089: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-n8nhf" in namespace "kubectl-7594" to be "running and ready"
Feb 28 16:45:51.124: INFO: Pod "e2e-test-nginx-rc-n8nhf": Phase="Pending", Reason="", readiness=false. Elapsed: 34.842727ms
Feb 28 16:45:53.159: INFO: Pod "e2e-test-nginx-rc-n8nhf": Phase="Running", Reason="", readiness=true. Elapsed: 2.069867876s
Feb 28 16:45:53.159: INFO: Pod "e2e-test-nginx-rc-n8nhf" satisfied condition "running and ready"
Feb 28 16:45:53.159: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-n8nhf]
Feb 28 16:45:53.159: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config logs rc/e2e-test-nginx-rc --namespace=kubectl-7594'
Feb 28 16:45:53.480: INFO: stderr: ""
Feb 28 16:45:53.480: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1461
Feb 28 16:45:53.480: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete rc e2e-test-nginx-rc --namespace=kubectl-7594'
Feb 28 16:45:53.759: INFO: stderr: ""
Feb 28 16:45:53.759: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:45:53.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7594" for this suite.
Feb 28 16:46:15.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:46:17.237: INFO: namespace kubectl-7594 deletion completed in 23.442804325s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:46:17.237: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Feb 28 16:46:17.399: INFO: namespace kubectl-4918
Feb 28 16:46:17.399: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-4918'
Feb 28 16:46:17.993: INFO: stderr: ""
Feb 28 16:46:17.993: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 28 16:46:19.028: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 16:46:19.028: INFO: Found 0 / 1
Feb 28 16:46:20.028: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 16:46:20.028: INFO: Found 0 / 1
Feb 28 16:46:21.028: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 16:46:21.028: INFO: Found 1 / 1
Feb 28 16:46:21.028: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 28 16:46:21.063: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 16:46:21.063: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 28 16:46:21.063: INFO: wait on redis-master startup in kubectl-4918 
Feb 28 16:46:21.063: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config logs redis-master-gxw7p redis-master --namespace=kubectl-4918'
Feb 28 16:46:21.348: INFO: stderr: ""
Feb 28 16:46:21.348: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 28 Feb 16:46:19.987 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Feb 16:46:19.987 # Server started, Redis version 3.2.12\n1:M 28 Feb 16:46:19.987 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Feb 16:46:19.987 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 28 16:46:21.349: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-4918'
Feb 28 16:46:21.645: INFO: stderr: ""
Feb 28 16:46:21.645: INFO: stdout: "service/rm2 exposed\n"
Feb 28 16:46:21.681: INFO: Service rm2 in namespace kubectl-4918 found.
STEP: exposing service
Feb 28 16:46:23.751: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-4918'
Feb 28 16:46:24.081: INFO: stderr: ""
Feb 28 16:46:24.081: INFO: stdout: "service/rm3 exposed\n"
Feb 28 16:46:24.116: INFO: Service rm3 in namespace kubectl-4918 found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:46:26.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4918" for this suite.
Feb 28 16:46:48.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:46:49.695: INFO: namespace kubectl-4918 deletion completed in 23.472514728s
•SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:46:49.695: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 28 16:46:56.219: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 16:46:56.254: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 16:46:58.255: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 16:46:58.289: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 16:47:00.255: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 16:47:00.289: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 16:47:02.255: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 16:47:02.289: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 16:47:04.255: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 16:47:04.289: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 16:47:06.255: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 16:47:06.290: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 16:47:08.255: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 16:47:08.289: INFO: Pod pod-with-poststart-http-hook still exists
Feb 28 16:47:10.254: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 28 16:47:10.290: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:47:10.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8740" for this suite.
Feb 28 16:47:32.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:47:33.763: INFO: namespace container-lifecycle-hook-8740 deletion completed in 23.438121136s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:47:33.764: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 28 16:47:38.138: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-eac95317-434d-4f46-b043-9e1d114bdce2,GenerateName:,Namespace:events-794,SelfLink:/api/v1/namespaces/events-794/pods/send-events-eac95317-434d-4f46-b043-9e1d114bdce2,UID:88de53d7-dd51-4bd8-ba2b-85f5480cab77,ResourceVersion:13398,Generation:0,CreationTimestamp:2020-02-28 16:47:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 950705218,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-smr5f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-smr5f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-smr5f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0000da500} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0000da540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:47:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:47:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:47:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 16:47:33 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.2.85,StartTime:2020-02-28 16:47:33 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2020-02-28 16:47:36 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://1492a72b8150662fc1ab69b1ebeaea635518d7eb48954a882afbfa68c0d3c720}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 28 16:47:40.174: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 28 16:47:42.210: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:47:42.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-794" for this suite.
Feb 28 16:48:20.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:48:21.745: INFO: namespace events-794 deletion completed in 39.459414594s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:48:21.745: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:48:26.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3396" for this suite.
Feb 28 16:49:06.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:49:07.566: INFO: namespace kubelet-test-3396 deletion completed in 41.44002381s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:49:07.567: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-8b1a354a-3b1a-4cf2-83cd-0b0d9d2b2db0
STEP: Creating a pod to test consume secrets
Feb 28 16:49:07.785: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5e3ca7e6-be9b-4483-9309-64c44cecb84f" in namespace "projected-1369" to be "success or failure"
Feb 28 16:49:07.825: INFO: Pod "pod-projected-secrets-5e3ca7e6-be9b-4483-9309-64c44cecb84f": Phase="Pending", Reason="", readiness=false. Elapsed: 39.589795ms
Feb 28 16:49:09.860: INFO: Pod "pod-projected-secrets-5e3ca7e6-be9b-4483-9309-64c44cecb84f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074516769s
Feb 28 16:49:11.896: INFO: Pod "pod-projected-secrets-5e3ca7e6-be9b-4483-9309-64c44cecb84f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.110199027s
STEP: Saw pod success
Feb 28 16:49:11.896: INFO: Pod "pod-projected-secrets-5e3ca7e6-be9b-4483-9309-64c44cecb84f" satisfied condition "success or failure"
Feb 28 16:49:11.930: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-projected-secrets-5e3ca7e6-be9b-4483-9309-64c44cecb84f container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 16:49:12.020: INFO: Waiting for pod pod-projected-secrets-5e3ca7e6-be9b-4483-9309-64c44cecb84f to disappear
Feb 28 16:49:12.055: INFO: Pod pod-projected-secrets-5e3ca7e6-be9b-4483-9309-64c44cecb84f no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:49:12.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1369" for this suite.
Feb 28 16:49:18.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:49:19.536: INFO: namespace projected-1369 deletion completed in 7.44178201s
•SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:49:19.536: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 16:49:19.880: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"cdc5ad70-fb14-4992-87f5-40e7b3d8ab90", Controller:(*bool)(0xc000bc865e), BlockOwnerDeletion:(*bool)(0xc000bc865f)}}
Feb 28 16:49:19.919: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8750e1f4-221e-43d8-9ac3-94498885d1c3", Controller:(*bool)(0xc0007d3796), BlockOwnerDeletion:(*bool)(0xc0007d3797)}}
Feb 28 16:49:19.958: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"54d67365-cea2-4b88-80ac-73203a16f31a", Controller:(*bool)(0xc002e404ce), BlockOwnerDeletion:(*bool)(0xc002e404cf)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:49:25.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8279" for this suite.
Feb 28 16:49:31.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:49:32.518: INFO: namespace gc-8279 deletion completed in 7.447674294s
•SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:49:32.518: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 28 16:49:32.713: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:49:35.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4886" for this suite.
Feb 28 16:49:41.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:49:43.167: INFO: namespace init-container-4886 deletion completed in 7.443594094s
•SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:49:43.168: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 28 16:49:43.395: INFO: Waiting up to 5m0s for pod "pod-ee2b7bfd-094f-4465-bfff-cf8a2993cb83" in namespace "emptydir-4432" to be "success or failure"
Feb 28 16:49:43.432: INFO: Pod "pod-ee2b7bfd-094f-4465-bfff-cf8a2993cb83": Phase="Pending", Reason="", readiness=false. Elapsed: 37.259055ms
Feb 28 16:49:45.467: INFO: Pod "pod-ee2b7bfd-094f-4465-bfff-cf8a2993cb83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.072058089s
STEP: Saw pod success
Feb 28 16:49:45.467: INFO: Pod "pod-ee2b7bfd-094f-4465-bfff-cf8a2993cb83" satisfied condition "success or failure"
Feb 28 16:49:45.502: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-ee2b7bfd-094f-4465-bfff-cf8a2993cb83 container test-container: <nil>
STEP: delete the pod
Feb 28 16:49:45.590: INFO: Waiting for pod pod-ee2b7bfd-094f-4465-bfff-cf8a2993cb83 to disappear
Feb 28 16:49:45.624: INFO: Pod pod-ee2b7bfd-094f-4465-bfff-cf8a2993cb83 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:49:45.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4432" for this suite.
Feb 28 16:49:51.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:49:53.146: INFO: namespace emptydir-4432 deletion completed in 7.471869379s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:49:53.147: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 16:49:53.329: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12cd92f0-da6a-49a5-ba55-30b2b5d7d152" in namespace "downward-api-9575" to be "success or failure"
Feb 28 16:49:53.367: INFO: Pod "downwardapi-volume-12cd92f0-da6a-49a5-ba55-30b2b5d7d152": Phase="Pending", Reason="", readiness=false. Elapsed: 37.980069ms
Feb 28 16:49:55.403: INFO: Pod "downwardapi-volume-12cd92f0-da6a-49a5-ba55-30b2b5d7d152": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.073766076s
STEP: Saw pod success
Feb 28 16:49:55.403: INFO: Pod "downwardapi-volume-12cd92f0-da6a-49a5-ba55-30b2b5d7d152" satisfied condition "success or failure"
Feb 28 16:49:55.437: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod downwardapi-volume-12cd92f0-da6a-49a5-ba55-30b2b5d7d152 container client-container: <nil>
STEP: delete the pod
Feb 28 16:49:55.523: INFO: Waiting for pod downwardapi-volume-12cd92f0-da6a-49a5-ba55-30b2b5d7d152 to disappear
Feb 28 16:49:55.557: INFO: Pod downwardapi-volume-12cd92f0-da6a-49a5-ba55-30b2b5d7d152 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:49:55.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9575" for this suite.
Feb 28 16:50:01.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:50:03.046: INFO: namespace downward-api-9575 deletion completed in 7.452657351s
•SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:50:03.046: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-35f71295-0db3-45ac-b8a7-9aa29ca2fd68
STEP: Creating a pod to test consume secrets
Feb 28 16:50:03.309: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f8116b32-aabc-4728-a8b4-2286e3450e27" in namespace "projected-3179" to be "success or failure"
Feb 28 16:50:03.344: INFO: Pod "pod-projected-secrets-f8116b32-aabc-4728-a8b4-2286e3450e27": Phase="Pending", Reason="", readiness=false. Elapsed: 34.387447ms
Feb 28 16:50:05.379: INFO: Pod "pod-projected-secrets-f8116b32-aabc-4728-a8b4-2286e3450e27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069597037s
Feb 28 16:50:07.414: INFO: Pod "pod-projected-secrets-f8116b32-aabc-4728-a8b4-2286e3450e27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.104893201s
STEP: Saw pod success
Feb 28 16:50:07.414: INFO: Pod "pod-projected-secrets-f8116b32-aabc-4728-a8b4-2286e3450e27" satisfied condition "success or failure"
Feb 28 16:50:07.449: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-projected-secrets-f8116b32-aabc-4728-a8b4-2286e3450e27 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 16:50:07.536: INFO: Waiting for pod pod-projected-secrets-f8116b32-aabc-4728-a8b4-2286e3450e27 to disappear
Feb 28 16:50:07.571: INFO: Pod pod-projected-secrets-f8116b32-aabc-4728-a8b4-2286e3450e27 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:50:07.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3179" for this suite.
Feb 28 16:50:13.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:50:15.059: INFO: namespace projected-3179 deletion completed in 7.451428715s
•SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:50:15.059: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Feb 28 16:50:55.582: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 6
	[quantile=0.9] = 285
	[quantile=0.99] = 31172
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 6170
	[quantile=0.9] = 476006
	[quantile=0.99] = 531441
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = 5
	[quantile=0.9] = 5
	[quantile=0.99] = 5
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = 613716
	[quantile=0.9] = 613716
	[quantile=0.99] = 613716
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 5
	[quantile=0.9] = 12
	[quantile=0.99] = 91
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 14
	[quantile=0.9] = 27
	[quantile=0.99] = 67
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 12
	[quantile=0.9] = 18
	[quantile=0.99] = 29
For namespace_queue_latency_sum:
	[] = 2869
For namespace_queue_latency_count:
	[] = 216
For namespace_retries:
	[] = 217
For namespace_work_duration:
	[quantile=0.5] = 305458
	[quantile=0.9] = 446298
	[quantile=0.99] = 791969
For namespace_work_duration_sum:
	[] = 75695854
For namespace_work_duration_count:
	[] = 216
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:50:55.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1757" for this suite.
Feb 28 16:51:01.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:51:03.098: INFO: namespace gc-1757 deletion completed in 7.481100223s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:51:03.099: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-3675d716-526d-4daa-a63e-860981d11b7a
STEP: Creating a pod to test consume configMaps
Feb 28 16:51:03.360: INFO: Waiting up to 5m0s for pod "pod-configmaps-db406db9-3a55-4ae5-b331-d738135dd801" in namespace "configmap-1839" to be "success or failure"
Feb 28 16:51:03.400: INFO: Pod "pod-configmaps-db406db9-3a55-4ae5-b331-d738135dd801": Phase="Pending", Reason="", readiness=false. Elapsed: 39.582853ms
Feb 28 16:51:05.446: INFO: Pod "pod-configmaps-db406db9-3a55-4ae5-b331-d738135dd801": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.085921494s
STEP: Saw pod success
Feb 28 16:51:05.447: INFO: Pod "pod-configmaps-db406db9-3a55-4ae5-b331-d738135dd801" satisfied condition "success or failure"
Feb 28 16:51:05.491: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-configmaps-db406db9-3a55-4ae5-b331-d738135dd801 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 16:51:05.674: INFO: Waiting for pod pod-configmaps-db406db9-3a55-4ae5-b331-d738135dd801 to disappear
Feb 28 16:51:05.714: INFO: Pod pod-configmaps-db406db9-3a55-4ae5-b331-d738135dd801 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:51:05.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1839" for this suite.
Feb 28 16:51:11.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:51:13.398: INFO: namespace configmap-1839 deletion completed in 7.640478307s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:51:13.399: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 16:51:13.629: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2feae35-3291-4764-9469-7cab994f70dd" in namespace "downward-api-2871" to be "success or failure"
Feb 28 16:51:13.663: INFO: Pod "downwardapi-volume-e2feae35-3291-4764-9469-7cab994f70dd": Phase="Pending", Reason="", readiness=false. Elapsed: 34.467446ms
Feb 28 16:51:15.718: INFO: Pod "downwardapi-volume-e2feae35-3291-4764-9469-7cab994f70dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.088941221s
STEP: Saw pod success
Feb 28 16:51:15.718: INFO: Pod "downwardapi-volume-e2feae35-3291-4764-9469-7cab994f70dd" satisfied condition "success or failure"
Feb 28 16:51:15.757: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod downwardapi-volume-e2feae35-3291-4764-9469-7cab994f70dd container client-container: <nil>
STEP: delete the pod
Feb 28 16:51:15.868: INFO: Waiting for pod downwardapi-volume-e2feae35-3291-4764-9469-7cab994f70dd to disappear
Feb 28 16:51:15.914: INFO: Pod downwardapi-volume-e2feae35-3291-4764-9469-7cab994f70dd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:51:15.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2871" for this suite.
Feb 28 16:51:22.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:51:23.450: INFO: namespace downward-api-2871 deletion completed in 7.494824024s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:51:23.450: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 28 16:51:26.353: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-4133 pod-service-account-c9c1607c-8647-4e1d-9722-0b4cc0939c4f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 28 16:51:27.031: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-4133 pod-service-account-c9c1607c-8647-4e1d-9722-0b4cc0939c4f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 28 16:51:27.694: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-4133 pod-service-account-c9c1607c-8647-4e1d-9722-0b4cc0939c4f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:51:28.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4133" for this suite.
Feb 28 16:51:34.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:51:36.049: INFO: namespace svcaccounts-4133 deletion completed in 7.606874856s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:51:36.049: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 28 16:51:38.207: INFO: Pod name wrapped-volume-race-06bf8537-db84-491d-98bc-c4961397c3ae: Found 3 pods out of 5
Feb 28 16:51:43.244: INFO: Pod name wrapped-volume-race-06bf8537-db84-491d-98bc-c4961397c3ae: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-06bf8537-db84-491d-98bc-c4961397c3ae in namespace emptydir-wrapper-8841, will wait for the garbage collector to delete the pods
Feb 28 16:51:55.579: INFO: Deleting ReplicationController wrapped-volume-race-06bf8537-db84-491d-98bc-c4961397c3ae took: 37.884964ms
Feb 28 16:51:58.179: INFO: Terminating ReplicationController wrapped-volume-race-06bf8537-db84-491d-98bc-c4961397c3ae pods took: 2.600357665s
STEP: Creating RC which spawns configmap-volume pods
Feb 28 16:52:39.352: INFO: Pod name wrapped-volume-race-8a8135ac-c0ab-4b4c-9db9-58f74455ebc2: Found 3 pods out of 5
Feb 28 16:52:44.389: INFO: Pod name wrapped-volume-race-8a8135ac-c0ab-4b4c-9db9-58f74455ebc2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8a8135ac-c0ab-4b4c-9db9-58f74455ebc2 in namespace emptydir-wrapper-8841, will wait for the garbage collector to delete the pods
Feb 28 16:52:56.725: INFO: Deleting ReplicationController wrapped-volume-race-8a8135ac-c0ab-4b4c-9db9-58f74455ebc2 took: 39.282029ms
Feb 28 16:52:57.426: INFO: Terminating ReplicationController wrapped-volume-race-8a8135ac-c0ab-4b4c-9db9-58f74455ebc2 pods took: 701.14834ms
STEP: Creating RC which spawns configmap-volume pods
Feb 28 16:53:39.884: INFO: Pod name wrapped-volume-race-88e7d1ae-1712-405f-9918-4d74b39db61f: Found 1 pods out of 5
Feb 28 16:53:44.922: INFO: Pod name wrapped-volume-race-88e7d1ae-1712-405f-9918-4d74b39db61f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-88e7d1ae-1712-405f-9918-4d74b39db61f in namespace emptydir-wrapper-8841, will wait for the garbage collector to delete the pods
Feb 28 16:53:57.259: INFO: Deleting ReplicationController wrapped-volume-race-88e7d1ae-1712-405f-9918-4d74b39db61f took: 39.808839ms
Feb 28 16:53:57.862: INFO: Terminating ReplicationController wrapped-volume-race-88e7d1ae-1712-405f-9918-4d74b39db61f pods took: 602.437892ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:54:41.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8841" for this suite.
Feb 28 16:54:49.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:54:51.238: INFO: namespace emptydir-wrapper-8841 deletion completed in 9.568449156s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:54:51.238: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-2a2a7828-a96e-4711-8cd6-9701628b496e
Feb 28 16:54:51.505: INFO: Pod name my-hostname-basic-2a2a7828-a96e-4711-8cd6-9701628b496e: Found 1 pods out of 1
Feb 28 16:54:51.505: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-2a2a7828-a96e-4711-8cd6-9701628b496e" are running
Feb 28 16:54:53.579: INFO: Pod "my-hostname-basic-2a2a7828-a96e-4711-8cd6-9701628b496e-hqx9w" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-28 16:54:51 +0000 UTC Reason: Message:}])
Feb 28 16:54:53.579: INFO: Trying to dial the pod
Feb 28 16:54:58.704: INFO: Controller my-hostname-basic-2a2a7828-a96e-4711-8cd6-9701628b496e: Got expected result from replica 1 [my-hostname-basic-2a2a7828-a96e-4711-8cd6-9701628b496e-hqx9w]: "my-hostname-basic-2a2a7828-a96e-4711-8cd6-9701628b496e-hqx9w", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:54:58.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1644" for this suite.
Feb 28 16:55:04.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:55:06.298: INFO: namespace replication-controller-1644 deletion completed in 7.554537632s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:55:06.298: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-d011de07-6cd2-49e1-894f-05555cd4aef7
STEP: Creating a pod to test consume secrets
Feb 28 16:55:06.532: INFO: Waiting up to 5m0s for pod "pod-secrets-3fcc89ae-c8b6-4a9b-b9cc-b3c516aab2eb" in namespace "secrets-457" to be "success or failure"
Feb 28 16:55:06.568: INFO: Pod "pod-secrets-3fcc89ae-c8b6-4a9b-b9cc-b3c516aab2eb": Phase="Pending", Reason="", readiness=false. Elapsed: 36.214406ms
Feb 28 16:55:08.609: INFO: Pod "pod-secrets-3fcc89ae-c8b6-4a9b-b9cc-b3c516aab2eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076877539s
STEP: Saw pod success
Feb 28 16:55:08.609: INFO: Pod "pod-secrets-3fcc89ae-c8b6-4a9b-b9cc-b3c516aab2eb" satisfied condition "success or failure"
Feb 28 16:55:08.647: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-secrets-3fcc89ae-c8b6-4a9b-b9cc-b3c516aab2eb container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 16:55:08.733: INFO: Waiting for pod pod-secrets-3fcc89ae-c8b6-4a9b-b9cc-b3c516aab2eb to disappear
Feb 28 16:55:08.770: INFO: Pod pod-secrets-3fcc89ae-c8b6-4a9b-b9cc-b3c516aab2eb no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:55:08.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-457" for this suite.
Feb 28 16:55:14.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:55:16.361: INFO: namespace secrets-457 deletion completed in 7.555192544s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:55:16.362: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Feb 28 16:55:16.554: INFO: >>> kubeConfig: /workspace/.kube/config
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Feb 28 16:55:18.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 16:55:20.113: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 16:55:22.119: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 16:55:24.114: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 16:55:26.114: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 16:55:28.115: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718505717, loc:(*time.Location)(0x7eb0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 28 16:55:31.683: INFO: Waited 1.518462731s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:55:34.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9057" for this suite.
Feb 28 16:55:40.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:55:41.751: INFO: namespace aggregator-9057 deletion completed in 7.516602072s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:55:41.752: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Feb 28 16:55:41.942: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-7494'
Feb 28 16:55:43.318: INFO: stderr: ""
Feb 28 16:55:43.318: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 28 16:55:44.354: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 16:55:44.354: INFO: Found 0 / 1
Feb 28 16:55:45.359: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 16:55:45.359: INFO: Found 1 / 1
Feb 28 16:55:45.359: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 28 16:55:45.395: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 16:55:45.395: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 28 16:55:45.395: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config patch pod redis-master-dbwpv --namespace=kubectl-7494 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 28 16:55:46.073: INFO: stderr: ""
Feb 28 16:55:46.073: INFO: stdout: "pod/redis-master-dbwpv patched\n"
STEP: checking annotations
Feb 28 16:55:46.110: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 16:55:46.110: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:55:46.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7494" for this suite.
Feb 28 16:56:08.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:56:09.686: INFO: namespace kubectl-7494 deletion completed in 23.525821408s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:56:09.687: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Feb 28 16:56:12.172: INFO: Asynchronously running '/workspace/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 28 16:56:22.678: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:56:22.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2704" for this suite.
Feb 28 16:56:28.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:56:30.313: INFO: namespace pods-2704 deletion completed in 7.539316751s
•
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:56:30.313: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 16:56:30.717: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 28 16:56:30.803: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:30.863: INFO: Number of nodes with available pods: 0
Feb 28 16:56:30.863: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:56:31.904: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:31.943: INFO: Number of nodes with available pods: 0
Feb 28 16:56:31.943: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:56:32.901: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:32.940: INFO: Number of nodes with available pods: 0
Feb 28 16:56:32.940: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 16:56:33.911: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:33.946: INFO: Number of nodes with available pods: 3
Feb 28 16:56:33.946: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 28 16:56:34.231: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:34.231: INFO: Wrong image for pod: daemon-set-fg5lc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:34.231: INFO: Wrong image for pod: daemon-set-vpcgw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:34.267: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:35.311: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:35.311: INFO: Wrong image for pod: daemon-set-fg5lc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:35.311: INFO: Wrong image for pod: daemon-set-vpcgw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:35.347: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:36.305: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:36.305: INFO: Wrong image for pod: daemon-set-fg5lc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:36.305: INFO: Pod daemon-set-fg5lc is not available
Feb 28 16:56:36.305: INFO: Wrong image for pod: daemon-set-vpcgw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:36.340: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:37.309: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:37.309: INFO: Pod daemon-set-slkw4 is not available
Feb 28 16:56:37.309: INFO: Wrong image for pod: daemon-set-vpcgw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:37.346: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:38.306: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:38.307: INFO: Pod daemon-set-slkw4 is not available
Feb 28 16:56:38.307: INFO: Wrong image for pod: daemon-set-vpcgw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:38.343: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:39.305: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:39.305: INFO: Pod daemon-set-slkw4 is not available
Feb 28 16:56:39.305: INFO: Wrong image for pod: daemon-set-vpcgw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:39.351: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:40.307: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:40.307: INFO: Pod daemon-set-slkw4 is not available
Feb 28 16:56:40.307: INFO: Wrong image for pod: daemon-set-vpcgw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:40.346: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:41.305: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:41.305: INFO: Pod daemon-set-slkw4 is not available
Feb 28 16:56:41.305: INFO: Wrong image for pod: daemon-set-vpcgw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:41.341: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:42.305: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:42.305: INFO: Wrong image for pod: daemon-set-vpcgw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:42.344: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:43.303: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:43.304: INFO: Wrong image for pod: daemon-set-vpcgw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:43.304: INFO: Pod daemon-set-vpcgw is not available
Feb 28 16:56:43.347: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:44.303: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:44.303: INFO: Wrong image for pod: daemon-set-vpcgw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:44.303: INFO: Pod daemon-set-vpcgw is not available
Feb 28 16:56:44.339: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:45.304: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:45.304: INFO: Wrong image for pod: daemon-set-vpcgw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:45.304: INFO: Pod daemon-set-vpcgw is not available
Feb 28 16:56:45.342: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:46.307: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:46.307: INFO: Wrong image for pod: daemon-set-vpcgw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:46.307: INFO: Pod daemon-set-vpcgw is not available
Feb 28 16:56:46.343: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:47.313: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:47.313: INFO: Wrong image for pod: daemon-set-vpcgw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:47.313: INFO: Pod daemon-set-vpcgw is not available
Feb 28 16:56:47.351: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:48.304: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:48.304: INFO: Wrong image for pod: daemon-set-vpcgw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:48.304: INFO: Pod daemon-set-vpcgw is not available
Feb 28 16:56:48.342: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:49.306: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:49.306: INFO: Pod daemon-set-d2jvc is not available
Feb 28 16:56:49.341: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:50.310: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:50.310: INFO: Pod daemon-set-d2jvc is not available
Feb 28 16:56:50.346: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:51.305: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:51.345: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:52.304: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:52.304: INFO: Pod daemon-set-6qlvz is not available
Feb 28 16:56:52.341: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:53.307: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:53.307: INFO: Pod daemon-set-6qlvz is not available
Feb 28 16:56:53.342: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:54.307: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:54.307: INFO: Pod daemon-set-6qlvz is not available
Feb 28 16:56:54.344: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:55.306: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:55.306: INFO: Pod daemon-set-6qlvz is not available
Feb 28 16:56:55.341: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:56.309: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:56.309: INFO: Pod daemon-set-6qlvz is not available
Feb 28 16:56:56.347: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:57.319: INFO: Wrong image for pod: daemon-set-6qlvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 28 16:56:57.319: INFO: Pod daemon-set-6qlvz is not available
Feb 28 16:56:57.363: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:58.306: INFO: Pod daemon-set-h4dk7 is not available
Feb 28 16:56:58.343: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 28 16:56:58.379: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:58.414: INFO: Number of nodes with available pods: 2
Feb 28 16:56:58.414: INFO: Node bootstrap-e2e-minion-group-s43f is running more than one daemon pod
Feb 28 16:56:59.450: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 16:56:59.485: INFO: Number of nodes with available pods: 3
Feb 28 16:56:59.485: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3947, will wait for the garbage collector to delete the pods
Feb 28 16:56:59.794: INFO: Deleting DaemonSet.extensions daemon-set took: 40.142128ms
Feb 28 16:57:00.295: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.405507ms
Feb 28 16:57:08.732: INFO: Number of nodes with available pods: 0
Feb 28 16:57:08.732: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 16:57:08.766: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3947/daemonsets","resourceVersion":"15350"},"items":null}

Feb 28 16:57:08.803: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3947/pods","resourceVersion":"15350"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:57:08.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3947" for this suite.
Feb 28 16:57:15.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:57:16.486: INFO: namespace daemonsets-3947 deletion completed in 7.499509746s
•SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:57:16.487: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4277
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 28 16:57:16.686: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 28 16:57:41.461: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.2.101:8080/dial?request=hostName&protocol=udp&host=10.64.3.124&port=8081&tries=1'] Namespace:pod-network-test-4277 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:57:41.461: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:57:41.895: INFO: Waiting for endpoints: map[]
Feb 28 16:57:41.935: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.2.101:8080/dial?request=hostName&protocol=udp&host=10.64.1.30&port=8081&tries=1'] Namespace:pod-network-test-4277 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:57:41.935: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:57:42.369: INFO: Waiting for endpoints: map[]
Feb 28 16:57:42.403: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.2.101:8080/dial?request=hostName&protocol=udp&host=10.64.2.100&port=8081&tries=1'] Namespace:pod-network-test-4277 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 28 16:57:42.404: INFO: >>> kubeConfig: /workspace/.kube/config
Feb 28 16:57:42.811: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:57:42.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4277" for this suite.
Feb 28 16:58:04.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:58:06.550: INFO: namespace pod-network-test-4277 deletion completed in 23.703543602s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:58:06.551: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Feb 28 16:58:06.739: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config cluster-info'
Feb 28 16:58:07.315: INFO: stderr: ""
Feb 28 16:58:07.315: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://34.82.216.130\x1b[0m\n\x1b[0;32mGLBCDefaultBackend\x1b[0m is running at \x1b[0;33mhttps://34.82.216.130/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy\x1b[0m\n\x1b[0;32mHeapster\x1b[0m is running at \x1b[0;33mhttps://34.82.216.130/api/v1/namespaces/kube-system/services/heapster/proxy\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://34.82.216.130/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://34.82.216.130/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://34.82.216.130/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:58:07.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-494" for this suite.
Feb 28 16:58:13.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:58:14.836: INFO: namespace kubectl-494 deletion completed in 7.481402842s
•
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:58:14.837: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 28 16:58:17.748: INFO: Successfully updated pod "pod-update-activedeadlineseconds-12f75619-03ff-409f-b9cc-e2394f88f793"
Feb 28 16:58:17.748: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-12f75619-03ff-409f-b9cc-e2394f88f793" in namespace "pods-3840" to be "terminated due to deadline exceeded"
Feb 28 16:58:17.782: INFO: Pod "pod-update-activedeadlineseconds-12f75619-03ff-409f-b9cc-e2394f88f793": Phase="Running", Reason="", readiness=true. Elapsed: 34.693508ms
Feb 28 16:58:19.817: INFO: Pod "pod-update-activedeadlineseconds-12f75619-03ff-409f-b9cc-e2394f88f793": Phase="Running", Reason="", readiness=true. Elapsed: 2.069642967s
Feb 28 16:58:21.852: INFO: Pod "pod-update-activedeadlineseconds-12f75619-03ff-409f-b9cc-e2394f88f793": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.104212421s
Feb 28 16:58:21.852: INFO: Pod "pod-update-activedeadlineseconds-12f75619-03ff-409f-b9cc-e2394f88f793" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:58:21.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3840" for this suite.
Feb 28 16:58:27.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:58:29.337: INFO: namespace pods-3840 deletion completed in 7.449983284s
•SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:58:29.337: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-7qlv
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 16:58:29.606: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-7qlv" in namespace "subpath-3743" to be "success or failure"
Feb 28 16:58:29.641: INFO: Pod "pod-subpath-test-secret-7qlv": Phase="Pending", Reason="", readiness=false. Elapsed: 34.944025ms
Feb 28 16:58:31.678: INFO: Pod "pod-subpath-test-secret-7qlv": Phase="Running", Reason="", readiness=true. Elapsed: 2.071883204s
Feb 28 16:58:33.713: INFO: Pod "pod-subpath-test-secret-7qlv": Phase="Running", Reason="", readiness=true. Elapsed: 4.106908501s
Feb 28 16:58:35.751: INFO: Pod "pod-subpath-test-secret-7qlv": Phase="Running", Reason="", readiness=true. Elapsed: 6.144262147s
Feb 28 16:58:37.786: INFO: Pod "pod-subpath-test-secret-7qlv": Phase="Running", Reason="", readiness=true. Elapsed: 8.179925828s
Feb 28 16:58:39.823: INFO: Pod "pod-subpath-test-secret-7qlv": Phase="Running", Reason="", readiness=true. Elapsed: 10.216217714s
Feb 28 16:58:41.860: INFO: Pod "pod-subpath-test-secret-7qlv": Phase="Running", Reason="", readiness=true. Elapsed: 12.253587505s
Feb 28 16:58:43.895: INFO: Pod "pod-subpath-test-secret-7qlv": Phase="Running", Reason="", readiness=true. Elapsed: 14.288373241s
Feb 28 16:58:45.930: INFO: Pod "pod-subpath-test-secret-7qlv": Phase="Running", Reason="", readiness=true. Elapsed: 16.324060086s
Feb 28 16:58:47.965: INFO: Pod "pod-subpath-test-secret-7qlv": Phase="Running", Reason="", readiness=true. Elapsed: 18.358823325s
Feb 28 16:58:50.000: INFO: Pod "pod-subpath-test-secret-7qlv": Phase="Running", Reason="", readiness=true. Elapsed: 20.393956896s
Feb 28 16:58:52.035: INFO: Pod "pod-subpath-test-secret-7qlv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.428973521s
STEP: Saw pod success
Feb 28 16:58:52.035: INFO: Pod "pod-subpath-test-secret-7qlv" satisfied condition "success or failure"
Feb 28 16:58:52.072: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-subpath-test-secret-7qlv container test-container-subpath-secret-7qlv: <nil>
STEP: delete the pod
Feb 28 16:58:52.161: INFO: Waiting for pod pod-subpath-test-secret-7qlv to disappear
Feb 28 16:58:52.195: INFO: Pod pod-subpath-test-secret-7qlv no longer exists
STEP: Deleting pod pod-subpath-test-secret-7qlv
Feb 28 16:58:52.195: INFO: Deleting pod "pod-subpath-test-secret-7qlv" in namespace "subpath-3743"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:58:52.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3743" for this suite.
Feb 28 16:58:58.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:58:59.789: INFO: namespace subpath-3743 deletion completed in 7.523836362s
•SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:58:59.790: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-12451885-143d-40f9-ba67-9920843c5089 in namespace container-probe-9737
Feb 28 16:59:02.086: INFO: Started pod liveness-12451885-143d-40f9-ba67-9920843c5089 in namespace container-probe-9737
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 16:59:02.121: INFO: Initial restart count of pod liveness-12451885-143d-40f9-ba67-9920843c5089 is 0
Feb 28 16:59:20.484: INFO: Restart count of pod container-probe-9737/liveness-12451885-143d-40f9-ba67-9920843c5089 is now 1 (18.36340687s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:59:20.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9737" for this suite.
Feb 28 16:59:26.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:59:28.034: INFO: namespace container-probe-9737 deletion completed in 7.470967395s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:59:28.035: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 28 16:59:28.221: INFO: Waiting up to 5m0s for pod "pod-a676e434-5aad-4cf0-a9db-17d28d185440" in namespace "emptydir-9491" to be "success or failure"
Feb 28 16:59:28.255: INFO: Pod "pod-a676e434-5aad-4cf0-a9db-17d28d185440": Phase="Pending", Reason="", readiness=false. Elapsed: 34.189688ms
Feb 28 16:59:30.290: INFO: Pod "pod-a676e434-5aad-4cf0-a9db-17d28d185440": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.068999974s
STEP: Saw pod success
Feb 28 16:59:30.290: INFO: Pod "pod-a676e434-5aad-4cf0-a9db-17d28d185440" satisfied condition "success or failure"
Feb 28 16:59:30.325: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-a676e434-5aad-4cf0-a9db-17d28d185440 container test-container: <nil>
STEP: delete the pod
Feb 28 16:59:30.430: INFO: Waiting for pod pod-a676e434-5aad-4cf0-a9db-17d28d185440 to disappear
Feb 28 16:59:30.465: INFO: Pod pod-a676e434-5aad-4cf0-a9db-17d28d185440 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:59:30.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9491" for this suite.
Feb 28 16:59:36.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:59:37.953: INFO: namespace emptydir-9491 deletion completed in 7.452313382s
•SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:59:37.953: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-a26c8546-549c-4684-9ca3-06055c049b63
STEP: Creating a pod to test consume secrets
Feb 28 16:59:38.221: INFO: Waiting up to 5m0s for pod "pod-secrets-7060e6d6-8167-4cd3-9659-865fa2107f06" in namespace "secrets-5297" to be "success or failure"
Feb 28 16:59:38.255: INFO: Pod "pod-secrets-7060e6d6-8167-4cd3-9659-865fa2107f06": Phase="Pending", Reason="", readiness=false. Elapsed: 34.282562ms
Feb 28 16:59:40.295: INFO: Pod "pod-secrets-7060e6d6-8167-4cd3-9659-865fa2107f06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.074418493s
STEP: Saw pod success
Feb 28 16:59:40.295: INFO: Pod "pod-secrets-7060e6d6-8167-4cd3-9659-865fa2107f06" satisfied condition "success or failure"
Feb 28 16:59:40.333: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-secrets-7060e6d6-8167-4cd3-9659-865fa2107f06 container secret-volume-test: <nil>
STEP: delete the pod
Feb 28 16:59:40.433: INFO: Waiting for pod pod-secrets-7060e6d6-8167-4cd3-9659-865fa2107f06 to disappear
Feb 28 16:59:40.487: INFO: Pod pod-secrets-7060e6d6-8167-4cd3-9659-865fa2107f06 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:59:40.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5297" for this suite.
Feb 28 16:59:46.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 16:59:48.898: INFO: namespace secrets-5297 deletion completed in 8.370201277s
•SSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 16:59:48.898: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 28 16:59:53.794: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 16:59:54.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7713" for this suite.
Feb 28 17:00:16.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:00:18.876: INFO: namespace replicaset-7713 deletion completed in 24.595303408s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:00:18.876: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-718a9a6e-d0b8-4a31-9e5e-d6f7df14da29
STEP: Creating a pod to test consume secrets
Feb 28 17:00:19.288: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-03a2318e-1cf5-4d39-a072-91d454edb8be" in namespace "projected-5608" to be "success or failure"
Feb 28 17:00:19.337: INFO: Pod "pod-projected-secrets-03a2318e-1cf5-4d39-a072-91d454edb8be": Phase="Pending", Reason="", readiness=false. Elapsed: 49.37713ms
Feb 28 17:00:21.376: INFO: Pod "pod-projected-secrets-03a2318e-1cf5-4d39-a072-91d454edb8be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08834352s
Feb 28 17:00:23.421: INFO: Pod "pod-projected-secrets-03a2318e-1cf5-4d39-a072-91d454edb8be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.132868139s
STEP: Saw pod success
Feb 28 17:00:23.421: INFO: Pod "pod-projected-secrets-03a2318e-1cf5-4d39-a072-91d454edb8be" satisfied condition "success or failure"
Feb 28 17:00:23.462: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-projected-secrets-03a2318e-1cf5-4d39-a072-91d454edb8be container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 17:00:23.564: INFO: Waiting for pod pod-projected-secrets-03a2318e-1cf5-4d39-a072-91d454edb8be to disappear
Feb 28 17:00:23.599: INFO: Pod pod-projected-secrets-03a2318e-1cf5-4d39-a072-91d454edb8be no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:00:23.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5608" for this suite.
Feb 28 17:00:29.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:00:32.778: INFO: namespace projected-5608 deletion completed in 9.142942315s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:00:32.783: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 28 17:00:38.063: INFO: Successfully updated pod "pod-update-79eb5927-d943-4529-b398-645cffcef60e"
STEP: verifying the updated pod is in kubernetes
Feb 28 17:00:38.226: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:00:38.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4079" for this suite.
Feb 28 17:01:00.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:01:02.263: INFO: namespace pods-4079 deletion completed in 23.979387321s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:01:02.263: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-ff0afd4b-ec20-4b47-a2ed-1901bdfe784d
STEP: Creating a pod to test consume secrets
Feb 28 17:01:02.540: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bd8a9ee6-53c6-4403-bd2e-9780755c97c3" in namespace "projected-2505" to be "success or failure"
Feb 28 17:01:02.575: INFO: Pod "pod-projected-secrets-bd8a9ee6-53c6-4403-bd2e-9780755c97c3": Phase="Pending", Reason="", readiness=false. Elapsed: 34.469763ms
Feb 28 17:01:04.615: INFO: Pod "pod-projected-secrets-bd8a9ee6-53c6-4403-bd2e-9780755c97c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075197812s
STEP: Saw pod success
Feb 28 17:01:04.615: INFO: Pod "pod-projected-secrets-bd8a9ee6-53c6-4403-bd2e-9780755c97c3" satisfied condition "success or failure"
Feb 28 17:01:04.669: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-projected-secrets-bd8a9ee6-53c6-4403-bd2e-9780755c97c3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 17:01:04.894: INFO: Waiting for pod pod-projected-secrets-bd8a9ee6-53c6-4403-bd2e-9780755c97c3 to disappear
Feb 28 17:01:05.066: INFO: Pod pod-projected-secrets-bd8a9ee6-53c6-4403-bd2e-9780755c97c3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:01:05.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2505" for this suite.
Feb 28 17:01:11.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:01:13.338: INFO: namespace projected-2505 deletion completed in 8.135328233s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:01:13.338: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 17:01:13.619: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config version'
Feb 28 17:01:19.579: INFO: stderr: ""
Feb 28 17:01:19.579: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15+\", GitVersion:\"v1.15.11-beta.0.18+89a7468341d588\", GitCommit:\"89a7468341d588301c6c8be360ed2abb871009e1\", GitTreeState:\"clean\", BuildDate:\"2020-02-28T00:28:38Z\", GoVersion:\"go1.12.17\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15+\", GitVersion:\"v1.15.11-beta.0.18+89a7468341d588\", GitCommit:\"89a7468341d588301c6c8be360ed2abb871009e1\", GitTreeState:\"clean\", BuildDate:\"2020-02-28T00:28:38Z\", GoVersion:\"go1.12.17\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:01:19.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9019" for this suite.
Feb 28 17:01:25.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:01:28.310: INFO: namespace kubectl-9019 deletion completed in 8.67813683s
•SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:01:28.311: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 28 17:01:28.849: INFO: Waiting up to 5m0s for pod "pod-a2c37c4a-458b-40b5-a43c-f76b203ecd25" in namespace "emptydir-3705" to be "success or failure"
Feb 28 17:01:28.899: INFO: Pod "pod-a2c37c4a-458b-40b5-a43c-f76b203ecd25": Phase="Pending", Reason="", readiness=false. Elapsed: 50.388933ms
Feb 28 17:01:30.937: INFO: Pod "pod-a2c37c4a-458b-40b5-a43c-f76b203ecd25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.088151384s
STEP: Saw pod success
Feb 28 17:01:30.937: INFO: Pod "pod-a2c37c4a-458b-40b5-a43c-f76b203ecd25" satisfied condition "success or failure"
Feb 28 17:01:30.972: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-a2c37c4a-458b-40b5-a43c-f76b203ecd25 container test-container: <nil>
STEP: delete the pod
Feb 28 17:01:31.208: INFO: Waiting for pod pod-a2c37c4a-458b-40b5-a43c-f76b203ecd25 to disappear
Feb 28 17:01:31.258: INFO: Pod pod-a2c37c4a-458b-40b5-a43c-f76b203ecd25 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:01:31.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3705" for this suite.
Feb 28 17:01:37.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:01:40.694: INFO: namespace emptydir-3705 deletion completed in 9.400672561s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:01:41.091: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3232.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3232.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3232.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3232.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 28 17:01:46.080: INFO: DNS probes using dns-test-afdaf037-c645-4746-af21-d0bf5d10f25d succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3232.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3232.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3232.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3232.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 28 17:01:51.427: INFO: File wheezy_udp@dns-test-service-3.dns-3232.svc.cluster.local from pod  dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 28 17:01:51.463: INFO: File jessie_udp@dns-test-service-3.dns-3232.svc.cluster.local from pod  dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 28 17:01:51.463: INFO: Lookups using dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce failed for: [wheezy_udp@dns-test-service-3.dns-3232.svc.cluster.local jessie_udp@dns-test-service-3.dns-3232.svc.cluster.local]

Feb 28 17:01:56.511: INFO: File wheezy_udp@dns-test-service-3.dns-3232.svc.cluster.local from pod  dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 28 17:01:56.596: INFO: File jessie_udp@dns-test-service-3.dns-3232.svc.cluster.local from pod  dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 28 17:01:56.596: INFO: Lookups using dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce failed for: [wheezy_udp@dns-test-service-3.dns-3232.svc.cluster.local jessie_udp@dns-test-service-3.dns-3232.svc.cluster.local]

Feb 28 17:02:01.499: INFO: File wheezy_udp@dns-test-service-3.dns-3232.svc.cluster.local from pod  dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 28 17:02:01.539: INFO: File jessie_udp@dns-test-service-3.dns-3232.svc.cluster.local from pod  dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 28 17:02:01.539: INFO: Lookups using dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce failed for: [wheezy_udp@dns-test-service-3.dns-3232.svc.cluster.local jessie_udp@dns-test-service-3.dns-3232.svc.cluster.local]

Feb 28 17:02:06.532: INFO: File wheezy_udp@dns-test-service-3.dns-3232.svc.cluster.local from pod  dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 28 17:02:06.577: INFO: File jessie_udp@dns-test-service-3.dns-3232.svc.cluster.local from pod  dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 28 17:02:06.577: INFO: Lookups using dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce failed for: [wheezy_udp@dns-test-service-3.dns-3232.svc.cluster.local jessie_udp@dns-test-service-3.dns-3232.svc.cluster.local]

Feb 28 17:02:11.515: INFO: File wheezy_udp@dns-test-service-3.dns-3232.svc.cluster.local from pod  dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 28 17:02:11.677: INFO: File jessie_udp@dns-test-service-3.dns-3232.svc.cluster.local from pod  dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 28 17:02:11.677: INFO: Lookups using dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce failed for: [wheezy_udp@dns-test-service-3.dns-3232.svc.cluster.local jessie_udp@dns-test-service-3.dns-3232.svc.cluster.local]

Feb 28 17:02:16.504: INFO: File wheezy_udp@dns-test-service-3.dns-3232.svc.cluster.local from pod  dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce contains '' instead of 'bar.example.com.'
Feb 28 17:02:16.545: INFO: Lookups using dns-3232/dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce failed for: [wheezy_udp@dns-test-service-3.dns-3232.svc.cluster.local]

Feb 28 17:02:21.554: INFO: DNS probes using dns-test-7d5ab986-5018-4201-b946-7a7353c8e6ce succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3232.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3232.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3232.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3232.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 28 17:02:24.460: INFO: DNS probes using dns-test-11f309af-c974-446d-ab13-bd32374e789e succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:02:24.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3232" for this suite.
Feb 28 17:02:30.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:02:34.050: INFO: namespace dns-3232 deletion completed in 9.442192809s
•SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:02:34.098: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 17:02:34.588: INFO: Waiting up to 5m0s for pod "downwardapi-volume-264ade57-148d-40ba-947a-08d54887fa48" in namespace "downward-api-4657" to be "success or failure"
Feb 28 17:02:34.824: INFO: Pod "downwardapi-volume-264ade57-148d-40ba-947a-08d54887fa48": Phase="Pending", Reason="", readiness=false. Elapsed: 236.020887ms
Feb 28 17:02:36.862: INFO: Pod "downwardapi-volume-264ade57-148d-40ba-947a-08d54887fa48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.273952772s
STEP: Saw pod success
Feb 28 17:02:36.862: INFO: Pod "downwardapi-volume-264ade57-148d-40ba-947a-08d54887fa48" satisfied condition "success or failure"
Feb 28 17:02:36.897: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod downwardapi-volume-264ade57-148d-40ba-947a-08d54887fa48 container client-container: <nil>
STEP: delete the pod
Feb 28 17:02:37.057: INFO: Waiting for pod downwardapi-volume-264ade57-148d-40ba-947a-08d54887fa48 to disappear
Feb 28 17:02:37.094: INFO: Pod downwardapi-volume-264ade57-148d-40ba-947a-08d54887fa48 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:02:37.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4657" for this suite.
Feb 28 17:02:43.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:02:46.216: INFO: namespace downward-api-4657 deletion completed in 9.056822019s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:02:46.216: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 17:02:46.590: INFO: Waiting up to 5m0s for pod "downwardapi-volume-63d69af1-1f7a-41bb-93bb-1cf0e8809203" in namespace "downward-api-2689" to be "success or failure"
Feb 28 17:02:46.759: INFO: Pod "downwardapi-volume-63d69af1-1f7a-41bb-93bb-1cf0e8809203": Phase="Pending", Reason="", readiness=false. Elapsed: 167.880217ms
Feb 28 17:02:48.798: INFO: Pod "downwardapi-volume-63d69af1-1f7a-41bb-93bb-1cf0e8809203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.206998415s
STEP: Saw pod success
Feb 28 17:02:48.798: INFO: Pod "downwardapi-volume-63d69af1-1f7a-41bb-93bb-1cf0e8809203" satisfied condition "success or failure"
Feb 28 17:02:48.837: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod downwardapi-volume-63d69af1-1f7a-41bb-93bb-1cf0e8809203 container client-container: <nil>
STEP: delete the pod
Feb 28 17:02:48.931: INFO: Waiting for pod downwardapi-volume-63d69af1-1f7a-41bb-93bb-1cf0e8809203 to disappear
Feb 28 17:02:48.969: INFO: Pod downwardapi-volume-63d69af1-1f7a-41bb-93bb-1cf0e8809203 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:02:48.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2689" for this suite.
Feb 28 17:02:55.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:02:57.596: INFO: namespace downward-api-2689 deletion completed in 8.57113851s
•S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:02:57.596: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-sn99
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 17:02:58.056: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-sn99" in namespace "subpath-6871" to be "success or failure"
Feb 28 17:02:58.131: INFO: Pod "pod-subpath-test-projected-sn99": Phase="Pending", Reason="", readiness=false. Elapsed: 74.529269ms
Feb 28 17:03:00.332: INFO: Pod "pod-subpath-test-projected-sn99": Phase="Running", Reason="", readiness=true. Elapsed: 2.275098378s
Feb 28 17:03:02.368: INFO: Pod "pod-subpath-test-projected-sn99": Phase="Running", Reason="", readiness=true. Elapsed: 4.311588578s
Feb 28 17:03:04.404: INFO: Pod "pod-subpath-test-projected-sn99": Phase="Running", Reason="", readiness=true. Elapsed: 6.347123526s
Feb 28 17:03:06.445: INFO: Pod "pod-subpath-test-projected-sn99": Phase="Running", Reason="", readiness=true. Elapsed: 8.38859311s
Feb 28 17:03:08.502: INFO: Pod "pod-subpath-test-projected-sn99": Phase="Running", Reason="", readiness=true. Elapsed: 10.445811478s
Feb 28 17:03:10.545: INFO: Pod "pod-subpath-test-projected-sn99": Phase="Running", Reason="", readiness=true. Elapsed: 12.488251532s
Feb 28 17:03:12.595: INFO: Pod "pod-subpath-test-projected-sn99": Phase="Running", Reason="", readiness=true. Elapsed: 14.538009337s
Feb 28 17:03:14.634: INFO: Pod "pod-subpath-test-projected-sn99": Phase="Running", Reason="", readiness=true. Elapsed: 16.577768956s
Feb 28 17:03:16.734: INFO: Pod "pod-subpath-test-projected-sn99": Phase="Running", Reason="", readiness=true. Elapsed: 18.677430318s
Feb 28 17:03:18.786: INFO: Pod "pod-subpath-test-projected-sn99": Phase="Running", Reason="", readiness=true. Elapsed: 20.729839788s
Feb 28 17:03:20.834: INFO: Pod "pod-subpath-test-projected-sn99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.77782357s
STEP: Saw pod success
Feb 28 17:03:20.834: INFO: Pod "pod-subpath-test-projected-sn99" satisfied condition "success or failure"
Feb 28 17:03:20.871: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-subpath-test-projected-sn99 container test-container-subpath-projected-sn99: <nil>
STEP: delete the pod
Feb 28 17:03:20.967: INFO: Waiting for pod pod-subpath-test-projected-sn99 to disappear
Feb 28 17:03:21.001: INFO: Pod pod-subpath-test-projected-sn99 no longer exists
STEP: Deleting pod pod-subpath-test-projected-sn99
Feb 28 17:03:21.001: INFO: Deleting pod "pod-subpath-test-projected-sn99" in namespace "subpath-6871"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:03:21.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6871" for this suite.
Feb 28 17:03:27.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:03:29.216: INFO: namespace subpath-6871 deletion completed in 8.142478534s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:03:29.217: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-1409/configmap-test-ace36089-7bc5-4331-835b-c0512a010ba9
STEP: Creating a pod to test consume configMaps
Feb 28 17:03:29.557: INFO: Waiting up to 5m0s for pod "pod-configmaps-1827dd53-f5fb-4fd6-aee1-81ce70982339" in namespace "configmap-1409" to be "success or failure"
Feb 28 17:03:29.631: INFO: Pod "pod-configmaps-1827dd53-f5fb-4fd6-aee1-81ce70982339": Phase="Pending", Reason="", readiness=false. Elapsed: 74.25422ms
Feb 28 17:03:31.673: INFO: Pod "pod-configmaps-1827dd53-f5fb-4fd6-aee1-81ce70982339": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.115684187s
STEP: Saw pod success
Feb 28 17:03:31.673: INFO: Pod "pod-configmaps-1827dd53-f5fb-4fd6-aee1-81ce70982339" satisfied condition "success or failure"
Feb 28 17:03:31.731: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-configmaps-1827dd53-f5fb-4fd6-aee1-81ce70982339 container env-test: <nil>
STEP: delete the pod
Feb 28 17:03:31.826: INFO: Waiting for pod pod-configmaps-1827dd53-f5fb-4fd6-aee1-81ce70982339 to disappear
Feb 28 17:03:31.871: INFO: Pod pod-configmaps-1827dd53-f5fb-4fd6-aee1-81ce70982339 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:03:31.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1409" for this suite.
Feb 28 17:03:38.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:03:39.990: INFO: namespace configmap-1409 deletion completed in 8.083773017s
•SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:03:39.991: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 17:03:40.308: INFO: Creating ReplicaSet my-hostname-basic-51a98416-2171-49ca-a7b2-ba694a549f7a
Feb 28 17:03:40.454: INFO: Pod name my-hostname-basic-51a98416-2171-49ca-a7b2-ba694a549f7a: Found 1 pods out of 1
Feb 28 17:03:40.454: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-51a98416-2171-49ca-a7b2-ba694a549f7a" is running
Feb 28 17:03:42.533: INFO: Pod "my-hostname-basic-51a98416-2171-49ca-a7b2-ba694a549f7a-6t9k5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-28 17:03:40 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-28 17:03:40 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-51a98416-2171-49ca-a7b2-ba694a549f7a]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-28 17:03:40 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-51a98416-2171-49ca-a7b2-ba694a549f7a]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-28 17:03:40 +0000 UTC Reason: Message:}])
Feb 28 17:03:42.533: INFO: Trying to dial the pod
Feb 28 17:03:47.642: INFO: Controller my-hostname-basic-51a98416-2171-49ca-a7b2-ba694a549f7a: Got expected result from replica 1 [my-hostname-basic-51a98416-2171-49ca-a7b2-ba694a549f7a-6t9k5]: "my-hostname-basic-51a98416-2171-49ca-a7b2-ba694a549f7a-6t9k5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:03:47.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5862" for this suite.
Feb 28 17:03:53.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:03:55.173: INFO: namespace replicaset-5862 deletion completed in 7.495041097s
•SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:03:55.174: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-02bd76c5-caf1-4966-9804-625b3eed8f61
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:03:55.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7301" for this suite.
Feb 28 17:04:01.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:04:02.976: INFO: namespace secrets-7301 deletion completed in 7.519383846s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:04:02.977: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 28 17:04:03.208: INFO: Waiting up to 5m0s for pod "pod-f0fdc78e-1eda-46e0-84ec-71523feba7ff" in namespace "emptydir-9745" to be "success or failure"
Feb 28 17:04:03.243: INFO: Pod "pod-f0fdc78e-1eda-46e0-84ec-71523feba7ff": Phase="Pending", Reason="", readiness=false. Elapsed: 34.938861ms
Feb 28 17:04:05.278: INFO: Pod "pod-f0fdc78e-1eda-46e0-84ec-71523feba7ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069732659s
Feb 28 17:04:07.316: INFO: Pod "pod-f0fdc78e-1eda-46e0-84ec-71523feba7ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.107962222s
STEP: Saw pod success
Feb 28 17:04:07.316: INFO: Pod "pod-f0fdc78e-1eda-46e0-84ec-71523feba7ff" satisfied condition "success or failure"
Feb 28 17:04:07.351: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-f0fdc78e-1eda-46e0-84ec-71523feba7ff container test-container: <nil>
STEP: delete the pod
Feb 28 17:04:07.446: INFO: Waiting for pod pod-f0fdc78e-1eda-46e0-84ec-71523feba7ff to disappear
Feb 28 17:04:07.481: INFO: Pod pod-f0fdc78e-1eda-46e0-84ec-71523feba7ff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:04:07.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9745" for this suite.
Feb 28 17:04:13.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:04:15.237: INFO: namespace emptydir-9745 deletion completed in 7.720446895s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:04:15.237: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 17:04:15.428: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f73331b5-f68b-4ecd-b32f-a2f01c2634b0" in namespace "downward-api-4060" to be "success or failure"
Feb 28 17:04:15.464: INFO: Pod "downwardapi-volume-f73331b5-f68b-4ecd-b32f-a2f01c2634b0": Phase="Pending", Reason="", readiness=false. Elapsed: 35.960472ms
Feb 28 17:04:17.499: INFO: Pod "downwardapi-volume-f73331b5-f68b-4ecd-b32f-a2f01c2634b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071358651s
Feb 28 17:04:19.538: INFO: Pod "downwardapi-volume-f73331b5-f68b-4ecd-b32f-a2f01c2634b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.109552767s
STEP: Saw pod success
Feb 28 17:04:19.538: INFO: Pod "downwardapi-volume-f73331b5-f68b-4ecd-b32f-a2f01c2634b0" satisfied condition "success or failure"
Feb 28 17:04:19.573: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod downwardapi-volume-f73331b5-f68b-4ecd-b32f-a2f01c2634b0 container client-container: <nil>
STEP: delete the pod
Feb 28 17:04:19.658: INFO: Waiting for pod downwardapi-volume-f73331b5-f68b-4ecd-b32f-a2f01c2634b0 to disappear
Feb 28 17:04:19.693: INFO: Pod downwardapi-volume-f73331b5-f68b-4ecd-b32f-a2f01c2634b0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:04:19.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4060" for this suite.
Feb 28 17:04:25.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:04:27.225: INFO: namespace downward-api-4060 deletion completed in 7.496221487s
•SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:04:27.225: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:04:34.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7981" for this suite.
Feb 28 17:04:40.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:04:41.854: INFO: namespace watch-7981 deletion completed in 7.514938968s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:04:41.855: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-wdhq
STEP: Creating a pod to test atomic-volume-subpath
Feb 28 17:04:42.167: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-wdhq" in namespace "subpath-3160" to be "success or failure"
Feb 28 17:04:42.201: INFO: Pod "pod-subpath-test-downwardapi-wdhq": Phase="Pending", Reason="", readiness=false. Elapsed: 34.693224ms
Feb 28 17:04:44.236: INFO: Pod "pod-subpath-test-downwardapi-wdhq": Phase="Running", Reason="", readiness=true. Elapsed: 2.069522384s
Feb 28 17:04:46.273: INFO: Pod "pod-subpath-test-downwardapi-wdhq": Phase="Running", Reason="", readiness=true. Elapsed: 4.105922453s
Feb 28 17:04:48.308: INFO: Pod "pod-subpath-test-downwardapi-wdhq": Phase="Running", Reason="", readiness=true. Elapsed: 6.140807486s
Feb 28 17:04:50.343: INFO: Pod "pod-subpath-test-downwardapi-wdhq": Phase="Running", Reason="", readiness=true. Elapsed: 8.175952634s
Feb 28 17:04:52.385: INFO: Pod "pod-subpath-test-downwardapi-wdhq": Phase="Running", Reason="", readiness=true. Elapsed: 10.218696938s
Feb 28 17:04:54.423: INFO: Pod "pod-subpath-test-downwardapi-wdhq": Phase="Running", Reason="", readiness=true. Elapsed: 12.255839526s
Feb 28 17:04:56.458: INFO: Pod "pod-subpath-test-downwardapi-wdhq": Phase="Running", Reason="", readiness=true. Elapsed: 14.29129116s
Feb 28 17:04:58.497: INFO: Pod "pod-subpath-test-downwardapi-wdhq": Phase="Running", Reason="", readiness=true. Elapsed: 16.330418407s
Feb 28 17:05:00.535: INFO: Pod "pod-subpath-test-downwardapi-wdhq": Phase="Running", Reason="", readiness=true. Elapsed: 18.368136501s
Feb 28 17:05:02.570: INFO: Pod "pod-subpath-test-downwardapi-wdhq": Phase="Running", Reason="", readiness=true. Elapsed: 20.403344041s
Feb 28 17:05:04.605: INFO: Pod "pod-subpath-test-downwardapi-wdhq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.438593163s
STEP: Saw pod success
Feb 28 17:05:04.605: INFO: Pod "pod-subpath-test-downwardapi-wdhq" satisfied condition "success or failure"
Feb 28 17:05:04.640: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-subpath-test-downwardapi-wdhq container test-container-subpath-downwardapi-wdhq: <nil>
STEP: delete the pod
Feb 28 17:05:04.725: INFO: Waiting for pod pod-subpath-test-downwardapi-wdhq to disappear
Feb 28 17:05:04.759: INFO: Pod pod-subpath-test-downwardapi-wdhq no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-wdhq
Feb 28 17:05:04.759: INFO: Deleting pod "pod-subpath-test-downwardapi-wdhq" in namespace "subpath-3160"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:05:04.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3160" for this suite.
Feb 28 17:05:10.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:05:12.330: INFO: namespace subpath-3160 deletion completed in 7.499875415s
•SSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:05:12.331: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 28 17:05:12.561: INFO: Waiting up to 5m0s for pod "downward-api-4863b90c-2e82-4b46-85ba-7828f241cda8" in namespace "downward-api-7273" to be "success or failure"
Feb 28 17:05:12.596: INFO: Pod "downward-api-4863b90c-2e82-4b46-85ba-7828f241cda8": Phase="Pending", Reason="", readiness=false. Elapsed: 35.399655ms
Feb 28 17:05:14.634: INFO: Pod "downward-api-4863b90c-2e82-4b46-85ba-7828f241cda8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.073656249s
STEP: Saw pod success
Feb 28 17:05:14.634: INFO: Pod "downward-api-4863b90c-2e82-4b46-85ba-7828f241cda8" satisfied condition "success or failure"
Feb 28 17:05:14.671: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod downward-api-4863b90c-2e82-4b46-85ba-7828f241cda8 container dapi-container: <nil>
STEP: delete the pod
Feb 28 17:05:14.757: INFO: Waiting for pod downward-api-4863b90c-2e82-4b46-85ba-7828f241cda8 to disappear
Feb 28 17:05:14.797: INFO: Pod downward-api-4863b90c-2e82-4b46-85ba-7828f241cda8 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:05:14.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7273" for this suite.
Feb 28 17:05:20.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:05:22.301: INFO: namespace downward-api-7273 deletion completed in 7.465078557s
•SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:05:22.301: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 28 17:05:22.741: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1561,SelfLink:/api/v1/namespaces/watch-1561/configmaps/e2e-watch-test-resource-version,UID:edaa10b4-ae2d-4710-8913-22fbd47b7b77,ResourceVersion:16950,Generation:0,CreationTimestamp:2020-02-28 17:05:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 28 17:05:22.742: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1561,SelfLink:/api/v1/namespaces/watch-1561/configmaps/e2e-watch-test-resource-version,UID:edaa10b4-ae2d-4710-8913-22fbd47b7b77,ResourceVersion:16951,Generation:0,CreationTimestamp:2020-02-28 17:05:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:05:22.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1561" for this suite.
Feb 28 17:05:30.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:05:32.249: INFO: namespace watch-1561 deletion completed in 9.46879782s
•SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:05:32.249: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:05:57.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6527" for this suite.
Feb 28 17:06:03.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:06:04.644: INFO: namespace container-runtime-6527 deletion completed in 7.482652506s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:06:04.645: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2343.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2343.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2343.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2343.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2343.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2343.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2343.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2343.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2343.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2343.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2343.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 30.159.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.159.30_udp@PTR;check="$$(dig +tcp +noall +answer +search 30.159.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.159.30_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2343.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2343.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2343.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2343.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2343.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2343.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2343.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2343.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2343.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2343.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2343.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 30.159.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.159.30_udp@PTR;check="$$(dig +tcp +noall +answer +search 30.159.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.159.30_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 28 17:06:07.069: INFO: Unable to read wheezy_udp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:07.105: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:07.141: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:07.177: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:07.432: INFO: Unable to read jessie_udp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:07.468: INFO: Unable to read jessie_tcp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:07.504: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:07.539: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:07.756: INFO: Lookups using dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec failed for: [wheezy_udp@dns-test-service.dns-2343.svc.cluster.local wheezy_tcp@dns-test-service.dns-2343.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local jessie_udp@dns-test-service.dns-2343.svc.cluster.local jessie_tcp@dns-test-service.dns-2343.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local]

Feb 28 17:06:12.794: INFO: Unable to read wheezy_udp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:12.829: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:12.866: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:12.904: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:13.162: INFO: Unable to read jessie_udp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:13.205: INFO: Unable to read jessie_tcp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:13.241: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:13.277: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:13.494: INFO: Lookups using dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec failed for: [wheezy_udp@dns-test-service.dns-2343.svc.cluster.local wheezy_tcp@dns-test-service.dns-2343.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local jessie_udp@dns-test-service.dns-2343.svc.cluster.local jessie_tcp@dns-test-service.dns-2343.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local]

Feb 28 17:06:17.799: INFO: Unable to read wheezy_udp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:17.834: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:17.872: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:17.907: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:18.162: INFO: Unable to read jessie_udp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:18.198: INFO: Unable to read jessie_tcp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:18.234: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:18.270: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:18.487: INFO: Lookups using dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec failed for: [wheezy_udp@dns-test-service.dns-2343.svc.cluster.local wheezy_tcp@dns-test-service.dns-2343.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local jessie_udp@dns-test-service.dns-2343.svc.cluster.local jessie_tcp@dns-test-service.dns-2343.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local]

Feb 28 17:06:22.793: INFO: Unable to read wheezy_udp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:22.828: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:22.864: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:22.900: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:23.152: INFO: Unable to read jessie_udp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:23.188: INFO: Unable to read jessie_tcp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:23.225: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:23.261: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:23.482: INFO: Lookups using dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec failed for: [wheezy_udp@dns-test-service.dns-2343.svc.cluster.local wheezy_tcp@dns-test-service.dns-2343.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local jessie_udp@dns-test-service.dns-2343.svc.cluster.local jessie_tcp@dns-test-service.dns-2343.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local]

Feb 28 17:06:27.802: INFO: Unable to read wheezy_udp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:27.838: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:27.875: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:27.911: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:28.168: INFO: Unable to read jessie_udp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:28.204: INFO: Unable to read jessie_tcp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:28.241: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:28.279: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:28.506: INFO: Lookups using dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec failed for: [wheezy_udp@dns-test-service.dns-2343.svc.cluster.local wheezy_tcp@dns-test-service.dns-2343.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local jessie_udp@dns-test-service.dns-2343.svc.cluster.local jessie_tcp@dns-test-service.dns-2343.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local]

Feb 28 17:06:32.792: INFO: Unable to read wheezy_udp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:32.829: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:32.864: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:32.900: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:33.155: INFO: Unable to read jessie_udp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:33.192: INFO: Unable to read jessie_tcp@dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:33.227: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:33.263: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local from pod dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec: the server could not find the requested resource (get pods dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec)
Feb 28 17:06:33.486: INFO: Lookups using dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec failed for: [wheezy_udp@dns-test-service.dns-2343.svc.cluster.local wheezy_tcp@dns-test-service.dns-2343.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local jessie_udp@dns-test-service.dns-2343.svc.cluster.local jessie_tcp@dns-test-service.dns-2343.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2343.svc.cluster.local]

Feb 28 17:06:38.487: INFO: DNS probes using dns-2343/dns-test-2e3c2ca6-2212-41a9-b4e5-ff9001d397ec succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:06:38.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2343" for this suite.
Feb 28 17:06:44.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:06:46.147: INFO: namespace dns-2343 deletion completed in 7.458752802s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:06:46.148: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-db57ebe8-4f0c-45e9-986f-1afbe967d557
STEP: Creating secret with name secret-projected-all-test-volume-321d2230-1e3a-47b7-8635-e88540c7c26d
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 28 17:06:46.449: INFO: Waiting up to 5m0s for pod "projected-volume-5d1b69e1-510b-4772-ba90-1bb7338c4f52" in namespace "projected-7341" to be "success or failure"
Feb 28 17:06:46.484: INFO: Pod "projected-volume-5d1b69e1-510b-4772-ba90-1bb7338c4f52": Phase="Pending", Reason="", readiness=false. Elapsed: 35.7461ms
Feb 28 17:06:48.520: INFO: Pod "projected-volume-5d1b69e1-510b-4772-ba90-1bb7338c4f52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.071825919s
STEP: Saw pod success
Feb 28 17:06:48.521: INFO: Pod "projected-volume-5d1b69e1-510b-4772-ba90-1bb7338c4f52" satisfied condition "success or failure"
Feb 28 17:06:48.555: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod projected-volume-5d1b69e1-510b-4772-ba90-1bb7338c4f52 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 28 17:06:48.650: INFO: Waiting for pod projected-volume-5d1b69e1-510b-4772-ba90-1bb7338c4f52 to disappear
Feb 28 17:06:48.684: INFO: Pod projected-volume-5d1b69e1-510b-4772-ba90-1bb7338c4f52 no longer exists
[AfterEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:06:48.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7341" for this suite.
Feb 28 17:06:54.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:06:56.235: INFO: namespace projected-7341 deletion completed in 7.514920822s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:06:56.236: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Feb 28 17:06:56.391: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config api-versions'
Feb 28 17:06:57.051: INFO: stderr: ""
Feb 28 17:06:57.051: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscalingpolicy.kope.io/v1alpha1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:06:57.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9741" for this suite.
Feb 28 17:07:03.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:07:04.579: INFO: namespace kubectl-9741 deletion completed in 7.491371517s
•S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:07:04.579: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Feb 28 17:07:04.772: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-2343'
Feb 28 17:07:05.905: INFO: stderr: ""
Feb 28 17:07:05.905: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 17:07:05.905: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2343'
Feb 28 17:07:06.277: INFO: stderr: ""
Feb 28 17:07:06.277: INFO: stdout: "update-demo-nautilus-cdwc8 update-demo-nautilus-mx6qp "
Feb 28 17:07:06.277: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-cdwc8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2343'
Feb 28 17:07:06.607: INFO: stderr: ""
Feb 28 17:07:06.607: INFO: stdout: ""
Feb 28 17:07:06.607: INFO: update-demo-nautilus-cdwc8 is created but not running
Feb 28 17:07:11.608: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2343'
Feb 28 17:07:11.996: INFO: stderr: ""
Feb 28 17:07:11.996: INFO: stdout: "update-demo-nautilus-cdwc8 update-demo-nautilus-mx6qp "
Feb 28 17:07:11.996: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-cdwc8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2343'
Feb 28 17:07:12.405: INFO: stderr: ""
Feb 28 17:07:12.405: INFO: stdout: "true"
Feb 28 17:07:12.405: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-cdwc8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2343'
Feb 28 17:07:12.697: INFO: stderr: ""
Feb 28 17:07:12.697: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 17:07:12.697: INFO: validating pod update-demo-nautilus-cdwc8
Feb 28 17:07:12.734: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 17:07:12.734: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 17:07:12.734: INFO: update-demo-nautilus-cdwc8 is verified up and running
Feb 28 17:07:12.734: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-mx6qp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2343'
Feb 28 17:07:13.018: INFO: stderr: ""
Feb 28 17:07:13.018: INFO: stdout: "true"
Feb 28 17:07:13.018: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-mx6qp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2343'
Feb 28 17:07:13.293: INFO: stderr: ""
Feb 28 17:07:13.293: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 17:07:13.293: INFO: validating pod update-demo-nautilus-mx6qp
Feb 28 17:07:13.330: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 17:07:13.331: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 17:07:13.331: INFO: update-demo-nautilus-mx6qp is verified up and running
STEP: using delete to clean up resources
Feb 28 17:07:13.331: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-2343'
Feb 28 17:07:13.640: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 17:07:13.640: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 28 17:07:13.640: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2343'
Feb 28 17:07:13.958: INFO: stderr: "No resources found.\n"
Feb 28 17:07:13.958: INFO: stdout: ""
Feb 28 17:07:13.958: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -l name=update-demo --namespace=kubectl-2343 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 17:07:14.226: INFO: stderr: ""
Feb 28 17:07:14.226: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:07:14.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2343" for this suite.
Feb 28 17:07:36.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:07:37.725: INFO: namespace kubectl-2343 deletion completed in 23.462554926s
•SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:07:37.725: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 17:07:37.925: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-2724'
Feb 28 17:07:38.469: INFO: stderr: ""
Feb 28 17:07:38.469: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 28 17:07:38.469: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-2724'
Feb 28 17:07:39.069: INFO: stderr: ""
Feb 28 17:07:39.069: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 28 17:07:40.105: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 17:07:40.105: INFO: Found 0 / 1
Feb 28 17:07:41.105: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 17:07:41.105: INFO: Found 1 / 1
Feb 28 17:07:41.105: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 28 17:07:41.140: INFO: Selector matched 1 pods for map[app:redis]
Feb 28 17:07:41.140: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 28 17:07:41.140: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config describe pod redis-master-hbkrh --namespace=kubectl-2724'
Feb 28 17:07:41.465: INFO: stderr: ""
Feb 28 17:07:41.465: INFO: stdout: "Name:           redis-master-hbkrh\nNamespace:      kubectl-2724\nPriority:       0\nNode:           bootstrap-e2e-minion-group-s43f/10.138.0.5\nStart Time:     Fri, 28 Feb 2020 17:07:38 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             10.64.2.116\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://59f16f544e9668b54f37a73f9d00caf20757969990157cff90ab4f64d31fc59b\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 28 Feb 2020 17:07:39 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-pfwsk (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-pfwsk:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-pfwsk\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                      Message\n  ----    ------     ----  ----                                      -------\n  Normal  Scheduled  3s    default-scheduler                         Successfully assigned kubectl-2724/redis-master-hbkrh to bootstrap-e2e-minion-group-s43f\n  Normal  Pulled     2s    kubelet, bootstrap-e2e-minion-group-s43f  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, bootstrap-e2e-minion-group-s43f  Created container redis-master\n  Normal  Started    2s    kubelet, bootstrap-e2e-minion-group-s43f  Started container redis-master\n"
Feb 28 17:07:41.465: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config describe rc redis-master --namespace=kubectl-2724'
Feb 28 17:07:41.863: INFO: stderr: ""
Feb 28 17:07:41.863: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-2724\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-hbkrh\n"
Feb 28 17:07:41.863: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config describe service redis-master --namespace=kubectl-2724'
Feb 28 17:07:42.287: INFO: stderr: ""
Feb 28 17:07:42.287: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-2724\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.0.162.20\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.64.2.116:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 28 17:07:42.322: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config describe node bootstrap-e2e-master'
Feb 28 17:07:42.797: INFO: stderr: ""
Feb 28 17:07:42.797: INFO: stdout: "Name:               bootstrap-e2e-master\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=n1-standard-1\n                    beta.kubernetes.io/metadata-proxy-ready=true\n                    beta.kubernetes.io/os=linux\n                    cloud.google.com/metadata-proxy-ready=true\n                    failure-domain.beta.kubernetes.io/region=us-west1\n                    failure-domain.beta.kubernetes.io/zone=us-west1-b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=bootstrap-e2e-master\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 28 Feb 2020 15:42:14 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\n                    node.kubernetes.io/unschedulable:NoSchedule\nUnschedulable:      true\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 28 Feb 2020 15:42:22 +0000   Fri, 28 Feb 2020 15:42:22 +0000   RouteCreated                 RouteController created a route\n  MemoryPressure       False   Fri, 28 Feb 2020 17:07:05 +0000   Fri, 28 Feb 2020 15:42:14 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 28 Feb 2020 17:07:05 +0000   Fri, 28 Feb 2020 15:42:14 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 28 Feb 2020 17:07:05 +0000   Fri, 28 Feb 2020 15:42:14 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 28 Feb 2020 17:07:05 +0000   Fri, 28 Feb 2020 15:42:14 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.138.0.2\n  ExternalIP:   34.82.216.130\n  InternalDNS:  bootstrap-e2e-master.c.gce-gci-upg-1-3-1-4-ctl-skew.internal\n  Hostname:     bootstrap-e2e-master.c.gce-gci-upg-1-3-1-4-ctl-skew.internal\nCapacity:\n attachable-volumes-gce-pd:  127\n cpu:                        1\n ephemeral-storage:          16293736Ki\n hugepages-2Mi:              0\n memory:                     3787516Ki\n pods:                       110\nAllocatable:\n attachable-volumes-gce-pd:  127\n cpu:                        1\n ephemeral-storage:          15016307073\n hugepages-2Mi:              0\n memory:                     3531516Ki\n pods:                       110\nSystem Info:\n Machine ID:                 221188c2dba163e84bb8262cfcb9fccb\n System UUID:                221188C2-DBA1-63E8-4BB8-262CFCB9FCCB\n Boot ID:                    86fcfe82-363d-4fe1-a8b5-0452b40a358e\n Kernel Version:             4.14.94+\n OS Image:                   Container-Optimized OS from Google\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.3\n Kubelet Version:            v1.15.11-beta.0.18+89a7468341d588\n Kube-Proxy Version:         v1.15.11-beta.0.18+89a7468341d588\nPodCIDR:                     10.64.0.0/24\nProviderID:                  gce://gce-gci-upg-1-3-1-4-ctl-skew/us-west1-b/bootstrap-e2e-master\nNon-terminated Pods:         (10 in total)\n  Namespace                  Name                                            CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                            ------------  ----------  ---------------  -------------  ---\n  kube-system                etcd-empty-dir-cleanup-bootstrap-e2e-master     0 (0%)        0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                etcd-server-bootstrap-e2e-master                200m (20%)    0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                etcd-server-events-bootstrap-e2e-master         100m (10%)    0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                fluentd-gcp-v3.2.0-bcfjp                        100m (10%)    1 (100%)    200Mi (5%)       500Mi (14%)    85m\n  kube-system                kube-addon-manager-bootstrap-e2e-master         5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         84m\n  kube-system                kube-apiserver-bootstrap-e2e-master             250m (25%)    0 (0%)      0 (0%)           0 (0%)         85m\n  kube-system                kube-controller-manager-bootstrap-e2e-master    200m (20%)    0 (0%)      0 (0%)           0 (0%)         85m\n  kube-system                kube-scheduler-bootstrap-e2e-master             75m (7%)      0 (0%)      0 (0%)           0 (0%)         85m\n  kube-system                l7-lb-controller-v1.2.3-bootstrap-e2e-master    10m (1%)      0 (0%)      50Mi (1%)        0 (0%)         84m\n  kube-system                metadata-proxy-v0.1-gplzl                       32m (3%)      32m (3%)    45Mi (1%)        45Mi (1%)      85m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests     Limits\n  --------                   --------     ------\n  cpu                        972m (97%)   1032m (103%)\n  memory                     345Mi (10%)  545Mi (15%)\n  ephemeral-storage          0 (0%)       0 (0%)\n  attachable-volumes-gce-pd  0            0\nEvents:                      <none>\n"
Feb 28 17:07:42.798: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config describe namespace kubectl-2724'
Feb 28 17:07:43.223: INFO: stderr: ""
Feb 28 17:07:43.223: INFO: stdout: "Name:         kubectl-2724\nLabels:       e2e-framework=kubectl\n              e2e-run=f2d2476f-1135-406d-947a-9a4fd4eebadc\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:07:43.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2724" for this suite.
Feb 28 17:08:05.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:08:06.759: INFO: namespace kubectl-2724 deletion completed in 23.49585648s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:08:06.760: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 28 17:08:06.994: INFO: Waiting up to 5m0s for pod "downward-api-106dbf63-1732-4a1e-9384-d6c20628a21a" in namespace "downward-api-7923" to be "success or failure"
Feb 28 17:08:07.028: INFO: Pod "downward-api-106dbf63-1732-4a1e-9384-d6c20628a21a": Phase="Pending", Reason="", readiness=false. Elapsed: 34.434901ms
Feb 28 17:08:09.063: INFO: Pod "downward-api-106dbf63-1732-4a1e-9384-d6c20628a21a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.069671622s
STEP: Saw pod success
Feb 28 17:08:09.063: INFO: Pod "downward-api-106dbf63-1732-4a1e-9384-d6c20628a21a" satisfied condition "success or failure"
Feb 28 17:08:09.099: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod downward-api-106dbf63-1732-4a1e-9384-d6c20628a21a container dapi-container: <nil>
STEP: delete the pod
Feb 28 17:08:09.187: INFO: Waiting for pod downward-api-106dbf63-1732-4a1e-9384-d6c20628a21a to disappear
Feb 28 17:08:09.227: INFO: Pod downward-api-106dbf63-1732-4a1e-9384-d6c20628a21a no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:08:09.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7923" for this suite.
Feb 28 17:08:15.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:08:16.805: INFO: namespace downward-api-7923 deletion completed in 7.541836739s
•SSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:08:16.807: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:08:17.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4079" for this suite.
Feb 28 17:08:23.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:08:24.739: INFO: namespace services-4079 deletion completed in 7.66907052s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:08:24.739: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-89cfcd6a-cbc0-474a-8d71-d0c04828415d
STEP: Creating a pod to test consume secrets
Feb 28 17:08:25.019: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-121dc12e-365e-485b-bedf-9f68be3a042e" in namespace "projected-6036" to be "success or failure"
Feb 28 17:08:25.081: INFO: Pod "pod-projected-secrets-121dc12e-365e-485b-bedf-9f68be3a042e": Phase="Pending", Reason="", readiness=false. Elapsed: 62.117417ms
Feb 28 17:08:27.118: INFO: Pod "pod-projected-secrets-121dc12e-365e-485b-bedf-9f68be3a042e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.099506839s
Feb 28 17:08:29.157: INFO: Pod "pod-projected-secrets-121dc12e-365e-485b-bedf-9f68be3a042e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.138636939s
STEP: Saw pod success
Feb 28 17:08:29.157: INFO: Pod "pod-projected-secrets-121dc12e-365e-485b-bedf-9f68be3a042e" satisfied condition "success or failure"
Feb 28 17:08:29.193: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-projected-secrets-121dc12e-365e-485b-bedf-9f68be3a042e container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 28 17:08:29.294: INFO: Waiting for pod pod-projected-secrets-121dc12e-365e-485b-bedf-9f68be3a042e to disappear
Feb 28 17:08:29.330: INFO: Pod pod-projected-secrets-121dc12e-365e-485b-bedf-9f68be3a042e no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:08:29.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6036" for this suite.
Feb 28 17:08:35.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:08:36.914: INFO: namespace projected-6036 deletion completed in 7.547701744s
•SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:08:36.915: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Feb 28 17:08:37.187: INFO: observed the pod list
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:08:48.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6513" for this suite.
Feb 28 17:08:54.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:08:56.334: INFO: namespace pods-6513 deletion completed in 7.592082307s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:08:56.337: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-b6f8fc3b-3eac-4741-bdf1-b2ec197208d2
STEP: Creating configMap with name cm-test-opt-upd-3d8cdacf-3fb2-4d2b-9e6f-53654544c656
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b6f8fc3b-3eac-4741-bdf1-b2ec197208d2
STEP: Updating configmap cm-test-opt-upd-3d8cdacf-3fb2-4d2b-9e6f-53654544c656
STEP: Creating configMap with name cm-test-opt-create-f54c4178-3303-4509-b2f9-b9b32a41a01b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:09:01.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4576" for this suite.
Feb 28 17:09:23.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:09:25.138: INFO: namespace projected-4576 deletion completed in 23.822910858s
•SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:09:25.138: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-42baf156-f2a1-4e99-b3c9-c3bf77c18a24 in namespace container-probe-6579
Feb 28 17:09:27.545: INFO: Started pod busybox-42baf156-f2a1-4e99-b3c9-c3bf77c18a24 in namespace container-probe-6579
STEP: checking the pod's current state and verifying that restartCount is present
Feb 28 17:09:27.582: INFO: Initial restart count of pod busybox-42baf156-f2a1-4e99-b3c9-c3bf77c18a24 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:13:28.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6579" for this suite.
Feb 28 17:13:34.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:13:35.912: INFO: namespace container-probe-6579 deletion completed in 7.509012704s
•SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:13:35.912: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Feb 28 17:13:36.110: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 28 17:13:36.110: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-1465'
Feb 28 17:13:37.604: INFO: stderr: ""
Feb 28 17:13:37.604: INFO: stdout: "service/redis-slave created\n"
Feb 28 17:13:37.605: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 28 17:13:37.605: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-1465'
Feb 28 17:13:39.031: INFO: stderr: ""
Feb 28 17:13:39.031: INFO: stdout: "service/redis-master created\n"
Feb 28 17:13:39.032: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 28 17:13:39.032: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-1465'
Feb 28 17:13:39.979: INFO: stderr: ""
Feb 28 17:13:39.979: INFO: stdout: "service/frontend created\n"
Feb 28 17:13:39.980: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 28 17:13:39.980: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-1465'
Feb 28 17:13:41.123: INFO: stderr: ""
Feb 28 17:13:41.123: INFO: stdout: "deployment.apps/frontend created\n"
Feb 28 17:13:41.123: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 28 17:13:41.124: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-1465'
Feb 28 17:13:42.038: INFO: stderr: ""
Feb 28 17:13:42.038: INFO: stdout: "deployment.apps/redis-master created\n"
Feb 28 17:13:42.039: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 28 17:13:42.039: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-1465'
Feb 28 17:13:43.166: INFO: stderr: ""
Feb 28 17:13:43.166: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Feb 28 17:13:43.166: INFO: Waiting for all frontend pods to be Running.
Feb 28 17:14:03.231: INFO: Waiting for frontend to serve content.
Feb 28 17:14:03.280: INFO: Trying to add a new entry to the guestbook.
Feb 28 17:14:03.328: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 28 17:14:03.374: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-1465'
Feb 28 17:14:03.813: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 17:14:03.813: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 17:14:03.813: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-1465'
Feb 28 17:14:04.294: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 17:14:04.294: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 17:14:04.295: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-1465'
Feb 28 17:14:04.782: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 17:14:04.782: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 17:14:04.782: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-1465'
Feb 28 17:14:05.367: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 17:14:05.367: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 17:14:05.368: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-1465'
Feb 28 17:14:06.126: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 17:14:06.126: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 28 17:14:06.126: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-1465'
Feb 28 17:14:06.755: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 17:14:06.755: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:14:06.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1465" for this suite.
Feb 28 17:14:50.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:14:52.489: INFO: namespace kubectl-1465 deletion completed in 45.698677656s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:14:52.490: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 17:14:52.673: INFO: Creating deployment "nginx-deployment"
Feb 28 17:14:52.715: INFO: Waiting for observed generation 1
Feb 28 17:14:52.779: INFO: Waiting for all required pods to come up
Feb 28 17:14:52.859: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 28 17:14:56.999: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 28 17:14:57.069: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 28 17:14:57.144: INFO: Updating deployment nginx-deployment
Feb 28 17:14:57.144: INFO: Waiting for observed generation 2
Feb 28 17:14:59.251: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 28 17:14:59.286: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 28 17:14:59.321: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 28 17:14:59.425: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 28 17:14:59.426: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 28 17:14:59.461: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 28 17:14:59.531: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 28 17:14:59.531: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 28 17:14:59.604: INFO: Updating deployment nginx-deployment
Feb 28 17:14:59.604: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 28 17:14:59.767: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 28 17:14:59.902: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 28 17:15:00.074: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-9625,SelfLink:/apis/apps/v1/namespaces/deployment-9625/deployments/nginx-deployment,UID:b3a74a04-5bfc-4202-9668-5561f8a30aa9,ResourceVersion:18648,Generation:3,CreationTimestamp:2020-02-28 17:14:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2020-02-28 17:14:57 +0000 UTC 2020-02-28 17:14:52 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2020-02-28 17:14:59 +0000 UTC 2020-02-28 17:14:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb 28 17:15:00.122: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-9625,SelfLink:/apis/apps/v1/namespaces/deployment-9625/replicasets/nginx-deployment-55fb7cb77f,UID:a28d5c54-1856-4d19-84f9-43b7ad5a0cfb,ResourceVersion:18690,Generation:3,CreationTimestamp:2020-02-28 17:14:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment b3a74a04-5bfc-4202-9668-5561f8a30aa9 0xc000b4acb7 0xc000b4acb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 28 17:15:00.122: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 28 17:15:00.122: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-9625,SelfLink:/apis/apps/v1/namespaces/deployment-9625/replicasets/nginx-deployment-7b8c6f4498,UID:1feb41ff-018e-4251-8425-445ad3e45658,ResourceVersion:18689,Generation:3,CreationTimestamp:2020-02-28 17:14:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment b3a74a04-5bfc-4202-9668-5561f8a30aa9 0xc000b4ada7 0xc000b4ada8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb 28 17:15:00.179: INFO: Pod "nginx-deployment-55fb7cb77f-5sn4j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-5sn4j,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-55fb7cb77f-5sn4j,UID:f2de5952-88ed-4fa3-8528-476313794e8d,ResourceVersion:18665,Generation:0,CreationTimestamp:2020-02-28 17:14:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a28d5c54-1856-4d19-84f9-43b7ad5a0cfb 0xc00369fae7 0xc00369fae8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-t863,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00369fb50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00369fb70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.3,PodIP:10.64.1.34,StartTime:2020-02-28 17:14:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.179: INFO: Pod "nginx-deployment-55fb7cb77f-g4ztz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-g4ztz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-55fb7cb77f-g4ztz,UID:92531bc4-b509-49ab-9c9d-313580429069,ResourceVersion:18628,Generation:0,CreationTimestamp:2020-02-28 17:14:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a28d5c54-1856-4d19-84f9-43b7ad5a0cfb 0xc00369fc60 0xc00369fc61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-q1l7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00369fcd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00369fcf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2020-02-28 17:14:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.179: INFO: Pod "nginx-deployment-55fb7cb77f-gtx5f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-gtx5f,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-55fb7cb77f-gtx5f,UID:3d95c6f9-aeae-4cf3-9b00-fc544f9d663d,ResourceVersion:18676,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a28d5c54-1856-4d19-84f9-43b7ad5a0cfb 0xc00369fdc0 0xc00369fdc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-q1l7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00369fe30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00369fe50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.180: INFO: Pod "nginx-deployment-55fb7cb77f-jbn6g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-jbn6g,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-55fb7cb77f-jbn6g,UID:1309ba76-a912-4682-9131-a425b43c13ab,ResourceVersion:18678,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a28d5c54-1856-4d19-84f9-43b7ad5a0cfb 0xc00369fed0 0xc00369fed1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00369ff40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00369ff60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.180: INFO: Pod "nginx-deployment-55fb7cb77f-jl7dl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-jl7dl,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-55fb7cb77f-jl7dl,UID:0d5f992c-0bd4-48e6-8301-5332f102c5b7,ResourceVersion:18616,Generation:0,CreationTimestamp:2020-02-28 17:14:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a28d5c54-1856-4d19-84f9-43b7ad5a0cfb 0xc00369ffe0 0xc00369ffe1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-q1l7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003270050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003270070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2020-02-28 17:14:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.180: INFO: Pod "nginx-deployment-55fb7cb77f-lmw5x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-lmw5x,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-55fb7cb77f-lmw5x,UID:258f72b5-0c58-4c03-a558-ea1c73c7981c,ResourceVersion:18680,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a28d5c54-1856-4d19-84f9-43b7ad5a0cfb 0xc003270140 0xc003270141}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-t863,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032701b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032701d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.180: INFO: Pod "nginx-deployment-55fb7cb77f-nvq6z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-nvq6z,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-55fb7cb77f-nvq6z,UID:6a7a82a9-b450-4b7c-8f84-afad0fa3ba2f,ResourceVersion:18630,Generation:0,CreationTimestamp:2020-02-28 17:14:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a28d5c54-1856-4d19-84f9-43b7ad5a0cfb 0xc003270250 0xc003270251}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032702c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032702e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2020-02-28 17:14:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.180: INFO: Pod "nginx-deployment-55fb7cb77f-rhjbv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rhjbv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-55fb7cb77f-rhjbv,UID:08bb2f04-8124-4378-b401-2129e1383c93,ResourceVersion:18691,Generation:0,CreationTimestamp:2020-02-28 17:14:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a28d5c54-1856-4d19-84f9-43b7ad5a0cfb 0xc0032703b0 0xc0032703b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003270420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003270440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:57 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.2.123,StartTime:2020-02-28 17:14:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.180: INFO: Pod "nginx-deployment-55fb7cb77f-sm7m4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-sm7m4,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-55fb7cb77f-sm7m4,UID:fb85d7f3-7062-4ea3-8768-133fb4f05ce2,ResourceVersion:18687,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a28d5c54-1856-4d19-84f9-43b7ad5a0cfb 0xc003270530 0xc003270531}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-t863,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032705a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032705c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2020-02-28 17:14:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.180: INFO: Pod "nginx-deployment-55fb7cb77f-td2pd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-td2pd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-55fb7cb77f-td2pd,UID:33cac1b4-37fc-49bf-9bbe-fc3c12b45258,ResourceVersion:18683,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a28d5c54-1856-4d19-84f9-43b7ad5a0cfb 0xc003270690 0xc003270691}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003270700} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003270720}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.180: INFO: Pod "nginx-deployment-55fb7cb77f-wcdb5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wcdb5,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-55fb7cb77f-wcdb5,UID:c56ffd4b-49de-4ca2-b729-c8e95c993a28,ResourceVersion:18668,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a28d5c54-1856-4d19-84f9-43b7ad5a0cfb 0xc0032707a0 0xc0032707a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003270810} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003270830}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2020-02-28 17:14:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.181: INFO: Pod "nginx-deployment-55fb7cb77f-wp468" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wp468,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-55fb7cb77f-wp468,UID:e74a1e95-94c2-46bd-ad9e-3b4b6782b6b4,ResourceVersion:18688,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a28d5c54-1856-4d19-84f9-43b7ad5a0cfb 0xc003270900 0xc003270901}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-q1l7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003270970} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003270990}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2020-02-28 17:14:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.181: INFO: Pod "nginx-deployment-55fb7cb77f-x4rck" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-x4rck,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-55fb7cb77f-x4rck,UID:e1fe2648-4062-44d8-8a78-d00bd8c03192,ResourceVersion:18675,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f a28d5c54-1856-4d19-84f9-43b7ad5a0cfb 0xc003270a60 0xc003270a61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003270ad0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003270af0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.181: INFO: Pod "nginx-deployment-7b8c6f4498-4stzp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4stzp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-4stzp,UID:dfc6ca83-d5b8-4bd8-8b8e-12513b6f58a4,ResourceVersion:18591,Generation:0,CreationTimestamp:2020-02-28 17:14:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003270b70 0xc003270b71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-q1l7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003270bd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003270bf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.4,PodIP:10.64.3.150,StartTime:2020-02-28 17:14:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-28 17:14:55 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://967c55206327cc8dc262951f40f4a8a32527b21cc2ab5cdf937bad932c8f59a8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.181: INFO: Pod "nginx-deployment-7b8c6f4498-65p7g" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-65p7g,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-65p7g,UID:24537ff9-f2f2-428f-b268-db3c8494be9b,ResourceVersion:18566,Generation:0,CreationTimestamp:2020-02-28 17:14:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003270cc0 0xc003270cc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003270d20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003270d40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.2.122,StartTime:2020-02-28 17:14:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-28 17:14:55 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://fba12fc37b7db9387d0ab448b19629fbc07b2c2bd48a1253d5ff6128b553ca48}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.181: INFO: Pod "nginx-deployment-7b8c6f4498-6cgxz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-6cgxz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-6cgxz,UID:9e9e316a-8ede-444a-8804-810b3b8a435f,ResourceVersion:18693,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003270e10 0xc003270e11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-t863,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003270e70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003270e90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2020-02-28 17:14:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.181: INFO: Pod "nginx-deployment-7b8c6f4498-7xbgx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7xbgx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-7xbgx,UID:cc4ef167-48d3-451a-a96e-6ebf02f2e62e,ResourceVersion:18696,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003270f50 0xc003270f51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003270fb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003270fd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2020-02-28 17:14:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.183: INFO: Pod "nginx-deployment-7b8c6f4498-8nwt4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8nwt4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-8nwt4,UID:6c88580c-70b0-4ff9-be4b-c686705f48e9,ResourceVersion:18681,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003271090 0xc003271091}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-t863,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032710f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003271110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.187: INFO: Pod "nginx-deployment-7b8c6f4498-8wbvv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8wbvv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-8wbvv,UID:204195b4-1a46-4e0d-95b2-12d7b262b0c7,ResourceVersion:18695,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003271190 0xc003271191}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-t863,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032711f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003271210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2020-02-28 17:14:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.190: INFO: Pod "nginx-deployment-7b8c6f4498-98cbs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-98cbs,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-98cbs,UID:5ca56e5e-fc22-4d27-bb91-b94421929ece,ResourceVersion:18694,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc0032712e0 0xc0032712e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-q1l7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003271340} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003271360}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2020-02-28 17:14:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.192: INFO: Pod "nginx-deployment-7b8c6f4498-b8hct" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-b8hct,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-b8hct,UID:ea1c118d-3d30-43e5-b30e-3ea0f37b1319,ResourceVersion:18684,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003271430 0xc003271431}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-q1l7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003271490} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032714b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.195: INFO: Pod "nginx-deployment-7b8c6f4498-d5fzv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-d5fzv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-d5fzv,UID:d5e3aba0-5c3f-453d-bd64-ab73e4d60ac7,ResourceVersion:18686,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003271530 0xc003271531}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003271590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032715b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.202: INFO: Pod "nginx-deployment-7b8c6f4498-fhdxm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fhdxm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-fhdxm,UID:b9796044-ddc2-4546-8123-1e0a93f95319,ResourceVersion:18575,Generation:0,CreationTimestamp:2020-02-28 17:14:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003271630 0xc003271631}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-t863,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003271690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032716b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.3,PodIP:10.64.1.33,StartTime:2020-02-28 17:14:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-28 17:14:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://9260666029b3a797ee20161a5a5c314aae804c194c2f102179730d6b9fb60f2c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.206: INFO: Pod "nginx-deployment-7b8c6f4498-fnlpz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fnlpz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-fnlpz,UID:6d0838cf-3d57-4c7e-a5d6-6a7dd471c1fe,ResourceVersion:18585,Generation:0,CreationTimestamp:2020-02-28 17:14:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003271780 0xc003271781}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-q1l7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032717e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003271800}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.4,PodIP:10.64.3.154,StartTime:2020-02-28 17:14:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-28 17:14:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://921d25030ef20269475c1b85594c17776f630bf5bc11e2b8b9cf2ba0614243cf}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.209: INFO: Pod "nginx-deployment-7b8c6f4498-gcqvk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gcqvk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-gcqvk,UID:45cb715e-c7f6-4dd5-8512-633e890e2014,ResourceVersion:18677,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc0032718d0 0xc0032718d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-q1l7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003271930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003271950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2020-02-28 17:14:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.213: INFO: Pod "nginx-deployment-7b8c6f4498-hppfv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hppfv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-hppfv,UID:b2d81b9f-b1dc-4d63-85de-0338f29949b0,ResourceVersion:18663,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003271a10 0xc003271a11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-q1l7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003271a70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003271a90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2020-02-28 17:14:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.216: INFO: Pod "nginx-deployment-7b8c6f4498-jrmxx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jrmxx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-jrmxx,UID:25422b31-c5e8-4e33-9d50-56cfea4a72ac,ResourceVersion:18572,Generation:0,CreationTimestamp:2020-02-28 17:14:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003271b50 0xc003271b51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003271bb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003271bd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.2.121,StartTime:2020-02-28 17:14:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-28 17:14:55 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://5df7f29ec39a2081630f29af67b9a391e65090cd7f51580e66e92840a9e62060}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.219: INFO: Pod "nginx-deployment-7b8c6f4498-qnxqn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qnxqn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-qnxqn,UID:c27b5524-c3eb-4c8e-9f2a-8e4fbbc49d75,ResourceVersion:18685,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003271ca0 0xc003271ca1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-q1l7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003271d00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003271d20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.227: INFO: Pod "nginx-deployment-7b8c6f4498-rffh5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rffh5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-rffh5,UID:f9e15fc1-e2e3-4aee-ab69-134967fee8c0,ResourceVersion:18682,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003271da0 0xc003271da1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003271e00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003271e20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2020-02-28 17:14:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.230: INFO: Pod "nginx-deployment-7b8c6f4498-srgkk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-srgkk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-srgkk,UID:25ee1350-91c4-4e51-8f26-4ee8ab28f398,ResourceVersion:18679,Generation:0,CreationTimestamp:2020-02-28 17:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003271ee0 0xc003271ee1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003271f40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003271f60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:59 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.231: INFO: Pod "nginx-deployment-7b8c6f4498-tl6qd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tl6qd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-tl6qd,UID:b7d8debc-358c-46c6-8cc7-5faafc2bde86,ResourceVersion:18569,Generation:0,CreationTimestamp:2020-02-28 17:14:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc003271fe0 0xc003271fe1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-s43f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036bc040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036bc060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.2.120,StartTime:2020-02-28 17:14:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-28 17:14:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://aec1d308be1c5426620eee85d7a636cf8a298150e0daac4d75304cc9ef5c9fcc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.245: INFO: Pod "nginx-deployment-7b8c6f4498-wv9fn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wv9fn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-wv9fn,UID:14cbed74-45c7-4556-a559-c4da7aa26cda,ResourceVersion:18578,Generation:0,CreationTimestamp:2020-02-28 17:14:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc0036bc130 0xc0036bc131}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-t863,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036bc190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036bc1b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.3,PodIP:10.64.1.32,StartTime:2020-02-28 17:14:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-28 17:14:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://aa4ccb8d95f362e0561041297b24e8e11203a69cb33586a41afa1e39dc4a1f05}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 28 17:15:00.245: INFO: Pod "nginx-deployment-7b8c6f4498-z5vj7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-z5vj7,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9625,SelfLink:/api/v1/namespaces/deployment-9625/pods/nginx-deployment-7b8c6f4498-z5vj7,UID:417af174-b298-45c0-aebc-57a30c51a45a,ResourceVersion:18582,Generation:0,CreationTimestamp:2020-02-28 17:14:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 1feb41ff-018e-4251-8425-445ad3e45658 0xc0036bc280 0xc0036bc281}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f44bj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f44bj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f44bj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-q1l7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0036bc2e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0036bc300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-28 17:14:52 +0000 UTC  }],Message:,Reason:,HostIP:10.138.0.4,PodIP:10.64.3.152,StartTime:2020-02-28 17:14:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-28 17:14:55 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://225c780dd8c74c159926ac45c976c8887aac7eb167013b4cf2c510a199435022}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:15:00.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9625" for this suite.
Feb 28 17:15:08.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:15:09.735: INFO: namespace deployment-9625 deletion completed in 9.451431829s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:15:09.736: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 17:15:09.961: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c6a1029-64bb-42e8-8d4d-0e40e7549967" in namespace "projected-9791" to be "success or failure"
Feb 28 17:15:09.997: INFO: Pod "downwardapi-volume-8c6a1029-64bb-42e8-8d4d-0e40e7549967": Phase="Pending", Reason="", readiness=false. Elapsed: 35.998157ms
Feb 28 17:15:12.033: INFO: Pod "downwardapi-volume-8c6a1029-64bb-42e8-8d4d-0e40e7549967": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071385778s
Feb 28 17:15:14.068: INFO: Pod "downwardapi-volume-8c6a1029-64bb-42e8-8d4d-0e40e7549967": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.106306716s
STEP: Saw pod success
Feb 28 17:15:14.068: INFO: Pod "downwardapi-volume-8c6a1029-64bb-42e8-8d4d-0e40e7549967" satisfied condition "success or failure"
Feb 28 17:15:14.106: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod downwardapi-volume-8c6a1029-64bb-42e8-8d4d-0e40e7549967 container client-container: <nil>
STEP: delete the pod
Feb 28 17:15:14.198: INFO: Waiting for pod downwardapi-volume-8c6a1029-64bb-42e8-8d4d-0e40e7549967 to disappear
Feb 28 17:15:14.234: INFO: Pod downwardapi-volume-8c6a1029-64bb-42e8-8d4d-0e40e7549967 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:15:14.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9791" for this suite.
Feb 28 17:15:20.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:15:21.782: INFO: namespace projected-9791 deletion completed in 7.509991342s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:15:21.783: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Feb 28 17:15:24.154: INFO: Pod pod-hostip-08defb3f-5079-4538-8fd6-8ea9b99a03c2 has hostIP: 10.138.0.5
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:15:24.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1294" for this suite.
Feb 28 17:15:46.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:15:47.632: INFO: namespace pods-1294 deletion completed in 23.44198707s
•SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:15:47.633: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Feb 28 17:15:58.098: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 7
	[quantile=0.9] = 510
	[quantile=0.99] = 681
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 36569
	[quantile=0.9] = 653848
	[quantile=0.99] = 657213
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 5
	[quantile=0.9] = 6
	[quantile=0.99] = 25
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 14
	[quantile=0.9] = 29
	[quantile=0.99] = 100
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 12
	[quantile=0.9] = 15
	[quantile=0.99] = 24
For namespace_queue_latency_sum:
	[] = 3797
For namespace_queue_latency_count:
	[] = 288
For namespace_retries:
	[] = 290
For namespace_work_duration:
	[quantile=0.5] = 281089
	[quantile=0.9] = 397336
	[quantile=0.99] = 1686057
For namespace_work_duration_sum:
	[] = 103514458
For namespace_work_duration_count:
	[] = 288
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:15:58.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4990" for this suite.
Feb 28 17:16:04.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:16:05.576: INFO: namespace gc-4990 deletion completed in 7.442455643s
•SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:16:05.576: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-7227
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7227
STEP: Deleting pre-stop pod
Feb 28 17:16:17.150: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:16:17.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7227" for this suite.
Feb 28 17:16:55.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:16:56.722: INFO: namespace prestop-7227 deletion completed in 39.495043737s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:16:56.722: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-7795/configmap-test-d2b86488-8e58-4774-99cd-1409ea39d27b
STEP: Creating a pod to test consume configMaps
Feb 28 17:16:56.942: INFO: Waiting up to 5m0s for pod "pod-configmaps-581103d0-c7d0-454e-b4cd-bd23605955ba" in namespace "configmap-7795" to be "success or failure"
Feb 28 17:16:56.976: INFO: Pod "pod-configmaps-581103d0-c7d0-454e-b4cd-bd23605955ba": Phase="Pending", Reason="", readiness=false. Elapsed: 34.110443ms
Feb 28 17:16:59.012: INFO: Pod "pod-configmaps-581103d0-c7d0-454e-b4cd-bd23605955ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.070563534s
STEP: Saw pod success
Feb 28 17:16:59.012: INFO: Pod "pod-configmaps-581103d0-c7d0-454e-b4cd-bd23605955ba" satisfied condition "success or failure"
Feb 28 17:16:59.047: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-configmaps-581103d0-c7d0-454e-b4cd-bd23605955ba container env-test: <nil>
STEP: delete the pod
Feb 28 17:16:59.137: INFO: Waiting for pod pod-configmaps-581103d0-c7d0-454e-b4cd-bd23605955ba to disappear
Feb 28 17:16:59.172: INFO: Pod pod-configmaps-581103d0-c7d0-454e-b4cd-bd23605955ba no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:16:59.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7795" for this suite.
Feb 28 17:17:05.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:17:06.655: INFO: namespace configmap-7795 deletion completed in 7.447753476s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:17:06.655: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 28 17:17:11.695: INFO: Successfully updated pod "annotationupdatefc736421-110d-4a92-bf7a-84e69f40cdd1"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:17:13.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6211" for this suite.
Feb 28 17:17:35.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:17:37.257: INFO: namespace projected-6211 deletion completed in 23.442803959s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:17:37.258: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 17:17:37.445: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bdb29ab9-9a4c-4a69-9e2e-76aee9ead408" in namespace "projected-853" to be "success or failure"
Feb 28 17:17:37.479: INFO: Pod "downwardapi-volume-bdb29ab9-9a4c-4a69-9e2e-76aee9ead408": Phase="Pending", Reason="", readiness=false. Elapsed: 34.779995ms
Feb 28 17:17:39.514: INFO: Pod "downwardapi-volume-bdb29ab9-9a4c-4a69-9e2e-76aee9ead408": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069586616s
Feb 28 17:17:41.550: INFO: Pod "downwardapi-volume-bdb29ab9-9a4c-4a69-9e2e-76aee9ead408": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.10509309s
STEP: Saw pod success
Feb 28 17:17:41.550: INFO: Pod "downwardapi-volume-bdb29ab9-9a4c-4a69-9e2e-76aee9ead408" satisfied condition "success or failure"
Feb 28 17:17:41.585: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod downwardapi-volume-bdb29ab9-9a4c-4a69-9e2e-76aee9ead408 container client-container: <nil>
STEP: delete the pod
Feb 28 17:17:41.682: INFO: Waiting for pod downwardapi-volume-bdb29ab9-9a4c-4a69-9e2e-76aee9ead408 to disappear
Feb 28 17:17:41.717: INFO: Pod downwardapi-volume-bdb29ab9-9a4c-4a69-9e2e-76aee9ead408 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:17:41.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-853" for this suite.
Feb 28 17:17:47.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:17:49.223: INFO: namespace projected-853 deletion completed in 7.470565152s
•SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:17:49.223: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Feb 28 17:18:19.804: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 7
	[quantile=0.9] = 510
	[quantile=0.99] = 681
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 30326
	[quantile=0.9] = 653848
	[quantile=0.99] = 657213
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = 5
	[quantile=0.9] = 536772
	[quantile=0.99] = 536772
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = 2413
	[quantile=0.9] = 537120
	[quantile=0.99] = 537120
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 5
	[quantile=0.9] = 6
	[quantile=0.99] = 24
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 14
	[quantile=0.9] = 28
	[quantile=0.99] = 100
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 12
	[quantile=0.9] = 18
	[quantile=0.99] = 68
For namespace_queue_latency_sum:
	[] = 3961
For namespace_queue_latency_count:
	[] = 296
For namespace_retries:
	[] = 298
For namespace_work_duration:
	[quantile=0.5] = 274966
	[quantile=0.9] = 465589
	[quantile=0.99] = 1686057
For namespace_work_duration_sum:
	[] = 105871979
For namespace_work_duration_count:
	[] = 296
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:18:19.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6923" for this suite.
Feb 28 17:18:25.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:18:27.292: INFO: namespace gc-6923 deletion completed in 7.453185318s
•SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:18:27.293: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-f3e2e0a9-8bba-4b60-a0e3-3b06883c46b8
STEP: Creating a pod to test consume configMaps
Feb 28 17:18:27.512: INFO: Waiting up to 5m0s for pod "pod-configmaps-dc4376fc-6935-4d0e-beaa-72bf437ac8c7" in namespace "configmap-7003" to be "success or failure"
Feb 28 17:18:27.546: INFO: Pod "pod-configmaps-dc4376fc-6935-4d0e-beaa-72bf437ac8c7": Phase="Pending", Reason="", readiness=false. Elapsed: 34.181663ms
Feb 28 17:18:29.582: INFO: Pod "pod-configmaps-dc4376fc-6935-4d0e-beaa-72bf437ac8c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.070243449s
STEP: Saw pod success
Feb 28 17:18:29.582: INFO: Pod "pod-configmaps-dc4376fc-6935-4d0e-beaa-72bf437ac8c7" satisfied condition "success or failure"
Feb 28 17:18:29.617: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-configmaps-dc4376fc-6935-4d0e-beaa-72bf437ac8c7 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 17:18:29.708: INFO: Waiting for pod pod-configmaps-dc4376fc-6935-4d0e-beaa-72bf437ac8c7 to disappear
Feb 28 17:18:29.742: INFO: Pod pod-configmaps-dc4376fc-6935-4d0e-beaa-72bf437ac8c7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:18:29.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7003" for this suite.
Feb 28 17:18:35.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:18:37.235: INFO: namespace configmap-7003 deletion completed in 7.451099143s
•SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:18:37.236: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 28 17:18:39.604: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:18:39.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5444" for this suite.
Feb 28 17:18:45.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:18:47.150: INFO: namespace container-runtime-5444 deletion completed in 7.430948201s
•SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:18:47.150: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:18:49.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4058" for this suite.
Feb 28 17:19:41.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:19:43.027: INFO: namespace kubelet-test-4058 deletion completed in 53.462606541s
•
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:19:43.027: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 17:19:43.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7cfc286-6784-4288-88c8-d865dffc5263" in namespace "projected-4750" to be "success or failure"
Feb 28 17:19:43.288: INFO: Pod "downwardapi-volume-f7cfc286-6784-4288-88c8-d865dffc5263": Phase="Pending", Reason="", readiness=false. Elapsed: 35.362723ms
Feb 28 17:19:45.324: INFO: Pod "downwardapi-volume-f7cfc286-6784-4288-88c8-d865dffc5263": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.071438909s
STEP: Saw pod success
Feb 28 17:19:45.324: INFO: Pod "downwardapi-volume-f7cfc286-6784-4288-88c8-d865dffc5263" satisfied condition "success or failure"
Feb 28 17:19:45.360: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod downwardapi-volume-f7cfc286-6784-4288-88c8-d865dffc5263 container client-container: <nil>
STEP: delete the pod
Feb 28 17:19:45.451: INFO: Waiting for pod downwardapi-volume-f7cfc286-6784-4288-88c8-d865dffc5263 to disappear
Feb 28 17:19:45.486: INFO: Pod downwardapi-volume-f7cfc286-6784-4288-88c8-d865dffc5263 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:19:45.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4750" for this suite.
Feb 28 17:19:51.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:19:52.966: INFO: namespace projected-4750 deletion completed in 7.445486928s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:19:52.967: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 28 17:19:55.923: INFO: Successfully updated pod "labelsupdate98d5e76f-fa4c-4f68-8085-fccdc0136b83"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:19:58.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2004" for this suite.
Feb 28 17:20:20.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:20:21.637: INFO: namespace downward-api-2004 deletion completed in 23.595498488s
•SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:20:21.637: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-a498f007-5b11-4e44-a06d-5abfda78fa19
STEP: Creating a pod to test consume secrets
Feb 28 17:20:21.905: INFO: Waiting up to 5m0s for pod "pod-secrets-0753cdd8-a693-4cb0-84cf-62fdd4e73e3b" in namespace "secrets-2993" to be "success or failure"
Feb 28 17:20:21.939: INFO: Pod "pod-secrets-0753cdd8-a693-4cb0-84cf-62fdd4e73e3b": Phase="Pending", Reason="", readiness=false. Elapsed: 34.465144ms
Feb 28 17:20:23.974: INFO: Pod "pod-secrets-0753cdd8-a693-4cb0-84cf-62fdd4e73e3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.069919803s
STEP: Saw pod success
Feb 28 17:20:23.975: INFO: Pod "pod-secrets-0753cdd8-a693-4cb0-84cf-62fdd4e73e3b" satisfied condition "success or failure"
Feb 28 17:20:24.010: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-secrets-0753cdd8-a693-4cb0-84cf-62fdd4e73e3b container secret-env-test: <nil>
STEP: delete the pod
Feb 28 17:20:24.113: INFO: Waiting for pod pod-secrets-0753cdd8-a693-4cb0-84cf-62fdd4e73e3b to disappear
Feb 28 17:20:24.147: INFO: Pod pod-secrets-0753cdd8-a693-4cb0-84cf-62fdd4e73e3b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:20:24.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2993" for this suite.
Feb 28 17:20:30.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:20:31.622: INFO: namespace secrets-2993 deletion completed in 7.439267197s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:20:31.622: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;check="$$(dig +notcp +noall +answer +search google.com A)" && test -n "$$check" && echo OK > /results/wheezy_udp@google.com;check="$$(dig +tcp +noall +answer +search google.com A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@google.com;check="$$(dig +notcp +noall +answer +search metadata A)" && test -n "$$check" && echo OK > /results/wheezy_udp@metadata;check="$$(dig +tcp +noall +answer +search metadata A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@metadata;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5159.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;check="$$(dig +notcp +noall +answer +search google.com A)" && test -n "$$check" && echo OK > /results/jessie_udp@google.com;check="$$(dig +tcp +noall +answer +search google.com A)" && test -n "$$check" && echo OK > /results/jessie_tcp@google.com;check="$$(dig +notcp +noall +answer +search metadata A)" && test -n "$$check" && echo OK > /results/jessie_udp@metadata;check="$$(dig +tcp +noall +answer +search metadata A)" && test -n "$$check" && echo OK > /results/jessie_tcp@metadata;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5159.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 28 17:20:34.543: INFO: DNS probes using dns-5159/dns-test-5fd74ff6-6911-4324-ab67-dffba5d5af8e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:20:34.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5159" for this suite.
Feb 28 17:20:40.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:20:42.130: INFO: namespace dns-5159 deletion completed in 7.505529282s
•SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:20:42.130: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-830600d5-402f-42fa-8d88-72cc441e1a42
STEP: Creating secret with name s-test-opt-upd-029f308a-f01a-4ff7-be5c-e6bc5af52a7f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-830600d5-402f-42fa-8d88-72cc441e1a42
STEP: Updating secret s-test-opt-upd-029f308a-f01a-4ff7-be5c-e6bc5af52a7f
STEP: Creating secret with name s-test-opt-create-eaf5d114-ab4c-48a9-a48a-433ea0a5afe2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:21:50.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7432" for this suite.
Feb 28 17:22:12.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:22:13.800: INFO: namespace projected-7432 deletion completed in 23.556449009s
•SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:22:13.800: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:22:20.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9060" for this suite.
Feb 28 17:22:26.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:22:28.204: INFO: namespace namespaces-9060 deletion completed in 7.645937767s
STEP: Destroying namespace "nsdeletetest-4143" for this suite.
Feb 28 17:22:28.242: INFO: Namespace nsdeletetest-4143 was already deleted
STEP: Destroying namespace "nsdeletetest-8437" for this suite.
Feb 28 17:22:34.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:22:35.819: INFO: namespace nsdeletetest-8437 deletion completed in 7.57676606s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:22:35.819: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 17:22:36.023: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b81aad0-0815-497c-a2d8-c533f1a430b6" in namespace "downward-api-9695" to be "success or failure"
Feb 28 17:22:36.063: INFO: Pod "downwardapi-volume-6b81aad0-0815-497c-a2d8-c533f1a430b6": Phase="Pending", Reason="", readiness=false. Elapsed: 39.814718ms
Feb 28 17:22:38.101: INFO: Pod "downwardapi-volume-6b81aad0-0815-497c-a2d8-c533f1a430b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.078216335s
STEP: Saw pod success
Feb 28 17:22:38.101: INFO: Pod "downwardapi-volume-6b81aad0-0815-497c-a2d8-c533f1a430b6" satisfied condition "success or failure"
Feb 28 17:22:38.139: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod downwardapi-volume-6b81aad0-0815-497c-a2d8-c533f1a430b6 container client-container: <nil>
STEP: delete the pod
Feb 28 17:22:38.233: INFO: Waiting for pod downwardapi-volume-6b81aad0-0815-497c-a2d8-c533f1a430b6 to disappear
Feb 28 17:22:38.271: INFO: Pod downwardapi-volume-6b81aad0-0815-497c-a2d8-c533f1a430b6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:22:38.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9695" for this suite.
Feb 28 17:22:44.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:22:45.892: INFO: namespace downward-api-9695 deletion completed in 7.582838135s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:22:45.892: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:22:46.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8228" for this suite.
Feb 28 17:22:52.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:22:53.740: INFO: namespace kubelet-test-8228 deletion completed in 7.565293106s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:22:53.740: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 17:22:56.149: INFO: Waiting up to 5m0s for pod "client-envvars-8df1018e-a932-4762-94e7-0f9b380059d9" in namespace "pods-8142" to be "success or failure"
Feb 28 17:22:56.187: INFO: Pod "client-envvars-8df1018e-a932-4762-94e7-0f9b380059d9": Phase="Pending", Reason="", readiness=false. Elapsed: 37.513048ms
Feb 28 17:22:58.225: INFO: Pod "client-envvars-8df1018e-a932-4762-94e7-0f9b380059d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075891714s
STEP: Saw pod success
Feb 28 17:22:58.225: INFO: Pod "client-envvars-8df1018e-a932-4762-94e7-0f9b380059d9" satisfied condition "success or failure"
Feb 28 17:22:58.263: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod client-envvars-8df1018e-a932-4762-94e7-0f9b380059d9 container env3cont: <nil>
STEP: delete the pod
Feb 28 17:22:58.355: INFO: Waiting for pod client-envvars-8df1018e-a932-4762-94e7-0f9b380059d9 to disappear
Feb 28 17:22:58.393: INFO: Pod client-envvars-8df1018e-a932-4762-94e7-0f9b380059d9 no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:22:58.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8142" for this suite.
Feb 28 17:23:38.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:23:40.005: INFO: namespace pods-8142 deletion completed in 41.574037419s
•SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:23:40.005: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-2cc0b088-9e0b-417f-b62e-ba2c31a3b0c7
STEP: Creating a pod to test consume configMaps
Feb 28 17:23:40.278: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6915cd5e-2fe7-4b1e-8966-12c68f253dbb" in namespace "projected-8580" to be "success or failure"
Feb 28 17:23:40.316: INFO: Pod "pod-projected-configmaps-6915cd5e-2fe7-4b1e-8966-12c68f253dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 38.091603ms
Feb 28 17:23:42.355: INFO: Pod "pod-projected-configmaps-6915cd5e-2fe7-4b1e-8966-12c68f253dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076221613s
Feb 28 17:23:44.393: INFO: Pod "pod-projected-configmaps-6915cd5e-2fe7-4b1e-8966-12c68f253dbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.114273792s
STEP: Saw pod success
Feb 28 17:23:44.393: INFO: Pod "pod-projected-configmaps-6915cd5e-2fe7-4b1e-8966-12c68f253dbb" satisfied condition "success or failure"
Feb 28 17:23:44.432: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-projected-configmaps-6915cd5e-2fe7-4b1e-8966-12c68f253dbb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 28 17:23:44.526: INFO: Waiting for pod pod-projected-configmaps-6915cd5e-2fe7-4b1e-8966-12c68f253dbb to disappear
Feb 28 17:23:44.564: INFO: Pod pod-projected-configmaps-6915cd5e-2fe7-4b1e-8966-12c68f253dbb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:23:44.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8580" for this suite.
Feb 28 17:23:50.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:23:52.234: INFO: namespace projected-8580 deletion completed in 7.631563633s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:23:52.235: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-b4h8z in namespace proxy-1026
I0228 17:23:52.513833    8260 runners.go:180] Created replication controller with name: proxy-service-b4h8z, namespace: proxy-1026, replica count: 1
I0228 17:23:53.564327    8260 runners.go:180] proxy-service-b4h8z Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 17:23:54.564564    8260 runners.go:180] proxy-service-b4h8z Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0228 17:23:55.564876    8260 runners.go:180] proxy-service-b4h8z Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 17:23:56.565181    8260 runners.go:180] proxy-service-b4h8z Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 17:23:57.565537    8260 runners.go:180] proxy-service-b4h8z Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 17:23:58.565837    8260 runners.go:180] proxy-service-b4h8z Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 17:23:59.566124    8260 runners.go:180] proxy-service-b4h8z Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 17:24:00.566510    8260 runners.go:180] proxy-service-b4h8z Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0228 17:24:01.566760    8260 runners.go:180] proxy-service-b4h8z Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 28 17:24:01.605: INFO: setup took 9.179641854s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 28 17:24:01.659: INFO: (0) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 53.754012ms)
Feb 28 17:24:01.663: INFO: (0) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 57.27152ms)
Feb 28 17:24:01.663: INFO: (0) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 57.235512ms)
Feb 28 17:24:01.663: INFO: (0) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 57.156463ms)
Feb 28 17:24:01.663: INFO: (0) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 57.241169ms)
Feb 28 17:24:01.663: INFO: (0) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 57.505551ms)
Feb 28 17:24:01.665: INFO: (0) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 59.297395ms)
Feb 28 17:24:01.665: INFO: (0) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 59.232913ms)
Feb 28 17:24:01.665: INFO: (0) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 60.108304ms)
Feb 28 17:24:01.669: INFO: (0) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 63.060973ms)
Feb 28 17:24:01.686: INFO: (0) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 80.943068ms)
Feb 28 17:24:01.688: INFO: (0) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 82.446905ms)
Feb 28 17:24:01.688: INFO: (0) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 82.422128ms)
Feb 28 17:24:01.688: INFO: (0) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 82.584369ms)
Feb 28 17:24:01.688: INFO: (0) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 82.583534ms)
Feb 28 17:24:01.697: INFO: (0) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 91.519257ms)
Feb 28 17:24:01.748: INFO: (1) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 50.984988ms)
Feb 28 17:24:01.748: INFO: (1) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 50.877602ms)
Feb 28 17:24:01.748: INFO: (1) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 50.756656ms)
Feb 28 17:24:01.748: INFO: (1) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 50.849312ms)
Feb 28 17:24:01.749: INFO: (1) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 51.483589ms)
Feb 28 17:24:01.753: INFO: (1) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 55.615987ms)
Feb 28 17:24:01.753: INFO: (1) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 55.93946ms)
Feb 28 17:24:01.753: INFO: (1) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 55.868533ms)
Feb 28 17:24:01.753: INFO: (1) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 55.84815ms)
Feb 28 17:24:01.753: INFO: (1) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 56.045301ms)
Feb 28 17:24:01.754: INFO: (1) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 56.533943ms)
Feb 28 17:24:01.756: INFO: (1) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 58.626538ms)
Feb 28 17:24:01.758: INFO: (1) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 60.90801ms)
Feb 28 17:24:01.758: INFO: (1) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 60.946953ms)
Feb 28 17:24:01.759: INFO: (1) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 61.192076ms)
Feb 28 17:24:01.759: INFO: (1) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 61.564062ms)
Feb 28 17:24:01.822: INFO: (2) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 63.340552ms)
Feb 28 17:24:01.822: INFO: (2) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 63.255939ms)
Feb 28 17:24:01.833: INFO: (2) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 74.019769ms)
Feb 28 17:24:01.834: INFO: (2) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 74.922071ms)
Feb 28 17:24:01.834: INFO: (2) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 75.280512ms)
Feb 28 17:24:01.834: INFO: (2) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 75.375439ms)
Feb 28 17:24:01.835: INFO: (2) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 75.761573ms)
Feb 28 17:24:01.835: INFO: (2) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 75.825338ms)
Feb 28 17:24:01.835: INFO: (2) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 75.867943ms)
Feb 28 17:24:01.838: INFO: (2) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 78.895437ms)
Feb 28 17:24:01.842: INFO: (2) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 82.758236ms)
Feb 28 17:24:01.842: INFO: (2) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 82.730825ms)
Feb 28 17:24:01.842: INFO: (2) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 83.07156ms)
Feb 28 17:24:01.842: INFO: (2) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 82.813925ms)
Feb 28 17:24:01.842: INFO: (2) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 83.083728ms)
Feb 28 17:24:01.844: INFO: (2) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 84.487302ms)
Feb 28 17:24:01.891: INFO: (3) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 47.251195ms)
Feb 28 17:24:01.893: INFO: (3) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 48.744042ms)
Feb 28 17:24:01.896: INFO: (3) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 52.378157ms)
Feb 28 17:24:01.896: INFO: (3) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 52.581185ms)
Feb 28 17:24:01.896: INFO: (3) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 52.612273ms)
Feb 28 17:24:01.896: INFO: (3) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 52.739587ms)
Feb 28 17:24:01.896: INFO: (3) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 52.623621ms)
Feb 28 17:24:01.896: INFO: (3) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 52.611146ms)
Feb 28 17:24:01.899: INFO: (3) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 55.143846ms)
Feb 28 17:24:01.899: INFO: (3) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 55.100534ms)
Feb 28 17:24:01.902: INFO: (3) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 57.978772ms)
Feb 28 17:24:01.904: INFO: (3) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 60.296469ms)
Feb 28 17:24:01.905: INFO: (3) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 60.870986ms)
Feb 28 17:24:01.905: INFO: (3) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 60.962545ms)
Feb 28 17:24:01.905: INFO: (3) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 61.14807ms)
Feb 28 17:24:01.905: INFO: (3) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 61.266299ms)
Feb 28 17:24:01.954: INFO: (4) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 49.157173ms)
Feb 28 17:24:01.954: INFO: (4) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 49.292666ms)
Feb 28 17:24:01.956: INFO: (4) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 50.76453ms)
Feb 28 17:24:01.956: INFO: (4) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 50.845159ms)
Feb 28 17:24:01.960: INFO: (4) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 54.463987ms)
Feb 28 17:24:01.960: INFO: (4) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 54.535207ms)
Feb 28 17:24:01.960: INFO: (4) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 54.406244ms)
Feb 28 17:24:01.960: INFO: (4) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 54.592987ms)
Feb 28 17:24:01.960: INFO: (4) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 54.591339ms)
Feb 28 17:24:01.960: INFO: (4) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 54.513813ms)
Feb 28 17:24:01.963: INFO: (4) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 58.257256ms)
Feb 28 17:24:01.966: INFO: (4) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 60.828835ms)
Feb 28 17:24:01.966: INFO: (4) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 61.029767ms)
Feb 28 17:24:01.966: INFO: (4) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 61.090439ms)
Feb 28 17:24:01.966: INFO: (4) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 61.141912ms)
Feb 28 17:24:01.966: INFO: (4) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 60.912914ms)
Feb 28 17:24:02.011: INFO: (5) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 44.828948ms)
Feb 28 17:24:02.013: INFO: (5) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 47.011761ms)
Feb 28 17:24:02.016: INFO: (5) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 49.710508ms)
Feb 28 17:24:02.016: INFO: (5) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 49.652246ms)
Feb 28 17:24:02.024: INFO: (5) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 57.173907ms)
Feb 28 17:24:02.024: INFO: (5) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 57.474552ms)
Feb 28 17:24:02.024: INFO: (5) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 57.602852ms)
Feb 28 17:24:02.025: INFO: (5) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 58.669277ms)
Feb 28 17:24:02.025: INFO: (5) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 58.644155ms)
Feb 28 17:24:02.025: INFO: (5) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 58.690397ms)
Feb 28 17:24:02.025: INFO: (5) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 58.858074ms)
Feb 28 17:24:02.025: INFO: (5) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 58.849006ms)
Feb 28 17:24:02.027: INFO: (5) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 60.917746ms)
Feb 28 17:24:02.027: INFO: (5) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 60.841629ms)
Feb 28 17:24:02.027: INFO: (5) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 60.887137ms)
Feb 28 17:24:02.027: INFO: (5) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 60.954268ms)
Feb 28 17:24:02.080: INFO: (6) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 52.859349ms)
Feb 28 17:24:02.080: INFO: (6) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 53.053017ms)
Feb 28 17:24:02.086: INFO: (6) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 58.944929ms)
Feb 28 17:24:02.087: INFO: (6) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 58.957802ms)
Feb 28 17:24:02.087: INFO: (6) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 59.271248ms)
Feb 28 17:24:02.087: INFO: (6) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 59.278597ms)
Feb 28 17:24:02.087: INFO: (6) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 59.427921ms)
Feb 28 17:24:02.089: INFO: (6) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 61.082415ms)
Feb 28 17:24:02.089: INFO: (6) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 62.103438ms)
Feb 28 17:24:02.090: INFO: (6) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 62.012572ms)
Feb 28 17:24:02.090: INFO: (6) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 62.071783ms)
Feb 28 17:24:02.090: INFO: (6) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 62.315276ms)
Feb 28 17:24:02.096: INFO: (6) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 68.206463ms)
Feb 28 17:24:02.096: INFO: (6) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 68.265978ms)
Feb 28 17:24:02.096: INFO: (6) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 68.328122ms)
Feb 28 17:24:02.096: INFO: (6) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 68.206349ms)
Feb 28 17:24:02.142: INFO: (7) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 46.525374ms)
Feb 28 17:24:02.146: INFO: (7) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 49.831868ms)
Feb 28 17:24:02.146: INFO: (7) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 50.19838ms)
Feb 28 17:24:02.146: INFO: (7) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 50.004593ms)
Feb 28 17:24:02.148: INFO: (7) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 52.365157ms)
Feb 28 17:24:02.148: INFO: (7) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 52.414287ms)
Feb 28 17:24:02.154: INFO: (7) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 57.867238ms)
Feb 28 17:24:02.154: INFO: (7) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 57.753833ms)
Feb 28 17:24:02.154: INFO: (7) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 57.780098ms)
Feb 28 17:24:02.154: INFO: (7) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 57.809718ms)
Feb 28 17:24:02.154: INFO: (7) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 57.9164ms)
Feb 28 17:24:02.154: INFO: (7) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 57.853464ms)
Feb 28 17:24:02.154: INFO: (7) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 57.82262ms)
Feb 28 17:24:02.154: INFO: (7) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 57.900328ms)
Feb 28 17:24:02.154: INFO: (7) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 57.919332ms)
Feb 28 17:24:02.155: INFO: (7) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 58.655732ms)
Feb 28 17:24:02.202: INFO: (8) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 47.677282ms)
Feb 28 17:24:02.210: INFO: (8) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 55.225786ms)
Feb 28 17:24:02.210: INFO: (8) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 55.458028ms)
Feb 28 17:24:02.210: INFO: (8) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 55.410236ms)
Feb 28 17:24:02.210: INFO: (8) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 55.340193ms)
Feb 28 17:24:02.210: INFO: (8) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 55.33251ms)
Feb 28 17:24:02.210: INFO: (8) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 55.533853ms)
Feb 28 17:24:02.210: INFO: (8) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 55.507112ms)
Feb 28 17:24:02.210: INFO: (8) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 55.371773ms)
Feb 28 17:24:02.210: INFO: (8) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 55.479168ms)
Feb 28 17:24:02.212: INFO: (8) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 57.119362ms)
Feb 28 17:24:02.212: INFO: (8) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 57.071414ms)
Feb 28 17:24:02.214: INFO: (8) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 59.278233ms)
Feb 28 17:24:02.214: INFO: (8) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 59.240233ms)
Feb 28 17:24:02.214: INFO: (8) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 59.269939ms)
Feb 28 17:24:02.214: INFO: (8) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 59.41316ms)
Feb 28 17:24:02.259: INFO: (9) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 44.691982ms)
Feb 28 17:24:02.265: INFO: (9) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 50.135661ms)
Feb 28 17:24:02.271: INFO: (9) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 56.254781ms)
Feb 28 17:24:02.271: INFO: (9) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 56.462399ms)
Feb 28 17:24:02.271: INFO: (9) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 56.650829ms)
Feb 28 17:24:02.271: INFO: (9) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 56.489146ms)
Feb 28 17:24:02.271: INFO: (9) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 56.470549ms)
Feb 28 17:24:02.271: INFO: (9) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 56.69623ms)
Feb 28 17:24:02.271: INFO: (9) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 56.495207ms)
Feb 28 17:24:02.271: INFO: (9) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 56.599659ms)
Feb 28 17:24:02.271: INFO: (9) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 56.597117ms)
Feb 28 17:24:02.273: INFO: (9) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 58.08237ms)
Feb 28 17:24:02.275: INFO: (9) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 60.211526ms)
Feb 28 17:24:02.276: INFO: (9) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 61.207744ms)
Feb 28 17:24:02.276: INFO: (9) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 61.193422ms)
Feb 28 17:24:02.277: INFO: (9) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 62.33886ms)
Feb 28 17:24:02.329: INFO: (10) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 52.353311ms)
Feb 28 17:24:02.329: INFO: (10) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 52.282295ms)
Feb 28 17:24:02.329: INFO: (10) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 52.241869ms)
Feb 28 17:24:02.330: INFO: (10) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 52.571024ms)
Feb 28 17:24:02.330: INFO: (10) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 52.6481ms)
Feb 28 17:24:02.332: INFO: (10) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 54.435699ms)
Feb 28 17:24:02.332: INFO: (10) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 54.489076ms)
Feb 28 17:24:02.334: INFO: (10) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 56.686701ms)
Feb 28 17:24:02.335: INFO: (10) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 57.863199ms)
Feb 28 17:24:02.335: INFO: (10) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 58.014517ms)
Feb 28 17:24:02.336: INFO: (10) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 58.44029ms)
Feb 28 17:24:02.338: INFO: (10) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 61.181257ms)
Feb 28 17:24:02.338: INFO: (10) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 61.19671ms)
Feb 28 17:24:02.338: INFO: (10) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 61.178214ms)
Feb 28 17:24:02.338: INFO: (10) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 60.883473ms)
Feb 28 17:24:02.339: INFO: (10) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 61.596967ms)
Feb 28 17:24:02.387: INFO: (11) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 47.64952ms)
Feb 28 17:24:02.390: INFO: (11) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 50.579156ms)
Feb 28 17:24:02.391: INFO: (11) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 51.992324ms)
Feb 28 17:24:02.391: INFO: (11) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 52.278804ms)
Feb 28 17:24:02.392: INFO: (11) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 52.072991ms)
Feb 28 17:24:02.392: INFO: (11) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 52.734384ms)
Feb 28 17:24:02.397: INFO: (11) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 58.088245ms)
Feb 28 17:24:02.397: INFO: (11) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 57.796926ms)
Feb 28 17:24:02.397: INFO: (11) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 57.399069ms)
Feb 28 17:24:02.397: INFO: (11) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 57.44404ms)
Feb 28 17:24:02.397: INFO: (11) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 57.994544ms)
Feb 28 17:24:02.397: INFO: (11) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 58.499668ms)
Feb 28 17:24:02.399: INFO: (11) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 59.608505ms)
Feb 28 17:24:02.399: INFO: (11) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 59.512162ms)
Feb 28 17:24:02.400: INFO: (11) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 60.283271ms)
Feb 28 17:24:02.400: INFO: (11) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 60.994639ms)
Feb 28 17:24:02.447: INFO: (12) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 46.570596ms)
Feb 28 17:24:02.450: INFO: (12) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 49.376725ms)
Feb 28 17:24:02.453: INFO: (12) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 52.533486ms)
Feb 28 17:24:02.459: INFO: (12) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 58.846421ms)
Feb 28 17:24:02.459: INFO: (12) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 58.567912ms)
Feb 28 17:24:02.459: INFO: (12) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 58.641448ms)
Feb 28 17:24:02.459: INFO: (12) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 58.696862ms)
Feb 28 17:24:02.460: INFO: (12) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 59.103147ms)
Feb 28 17:24:02.460: INFO: (12) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 59.219254ms)
Feb 28 17:24:02.460: INFO: (12) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 59.651563ms)
Feb 28 17:24:02.461: INFO: (12) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 60.048819ms)
Feb 28 17:24:02.461: INFO: (12) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 60.371026ms)
Feb 28 17:24:02.461: INFO: (12) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 60.259462ms)
Feb 28 17:24:02.462: INFO: (12) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 61.204039ms)
Feb 28 17:24:02.462: INFO: (12) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 61.70943ms)
Feb 28 17:24:02.462: INFO: (12) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 61.559431ms)
Feb 28 17:24:02.513: INFO: (13) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 50.964442ms)
Feb 28 17:24:02.513: INFO: (13) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 50.888971ms)
Feb 28 17:24:02.513: INFO: (13) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 50.975843ms)
Feb 28 17:24:02.514: INFO: (13) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 51.305649ms)
Feb 28 17:24:02.518: INFO: (13) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 56.019837ms)
Feb 28 17:24:02.520: INFO: (13) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 57.639128ms)
Feb 28 17:24:02.520: INFO: (13) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 57.693885ms)
Feb 28 17:24:02.520: INFO: (13) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 57.62528ms)
Feb 28 17:24:02.520: INFO: (13) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 57.420535ms)
Feb 28 17:24:02.520: INFO: (13) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 57.743074ms)
Feb 28 17:24:02.523: INFO: (13) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 60.104145ms)
Feb 28 17:24:02.523: INFO: (13) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 60.051915ms)
Feb 28 17:24:02.523: INFO: (13) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 60.035923ms)
Feb 28 17:24:02.523: INFO: (13) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 60.116285ms)
Feb 28 17:24:02.523: INFO: (13) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 60.021686ms)
Feb 28 17:24:02.523: INFO: (13) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 60.508893ms)
Feb 28 17:24:02.573: INFO: (14) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 49.621897ms)
Feb 28 17:24:02.573: INFO: (14) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 49.620779ms)
Feb 28 17:24:02.573: INFO: (14) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 49.796201ms)
Feb 28 17:24:02.586: INFO: (14) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 63.111268ms)
Feb 28 17:24:02.593: INFO: (14) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 70.165399ms)
Feb 28 17:24:02.593: INFO: (14) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 70.364149ms)
Feb 28 17:24:02.593: INFO: (14) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 70.227554ms)
Feb 28 17:24:02.593: INFO: (14) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 70.267332ms)
Feb 28 17:24:02.593: INFO: (14) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 70.231166ms)
Feb 28 17:24:02.593: INFO: (14) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 70.248154ms)
Feb 28 17:24:02.593: INFO: (14) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 70.278472ms)
Feb 28 17:24:02.594: INFO: (14) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 70.289692ms)
Feb 28 17:24:02.594: INFO: (14) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 70.695088ms)
Feb 28 17:24:02.595: INFO: (14) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 72.329765ms)
Feb 28 17:24:02.598: INFO: (14) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 74.64141ms)
Feb 28 17:24:02.598: INFO: (14) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 74.729013ms)
Feb 28 17:24:02.645: INFO: (15) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 46.482156ms)
Feb 28 17:24:02.651: INFO: (15) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 52.550997ms)
Feb 28 17:24:02.651: INFO: (15) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 52.54056ms)
Feb 28 17:24:02.651: INFO: (15) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 52.612114ms)
Feb 28 17:24:02.651: INFO: (15) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 52.633076ms)
Feb 28 17:24:02.651: INFO: (15) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 52.711627ms)
Feb 28 17:24:02.651: INFO: (15) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 52.606631ms)
Feb 28 17:24:02.651: INFO: (15) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 52.680038ms)
Feb 28 17:24:02.651: INFO: (15) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 53.074606ms)
Feb 28 17:24:02.652: INFO: (15) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 54.349041ms)
Feb 28 17:24:02.653: INFO: (15) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 54.731905ms)
Feb 28 17:24:02.656: INFO: (15) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 57.811925ms)
Feb 28 17:24:02.656: INFO: (15) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 57.70909ms)
Feb 28 17:24:02.656: INFO: (15) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 57.882387ms)
Feb 28 17:24:02.656: INFO: (15) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 58.049412ms)
Feb 28 17:24:02.656: INFO: (15) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 58.027258ms)
Feb 28 17:24:02.702: INFO: (16) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 45.291265ms)
Feb 28 17:24:02.704: INFO: (16) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 47.515041ms)
Feb 28 17:24:02.704: INFO: (16) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 47.48574ms)
Feb 28 17:24:02.705: INFO: (16) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 48.663703ms)
Feb 28 17:24:02.705: INFO: (16) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 48.684703ms)
Feb 28 17:24:02.712: INFO: (16) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 55.402902ms)
Feb 28 17:24:02.712: INFO: (16) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 55.44256ms)
Feb 28 17:24:02.712: INFO: (16) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 55.476168ms)
Feb 28 17:24:02.712: INFO: (16) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 55.473741ms)
Feb 28 17:24:02.712: INFO: (16) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 55.592064ms)
Feb 28 17:24:02.713: INFO: (16) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 56.361326ms)
Feb 28 17:24:02.715: INFO: (16) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 58.786612ms)
Feb 28 17:24:02.715: INFO: (16) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 58.89361ms)
Feb 28 17:24:02.715: INFO: (16) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 58.980807ms)
Feb 28 17:24:02.715: INFO: (16) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 59.015829ms)
Feb 28 17:24:02.715: INFO: (16) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 59.133814ms)
Feb 28 17:24:02.762: INFO: (17) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 46.096571ms)
Feb 28 17:24:02.771: INFO: (17) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 55.192643ms)
Feb 28 17:24:02.771: INFO: (17) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 55.344107ms)
Feb 28 17:24:02.771: INFO: (17) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 55.174906ms)
Feb 28 17:24:02.771: INFO: (17) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 55.22334ms)
Feb 28 17:24:02.771: INFO: (17) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 55.241078ms)
Feb 28 17:24:02.771: INFO: (17) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 55.311662ms)
Feb 28 17:24:02.771: INFO: (17) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 55.532022ms)
Feb 28 17:24:02.772: INFO: (17) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 56.407524ms)
Feb 28 17:24:02.772: INFO: (17) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 56.308529ms)
Feb 28 17:24:02.785: INFO: (17) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 69.348006ms)
Feb 28 17:24:02.785: INFO: (17) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 69.277058ms)
Feb 28 17:24:02.785: INFO: (17) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 69.489008ms)
Feb 28 17:24:02.785: INFO: (17) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 69.760357ms)
Feb 28 17:24:02.785: INFO: (17) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 69.8093ms)
Feb 28 17:24:02.785: INFO: (17) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 69.70934ms)
Feb 28 17:24:02.833: INFO: (18) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 47.73593ms)
Feb 28 17:24:02.842: INFO: (18) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 56.15934ms)
Feb 28 17:24:02.842: INFO: (18) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 56.299027ms)
Feb 28 17:24:02.842: INFO: (18) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 56.236576ms)
Feb 28 17:24:02.842: INFO: (18) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 56.366297ms)
Feb 28 17:24:02.842: INFO: (18) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 56.248819ms)
Feb 28 17:24:02.842: INFO: (18) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 56.357056ms)
Feb 28 17:24:02.842: INFO: (18) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 56.429178ms)
Feb 28 17:24:02.842: INFO: (18) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 56.304742ms)
Feb 28 17:24:02.842: INFO: (18) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 56.332316ms)
Feb 28 17:24:02.845: INFO: (18) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 60.253044ms)
Feb 28 17:24:02.846: INFO: (18) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 60.487296ms)
Feb 28 17:24:02.846: INFO: (18) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 60.421615ms)
Feb 28 17:24:02.846: INFO: (18) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 60.311524ms)
Feb 28 17:24:02.846: INFO: (18) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 60.389871ms)
Feb 28 17:24:02.846: INFO: (18) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 60.33987ms)
Feb 28 17:24:02.894: INFO: (19) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t/proxy/rewriteme">test</a> (200; 47.908267ms)
Feb 28 17:24:02.897: INFO: (19) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 50.978034ms)
Feb 28 17:24:02.897: INFO: (19) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:460/proxy/: tls baz (200; 51.150875ms)
Feb 28 17:24:02.897: INFO: (19) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 51.348968ms)
Feb 28 17:24:02.897: INFO: (19) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:160/proxy/: foo (200; 51.25243ms)
Feb 28 17:24:02.899: INFO: (19) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:443/proxy/tlsrewritem... (200; 53.190223ms)
Feb 28 17:24:02.905: INFO: (19) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname1/proxy/: tls baz (200; 58.853158ms)
Feb 28 17:24:02.905: INFO: (19) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">... (200; 58.794907ms)
Feb 28 17:24:02.905: INFO: (19) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname1/proxy/: foo (200; 58.885793ms)
Feb 28 17:24:02.905: INFO: (19) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname2/proxy/: bar (200; 59.09602ms)
Feb 28 17:24:02.905: INFO: (19) /api/v1/namespaces/proxy-1026/services/http:proxy-service-b4h8z:portname1/proxy/: foo (200; 59.211402ms)
Feb 28 17:24:02.905: INFO: (19) /api/v1/namespaces/proxy-1026/services/proxy-service-b4h8z:portname2/proxy/: bar (200; 58.988514ms)
Feb 28 17:24:02.906: INFO: (19) /api/v1/namespaces/proxy-1026/pods/https:proxy-service-b4h8z-dnf6t:462/proxy/: tls qux (200; 60.061072ms)
Feb 28 17:24:02.906: INFO: (19) /api/v1/namespaces/proxy-1026/services/https:proxy-service-b4h8z:tlsportname2/proxy/: tls qux (200; 59.977349ms)
Feb 28 17:24:02.906: INFO: (19) /api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/: <a href="/api/v1/namespaces/proxy-1026/pods/proxy-service-b4h8z-dnf6t:1080/proxy/rewriteme">test<... (200; 60.008795ms)
Feb 28 17:24:02.906: INFO: (19) /api/v1/namespaces/proxy-1026/pods/http:proxy-service-b4h8z-dnf6t:162/proxy/: bar (200; 60.363112ms)
STEP: deleting ReplicationController proxy-service-b4h8z in namespace proxy-1026, will wait for the garbage collector to delete the pods
Feb 28 17:24:03.037: INFO: Deleting ReplicationController proxy-service-b4h8z took: 42.95059ms
Feb 28 17:24:03.638: INFO: Terminating ReplicationController proxy-service-b4h8z pods took: 600.368959ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:24:05.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1026" for this suite.
Feb 28 17:24:11.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:24:13.158: INFO: namespace proxy-1026 deletion completed in 7.580181951s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:24:13.158: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 28 17:24:13.398: INFO: Waiting up to 5m0s for pod "downward-api-f3e7784d-1fc4-47a2-8891-ad500831148c" in namespace "downward-api-5941" to be "success or failure"
Feb 28 17:24:13.436: INFO: Pod "downward-api-f3e7784d-1fc4-47a2-8891-ad500831148c": Phase="Pending", Reason="", readiness=false. Elapsed: 38.195363ms
Feb 28 17:24:15.474: INFO: Pod "downward-api-f3e7784d-1fc4-47a2-8891-ad500831148c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076737251s
Feb 28 17:24:17.513: INFO: Pod "downward-api-f3e7784d-1fc4-47a2-8891-ad500831148c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.114901789s
STEP: Saw pod success
Feb 28 17:24:17.513: INFO: Pod "downward-api-f3e7784d-1fc4-47a2-8891-ad500831148c" satisfied condition "success or failure"
Feb 28 17:24:17.551: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod downward-api-f3e7784d-1fc4-47a2-8891-ad500831148c container dapi-container: <nil>
STEP: delete the pod
Feb 28 17:24:17.645: INFO: Waiting for pod downward-api-f3e7784d-1fc4-47a2-8891-ad500831148c to disappear
Feb 28 17:24:17.683: INFO: Pod downward-api-f3e7784d-1fc4-47a2-8891-ad500831148c no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:24:17.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5941" for this suite.
Feb 28 17:24:23.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:24:25.304: INFO: namespace downward-api-5941 deletion completed in 7.582577624s
•SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:24:25.305: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 28 17:24:31.822: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 17:24:31.860: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 17:24:33.860: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 17:24:33.898: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 17:24:35.860: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 17:24:35.898: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 17:24:37.860: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 17:24:37.898: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 17:24:39.860: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 17:24:39.898: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 17:24:41.860: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 17:24:41.904: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 17:24:43.860: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 17:24:43.899: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 17:24:45.860: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 17:24:45.898: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 17:24:47.860: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 17:24:47.898: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 17:24:49.860: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 17:24:49.898: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 17:24:51.860: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 17:24:51.898: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 17:24:53.860: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 17:24:53.899: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 17:24:55.860: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 17:24:55.898: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 28 17:24:57.860: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 28 17:24:57.899: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:24:57.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3642" for this suite.
Feb 28 17:25:20.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:25:21.551: INFO: namespace container-lifecycle-hook-3642 deletion completed in 23.56956103s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:25:21.552: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:25:46.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5368" for this suite.
Feb 28 17:25:52.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:25:53.870: INFO: namespace namespaces-5368 deletion completed in 7.575769658s
STEP: Destroying namespace "nsdeletetest-7354" for this suite.
Feb 28 17:25:53.908: INFO: Namespace nsdeletetest-7354 was already deleted
STEP: Destroying namespace "nsdeletetest-7436" for this suite.
Feb 28 17:26:00.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:26:01.474: INFO: namespace nsdeletetest-7436 deletion completed in 7.565521574s
•SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:26:01.474: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 17:26:01.688: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58c01862-857b-4578-9077-91a5262bd10f" in namespace "projected-227" to be "success or failure"
Feb 28 17:26:01.727: INFO: Pod "downwardapi-volume-58c01862-857b-4578-9077-91a5262bd10f": Phase="Pending", Reason="", readiness=false. Elapsed: 38.219479ms
Feb 28 17:26:03.765: INFO: Pod "downwardapi-volume-58c01862-857b-4578-9077-91a5262bd10f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076308614s
Feb 28 17:26:05.803: INFO: Pod "downwardapi-volume-58c01862-857b-4578-9077-91a5262bd10f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.114370121s
STEP: Saw pod success
Feb 28 17:26:05.803: INFO: Pod "downwardapi-volume-58c01862-857b-4578-9077-91a5262bd10f" satisfied condition "success or failure"
Feb 28 17:26:05.841: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod downwardapi-volume-58c01862-857b-4578-9077-91a5262bd10f container client-container: <nil>
STEP: delete the pod
Feb 28 17:26:05.940: INFO: Waiting for pod downwardapi-volume-58c01862-857b-4578-9077-91a5262bd10f to disappear
Feb 28 17:26:05.978: INFO: Pod downwardapi-volume-58c01862-857b-4578-9077-91a5262bd10f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:26:05.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-227" for this suite.
Feb 28 17:26:12.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:26:13.617: INFO: namespace projected-227 deletion completed in 7.601236568s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:26:13.617: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 28 17:26:13.861: INFO: Waiting up to 5m0s for pod "pod-8c037a7e-3a4a-4522-87a7-818689c6d129" in namespace "emptydir-9018" to be "success or failure"
Feb 28 17:26:13.899: INFO: Pod "pod-8c037a7e-3a4a-4522-87a7-818689c6d129": Phase="Pending", Reason="", readiness=false. Elapsed: 37.745861ms
Feb 28 17:26:15.937: INFO: Pod "pod-8c037a7e-3a4a-4522-87a7-818689c6d129": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075935439s
STEP: Saw pod success
Feb 28 17:26:15.937: INFO: Pod "pod-8c037a7e-3a4a-4522-87a7-818689c6d129" satisfied condition "success or failure"
Feb 28 17:26:15.976: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-8c037a7e-3a4a-4522-87a7-818689c6d129 container test-container: <nil>
STEP: delete the pod
Feb 28 17:26:16.070: INFO: Waiting for pod pod-8c037a7e-3a4a-4522-87a7-818689c6d129 to disappear
Feb 28 17:26:16.107: INFO: Pod pod-8c037a7e-3a4a-4522-87a7-818689c6d129 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:26:16.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9018" for this suite.
Feb 28 17:26:22.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:26:23.721: INFO: namespace emptydir-9018 deletion completed in 7.57540254s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:26:23.721: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 28 17:26:23.937: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ce139d37-7aa2-4074-8858-385f466a563b" in namespace "downward-api-3527" to be "success or failure"
Feb 28 17:26:23.976: INFO: Pod "downwardapi-volume-ce139d37-7aa2-4074-8858-385f466a563b": Phase="Pending", Reason="", readiness=false. Elapsed: 39.776082ms
Feb 28 17:26:26.015: INFO: Pod "downwardapi-volume-ce139d37-7aa2-4074-8858-385f466a563b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078453115s
Feb 28 17:26:28.053: INFO: Pod "downwardapi-volume-ce139d37-7aa2-4074-8858-385f466a563b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.116687688s
STEP: Saw pod success
Feb 28 17:26:28.053: INFO: Pod "downwardapi-volume-ce139d37-7aa2-4074-8858-385f466a563b" satisfied condition "success or failure"
Feb 28 17:26:28.091: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod downwardapi-volume-ce139d37-7aa2-4074-8858-385f466a563b container client-container: <nil>
STEP: delete the pod
Feb 28 17:26:28.185: INFO: Waiting for pod downwardapi-volume-ce139d37-7aa2-4074-8858-385f466a563b to disappear
Feb 28 17:26:28.223: INFO: Pod downwardapi-volume-ce139d37-7aa2-4074-8858-385f466a563b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:26:28.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3527" for this suite.
Feb 28 17:26:34.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:26:35.832: INFO: namespace downward-api-3527 deletion completed in 7.570845943s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:26:35.832: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 28 17:26:36.033: INFO: Waiting up to 5m0s for pod "pod-cc2821bb-b6a6-41fe-bb5e-b1b44ee913cc" in namespace "emptydir-4935" to be "success or failure"
Feb 28 17:26:36.071: INFO: Pod "pod-cc2821bb-b6a6-41fe-bb5e-b1b44ee913cc": Phase="Pending", Reason="", readiness=false. Elapsed: 37.972585ms
Feb 28 17:26:38.109: INFO: Pod "pod-cc2821bb-b6a6-41fe-bb5e-b1b44ee913cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076176354s
STEP: Saw pod success
Feb 28 17:26:38.109: INFO: Pod "pod-cc2821bb-b6a6-41fe-bb5e-b1b44ee913cc" satisfied condition "success or failure"
Feb 28 17:26:38.150: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod pod-cc2821bb-b6a6-41fe-bb5e-b1b44ee913cc container test-container: <nil>
STEP: delete the pod
Feb 28 17:26:38.242: INFO: Waiting for pod pod-cc2821bb-b6a6-41fe-bb5e-b1b44ee913cc to disappear
Feb 28 17:26:38.280: INFO: Pod pod-cc2821bb-b6a6-41fe-bb5e-b1b44ee913cc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:26:38.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4935" for this suite.
Feb 28 17:26:44.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:26:45.895: INFO: namespace emptydir-4935 deletion completed in 7.576551334s
•
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:26:45.895: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Feb 28 17:26:50.317: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config exec pod-sharedvolume-f6eb4c02-f85a-443c-b0c1-f4e3b580d486 -c busybox-main-container --namespace=emptydir-214 -- cat /usr/share/volumeshare/shareddata.txt'
Feb 28 17:26:51.282: INFO: stderr: ""
Feb 28 17:26:51.282: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:26:51.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-214" for this suite.
Feb 28 17:26:57.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:26:58.925: INFO: namespace emptydir-214 deletion completed in 7.605270512s
•SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:26:58.926: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 28 17:26:59.159: INFO: Waiting up to 5m0s for pod "downward-api-1b9547c4-06b2-4dbd-ab7c-d064330e0e64" in namespace "downward-api-2978" to be "success or failure"
Feb 28 17:26:59.197: INFO: Pod "downward-api-1b9547c4-06b2-4dbd-ab7c-d064330e0e64": Phase="Pending", Reason="", readiness=false. Elapsed: 37.724505ms
Feb 28 17:27:01.238: INFO: Pod "downward-api-1b9547c4-06b2-4dbd-ab7c-d064330e0e64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079161087s
Feb 28 17:27:03.276: INFO: Pod "downward-api-1b9547c4-06b2-4dbd-ab7c-d064330e0e64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.117227512s
STEP: Saw pod success
Feb 28 17:27:03.276: INFO: Pod "downward-api-1b9547c4-06b2-4dbd-ab7c-d064330e0e64" satisfied condition "success or failure"
Feb 28 17:27:03.314: INFO: Trying to get logs from node bootstrap-e2e-minion-group-q1l7 pod downward-api-1b9547c4-06b2-4dbd-ab7c-d064330e0e64 container dapi-container: <nil>
STEP: delete the pod
Feb 28 17:27:03.409: INFO: Waiting for pod downward-api-1b9547c4-06b2-4dbd-ab7c-d064330e0e64 to disappear
Feb 28 17:27:03.447: INFO: Pod downward-api-1b9547c4-06b2-4dbd-ab7c-d064330e0e64 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:27:03.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2978" for this suite.
Feb 28 17:27:09.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:27:11.058: INFO: namespace downward-api-2978 deletion completed in 7.571543996s
•SSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:27:11.058: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 17:27:11.253: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 28 17:27:12.554: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:27:12.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-677" for this suite.
Feb 28 17:27:18.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:27:20.190: INFO: namespace replication-controller-677 deletion completed in 7.559134662s
•SSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:27:20.191: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Feb 28 17:27:20.390: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-5767" to be "success or failure"
Feb 28 17:27:20.427: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 37.440404ms
Feb 28 17:27:22.465: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075731645s
Feb 28 17:27:24.504: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.113879532s
STEP: Saw pod success
Feb 28 17:27:24.504: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 28 17:27:24.541: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 28 17:27:24.634: INFO: Waiting for pod pod-host-path-test to disappear
Feb 28 17:27:24.672: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:27:24.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-5767" for this suite.
Feb 28 17:27:30.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:27:32.281: INFO: namespace hostpath-5767 deletion completed in 7.566915779s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:27:32.281: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-48d2a0c0-21ff-45a4-ac32-4f546423eec9
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-48d2a0c0-21ff-45a4-ac32-4f546423eec9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:28:44.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-477" for this suite.
Feb 28 17:29:06.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:29:07.873: INFO: namespace configmap-477 deletion completed in 23.573197366s
•SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:29:07.873: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 28 17:29:08.072: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:29:13.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3239" for this suite.
Feb 28 17:29:35.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:29:36.624: INFO: namespace init-container-3239 deletion completed in 23.572010153s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:29:36.625: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Feb 28 17:29:36.826: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-6385'
Feb 28 17:29:37.317: INFO: stderr: ""
Feb 28 17:29:37.317: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 17:29:37.317: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6385'
Feb 28 17:29:37.574: INFO: stderr: ""
Feb 28 17:29:37.574: INFO: stdout: "update-demo-nautilus-t5wzw update-demo-nautilus-xlgw5 "
Feb 28 17:29:37.574: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-t5wzw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6385'
Feb 28 17:29:37.842: INFO: stderr: ""
Feb 28 17:29:37.842: INFO: stdout: ""
Feb 28 17:29:37.842: INFO: update-demo-nautilus-t5wzw is created but not running
Feb 28 17:29:42.843: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6385'
Feb 28 17:29:43.078: INFO: stderr: ""
Feb 28 17:29:43.078: INFO: stdout: "update-demo-nautilus-t5wzw update-demo-nautilus-xlgw5 "
Feb 28 17:29:43.078: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-t5wzw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6385'
Feb 28 17:29:43.307: INFO: stderr: ""
Feb 28 17:29:43.307: INFO: stdout: "true"
Feb 28 17:29:43.307: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-t5wzw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6385'
Feb 28 17:29:43.546: INFO: stderr: ""
Feb 28 17:29:43.546: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 17:29:43.546: INFO: validating pod update-demo-nautilus-t5wzw
Feb 28 17:29:43.587: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 17:29:43.587: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 17:29:43.587: INFO: update-demo-nautilus-t5wzw is verified up and running
Feb 28 17:29:43.587: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-xlgw5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6385'
Feb 28 17:29:43.822: INFO: stderr: ""
Feb 28 17:29:43.822: INFO: stdout: "true"
Feb 28 17:29:43.822: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-xlgw5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6385'
Feb 28 17:29:44.085: INFO: stderr: ""
Feb 28 17:29:44.085: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 17:29:44.085: INFO: validating pod update-demo-nautilus-xlgw5
Feb 28 17:29:44.126: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 17:29:44.126: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 17:29:44.126: INFO: update-demo-nautilus-xlgw5 is verified up and running
STEP: scaling down the replication controller
Feb 28 17:29:44.129: INFO: scanned /workspace for discovery docs: <nil>
Feb 28 17:29:44.129: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-6385'
Feb 28 17:29:44.488: INFO: stderr: ""
Feb 28 17:29:44.488: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 17:29:44.488: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6385'
Feb 28 17:29:44.754: INFO: stderr: ""
Feb 28 17:29:44.754: INFO: stdout: "update-demo-nautilus-t5wzw update-demo-nautilus-xlgw5 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 28 17:29:49.755: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6385'
Feb 28 17:29:50.023: INFO: stderr: ""
Feb 28 17:29:50.023: INFO: stdout: "update-demo-nautilus-t5wzw update-demo-nautilus-xlgw5 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 28 17:29:55.023: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6385'
Feb 28 17:29:55.301: INFO: stderr: ""
Feb 28 17:29:55.301: INFO: stdout: "update-demo-nautilus-t5wzw update-demo-nautilus-xlgw5 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 28 17:30:00.301: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6385'
Feb 28 17:30:00.557: INFO: stderr: ""
Feb 28 17:30:00.557: INFO: stdout: "update-demo-nautilus-xlgw5 "
Feb 28 17:30:00.557: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-xlgw5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6385'
Feb 28 17:30:00.799: INFO: stderr: ""
Feb 28 17:30:00.799: INFO: stdout: "true"
Feb 28 17:30:00.799: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-xlgw5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6385'
Feb 28 17:30:01.050: INFO: stderr: ""
Feb 28 17:30:01.050: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 17:30:01.050: INFO: validating pod update-demo-nautilus-xlgw5
Feb 28 17:30:01.090: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 17:30:01.090: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 17:30:01.090: INFO: update-demo-nautilus-xlgw5 is verified up and running
STEP: scaling up the replication controller
Feb 28 17:30:01.092: INFO: scanned /workspace for discovery docs: <nil>
Feb 28 17:30:01.092: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-6385'
Feb 28 17:30:01.451: INFO: stderr: ""
Feb 28 17:30:01.451: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 28 17:30:01.451: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6385'
Feb 28 17:30:01.699: INFO: stderr: ""
Feb 28 17:30:01.699: INFO: stdout: "update-demo-nautilus-v6p62 update-demo-nautilus-xlgw5 "
Feb 28 17:30:01.699: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-v6p62 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6385'
Feb 28 17:30:01.958: INFO: stderr: ""
Feb 28 17:30:01.958: INFO: stdout: ""
Feb 28 17:30:01.958: INFO: update-demo-nautilus-v6p62 is created but not running
Feb 28 17:30:06.959: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6385'
Feb 28 17:30:07.208: INFO: stderr: ""
Feb 28 17:30:07.208: INFO: stdout: "update-demo-nautilus-v6p62 update-demo-nautilus-xlgw5 "
Feb 28 17:30:07.208: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-v6p62 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6385'
Feb 28 17:30:07.464: INFO: stderr: ""
Feb 28 17:30:07.464: INFO: stdout: "true"
Feb 28 17:30:07.464: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-v6p62 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6385'
Feb 28 17:30:07.715: INFO: stderr: ""
Feb 28 17:30:07.715: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 17:30:07.715: INFO: validating pod update-demo-nautilus-v6p62
Feb 28 17:30:07.755: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 17:30:07.755: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 17:30:07.755: INFO: update-demo-nautilus-v6p62 is verified up and running
Feb 28 17:30:07.756: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-xlgw5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6385'
Feb 28 17:30:07.998: INFO: stderr: ""
Feb 28 17:30:07.998: INFO: stdout: "true"
Feb 28 17:30:07.998: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-xlgw5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6385'
Feb 28 17:30:08.243: INFO: stderr: ""
Feb 28 17:30:08.243: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 28 17:30:08.243: INFO: validating pod update-demo-nautilus-xlgw5
Feb 28 17:30:08.283: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 28 17:30:08.283: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 28 17:30:08.283: INFO: update-demo-nautilus-xlgw5 is verified up and running
STEP: using delete to clean up resources
Feb 28 17:30:08.283: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-6385'
Feb 28 17:30:08.589: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 28 17:30:08.589: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 28 17:30:08.589: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6385'
Feb 28 17:30:08.876: INFO: stderr: "No resources found.\n"
Feb 28 17:30:08.876: INFO: stdout: ""
Feb 28 17:30:08.876: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.82.216.130 --kubeconfig=/workspace/.kube/config get pods -l name=update-demo --namespace=kubectl-6385 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 28 17:30:09.126: INFO: stderr: ""
Feb 28 17:30:09.126: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:30:09.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6385" for this suite.
Feb 28 17:30:31.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:30:32.736: INFO: namespace kubectl-6385 deletion completed in 23.571629645s
•SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:30:32.736: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 28 17:30:33.273: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:33.328: INFO: Number of nodes with available pods: 0
Feb 28 17:30:33.328: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 17:30:34.367: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:34.405: INFO: Number of nodes with available pods: 0
Feb 28 17:30:34.405: INFO: Node bootstrap-e2e-minion-group-q1l7 is running more than one daemon pod
Feb 28 17:30:35.367: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:35.405: INFO: Number of nodes with available pods: 3
Feb 28 17:30:35.405: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 28 17:30:35.565: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:35.603: INFO: Number of nodes with available pods: 2
Feb 28 17:30:35.603: INFO: Node bootstrap-e2e-minion-group-t863 is running more than one daemon pod
Feb 28 17:30:36.642: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:36.680: INFO: Number of nodes with available pods: 2
Feb 28 17:30:36.680: INFO: Node bootstrap-e2e-minion-group-t863 is running more than one daemon pod
Feb 28 17:30:37.645: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:37.683: INFO: Number of nodes with available pods: 2
Feb 28 17:30:37.683: INFO: Node bootstrap-e2e-minion-group-t863 is running more than one daemon pod
Feb 28 17:30:38.642: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:38.680: INFO: Number of nodes with available pods: 2
Feb 28 17:30:38.680: INFO: Node bootstrap-e2e-minion-group-t863 is running more than one daemon pod
Feb 28 17:30:39.642: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:39.680: INFO: Number of nodes with available pods: 2
Feb 28 17:30:39.680: INFO: Node bootstrap-e2e-minion-group-t863 is running more than one daemon pod
Feb 28 17:30:40.642: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:40.680: INFO: Number of nodes with available pods: 2
Feb 28 17:30:40.680: INFO: Node bootstrap-e2e-minion-group-t863 is running more than one daemon pod
Feb 28 17:30:41.642: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:41.680: INFO: Number of nodes with available pods: 2
Feb 28 17:30:41.680: INFO: Node bootstrap-e2e-minion-group-t863 is running more than one daemon pod
Feb 28 17:30:42.642: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:42.680: INFO: Number of nodes with available pods: 2
Feb 28 17:30:42.680: INFO: Node bootstrap-e2e-minion-group-t863 is running more than one daemon pod
Feb 28 17:30:43.642: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:43.684: INFO: Number of nodes with available pods: 2
Feb 28 17:30:43.684: INFO: Node bootstrap-e2e-minion-group-t863 is running more than one daemon pod
Feb 28 17:30:44.642: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:44.680: INFO: Number of nodes with available pods: 2
Feb 28 17:30:44.680: INFO: Node bootstrap-e2e-minion-group-t863 is running more than one daemon pod
Feb 28 17:30:45.642: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:45.680: INFO: Number of nodes with available pods: 2
Feb 28 17:30:45.680: INFO: Node bootstrap-e2e-minion-group-t863 is running more than one daemon pod
Feb 28 17:30:46.642: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:46.680: INFO: Number of nodes with available pods: 2
Feb 28 17:30:46.680: INFO: Node bootstrap-e2e-minion-group-t863 is running more than one daemon pod
Feb 28 17:30:47.642: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:47.680: INFO: Number of nodes with available pods: 2
Feb 28 17:30:47.680: INFO: Node bootstrap-e2e-minion-group-t863 is running more than one daemon pod
Feb 28 17:30:48.642: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:48.680: INFO: Number of nodes with available pods: 2
Feb 28 17:30:48.680: INFO: Node bootstrap-e2e-minion-group-t863 is running more than one daemon pod
Feb 28 17:30:49.642: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 28 17:30:49.680: INFO: Number of nodes with available pods: 3
Feb 28 17:30:49.680: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2539, will wait for the garbage collector to delete the pods
Feb 28 17:30:49.847: INFO: Deleting DaemonSet.extensions daemon-set took: 41.744026ms
Feb 28 17:30:50.547: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.295683ms
Feb 28 17:30:58.786: INFO: Number of nodes with available pods: 0
Feb 28 17:30:58.786: INFO: Number of running nodes: 0, number of available pods: 0
Feb 28 17:30:58.824: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2539/daemonsets","resourceVersion":"21646"},"items":null}

Feb 28 17:30:58.862: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2539/pods","resourceVersion":"21646"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:30:59.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2539" for this suite.
Feb 28 17:31:05.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:31:06.697: INFO: namespace daemonsets-2539 deletion completed in 7.643897631s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:31:06.697: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 28 17:31:09.090: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:31:09.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4803" for this suite.
Feb 28 17:31:15.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:31:16.829: INFO: namespace container-runtime-4803 deletion completed in 7.615320781s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:31:16.829: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 28 17:31:37.141: INFO: Container started at 2020-02-28 17:31:18 +0000 UTC, pod became ready at 2020-02-28 17:31:35 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:31:37.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7308" for this suite.
Feb 28 17:31:59.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:32:00.750: INFO: namespace container-probe-7308 deletion completed in 23.570775539s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:32:00.751: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 28 17:32:00.988: INFO: Waiting up to 5m0s for pod "pod-d647fab8-2efc-4641-a444-68ab6da0bdcb" in namespace "emptydir-122" to be "success or failure"
Feb 28 17:32:01.026: INFO: Pod "pod-d647fab8-2efc-4641-a444-68ab6da0bdcb": Phase="Pending", Reason="", readiness=false. Elapsed: 37.625042ms
Feb 28 17:32:03.064: INFO: Pod "pod-d647fab8-2efc-4641-a444-68ab6da0bdcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076095451s
STEP: Saw pod success
Feb 28 17:32:03.065: INFO: Pod "pod-d647fab8-2efc-4641-a444-68ab6da0bdcb" satisfied condition "success or failure"
Feb 28 17:32:03.103: INFO: Trying to get logs from node bootstrap-e2e-minion-group-s43f pod pod-d647fab8-2efc-4641-a444-68ab6da0bdcb container test-container: <nil>
STEP: delete the pod
Feb 28 17:32:03.198: INFO: Waiting for pod pod-d647fab8-2efc-4641-a444-68ab6da0bdcb to disappear
Feb 28 17:32:03.236: INFO: Pod pod-d647fab8-2efc-4641-a444-68ab6da0bdcb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:32:03.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-122" for this suite.
Feb 28 17:32:09.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:32:10.830: INFO: namespace emptydir-122 deletion completed in 7.555794565s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 28 17:32:10.831: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-4406
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4406 to expose endpoints map[]
Feb 28 17:32:11.069: INFO: successfully validated that service endpoint-test2 in namespace services-4406 exposes endpoints map[] (37.583426ms elapsed)
STEP: Creating pod pod1 in namespace services-4406
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4406 to expose endpoints map[pod1:[80]]
Feb 28 17:32:14.415: INFO: successfully validated that service endpoint-test2 in namespace services-4406 exposes endpoints map[pod1:[80]] (3.302295791s elapsed)
STEP: Creating pod pod2 in namespace services-4406
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4406 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 28 17:32:16.799: INFO: successfully validated that service endpoint-test2 in namespace services-4406 exposes endpoints map[pod1:[80] pod2:[80]] (2.340839345s elapsed)
STEP: Deleting pod pod1 in namespace services-4406
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4406 to expose endpoints map[pod2:[80]]
Feb 28 17:32:16.921: INFO: successfully validated that service endpoint-test2 in namespace services-4406 exposes endpoints map[pod2:[80]] (78.659675ms elapsed)
STEP: Deleting pod pod2 in namespace services-4406
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4406 to expose endpoints map[]
Feb 28 17:32:17.002: INFO: successfully validated that service endpoint-test2 in namespace services-4406 exposes endpoints map[] (37.726477ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 28 17:32:17.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4406" for this suite.
Feb 28 17:32:39.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 28 17:32:40.655: INFO: namespace services-4406 deletion completed in 23.56035076s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSSSSSSSSSSSSSFeb 28 17:32:40.655: INFO: Running AfterSuite actions on all nodes
Feb 28 17:32:40.657: INFO: Running AfterSuite actions on node 1
Feb 28 17:32:40.657: INFO: Skipping dumping logs from cluster

Ran 215 of 4412 Specs in 6594.078 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4197 Skipped
