I0623 17:39:00.461204      20 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-309105263
I0623 17:39:00.461366      20 e2e.go:241] Starting e2e run "049fc179-ac49-4759-ad65-b2168bdb0223" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1561311539 - Will randomize all specs
Will run 215 of 4411 specs

Jun 23 17:39:00.589: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 17:39:00.590: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 23 17:39:16.036: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 23 17:39:16.083: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 23 17:39:16.083: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jun 23 17:39:16.083: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 23 17:39:16.089: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun 23 17:39:16.089: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
Jun 23 17:39:16.089: INFO: e2e test version: v1.15.0
Jun 23 17:39:16.090: INFO: kube-apiserver version: v1.15.0
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:39:16.091: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir
Jun 23 17:39:16.115: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 23 17:39:16.122: INFO: Waiting up to 5m0s for pod "pod-2321702e-b88f-4403-a269-72b3e6c12c1d" in namespace "emptydir-1264" to be "success or failure"
Jun 23 17:39:16.131: INFO: Pod "pod-2321702e-b88f-4403-a269-72b3e6c12c1d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.637773ms
Jun 23 17:39:18.139: INFO: Pod "pod-2321702e-b88f-4403-a269-72b3e6c12c1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016695666s
Jun 23 17:39:20.147: INFO: Pod "pod-2321702e-b88f-4403-a269-72b3e6c12c1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024380816s
STEP: Saw pod success
Jun 23 17:39:20.147: INFO: Pod "pod-2321702e-b88f-4403-a269-72b3e6c12c1d" satisfied condition "success or failure"
Jun 23 17:39:20.152: INFO: Trying to get logs from node worker-1 pod pod-2321702e-b88f-4403-a269-72b3e6c12c1d container test-container: <nil>
STEP: delete the pod
Jun 23 17:39:20.216: INFO: Waiting for pod pod-2321702e-b88f-4403-a269-72b3e6c12c1d to disappear
Jun 23 17:39:20.219: INFO: Pod pod-2321702e-b88f-4403-a269-72b3e6c12c1d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:39:20.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1264" for this suite.
Jun 23 17:39:26.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:39:26.451: INFO: namespace emptydir-1264 deletion completed in 6.22903248s

• [SLOW TEST:10.360 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:39:26.452: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-4f319152-a885-4ce2-b7c7-566f0efce713
STEP: Creating a pod to test consume configMaps
Jun 23 17:39:26.487: INFO: Waiting up to 5m0s for pod "pod-configmaps-0864df8f-3ed5-4bfe-ad7f-f16c5e1c97ee" in namespace "configmap-1678" to be "success or failure"
Jun 23 17:39:26.493: INFO: Pod "pod-configmaps-0864df8f-3ed5-4bfe-ad7f-f16c5e1c97ee": Phase="Pending", Reason="", readiness=false. Elapsed: 5.947683ms
Jun 23 17:39:28.501: INFO: Pod "pod-configmaps-0864df8f-3ed5-4bfe-ad7f-f16c5e1c97ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013823338s
Jun 23 17:39:30.507: INFO: Pod "pod-configmaps-0864df8f-3ed5-4bfe-ad7f-f16c5e1c97ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019793126s
STEP: Saw pod success
Jun 23 17:39:30.507: INFO: Pod "pod-configmaps-0864df8f-3ed5-4bfe-ad7f-f16c5e1c97ee" satisfied condition "success or failure"
Jun 23 17:39:30.511: INFO: Trying to get logs from node worker-1 pod pod-configmaps-0864df8f-3ed5-4bfe-ad7f-f16c5e1c97ee container configmap-volume-test: <nil>
STEP: delete the pod
Jun 23 17:39:30.534: INFO: Waiting for pod pod-configmaps-0864df8f-3ed5-4bfe-ad7f-f16c5e1c97ee to disappear
Jun 23 17:39:30.543: INFO: Pod pod-configmaps-0864df8f-3ed5-4bfe-ad7f-f16c5e1c97ee no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:39:30.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1678" for this suite.
Jun 23 17:39:36.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:39:36.649: INFO: namespace configmap-1678 deletion completed in 6.101496829s

• [SLOW TEST:10.197 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:39:36.651: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 23 17:39:36.673: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:39:42.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9017" for this suite.
Jun 23 17:39:48.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:39:48.220: INFO: namespace init-container-9017 deletion completed in 6.115520259s

• [SLOW TEST:11.570 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:39:48.220: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 23 17:39:48.258: INFO: Waiting up to 5m0s for pod "downward-api-525765d9-a8e3-4bcc-9120-c98b4b4d4c60" in namespace "downward-api-9343" to be "success or failure"
Jun 23 17:39:48.262: INFO: Pod "downward-api-525765d9-a8e3-4bcc-9120-c98b4b4d4c60": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018446ms
Jun 23 17:39:50.268: INFO: Pod "downward-api-525765d9-a8e3-4bcc-9120-c98b4b4d4c60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010151944s
Jun 23 17:39:52.272: INFO: Pod "downward-api-525765d9-a8e3-4bcc-9120-c98b4b4d4c60": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013400457s
Jun 23 17:39:54.282: INFO: Pod "downward-api-525765d9-a8e3-4bcc-9120-c98b4b4d4c60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024143815s
STEP: Saw pod success
Jun 23 17:39:54.282: INFO: Pod "downward-api-525765d9-a8e3-4bcc-9120-c98b4b4d4c60" satisfied condition "success or failure"
Jun 23 17:39:54.289: INFO: Trying to get logs from node worker-2 pod downward-api-525765d9-a8e3-4bcc-9120-c98b4b4d4c60 container dapi-container: <nil>
STEP: delete the pod
Jun 23 17:39:54.348: INFO: Waiting for pod downward-api-525765d9-a8e3-4bcc-9120-c98b4b4d4c60 to disappear
Jun 23 17:39:54.354: INFO: Pod downward-api-525765d9-a8e3-4bcc-9120-c98b4b4d4c60 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:39:54.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9343" for this suite.
Jun 23 17:40:00.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:40:00.469: INFO: namespace downward-api-9343 deletion completed in 6.112796825s

• [SLOW TEST:12.249 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:40:00.474: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 23 17:40:00.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1330'
Jun 23 17:40:01.061: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 23 17:40:01.061: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Jun 23 17:40:01.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete jobs e2e-test-nginx-job --namespace=kubectl-1330'
Jun 23 17:40:01.148: INFO: stderr: ""
Jun 23 17:40:01.148: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:40:01.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1330" for this suite.
Jun 23 17:40:23.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:40:23.215: INFO: namespace kubectl-1330 deletion completed in 22.065360001s

• [SLOW TEST:22.742 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:40:23.216: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:40:27.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7041" for this suite.
Jun 23 17:41:05.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:41:05.393: INFO: namespace kubelet-test-7041 deletion completed in 38.104889608s

• [SLOW TEST:42.178 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:41:05.396: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 23 17:41:05.421: INFO: Waiting up to 5m0s for pod "pod-264c54f0-412d-4749-a884-abfc6f82353f" in namespace "emptydir-3287" to be "success or failure"
Jun 23 17:41:05.426: INFO: Pod "pod-264c54f0-412d-4749-a884-abfc6f82353f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.866846ms
Jun 23 17:41:07.431: INFO: Pod "pod-264c54f0-412d-4749-a884-abfc6f82353f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010594265s
Jun 23 17:41:09.437: INFO: Pod "pod-264c54f0-412d-4749-a884-abfc6f82353f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015868519s
STEP: Saw pod success
Jun 23 17:41:09.437: INFO: Pod "pod-264c54f0-412d-4749-a884-abfc6f82353f" satisfied condition "success or failure"
Jun 23 17:41:09.444: INFO: Trying to get logs from node worker-1 pod pod-264c54f0-412d-4749-a884-abfc6f82353f container test-container: <nil>
STEP: delete the pod
Jun 23 17:41:09.473: INFO: Waiting for pod pod-264c54f0-412d-4749-a884-abfc6f82353f to disappear
Jun 23 17:41:09.476: INFO: Pod pod-264c54f0-412d-4749-a884-abfc6f82353f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:41:09.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3287" for this suite.
Jun 23 17:41:15.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:41:15.561: INFO: namespace emptydir-3287 deletion completed in 6.08257025s

• [SLOW TEST:10.166 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:41:15.564: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 23 17:41:15.586: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:41:19.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3301" for this suite.
Jun 23 17:41:25.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:41:25.737: INFO: namespace init-container-3301 deletion completed in 6.101290062s

• [SLOW TEST:10.173 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:41:25.737: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 17:41:25.764: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f6183dea-ec18-4cd4-8c00-ae509afff18d" in namespace "downward-api-5900" to be "success or failure"
Jun 23 17:41:25.771: INFO: Pod "downwardapi-volume-f6183dea-ec18-4cd4-8c00-ae509afff18d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.851539ms
Jun 23 17:41:27.774: INFO: Pod "downwardapi-volume-f6183dea-ec18-4cd4-8c00-ae509afff18d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009763315s
STEP: Saw pod success
Jun 23 17:41:27.774: INFO: Pod "downwardapi-volume-f6183dea-ec18-4cd4-8c00-ae509afff18d" satisfied condition "success or failure"
Jun 23 17:41:27.776: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-f6183dea-ec18-4cd4-8c00-ae509afff18d container client-container: <nil>
STEP: delete the pod
Jun 23 17:41:27.798: INFO: Waiting for pod downwardapi-volume-f6183dea-ec18-4cd4-8c00-ae509afff18d to disappear
Jun 23 17:41:27.800: INFO: Pod downwardapi-volume-f6183dea-ec18-4cd4-8c00-ae509afff18d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:41:27.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5900" for this suite.
Jun 23 17:41:33.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:41:33.873: INFO: namespace downward-api-5900 deletion completed in 6.071092743s

• [SLOW TEST:8.136 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:41:33.873: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 23 17:41:33.897: INFO: Waiting up to 5m0s for pod "pod-b53fb246-6128-45bd-b221-6a52d5166309" in namespace "emptydir-2514" to be "success or failure"
Jun 23 17:41:33.902: INFO: Pod "pod-b53fb246-6128-45bd-b221-6a52d5166309": Phase="Pending", Reason="", readiness=false. Elapsed: 4.541898ms
Jun 23 17:41:35.905: INFO: Pod "pod-b53fb246-6128-45bd-b221-6a52d5166309": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007121989s
STEP: Saw pod success
Jun 23 17:41:35.905: INFO: Pod "pod-b53fb246-6128-45bd-b221-6a52d5166309" satisfied condition "success or failure"
Jun 23 17:41:35.907: INFO: Trying to get logs from node worker-1 pod pod-b53fb246-6128-45bd-b221-6a52d5166309 container test-container: <nil>
STEP: delete the pod
Jun 23 17:41:35.920: INFO: Waiting for pod pod-b53fb246-6128-45bd-b221-6a52d5166309 to disappear
Jun 23 17:41:35.922: INFO: Pod pod-b53fb246-6128-45bd-b221-6a52d5166309 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:41:35.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2514" for this suite.
Jun 23 17:41:41.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:41:42.014: INFO: namespace emptydir-2514 deletion completed in 6.089675516s

• [SLOW TEST:8.141 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:41:42.016: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 23 17:41:45.069: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:41:45.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1788" for this suite.
Jun 23 17:41:51.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:41:51.199: INFO: namespace container-runtime-1788 deletion completed in 6.101370452s

• [SLOW TEST:9.183 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:41:51.199: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Jun 23 17:41:51.229: INFO: Waiting up to 5m0s for pod "var-expansion-e5c4b8d4-eca9-49b2-b1b5-26a3fbc82be0" in namespace "var-expansion-3314" to be "success or failure"
Jun 23 17:41:51.237: INFO: Pod "var-expansion-e5c4b8d4-eca9-49b2-b1b5-26a3fbc82be0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006959ms
Jun 23 17:41:53.239: INFO: Pod "var-expansion-e5c4b8d4-eca9-49b2-b1b5-26a3fbc82be0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010050324s
Jun 23 17:41:55.246: INFO: Pod "var-expansion-e5c4b8d4-eca9-49b2-b1b5-26a3fbc82be0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017685039s
STEP: Saw pod success
Jun 23 17:41:55.246: INFO: Pod "var-expansion-e5c4b8d4-eca9-49b2-b1b5-26a3fbc82be0" satisfied condition "success or failure"
Jun 23 17:41:55.250: INFO: Trying to get logs from node worker-1 pod var-expansion-e5c4b8d4-eca9-49b2-b1b5-26a3fbc82be0 container dapi-container: <nil>
STEP: delete the pod
Jun 23 17:41:55.279: INFO: Waiting for pod var-expansion-e5c4b8d4-eca9-49b2-b1b5-26a3fbc82be0 to disappear
Jun 23 17:41:55.284: INFO: Pod var-expansion-e5c4b8d4-eca9-49b2-b1b5-26a3fbc82be0 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:41:55.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3314" for this suite.
Jun 23 17:42:01.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:42:01.427: INFO: namespace var-expansion-3314 deletion completed in 6.118603767s

• [SLOW TEST:10.228 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:42:01.428: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 23 17:42:01.452: INFO: Waiting up to 5m0s for pod "downward-api-2e63b87a-2ef5-4100-ad28-a18558d90fe1" in namespace "downward-api-3989" to be "success or failure"
Jun 23 17:42:01.456: INFO: Pod "downward-api-2e63b87a-2ef5-4100-ad28-a18558d90fe1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.594584ms
Jun 23 17:42:03.464: INFO: Pod "downward-api-2e63b87a-2ef5-4100-ad28-a18558d90fe1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012624206s
Jun 23 17:42:05.476: INFO: Pod "downward-api-2e63b87a-2ef5-4100-ad28-a18558d90fe1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024158329s
STEP: Saw pod success
Jun 23 17:42:05.476: INFO: Pod "downward-api-2e63b87a-2ef5-4100-ad28-a18558d90fe1" satisfied condition "success or failure"
Jun 23 17:42:05.483: INFO: Trying to get logs from node worker-1 pod downward-api-2e63b87a-2ef5-4100-ad28-a18558d90fe1 container dapi-container: <nil>
STEP: delete the pod
Jun 23 17:42:05.521: INFO: Waiting for pod downward-api-2e63b87a-2ef5-4100-ad28-a18558d90fe1 to disappear
Jun 23 17:42:05.523: INFO: Pod downward-api-2e63b87a-2ef5-4100-ad28-a18558d90fe1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:42:05.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3989" for this suite.
Jun 23 17:42:11.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:42:11.586: INFO: namespace downward-api-3989 deletion completed in 6.060454128s

• [SLOW TEST:10.158 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:42:11.586: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-4ef46173-4290-4095-bcc4-b51cd32c160a
STEP: Creating a pod to test consume configMaps
Jun 23 17:42:11.620: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f7d00050-0b35-45cf-a9de-a1bd3cd2fdce" in namespace "projected-6416" to be "success or failure"
Jun 23 17:42:11.624: INFO: Pod "pod-projected-configmaps-f7d00050-0b35-45cf-a9de-a1bd3cd2fdce": Phase="Pending", Reason="", readiness=false. Elapsed: 3.263517ms
Jun 23 17:42:13.631: INFO: Pod "pod-projected-configmaps-f7d00050-0b35-45cf-a9de-a1bd3cd2fdce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010270241s
Jun 23 17:42:15.638: INFO: Pod "pod-projected-configmaps-f7d00050-0b35-45cf-a9de-a1bd3cd2fdce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017245558s
STEP: Saw pod success
Jun 23 17:42:15.638: INFO: Pod "pod-projected-configmaps-f7d00050-0b35-45cf-a9de-a1bd3cd2fdce" satisfied condition "success or failure"
Jun 23 17:42:15.643: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-f7d00050-0b35-45cf-a9de-a1bd3cd2fdce container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 23 17:42:15.692: INFO: Waiting for pod pod-projected-configmaps-f7d00050-0b35-45cf-a9de-a1bd3cd2fdce to disappear
Jun 23 17:42:15.695: INFO: Pod pod-projected-configmaps-f7d00050-0b35-45cf-a9de-a1bd3cd2fdce no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:42:15.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6416" for this suite.
Jun 23 17:42:21.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:42:21.843: INFO: namespace projected-6416 deletion completed in 6.14461706s

• [SLOW TEST:10.257 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:42:21.844: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2544
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2544
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2544
Jun 23 17:42:21.876: INFO: Found 0 stateful pods, waiting for 1
Jun 23 17:42:31.884: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 23 17:42:31.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-2544 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 23 17:42:32.099: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 23 17:42:32.099: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 23 17:42:32.099: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 23 17:42:32.101: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 23 17:42:42.111: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 23 17:42:42.111: INFO: Waiting for statefulset status.replicas updated to 0
Jun 23 17:42:42.146: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999259s
Jun 23 17:42:43.154: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.984314344s
Jun 23 17:42:44.162: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.97673124s
Jun 23 17:42:45.177: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.960138746s
Jun 23 17:42:46.190: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.947262192s
Jun 23 17:42:47.197: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.940738818s
Jun 23 17:42:48.212: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.926051547s
Jun 23 17:42:49.218: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.919356477s
Jun 23 17:42:50.225: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.912386168s
Jun 23 17:42:51.232: INFO: Verifying statefulset ss doesn't scale past 1 for another 905.352245ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2544
Jun 23 17:42:52.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-2544 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 23 17:42:52.398: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 23 17:42:52.398: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 23 17:42:52.398: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 23 17:42:52.400: INFO: Found 1 stateful pods, waiting for 3
Jun 23 17:43:02.408: INFO: Found 2 stateful pods, waiting for 3
Jun 23 17:43:12.409: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 23 17:43:12.409: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 23 17:43:12.409: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun 23 17:43:22.407: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 23 17:43:22.407: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 23 17:43:22.407: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 23 17:43:22.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-2544 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 23 17:43:22.576: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 23 17:43:22.576: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 23 17:43:22.576: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 23 17:43:22.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-2544 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 23 17:43:22.741: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 23 17:43:22.741: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 23 17:43:22.741: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 23 17:43:22.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-2544 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 23 17:43:22.916: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 23 17:43:22.916: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 23 17:43:22.916: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 23 17:43:22.916: INFO: Waiting for statefulset status.replicas updated to 0
Jun 23 17:43:22.917: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 23 17:43:32.931: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 23 17:43:32.931: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 23 17:43:32.931: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 23 17:43:32.950: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999569s
Jun 23 17:43:33.952: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996000049s
Jun 23 17:43:34.955: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993279407s
Jun 23 17:43:35.957: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991009759s
Jun 23 17:43:36.962: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988340675s
Jun 23 17:43:37.964: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983741668s
Jun 23 17:43:38.984: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.981396688s
Jun 23 17:43:39.995: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961819353s
Jun 23 17:43:41.013: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.950758146s
Jun 23 17:43:42.038: INFO: Verifying statefulset ss doesn't scale past 3 for another 914.899672ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2544
Jun 23 17:43:43.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-2544 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 23 17:43:43.187: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 23 17:43:43.188: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 23 17:43:43.188: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 23 17:43:43.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-2544 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 23 17:43:43.353: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 23 17:43:43.353: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 23 17:43:43.353: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 23 17:43:43.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-2544 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 23 17:43:43.503: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 23 17:43:43.503: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 23 17:43:43.503: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 23 17:43:43.503: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 23 17:43:53.515: INFO: Deleting all statefulset in ns statefulset-2544
Jun 23 17:43:53.518: INFO: Scaling statefulset ss to 0
Jun 23 17:43:53.525: INFO: Waiting for statefulset status.replicas updated to 0
Jun 23 17:43:53.527: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:43:53.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2544" for this suite.
Jun 23 17:43:59.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:43:59.664: INFO: namespace statefulset-2544 deletion completed in 6.115571618s

• [SLOW TEST:97.821 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:43:59.667: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 17:43:59.712: INFO: Waiting up to 5m0s for pod "downwardapi-volume-52309fbd-3d8e-4327-8341-ea62581189b1" in namespace "projected-7824" to be "success or failure"
Jun 23 17:43:59.726: INFO: Pod "downwardapi-volume-52309fbd-3d8e-4327-8341-ea62581189b1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.019487ms
Jun 23 17:44:01.732: INFO: Pod "downwardapi-volume-52309fbd-3d8e-4327-8341-ea62581189b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019105582s
Jun 23 17:44:03.734: INFO: Pod "downwardapi-volume-52309fbd-3d8e-4327-8341-ea62581189b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021738857s
STEP: Saw pod success
Jun 23 17:44:03.734: INFO: Pod "downwardapi-volume-52309fbd-3d8e-4327-8341-ea62581189b1" satisfied condition "success or failure"
Jun 23 17:44:03.736: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-52309fbd-3d8e-4327-8341-ea62581189b1 container client-container: <nil>
STEP: delete the pod
Jun 23 17:44:03.759: INFO: Waiting for pod downwardapi-volume-52309fbd-3d8e-4327-8341-ea62581189b1 to disappear
Jun 23 17:44:03.762: INFO: Pod downwardapi-volume-52309fbd-3d8e-4327-8341-ea62581189b1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:44:03.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7824" for this suite.
Jun 23 17:44:09.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:44:09.855: INFO: namespace projected-7824 deletion completed in 6.089623394s

• [SLOW TEST:10.189 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:44:09.855: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 17:44:09.900: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8bc44567-def0-4bd4-97f8-f603287a2971" in namespace "downward-api-5599" to be "success or failure"
Jun 23 17:44:09.906: INFO: Pod "downwardapi-volume-8bc44567-def0-4bd4-97f8-f603287a2971": Phase="Pending", Reason="", readiness=false. Elapsed: 6.472153ms
Jun 23 17:44:11.910: INFO: Pod "downwardapi-volume-8bc44567-def0-4bd4-97f8-f603287a2971": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010340893s
Jun 23 17:44:13.915: INFO: Pod "downwardapi-volume-8bc44567-def0-4bd4-97f8-f603287a2971": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015160439s
STEP: Saw pod success
Jun 23 17:44:13.915: INFO: Pod "downwardapi-volume-8bc44567-def0-4bd4-97f8-f603287a2971" satisfied condition "success or failure"
Jun 23 17:44:13.920: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-8bc44567-def0-4bd4-97f8-f603287a2971 container client-container: <nil>
STEP: delete the pod
Jun 23 17:44:13.953: INFO: Waiting for pod downwardapi-volume-8bc44567-def0-4bd4-97f8-f603287a2971 to disappear
Jun 23 17:44:13.959: INFO: Pod downwardapi-volume-8bc44567-def0-4bd4-97f8-f603287a2971 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:44:13.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5599" for this suite.
Jun 23 17:44:19.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:44:20.069: INFO: namespace downward-api-5599 deletion completed in 6.103284888s

• [SLOW TEST:10.213 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:44:20.070: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5590
I0623 17:44:20.095660      20 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5590, replica count: 1
I0623 17:44:21.146762      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0623 17:44:22.147407      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0623 17:44:23.147856      20 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 23 17:44:23.273: INFO: Created: latency-svc-xwfsg
Jun 23 17:44:23.283: INFO: Got endpoints: latency-svc-xwfsg [34.858923ms]
Jun 23 17:44:23.298: INFO: Created: latency-svc-h2wh9
Jun 23 17:44:23.303: INFO: Got endpoints: latency-svc-h2wh9 [19.871699ms]
Jun 23 17:44:23.313: INFO: Created: latency-svc-jdhfz
Jun 23 17:44:23.323: INFO: Got endpoints: latency-svc-jdhfz [40.053057ms]
Jun 23 17:44:23.335: INFO: Created: latency-svc-znmhr
Jun 23 17:44:23.341: INFO: Got endpoints: latency-svc-znmhr [58.125232ms]
Jun 23 17:44:23.356: INFO: Created: latency-svc-mqc9v
Jun 23 17:44:23.362: INFO: Got endpoints: latency-svc-mqc9v [78.777277ms]
Jun 23 17:44:23.370: INFO: Created: latency-svc-mcmcf
Jun 23 17:44:23.375: INFO: Got endpoints: latency-svc-mcmcf [92.369676ms]
Jun 23 17:44:23.385: INFO: Created: latency-svc-b27tm
Jun 23 17:44:23.390: INFO: Got endpoints: latency-svc-b27tm [106.783993ms]
Jun 23 17:44:23.405: INFO: Created: latency-svc-8zsp9
Jun 23 17:44:23.414: INFO: Got endpoints: latency-svc-8zsp9 [131.160314ms]
Jun 23 17:44:23.434: INFO: Created: latency-svc-r9chk
Jun 23 17:44:23.437: INFO: Got endpoints: latency-svc-r9chk [153.813549ms]
Jun 23 17:44:23.448: INFO: Created: latency-svc-hbkph
Jun 23 17:44:23.450: INFO: Got endpoints: latency-svc-hbkph [167.060152ms]
Jun 23 17:44:23.474: INFO: Created: latency-svc-5tmr9
Jun 23 17:44:23.478: INFO: Got endpoints: latency-svc-5tmr9 [194.636042ms]
Jun 23 17:44:23.505: INFO: Created: latency-svc-nx7pn
Jun 23 17:44:23.509: INFO: Got endpoints: latency-svc-nx7pn [225.293033ms]
Jun 23 17:44:23.515: INFO: Created: latency-svc-2dtsz
Jun 23 17:44:23.520: INFO: Got endpoints: latency-svc-2dtsz [236.248906ms]
Jun 23 17:44:23.534: INFO: Created: latency-svc-mww4q
Jun 23 17:44:23.538: INFO: Got endpoints: latency-svc-mww4q [254.693898ms]
Jun 23 17:44:23.554: INFO: Created: latency-svc-ccvlh
Jun 23 17:44:23.554: INFO: Got endpoints: latency-svc-ccvlh [270.595763ms]
Jun 23 17:44:23.583: INFO: Created: latency-svc-zc4q7
Jun 23 17:44:23.583: INFO: Got endpoints: latency-svc-zc4q7 [299.69185ms]
Jun 23 17:44:23.601: INFO: Created: latency-svc-h8brp
Jun 23 17:44:23.605: INFO: Got endpoints: latency-svc-h8brp [302.261621ms]
Jun 23 17:44:23.612: INFO: Created: latency-svc-lq9jj
Jun 23 17:44:23.618: INFO: Got endpoints: latency-svc-lq9jj [294.430631ms]
Jun 23 17:44:23.624: INFO: Created: latency-svc-jq8q8
Jun 23 17:44:23.626: INFO: Got endpoints: latency-svc-jq8q8 [284.936041ms]
Jun 23 17:44:23.655: INFO: Created: latency-svc-g6cl5
Jun 23 17:44:23.663: INFO: Created: latency-svc-pgtvj
Jun 23 17:44:23.666: INFO: Got endpoints: latency-svc-g6cl5 [303.62285ms]
Jun 23 17:44:23.677: INFO: Got endpoints: latency-svc-pgtvj [301.117341ms]
Jun 23 17:44:23.697: INFO: Created: latency-svc-d9267
Jun 23 17:44:23.703: INFO: Got endpoints: latency-svc-d9267 [312.638959ms]
Jun 23 17:44:23.710: INFO: Created: latency-svc-nllpp
Jun 23 17:44:23.713: INFO: Got endpoints: latency-svc-nllpp [298.34515ms]
Jun 23 17:44:23.718: INFO: Created: latency-svc-8mgzp
Jun 23 17:44:23.721: INFO: Got endpoints: latency-svc-8mgzp [18.211258ms]
Jun 23 17:44:23.728: INFO: Created: latency-svc-mbfvn
Jun 23 17:44:23.732: INFO: Got endpoints: latency-svc-mbfvn [294.226963ms]
Jun 23 17:44:23.736: INFO: Created: latency-svc-p69hh
Jun 23 17:44:23.740: INFO: Got endpoints: latency-svc-p69hh [290.119754ms]
Jun 23 17:44:23.748: INFO: Created: latency-svc-gchjr
Jun 23 17:44:23.759: INFO: Got endpoints: latency-svc-gchjr [281.05659ms]
Jun 23 17:44:23.769: INFO: Created: latency-svc-gcn5r
Jun 23 17:44:23.771: INFO: Got endpoints: latency-svc-gcn5r [262.635784ms]
Jun 23 17:44:23.776: INFO: Created: latency-svc-jbf7l
Jun 23 17:44:23.799: INFO: Got endpoints: latency-svc-jbf7l [279.1437ms]
Jun 23 17:44:23.843: INFO: Created: latency-svc-2mt4v
Jun 23 17:44:23.848: INFO: Created: latency-svc-xfbgr
Jun 23 17:44:23.863: INFO: Got endpoints: latency-svc-xfbgr [309.344936ms]
Jun 23 17:44:23.863: INFO: Got endpoints: latency-svc-2mt4v [325.155592ms]
Jun 23 17:44:23.879: INFO: Created: latency-svc-gp4pq
Jun 23 17:44:23.884: INFO: Got endpoints: latency-svc-gp4pq [300.646893ms]
Jun 23 17:44:23.888: INFO: Created: latency-svc-v4fz8
Jun 23 17:44:23.893: INFO: Got endpoints: latency-svc-v4fz8 [288.178886ms]
Jun 23 17:44:23.899: INFO: Created: latency-svc-rkhq9
Jun 23 17:44:23.956: INFO: Got endpoints: latency-svc-rkhq9 [337.529979ms]
Jun 23 17:44:23.979: INFO: Created: latency-svc-h2plz
Jun 23 17:44:23.991: INFO: Got endpoints: latency-svc-h2plz [364.282957ms]
Jun 23 17:44:24.004: INFO: Created: latency-svc-42jth
Jun 23 17:44:24.019: INFO: Got endpoints: latency-svc-42jth [351.302307ms]
Jun 23 17:44:24.028: INFO: Created: latency-svc-c26dz
Jun 23 17:44:24.039: INFO: Got endpoints: latency-svc-c26dz [361.193403ms]
Jun 23 17:44:24.044: INFO: Created: latency-svc-4bbq7
Jun 23 17:44:24.049: INFO: Got endpoints: latency-svc-4bbq7 [336.038279ms]
Jun 23 17:44:24.060: INFO: Created: latency-svc-5cnc4
Jun 23 17:44:24.072: INFO: Got endpoints: latency-svc-5cnc4 [350.623045ms]
Jun 23 17:44:24.081: INFO: Created: latency-svc-4vtmn
Jun 23 17:44:24.085: INFO: Got endpoints: latency-svc-4vtmn [353.353157ms]
Jun 23 17:44:24.103: INFO: Created: latency-svc-dg426
Jun 23 17:44:24.105: INFO: Got endpoints: latency-svc-dg426 [364.666851ms]
Jun 23 17:44:24.123: INFO: Created: latency-svc-hlww6
Jun 23 17:44:24.138: INFO: Got endpoints: latency-svc-hlww6 [377.595497ms]
Jun 23 17:44:24.155: INFO: Created: latency-svc-48fc5
Jun 23 17:44:24.155: INFO: Got endpoints: latency-svc-48fc5 [383.076922ms]
Jun 23 17:44:24.164: INFO: Created: latency-svc-vb4zn
Jun 23 17:44:24.164: INFO: Got endpoints: latency-svc-vb4zn [365.370911ms]
Jun 23 17:44:24.176: INFO: Created: latency-svc-xs9ck
Jun 23 17:44:24.181: INFO: Got endpoints: latency-svc-xs9ck [317.685976ms]
Jun 23 17:44:24.201: INFO: Created: latency-svc-jwx4n
Jun 23 17:44:24.220: INFO: Created: latency-svc-chc9d
Jun 23 17:44:24.221: INFO: Got endpoints: latency-svc-jwx4n [357.164161ms]
Jun 23 17:44:24.229: INFO: Got endpoints: latency-svc-chc9d [345.658931ms]
Jun 23 17:44:24.237: INFO: Created: latency-svc-rmrjz
Jun 23 17:44:24.251: INFO: Got endpoints: latency-svc-rmrjz [357.438776ms]
Jun 23 17:44:24.256: INFO: Created: latency-svc-9qjfz
Jun 23 17:44:24.265: INFO: Got endpoints: latency-svc-9qjfz [309.693388ms]
Jun 23 17:44:24.294: INFO: Created: latency-svc-qvzpc
Jun 23 17:44:24.294: INFO: Got endpoints: latency-svc-qvzpc [303.302522ms]
Jun 23 17:44:24.356: INFO: Created: latency-svc-j8qqh
Jun 23 17:44:24.356: INFO: Created: latency-svc-gcldf
Jun 23 17:44:24.365: INFO: Got endpoints: latency-svc-gcldf [345.37336ms]
Jun 23 17:44:24.397: INFO: Created: latency-svc-zcpzp
Jun 23 17:44:24.397: INFO: Got endpoints: latency-svc-j8qqh [358.270975ms]
Jun 23 17:44:24.420: INFO: Created: latency-svc-wm2rx
Jun 23 17:44:24.438: INFO: Created: latency-svc-m5cnl
Jun 23 17:44:24.444: INFO: Got endpoints: latency-svc-zcpzp [395.563418ms]
Jun 23 17:44:24.455: INFO: Created: latency-svc-tgt62
Jun 23 17:44:24.477: INFO: Created: latency-svc-5hkvg
Jun 23 17:44:24.479: INFO: Got endpoints: latency-svc-wm2rx [406.938748ms]
Jun 23 17:44:24.503: INFO: Created: latency-svc-bjz54
Jun 23 17:44:24.515: INFO: Created: latency-svc-w4dld
Jun 23 17:44:24.529: INFO: Created: latency-svc-4dq9l
Jun 23 17:44:24.530: INFO: Got endpoints: latency-svc-m5cnl [444.648191ms]
Jun 23 17:44:24.543: INFO: Created: latency-svc-97kb7
Jun 23 17:44:24.553: INFO: Created: latency-svc-k4lhm
Jun 23 17:44:24.566: INFO: Created: latency-svc-ksdr5
Jun 23 17:44:24.570: INFO: Created: latency-svc-8tc7v
Jun 23 17:44:24.594: INFO: Got endpoints: latency-svc-tgt62 [488.449155ms]
Jun 23 17:44:24.620: INFO: Created: latency-svc-ktn49
Jun 23 17:44:24.621: INFO: Created: latency-svc-mx5fc
Jun 23 17:44:24.631: INFO: Got endpoints: latency-svc-5hkvg [493.654209ms]
Jun 23 17:44:24.641: INFO: Created: latency-svc-xq9ct
Jun 23 17:44:24.654: INFO: Created: latency-svc-h2ltd
Jun 23 17:44:24.667: INFO: Created: latency-svc-mggn7
Jun 23 17:44:24.684: INFO: Got endpoints: latency-svc-bjz54 [529.34724ms]
Jun 23 17:44:24.702: INFO: Created: latency-svc-28vvv
Jun 23 17:44:24.736: INFO: Created: latency-svc-4kbhw
Jun 23 17:44:24.743: INFO: Got endpoints: latency-svc-w4dld [578.341689ms]
Jun 23 17:44:24.770: INFO: Created: latency-svc-cknsd
Jun 23 17:44:24.787: INFO: Created: latency-svc-nspkl
Jun 23 17:44:24.789: INFO: Got endpoints: latency-svc-4dq9l [607.149753ms]
Jun 23 17:44:24.811: INFO: Created: latency-svc-4h8d5
Jun 23 17:44:24.818: INFO: Created: latency-svc-zbz2g
Jun 23 17:44:24.829: INFO: Got endpoints: latency-svc-97kb7 [607.929077ms]
Jun 23 17:44:24.846: INFO: Created: latency-svc-q2t4l
Jun 23 17:44:24.880: INFO: Got endpoints: latency-svc-k4lhm [650.138459ms]
Jun 23 17:44:24.910: INFO: Created: latency-svc-kb4jv
Jun 23 17:44:24.941: INFO: Got endpoints: latency-svc-ksdr5 [690.015124ms]
Jun 23 17:44:24.979: INFO: Created: latency-svc-p2jh4
Jun 23 17:44:24.987: INFO: Got endpoints: latency-svc-8tc7v [720.844117ms]
Jun 23 17:44:25.032: INFO: Got endpoints: latency-svc-ktn49 [738.250757ms]
Jun 23 17:44:25.047: INFO: Created: latency-svc-pxbvh
Jun 23 17:44:25.064: INFO: Created: latency-svc-nkzsn
Jun 23 17:44:25.080: INFO: Got endpoints: latency-svc-mx5fc [714.489429ms]
Jun 23 17:44:25.100: INFO: Created: latency-svc-wg5pp
Jun 23 17:44:25.129: INFO: Got endpoints: latency-svc-xq9ct [731.958242ms]
Jun 23 17:44:25.142: INFO: Created: latency-svc-d7b4s
Jun 23 17:44:25.181: INFO: Got endpoints: latency-svc-h2ltd [736.370003ms]
Jun 23 17:44:25.207: INFO: Created: latency-svc-vz5sd
Jun 23 17:44:25.235: INFO: Got endpoints: latency-svc-mggn7 [756.131898ms]
Jun 23 17:44:25.248: INFO: Created: latency-svc-xnzcj
Jun 23 17:44:25.279: INFO: Got endpoints: latency-svc-28vvv [748.731557ms]
Jun 23 17:44:25.292: INFO: Created: latency-svc-79tmh
Jun 23 17:44:25.328: INFO: Got endpoints: latency-svc-4kbhw [734.34363ms]
Jun 23 17:44:25.362: INFO: Created: latency-svc-26gbv
Jun 23 17:44:25.378: INFO: Got endpoints: latency-svc-cknsd [746.9359ms]
Jun 23 17:44:25.392: INFO: Created: latency-svc-h25m9
Jun 23 17:44:25.430: INFO: Got endpoints: latency-svc-nspkl [745.560453ms]
Jun 23 17:44:25.457: INFO: Created: latency-svc-ncl7x
Jun 23 17:44:25.482: INFO: Got endpoints: latency-svc-4h8d5 [738.924992ms]
Jun 23 17:44:25.503: INFO: Created: latency-svc-bs8vf
Jun 23 17:44:25.530: INFO: Got endpoints: latency-svc-zbz2g [741.617148ms]
Jun 23 17:44:25.622: INFO: Got endpoints: latency-svc-q2t4l [792.880229ms]
Jun 23 17:44:25.635: INFO: Got endpoints: latency-svc-kb4jv [754.945396ms]
Jun 23 17:44:25.638: INFO: Created: latency-svc-wpmqp
Jun 23 17:44:25.657: INFO: Created: latency-svc-5r74w
Jun 23 17:44:25.667: INFO: Created: latency-svc-6wtz9
Jun 23 17:44:25.678: INFO: Got endpoints: latency-svc-p2jh4 [736.717078ms]
Jun 23 17:44:25.722: INFO: Created: latency-svc-8rfgl
Jun 23 17:44:25.735: INFO: Got endpoints: latency-svc-pxbvh [747.931642ms]
Jun 23 17:44:25.747: INFO: Created: latency-svc-p2ll4
Jun 23 17:44:25.787: INFO: Got endpoints: latency-svc-nkzsn [754.714496ms]
Jun 23 17:44:25.814: INFO: Created: latency-svc-lfft8
Jun 23 17:44:25.836: INFO: Got endpoints: latency-svc-wg5pp [756.557452ms]
Jun 23 17:44:25.863: INFO: Created: latency-svc-2c8gx
Jun 23 17:44:25.883: INFO: Got endpoints: latency-svc-d7b4s [753.302892ms]
Jun 23 17:44:25.901: INFO: Created: latency-svc-7sbkt
Jun 23 17:44:25.931: INFO: Got endpoints: latency-svc-vz5sd [749.942876ms]
Jun 23 17:44:25.961: INFO: Created: latency-svc-p284n
Jun 23 17:44:25.980: INFO: Got endpoints: latency-svc-xnzcj [745.227054ms]
Jun 23 17:44:25.992: INFO: Created: latency-svc-wwgsp
Jun 23 17:44:26.032: INFO: Got endpoints: latency-svc-79tmh [752.278747ms]
Jun 23 17:44:26.045: INFO: Created: latency-svc-ctdmr
Jun 23 17:44:26.081: INFO: Got endpoints: latency-svc-26gbv [751.542312ms]
Jun 23 17:44:26.102: INFO: Created: latency-svc-92w2z
Jun 23 17:44:26.129: INFO: Got endpoints: latency-svc-h25m9 [750.582436ms]
Jun 23 17:44:26.143: INFO: Created: latency-svc-7ndkr
Jun 23 17:44:26.182: INFO: Got endpoints: latency-svc-ncl7x [749.702651ms]
Jun 23 17:44:26.197: INFO: Created: latency-svc-26lsp
Jun 23 17:44:26.235: INFO: Got endpoints: latency-svc-bs8vf [753.512843ms]
Jun 23 17:44:26.257: INFO: Created: latency-svc-khcnx
Jun 23 17:44:26.282: INFO: Got endpoints: latency-svc-wpmqp [751.656066ms]
Jun 23 17:44:26.299: INFO: Created: latency-svc-4hz42
Jun 23 17:44:26.343: INFO: Got endpoints: latency-svc-5r74w [720.217389ms]
Jun 23 17:44:26.369: INFO: Created: latency-svc-hrgcw
Jun 23 17:44:26.379: INFO: Got endpoints: latency-svc-6wtz9 [743.564428ms]
Jun 23 17:44:26.405: INFO: Created: latency-svc-rcvgd
Jun 23 17:44:26.431: INFO: Got endpoints: latency-svc-8rfgl [752.593897ms]
Jun 23 17:44:26.462: INFO: Created: latency-svc-28tj8
Jun 23 17:44:26.501: INFO: Got endpoints: latency-svc-p2ll4 [766.327223ms]
Jun 23 17:44:26.545: INFO: Got endpoints: latency-svc-lfft8 [756.951035ms]
Jun 23 17:44:26.549: INFO: Created: latency-svc-bdhnv
Jun 23 17:44:26.571: INFO: Created: latency-svc-v7vf4
Jun 23 17:44:26.593: INFO: Got endpoints: latency-svc-2c8gx [756.77063ms]
Jun 23 17:44:26.615: INFO: Created: latency-svc-jmq7h
Jun 23 17:44:26.630: INFO: Got endpoints: latency-svc-7sbkt [747.030518ms]
Jun 23 17:44:26.647: INFO: Created: latency-svc-zpz8x
Jun 23 17:44:26.684: INFO: Got endpoints: latency-svc-p284n [752.655925ms]
Jun 23 17:44:26.703: INFO: Created: latency-svc-xnvcr
Jun 23 17:44:26.731: INFO: Got endpoints: latency-svc-wwgsp [749.941604ms]
Jun 23 17:44:26.744: INFO: Created: latency-svc-cdg46
Jun 23 17:44:26.779: INFO: Got endpoints: latency-svc-ctdmr [747.067156ms]
Jun 23 17:44:26.802: INFO: Created: latency-svc-gc9vc
Jun 23 17:44:26.831: INFO: Got endpoints: latency-svc-92w2z [749.762923ms]
Jun 23 17:44:26.841: INFO: Created: latency-svc-k6gkk
Jun 23 17:44:26.878: INFO: Got endpoints: latency-svc-7ndkr [749.137198ms]
Jun 23 17:44:26.902: INFO: Created: latency-svc-5bnzv
Jun 23 17:44:26.931: INFO: Got endpoints: latency-svc-26lsp [748.305845ms]
Jun 23 17:44:26.944: INFO: Created: latency-svc-mb7lw
Jun 23 17:44:26.981: INFO: Got endpoints: latency-svc-khcnx [745.017737ms]
Jun 23 17:44:26.991: INFO: Created: latency-svc-j7t2v
Jun 23 17:44:27.033: INFO: Got endpoints: latency-svc-4hz42 [749.982722ms]
Jun 23 17:44:27.051: INFO: Created: latency-svc-xscc6
Jun 23 17:44:27.079: INFO: Got endpoints: latency-svc-hrgcw [735.679725ms]
Jun 23 17:44:27.100: INFO: Created: latency-svc-kr4vd
Jun 23 17:44:27.130: INFO: Got endpoints: latency-svc-rcvgd [750.633296ms]
Jun 23 17:44:27.137: INFO: Created: latency-svc-l97gc
Jun 23 17:44:27.185: INFO: Got endpoints: latency-svc-28tj8 [753.725535ms]
Jun 23 17:44:27.220: INFO: Created: latency-svc-dnh4w
Jun 23 17:44:27.231: INFO: Got endpoints: latency-svc-bdhnv [729.365947ms]
Jun 23 17:44:27.241: INFO: Created: latency-svc-kvkvv
Jun 23 17:44:27.281: INFO: Got endpoints: latency-svc-v7vf4 [735.589175ms]
Jun 23 17:44:27.295: INFO: Created: latency-svc-k4zcg
Jun 23 17:44:27.331: INFO: Got endpoints: latency-svc-jmq7h [737.649658ms]
Jun 23 17:44:27.345: INFO: Created: latency-svc-kbppp
Jun 23 17:44:27.381: INFO: Got endpoints: latency-svc-zpz8x [750.716885ms]
Jun 23 17:44:27.396: INFO: Created: latency-svc-lnt4t
Jun 23 17:44:27.428: INFO: Got endpoints: latency-svc-xnvcr [743.870113ms]
Jun 23 17:44:27.441: INFO: Created: latency-svc-2q9vm
Jun 23 17:44:27.493: INFO: Got endpoints: latency-svc-cdg46 [761.793604ms]
Jun 23 17:44:27.517: INFO: Created: latency-svc-cxf4f
Jun 23 17:44:27.529: INFO: Got endpoints: latency-svc-gc9vc [749.233781ms]
Jun 23 17:44:27.546: INFO: Created: latency-svc-2p8b5
Jun 23 17:44:27.578: INFO: Got endpoints: latency-svc-k6gkk [747.295297ms]
Jun 23 17:44:27.593: INFO: Created: latency-svc-cm8wh
Jun 23 17:44:27.629: INFO: Got endpoints: latency-svc-5bnzv [750.40937ms]
Jun 23 17:44:27.642: INFO: Created: latency-svc-f2b9k
Jun 23 17:44:27.681: INFO: Got endpoints: latency-svc-mb7lw [750.130741ms]
Jun 23 17:44:27.694: INFO: Created: latency-svc-66qnm
Jun 23 17:44:27.731: INFO: Got endpoints: latency-svc-j7t2v [750.161798ms]
Jun 23 17:44:27.748: INFO: Created: latency-svc-pvcbg
Jun 23 17:44:27.780: INFO: Got endpoints: latency-svc-xscc6 [746.955509ms]
Jun 23 17:44:27.812: INFO: Created: latency-svc-pk26m
Jun 23 17:44:27.858: INFO: Got endpoints: latency-svc-kr4vd [778.949555ms]
Jun 23 17:44:27.882: INFO: Got endpoints: latency-svc-l97gc [751.223188ms]
Jun 23 17:44:27.900: INFO: Created: latency-svc-98p74
Jun 23 17:44:27.901: INFO: Created: latency-svc-t7jcb
Jun 23 17:44:27.938: INFO: Got endpoints: latency-svc-dnh4w [753.121459ms]
Jun 23 17:44:27.953: INFO: Created: latency-svc-jh4m6
Jun 23 17:44:27.980: INFO: Got endpoints: latency-svc-kvkvv [749.385972ms]
Jun 23 17:44:28.015: INFO: Created: latency-svc-gz5jg
Jun 23 17:44:28.034: INFO: Got endpoints: latency-svc-k4zcg [753.139757ms]
Jun 23 17:44:28.046: INFO: Created: latency-svc-jqdhj
Jun 23 17:44:28.079: INFO: Got endpoints: latency-svc-kbppp [748.07758ms]
Jun 23 17:44:28.107: INFO: Created: latency-svc-tx5n6
Jun 23 17:44:28.138: INFO: Got endpoints: latency-svc-lnt4t [756.687298ms]
Jun 23 17:44:28.169: INFO: Created: latency-svc-6gvf9
Jun 23 17:44:28.180: INFO: Got endpoints: latency-svc-2q9vm [751.104276ms]
Jun 23 17:44:28.230: INFO: Created: latency-svc-mq4nb
Jun 23 17:44:28.256: INFO: Got endpoints: latency-svc-cxf4f [763.180654ms]
Jun 23 17:44:28.270: INFO: Created: latency-svc-z4stx
Jun 23 17:44:28.282: INFO: Got endpoints: latency-svc-2p8b5 [753.360846ms]
Jun 23 17:44:28.304: INFO: Created: latency-svc-rfmnt
Jun 23 17:44:28.333: INFO: Got endpoints: latency-svc-cm8wh [754.966371ms]
Jun 23 17:44:28.346: INFO: Created: latency-svc-fmxkr
Jun 23 17:44:28.380: INFO: Got endpoints: latency-svc-f2b9k [751.086492ms]
Jun 23 17:44:28.394: INFO: Created: latency-svc-85tjg
Jun 23 17:44:28.442: INFO: Got endpoints: latency-svc-66qnm [761.476734ms]
Jun 23 17:44:28.458: INFO: Created: latency-svc-sht79
Jun 23 17:44:28.480: INFO: Got endpoints: latency-svc-pvcbg [748.638007ms]
Jun 23 17:44:28.499: INFO: Created: latency-svc-s8zbk
Jun 23 17:44:28.532: INFO: Got endpoints: latency-svc-pk26m [751.23143ms]
Jun 23 17:44:28.551: INFO: Created: latency-svc-d5kcb
Jun 23 17:44:28.586: INFO: Got endpoints: latency-svc-98p74 [727.217561ms]
Jun 23 17:44:28.604: INFO: Created: latency-svc-kt64g
Jun 23 17:44:28.629: INFO: Got endpoints: latency-svc-t7jcb [747.727525ms]
Jun 23 17:44:28.645: INFO: Created: latency-svc-jnv7d
Jun 23 17:44:28.679: INFO: Got endpoints: latency-svc-jh4m6 [741.098159ms]
Jun 23 17:44:28.694: INFO: Created: latency-svc-xrb7w
Jun 23 17:44:28.731: INFO: Got endpoints: latency-svc-gz5jg [751.158475ms]
Jun 23 17:44:28.750: INFO: Created: latency-svc-7qs9m
Jun 23 17:44:28.779: INFO: Got endpoints: latency-svc-jqdhj [744.913816ms]
Jun 23 17:44:28.794: INFO: Created: latency-svc-9mzz5
Jun 23 17:44:28.830: INFO: Got endpoints: latency-svc-tx5n6 [750.106186ms]
Jun 23 17:44:28.870: INFO: Created: latency-svc-pt2wq
Jun 23 17:44:28.879: INFO: Got endpoints: latency-svc-6gvf9 [740.426895ms]
Jun 23 17:44:28.890: INFO: Created: latency-svc-6tk8l
Jun 23 17:44:28.930: INFO: Got endpoints: latency-svc-mq4nb [750.779346ms]
Jun 23 17:44:28.943: INFO: Created: latency-svc-pwbrq
Jun 23 17:44:28.982: INFO: Got endpoints: latency-svc-z4stx [725.556289ms]
Jun 23 17:44:28.995: INFO: Created: latency-svc-wkdrg
Jun 23 17:44:29.031: INFO: Got endpoints: latency-svc-rfmnt [748.326981ms]
Jun 23 17:44:29.042: INFO: Created: latency-svc-xkd6v
Jun 23 17:44:29.079: INFO: Got endpoints: latency-svc-fmxkr [745.218495ms]
Jun 23 17:44:29.088: INFO: Created: latency-svc-hkgq2
Jun 23 17:44:29.131: INFO: Got endpoints: latency-svc-85tjg [750.508651ms]
Jun 23 17:44:29.156: INFO: Created: latency-svc-wfkjn
Jun 23 17:44:29.180: INFO: Got endpoints: latency-svc-sht79 [737.444654ms]
Jun 23 17:44:29.204: INFO: Created: latency-svc-vg67g
Jun 23 17:44:29.234: INFO: Got endpoints: latency-svc-s8zbk [752.689351ms]
Jun 23 17:44:29.259: INFO: Created: latency-svc-54wv2
Jun 23 17:44:29.282: INFO: Got endpoints: latency-svc-d5kcb [750.586899ms]
Jun 23 17:44:29.315: INFO: Created: latency-svc-tzpbm
Jun 23 17:44:29.333: INFO: Got endpoints: latency-svc-kt64g [746.387622ms]
Jun 23 17:44:29.352: INFO: Created: latency-svc-pvmfh
Jun 23 17:44:29.387: INFO: Got endpoints: latency-svc-jnv7d [757.830661ms]
Jun 23 17:44:29.408: INFO: Created: latency-svc-lv9h2
Jun 23 17:44:29.442: INFO: Got endpoints: latency-svc-xrb7w [762.456929ms]
Jun 23 17:44:29.464: INFO: Created: latency-svc-txdwq
Jun 23 17:44:29.479: INFO: Got endpoints: latency-svc-7qs9m [747.33371ms]
Jun 23 17:44:29.517: INFO: Created: latency-svc-mcbtl
Jun 23 17:44:29.531: INFO: Got endpoints: latency-svc-9mzz5 [752.401796ms]
Jun 23 17:44:29.546: INFO: Created: latency-svc-6fwsv
Jun 23 17:44:29.581: INFO: Got endpoints: latency-svc-pt2wq [751.355483ms]
Jun 23 17:44:29.600: INFO: Created: latency-svc-8qqcs
Jun 23 17:44:29.630: INFO: Got endpoints: latency-svc-6tk8l [751.055372ms]
Jun 23 17:44:29.655: INFO: Created: latency-svc-srhj4
Jun 23 17:44:29.679: INFO: Got endpoints: latency-svc-pwbrq [748.0337ms]
Jun 23 17:44:29.724: INFO: Created: latency-svc-t8226
Jun 23 17:44:29.735: INFO: Got endpoints: latency-svc-wkdrg [752.581014ms]
Jun 23 17:44:29.750: INFO: Created: latency-svc-bcdz2
Jun 23 17:44:29.780: INFO: Got endpoints: latency-svc-xkd6v [749.630375ms]
Jun 23 17:44:29.807: INFO: Created: latency-svc-pgtcx
Jun 23 17:44:29.833: INFO: Got endpoints: latency-svc-hkgq2 [753.946891ms]
Jun 23 17:44:29.860: INFO: Created: latency-svc-fgnq8
Jun 23 17:44:29.882: INFO: Got endpoints: latency-svc-wfkjn [750.801096ms]
Jun 23 17:44:29.902: INFO: Created: latency-svc-tchkt
Jun 23 17:44:29.933: INFO: Got endpoints: latency-svc-vg67g [752.829447ms]
Jun 23 17:44:29.946: INFO: Created: latency-svc-bh4fr
Jun 23 17:44:29.982: INFO: Got endpoints: latency-svc-54wv2 [748.515193ms]
Jun 23 17:44:30.008: INFO: Created: latency-svc-g7jt6
Jun 23 17:44:30.030: INFO: Got endpoints: latency-svc-tzpbm [746.840315ms]
Jun 23 17:44:30.044: INFO: Created: latency-svc-mldsw
Jun 23 17:44:30.081: INFO: Got endpoints: latency-svc-pvmfh [747.998984ms]
Jun 23 17:44:30.094: INFO: Created: latency-svc-gmxb5
Jun 23 17:44:30.137: INFO: Got endpoints: latency-svc-lv9h2 [749.451782ms]
Jun 23 17:44:30.159: INFO: Created: latency-svc-t9hgc
Jun 23 17:44:30.183: INFO: Got endpoints: latency-svc-txdwq [740.155168ms]
Jun 23 17:44:30.199: INFO: Created: latency-svc-95277
Jun 23 17:44:30.229: INFO: Got endpoints: latency-svc-mcbtl [749.998475ms]
Jun 23 17:44:30.242: INFO: Created: latency-svc-lk55b
Jun 23 17:44:30.285: INFO: Got endpoints: latency-svc-6fwsv [753.138711ms]
Jun 23 17:44:30.309: INFO: Created: latency-svc-8trx7
Jun 23 17:44:30.333: INFO: Got endpoints: latency-svc-8qqcs [751.269099ms]
Jun 23 17:44:30.343: INFO: Created: latency-svc-8z6fd
Jun 23 17:44:30.380: INFO: Got endpoints: latency-svc-srhj4 [749.281912ms]
Jun 23 17:44:30.398: INFO: Created: latency-svc-bx6j8
Jun 23 17:44:30.431: INFO: Got endpoints: latency-svc-t8226 [751.75734ms]
Jun 23 17:44:30.453: INFO: Created: latency-svc-gqpkl
Jun 23 17:44:30.482: INFO: Got endpoints: latency-svc-bcdz2 [747.650544ms]
Jun 23 17:44:30.494: INFO: Created: latency-svc-cbs5q
Jun 23 17:44:30.530: INFO: Got endpoints: latency-svc-pgtcx [749.634935ms]
Jun 23 17:44:30.547: INFO: Created: latency-svc-22npt
Jun 23 17:44:30.594: INFO: Got endpoints: latency-svc-fgnq8 [761.226468ms]
Jun 23 17:44:30.612: INFO: Created: latency-svc-w9695
Jun 23 17:44:30.631: INFO: Got endpoints: latency-svc-tchkt [748.451844ms]
Jun 23 17:44:30.646: INFO: Created: latency-svc-9pp4b
Jun 23 17:44:30.685: INFO: Got endpoints: latency-svc-bh4fr [751.644044ms]
Jun 23 17:44:30.710: INFO: Created: latency-svc-k6dsb
Jun 23 17:44:30.730: INFO: Got endpoints: latency-svc-g7jt6 [747.050996ms]
Jun 23 17:44:30.765: INFO: Created: latency-svc-2x8vl
Jun 23 17:44:30.781: INFO: Got endpoints: latency-svc-mldsw [750.892309ms]
Jun 23 17:44:30.791: INFO: Created: latency-svc-b6zh9
Jun 23 17:44:30.835: INFO: Got endpoints: latency-svc-gmxb5 [754.656918ms]
Jun 23 17:44:30.855: INFO: Created: latency-svc-42gk2
Jun 23 17:44:30.885: INFO: Got endpoints: latency-svc-t9hgc [748.347824ms]
Jun 23 17:44:30.993: INFO: Got endpoints: latency-svc-95277 [809.603874ms]
Jun 23 17:44:30.999: INFO: Got endpoints: latency-svc-lk55b [768.997593ms]
Jun 23 17:44:31.016: INFO: Created: latency-svc-qgmsn
Jun 23 17:44:31.035: INFO: Got endpoints: latency-svc-8trx7 [750.105346ms]
Jun 23 17:44:31.035: INFO: Created: latency-svc-csbb9
Jun 23 17:44:31.049: INFO: Created: latency-svc-vqj64
Jun 23 17:44:31.060: INFO: Created: latency-svc-46hkb
Jun 23 17:44:31.091: INFO: Got endpoints: latency-svc-8z6fd [757.696095ms]
Jun 23 17:44:31.112: INFO: Created: latency-svc-kmvcq
Jun 23 17:44:31.129: INFO: Got endpoints: latency-svc-bx6j8 [748.900055ms]
Jun 23 17:44:31.183: INFO: Got endpoints: latency-svc-gqpkl [752.339131ms]
Jun 23 17:44:31.229: INFO: Got endpoints: latency-svc-cbs5q [745.826587ms]
Jun 23 17:44:31.279: INFO: Got endpoints: latency-svc-22npt [748.79022ms]
Jun 23 17:44:31.334: INFO: Got endpoints: latency-svc-w9695 [740.171925ms]
Jun 23 17:44:31.383: INFO: Got endpoints: latency-svc-9pp4b [751.543665ms]
Jun 23 17:44:31.437: INFO: Got endpoints: latency-svc-k6dsb [752.577934ms]
Jun 23 17:44:31.485: INFO: Got endpoints: latency-svc-2x8vl [754.691088ms]
Jun 23 17:44:31.531: INFO: Got endpoints: latency-svc-b6zh9 [749.930308ms]
Jun 23 17:44:31.583: INFO: Got endpoints: latency-svc-42gk2 [747.269598ms]
Jun 23 17:44:31.635: INFO: Got endpoints: latency-svc-qgmsn [749.335844ms]
Jun 23 17:44:31.679: INFO: Got endpoints: latency-svc-csbb9 [685.274195ms]
Jun 23 17:44:31.732: INFO: Got endpoints: latency-svc-vqj64 [732.69805ms]
Jun 23 17:44:31.785: INFO: Got endpoints: latency-svc-46hkb [749.836312ms]
Jun 23 17:44:31.832: INFO: Got endpoints: latency-svc-kmvcq [740.718477ms]
Jun 23 17:44:31.832: INFO: Latencies: [18.211258ms 19.871699ms 40.053057ms 58.125232ms 78.777277ms 92.369676ms 106.783993ms 131.160314ms 153.813549ms 167.060152ms 194.636042ms 225.293033ms 236.248906ms 254.693898ms 262.635784ms 270.595763ms 279.1437ms 281.05659ms 284.936041ms 288.178886ms 290.119754ms 294.226963ms 294.430631ms 298.34515ms 299.69185ms 300.646893ms 301.117341ms 302.261621ms 303.302522ms 303.62285ms 309.344936ms 309.693388ms 312.638959ms 317.685976ms 325.155592ms 336.038279ms 337.529979ms 345.37336ms 345.658931ms 350.623045ms 351.302307ms 353.353157ms 357.164161ms 357.438776ms 358.270975ms 361.193403ms 364.282957ms 364.666851ms 365.370911ms 377.595497ms 383.076922ms 395.563418ms 406.938748ms 444.648191ms 488.449155ms 493.654209ms 529.34724ms 578.341689ms 607.149753ms 607.929077ms 650.138459ms 685.274195ms 690.015124ms 714.489429ms 720.217389ms 720.844117ms 725.556289ms 727.217561ms 729.365947ms 731.958242ms 732.69805ms 734.34363ms 735.589175ms 735.679725ms 736.370003ms 736.717078ms 737.444654ms 737.649658ms 738.250757ms 738.924992ms 740.155168ms 740.171925ms 740.426895ms 740.718477ms 741.098159ms 741.617148ms 743.564428ms 743.870113ms 744.913816ms 745.017737ms 745.218495ms 745.227054ms 745.560453ms 745.826587ms 746.387622ms 746.840315ms 746.9359ms 746.955509ms 747.030518ms 747.050996ms 747.067156ms 747.269598ms 747.295297ms 747.33371ms 747.650544ms 747.727525ms 747.931642ms 747.998984ms 748.0337ms 748.07758ms 748.305845ms 748.326981ms 748.347824ms 748.451844ms 748.515193ms 748.638007ms 748.731557ms 748.79022ms 748.900055ms 749.137198ms 749.233781ms 749.281912ms 749.335844ms 749.385972ms 749.451782ms 749.630375ms 749.634935ms 749.702651ms 749.762923ms 749.836312ms 749.930308ms 749.941604ms 749.942876ms 749.982722ms 749.998475ms 750.105346ms 750.106186ms 750.130741ms 750.161798ms 750.40937ms 750.508651ms 750.582436ms 750.586899ms 750.633296ms 750.716885ms 750.779346ms 750.801096ms 750.892309ms 751.055372ms 751.086492ms 751.104276ms 751.158475ms 751.223188ms 751.23143ms 751.269099ms 751.355483ms 751.542312ms 751.543665ms 751.644044ms 751.656066ms 751.75734ms 752.278747ms 752.339131ms 752.401796ms 752.577934ms 752.581014ms 752.593897ms 752.655925ms 752.689351ms 752.829447ms 753.121459ms 753.138711ms 753.139757ms 753.302892ms 753.360846ms 753.512843ms 753.725535ms 753.946891ms 754.656918ms 754.691088ms 754.714496ms 754.945396ms 754.966371ms 756.131898ms 756.557452ms 756.687298ms 756.77063ms 756.951035ms 757.696095ms 757.830661ms 761.226468ms 761.476734ms 761.793604ms 762.456929ms 763.180654ms 766.327223ms 768.997593ms 778.949555ms 792.880229ms 809.603874ms]
Jun 23 17:44:31.832: INFO: 50 %ile: 747.067156ms
Jun 23 17:44:31.832: INFO: 90 %ile: 754.714496ms
Jun 23 17:44:31.832: INFO: 99 %ile: 792.880229ms
Jun 23 17:44:31.833: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:44:31.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5590" for this suite.
Jun 23 17:44:45.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:44:45.941: INFO: namespace svc-latency-5590 deletion completed in 14.104458971s

• [SLOW TEST:25.872 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:44:45.942: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0623 17:45:16.625724      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 23 17:45:16.626: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:45:16.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-746" for this suite.
Jun 23 17:45:22.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:45:22.701: INFO: namespace gc-746 deletion completed in 6.067760165s

• [SLOW TEST:36.759 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:45:22.703: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 17:45:22.751: INFO: Create a RollingUpdate DaemonSet
Jun 23 17:45:22.757: INFO: Check that daemon pods launch on every node of the cluster
Jun 23 17:45:22.793: INFO: Number of nodes with available pods: 0
Jun 23 17:45:22.793: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 17:45:23.809: INFO: Number of nodes with available pods: 0
Jun 23 17:45:23.809: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 17:45:24.819: INFO: Number of nodes with available pods: 1
Jun 23 17:45:24.819: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 17:45:25.808: INFO: Number of nodes with available pods: 3
Jun 23 17:45:25.808: INFO: Number of running nodes: 3, number of available pods: 3
Jun 23 17:45:25.808: INFO: Update the DaemonSet to trigger a rollout
Jun 23 17:45:25.831: INFO: Updating DaemonSet daemon-set
Jun 23 17:45:29.847: INFO: Roll back the DaemonSet before rollout is complete
Jun 23 17:45:29.853: INFO: Updating DaemonSet daemon-set
Jun 23 17:45:29.853: INFO: Make sure DaemonSet rollback is complete
Jun 23 17:45:29.856: INFO: Wrong image for pod: daemon-set-8mhpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 23 17:45:29.856: INFO: Pod daemon-set-8mhpz is not available
Jun 23 17:45:30.860: INFO: Wrong image for pod: daemon-set-8mhpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 23 17:45:30.860: INFO: Pod daemon-set-8mhpz is not available
Jun 23 17:45:31.870: INFO: Wrong image for pod: daemon-set-8mhpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 23 17:45:31.870: INFO: Pod daemon-set-8mhpz is not available
Jun 23 17:45:32.870: INFO: Wrong image for pod: daemon-set-8mhpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 23 17:45:32.870: INFO: Pod daemon-set-8mhpz is not available
Jun 23 17:45:33.865: INFO: Wrong image for pod: daemon-set-8mhpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 23 17:45:33.865: INFO: Pod daemon-set-8mhpz is not available
Jun 23 17:45:34.865: INFO: Wrong image for pod: daemon-set-8mhpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 23 17:45:34.865: INFO: Pod daemon-set-8mhpz is not available
Jun 23 17:45:35.865: INFO: Wrong image for pod: daemon-set-8mhpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 23 17:45:35.865: INFO: Pod daemon-set-8mhpz is not available
Jun 23 17:45:36.866: INFO: Wrong image for pod: daemon-set-8mhpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 23 17:45:36.866: INFO: Pod daemon-set-8mhpz is not available
Jun 23 17:45:37.866: INFO: Wrong image for pod: daemon-set-8mhpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 23 17:45:37.866: INFO: Pod daemon-set-8mhpz is not available
Jun 23 17:45:38.865: INFO: Wrong image for pod: daemon-set-8mhpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 23 17:45:38.865: INFO: Pod daemon-set-8mhpz is not available
Jun 23 17:45:39.863: INFO: Wrong image for pod: daemon-set-8mhpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 23 17:45:39.863: INFO: Pod daemon-set-8mhpz is not available
Jun 23 17:45:40.860: INFO: Wrong image for pod: daemon-set-8mhpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 23 17:45:40.860: INFO: Pod daemon-set-8mhpz is not available
Jun 23 17:45:41.868: INFO: Pod daemon-set-v4vhp is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3757, will wait for the garbage collector to delete the pods
Jun 23 17:45:41.954: INFO: Deleting DaemonSet.extensions daemon-set took: 4.631106ms
Jun 23 17:45:42.254: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.333286ms
Jun 23 17:45:44.563: INFO: Number of nodes with available pods: 0
Jun 23 17:45:44.563: INFO: Number of running nodes: 0, number of available pods: 0
Jun 23 17:45:44.577: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3757/daemonsets","resourceVersion":"3460"},"items":null}

Jun 23 17:45:44.584: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3757/pods","resourceVersion":"3460"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:45:44.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3757" for this suite.
Jun 23 17:45:50.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:45:50.716: INFO: namespace daemonsets-3757 deletion completed in 6.100404495s

• [SLOW TEST:28.013 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:45:50.716: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-9fe19723-8f39-4998-81c7-7c6e980a4276
STEP: Creating a pod to test consume configMaps
Jun 23 17:45:50.744: INFO: Waiting up to 5m0s for pod "pod-configmaps-5209f9d2-1402-4f88-8373-82783c24b0a8" in namespace "configmap-3005" to be "success or failure"
Jun 23 17:45:50.749: INFO: Pod "pod-configmaps-5209f9d2-1402-4f88-8373-82783c24b0a8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.36443ms
Jun 23 17:45:52.775: INFO: Pod "pod-configmaps-5209f9d2-1402-4f88-8373-82783c24b0a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031336129s
STEP: Saw pod success
Jun 23 17:45:52.775: INFO: Pod "pod-configmaps-5209f9d2-1402-4f88-8373-82783c24b0a8" satisfied condition "success or failure"
Jun 23 17:45:52.782: INFO: Trying to get logs from node worker-1 pod pod-configmaps-5209f9d2-1402-4f88-8373-82783c24b0a8 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 23 17:45:52.825: INFO: Waiting for pod pod-configmaps-5209f9d2-1402-4f88-8373-82783c24b0a8 to disappear
Jun 23 17:45:52.831: INFO: Pod pod-configmaps-5209f9d2-1402-4f88-8373-82783c24b0a8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:45:52.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3005" for this suite.
Jun 23 17:45:58.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:45:58.945: INFO: namespace configmap-3005 deletion completed in 6.110127773s

• [SLOW TEST:8.229 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:45:58.945: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 17:45:58.998: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9f34c0e-f502-4964-add8-d78049442ecb" in namespace "projected-8594" to be "success or failure"
Jun 23 17:45:59.010: INFO: Pod "downwardapi-volume-f9f34c0e-f502-4964-add8-d78049442ecb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.322745ms
Jun 23 17:46:01.014: INFO: Pod "downwardapi-volume-f9f34c0e-f502-4964-add8-d78049442ecb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01568514s
Jun 23 17:46:03.027: INFO: Pod "downwardapi-volume-f9f34c0e-f502-4964-add8-d78049442ecb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028729921s
STEP: Saw pod success
Jun 23 17:46:03.027: INFO: Pod "downwardapi-volume-f9f34c0e-f502-4964-add8-d78049442ecb" satisfied condition "success or failure"
Jun 23 17:46:03.032: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-f9f34c0e-f502-4964-add8-d78049442ecb container client-container: <nil>
STEP: delete the pod
Jun 23 17:46:03.066: INFO: Waiting for pod downwardapi-volume-f9f34c0e-f502-4964-add8-d78049442ecb to disappear
Jun 23 17:46:03.068: INFO: Pod downwardapi-volume-f9f34c0e-f502-4964-add8-d78049442ecb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:46:03.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8594" for this suite.
Jun 23 17:46:09.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:46:09.172: INFO: namespace projected-8594 deletion completed in 6.102166064s

• [SLOW TEST:10.228 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:46:09.175: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 23 17:46:11.757: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7719 pod-service-account-4cc93b1c-1ac7-454e-9f90-ed88a8f7bf03 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 23 17:46:11.952: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7719 pod-service-account-4cc93b1c-1ac7-454e-9f90-ed88a8f7bf03 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 23 17:46:12.169: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7719 pod-service-account-4cc93b1c-1ac7-454e-9f90-ed88a8f7bf03 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:46:12.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7719" for this suite.
Jun 23 17:46:18.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:46:18.426: INFO: namespace svcaccounts-7719 deletion completed in 6.122166075s

• [SLOW TEST:9.251 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:46:18.426: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-5404
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5404 to expose endpoints map[]
Jun 23 17:46:18.463: INFO: successfully validated that service multi-endpoint-test in namespace services-5404 exposes endpoints map[] (6.108305ms elapsed)
STEP: Creating pod pod1 in namespace services-5404
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5404 to expose endpoints map[pod1:[100]]
Jun 23 17:46:21.522: INFO: successfully validated that service multi-endpoint-test in namespace services-5404 exposes endpoints map[pod1:[100]] (3.047213018s elapsed)
STEP: Creating pod pod2 in namespace services-5404
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5404 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 23 17:46:24.607: INFO: successfully validated that service multi-endpoint-test in namespace services-5404 exposes endpoints map[pod1:[100] pod2:[101]] (3.072922656s elapsed)
STEP: Deleting pod pod1 in namespace services-5404
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5404 to expose endpoints map[pod2:[101]]
Jun 23 17:46:25.652: INFO: successfully validated that service multi-endpoint-test in namespace services-5404 exposes endpoints map[pod2:[101]] (1.035059366s elapsed)
STEP: Deleting pod pod2 in namespace services-5404
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5404 to expose endpoints map[]
Jun 23 17:46:25.678: INFO: successfully validated that service multi-endpoint-test in namespace services-5404 exposes endpoints map[] (8.520821ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:46:25.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5404" for this suite.
Jun 23 17:46:47.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:46:47.811: INFO: namespace services-5404 deletion completed in 22.106500205s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:29.385 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:46:47.811: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 23 17:46:47.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-1481'
Jun 23 17:46:47.927: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 23 17:46:47.927: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Jun 23 17:46:49.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete deployment e2e-test-nginx-deployment --namespace=kubectl-1481'
Jun 23 17:46:50.036: INFO: stderr: ""
Jun 23 17:46:50.036: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:46:50.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1481" for this suite.
Jun 23 17:48:52.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:48:52.117: INFO: namespace kubectl-1481 deletion completed in 2m2.072078792s

• [SLOW TEST:124.305 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:48:52.117: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 17:48:52.235: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 23 17:48:54.295: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:48:54.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3502" for this suite.
Jun 23 17:49:00.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:49:00.404: INFO: namespace replication-controller-3502 deletion completed in 6.096614201s

• [SLOW TEST:8.287 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:49:00.405: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 17:49:00.441: INFO: Creating deployment "nginx-deployment"
Jun 23 17:49:00.445: INFO: Waiting for observed generation 1
Jun 23 17:49:02.452: INFO: Waiting for all required pods to come up
Jun 23 17:49:02.459: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 23 17:49:04.486: INFO: Waiting for deployment "nginx-deployment" to complete
Jun 23 17:49:04.496: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jun 23 17:49:04.510: INFO: Updating deployment nginx-deployment
Jun 23 17:49:04.510: INFO: Waiting for observed generation 2
Jun 23 17:49:06.521: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 23 17:49:06.523: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 23 17:49:06.524: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 23 17:49:06.529: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 23 17:49:06.529: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 23 17:49:06.530: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 23 17:49:06.533: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jun 23 17:49:06.533: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jun 23 17:49:06.539: INFO: Updating deployment nginx-deployment
Jun 23 17:49:06.539: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jun 23 17:49:06.560: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 23 17:49:08.593: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 23 17:49:08.611: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-208,SelfLink:/apis/apps/v1/namespaces/deployment-208/deployments/nginx-deployment,UID:24fab9c1-1cbc-4645-a9d6-7fa64648cc8d,ResourceVersion:4287,Generation:3,CreationTimestamp:2019-06-23 17:49:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-06-23 17:49:07 +0000 UTC 2019-06-23 17:49:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-06-23 17:49:07 +0000 UTC 2019-06-23 17:49:01 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Jun 23 17:49:08.614: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-208,SelfLink:/apis/apps/v1/namespaces/deployment-208/replicasets/nginx-deployment-55fb7cb77f,UID:1843e7b2-f790-4be8-b132-c06f5e98f4cf,ResourceVersion:4283,Generation:3,CreationTimestamp:2019-06-23 17:49:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 24fab9c1-1cbc-4645-a9d6-7fa64648cc8d 0xc0033615e7 0xc0033615e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 23 17:49:08.614: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jun 23 17:49:08.615: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-208,SelfLink:/apis/apps/v1/namespaces/deployment-208/replicasets/nginx-deployment-7b8c6f4498,UID:58424ecc-81ef-4296-a39e-b2571541c688,ResourceVersion:4273,Generation:3,CreationTimestamp:2019-06-23 17:49:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 24fab9c1-1cbc-4645-a9d6-7fa64648cc8d 0xc003361797 0xc003361798}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jun 23 17:49:08.669: INFO: Pod "nginx-deployment-55fb7cb77f-5fqxd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-5fqxd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-55fb7cb77f-5fqxd,UID:5c6a4af8-e443-46a9-b631-925276b75d59,ResourceVersion:4195,Generation:0,CreationTimestamp:2019-06-23 17:49:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1843e7b2-f790-4be8-b132-c06f5e98f4cf 0xc0030367c0 0xc0030367c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003036840} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003036860}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:05 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.11,PodIP:10.32.0.7,StartTime:2019-06-23 17:49:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.669: INFO: Pod "nginx-deployment-55fb7cb77f-9q5vk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9q5vk,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-55fb7cb77f-9q5vk,UID:cd7d8bc2-dca7-478e-9a99-d2956ca8da06,ResourceVersion:4251,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1843e7b2-f790-4be8-b132-c06f5e98f4cf 0xc003036960 0xc003036961}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030369e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003036a00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.102,PodIP:,StartTime:2019-06-23 17:49:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.670: INFO: Pod "nginx-deployment-55fb7cb77f-bmf5v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-bmf5v,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-55fb7cb77f-bmf5v,UID:a5ef3d1f-0f55-4b67-aa8e-e5b74695514e,ResourceVersion:4289,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1843e7b2-f790-4be8-b132-c06f5e98f4cf 0xc003036ad0 0xc003036ad1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003036b50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003036b70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.670: INFO: Pod "nginx-deployment-55fb7cb77f-cw9bm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cw9bm,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-55fb7cb77f-cw9bm,UID:50a2da68-69df-4a20-8575-d93997915d00,ResourceVersion:4264,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1843e7b2-f790-4be8-b132-c06f5e98f4cf 0xc003036c40 0xc003036c41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003036cc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003036ce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.11,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.671: INFO: Pod "nginx-deployment-55fb7cb77f-dwzw9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-dwzw9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-55fb7cb77f-dwzw9,UID:3e0c381b-6c80-4160-a46a-2251ea98d1a9,ResourceVersion:4339,Generation:0,CreationTimestamp:2019-06-23 17:49:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1843e7b2-f790-4be8-b132-c06f5e98f4cf 0xc003036db0 0xc003036db1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003036e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003036e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:05 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:10.38.0.3,StartTime:2019-06-23 17:49:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.671: INFO: Pod "nginx-deployment-55fb7cb77f-n99qx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-n99qx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-55fb7cb77f-n99qx,UID:31cbdcd7-e191-4e6f-8311-cf15cf9546cb,ResourceVersion:4269,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1843e7b2-f790-4be8-b132-c06f5e98f4cf 0xc003036f40 0xc003036f41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003036fc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003036fe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.671: INFO: Pod "nginx-deployment-55fb7cb77f-nt4jx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-nt4jx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-55fb7cb77f-nt4jx,UID:71060adb-5bfa-427e-b7c9-79543453ba58,ResourceVersion:4333,Generation:0,CreationTimestamp:2019-06-23 17:49:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1843e7b2-f790-4be8-b132-c06f5e98f4cf 0xc0030370b0 0xc0030370b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003037140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003037160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:05 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.102,PodIP:10.40.0.6,StartTime:2019-06-23 17:49:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.672: INFO: Pod "nginx-deployment-55fb7cb77f-r6c4h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-r6c4h,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-55fb7cb77f-r6c4h,UID:c48c7013-a878-4d46-84c5-c15a3f142a0f,ResourceVersion:4275,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1843e7b2-f790-4be8-b132-c06f5e98f4cf 0xc003037260 0xc003037261}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030372e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003037300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.102,PodIP:,StartTime:2019-06-23 17:49:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.672: INFO: Pod "nginx-deployment-55fb7cb77f-v6jdz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-v6jdz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-55fb7cb77f-v6jdz,UID:e6c33eae-1987-47f7-8426-956ea2ee5364,ResourceVersion:4284,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1843e7b2-f790-4be8-b132-c06f5e98f4cf 0xc0030373f0 0xc0030373f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030374a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030374c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.11,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.672: INFO: Pod "nginx-deployment-55fb7cb77f-wktf7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wktf7,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-55fb7cb77f-wktf7,UID:57d5be7d-cbd8-4c54-b517-69e112e77299,ResourceVersion:4331,Generation:0,CreationTimestamp:2019-06-23 17:49:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1843e7b2-f790-4be8-b132-c06f5e98f4cf 0xc0030375a0 0xc0030375a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003037620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003037640}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:05 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.102,PodIP:10.40.0.7,StartTime:2019-06-23 17:49:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.672: INFO: Pod "nginx-deployment-55fb7cb77f-xfqt8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-xfqt8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-55fb7cb77f-xfqt8,UID:975a33d5-0acd-4acd-b0ff-d542260c4c1c,ResourceVersion:4320,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1843e7b2-f790-4be8-b132-c06f5e98f4cf 0xc003037730 0xc003037731}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030377b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030377d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.672: INFO: Pod "nginx-deployment-55fb7cb77f-z88cb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-z88cb,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-55fb7cb77f-z88cb,UID:1de92721-99c5-49a7-8236-efb58f7e2d4c,ResourceVersion:4178,Generation:0,CreationTimestamp:2019-06-23 17:49:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1843e7b2-f790-4be8-b132-c06f5e98f4cf 0xc0030378c0 0xc0030378c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003037940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003037960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:05 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-06-23 17:49:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.673: INFO: Pod "nginx-deployment-55fb7cb77f-zlk9v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-zlk9v,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-55fb7cb77f-zlk9v,UID:4dc315e8-535d-4a7a-b6c4-cbbe5575be62,ResourceVersion:4231,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1843e7b2-f790-4be8-b132-c06f5e98f4cf 0xc003037a30 0xc003037a31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003037ab0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003037ad0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.11,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.673: INFO: Pod "nginx-deployment-7b8c6f4498-4dqkx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4dqkx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-4dqkx,UID:f668a85c-71f6-41e9-a209-1634544ee03d,ResourceVersion:4291,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc003037ba0 0xc003037ba1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003037c10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003037c30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.11,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.673: INFO: Pod "nginx-deployment-7b8c6f4498-4xlhx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4xlhx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-4xlhx,UID:4da37396-8605-46f6-b301-da6540cc999a,ResourceVersion:4298,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc003037cf7 0xc003037cf8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003037d70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003037d90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.673: INFO: Pod "nginx-deployment-7b8c6f4498-75t6g" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-75t6g,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-75t6g,UID:0f67032e-5f6c-4a4d-97d3-c526b6ee7b1d,ResourceVersion:4077,Generation:0,CreationTimestamp:2019-06-23 17:49:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc003037e57 0xc003037e58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003037ed0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003037f10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:01 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.102,PodIP:10.40.0.2,StartTime:2019-06-23 17:49:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-23 17:49:01 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://602d1e1cfe3e6d70ea82ec73ba5bd255a490d20f80cb2989f8b9157d8417e291}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.673: INFO: Pod "nginx-deployment-7b8c6f4498-76jrd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-76jrd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-76jrd,UID:af787224-768c-4e25-9d38-f1a618ed439c,ResourceVersion:4119,Generation:0,CreationTimestamp:2019-06-23 17:49:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc003037fe0 0xc003037fe1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030a0090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030a00c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:01 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.11,PodIP:10.32.0.4,StartTime:2019-06-23 17:49:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-23 17:49:02 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e4b031b3c099d1a0cc5409902a193bce565f535c2842d25e1d69759f5d5dac22}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.673: INFO: Pod "nginx-deployment-7b8c6f4498-97cnn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-97cnn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-97cnn,UID:bac45436-11c6-4ffb-921f-94f8e4cc5503,ResourceVersion:4312,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc0030a02f0 0xc0030a02f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030a0890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030a08d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.11,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.673: INFO: Pod "nginx-deployment-7b8c6f4498-c4gk8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-c4gk8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-c4gk8,UID:04c35fbf-f04f-4af1-94b2-c2999badd503,ResourceVersion:4323,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc0030a0b97 0xc0030a0b98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030a0c20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030a0c40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.673: INFO: Pod "nginx-deployment-7b8c6f4498-d58vn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-d58vn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-d58vn,UID:c12430d7-52e4-4dbc-b463-ed68badd6f84,ResourceVersion:4109,Generation:0,CreationTimestamp:2019-06-23 17:49:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc0030a0df7 0xc0030a0df8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030a0f90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030a0fc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:01 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.102,PodIP:10.40.0.3,StartTime:2019-06-23 17:49:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-23 17:49:02 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://804146f75eceb6b224ecb0a36834d1ae53e810ea2e1a2f853fcdff1e3ce7511c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.674: INFO: Pod "nginx-deployment-7b8c6f4498-dbsnt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dbsnt,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-dbsnt,UID:ad130beb-8e7c-4ea8-a9a4-11624ebd66c1,ResourceVersion:4281,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc0030a1310 0xc0030a1311}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030a14a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030a1550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.102,PodIP:,StartTime:2019-06-23 17:49:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.674: INFO: Pod "nginx-deployment-7b8c6f4498-dfjzx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dfjzx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-dfjzx,UID:261c0605-268b-474f-95c1-67cc68bdbba0,ResourceVersion:4106,Generation:0,CreationTimestamp:2019-06-23 17:49:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc0030a16e7 0xc0030a16e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030a17a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030a1810}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:01 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.102,PodIP:10.40.0.5,StartTime:2019-06-23 17:49:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-23 17:49:02 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1e6b3dd8f78d4a4bafd066e81c8baa70802f8eaaf618ef68a1ff1faed2774835}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.674: INFO: Pod "nginx-deployment-7b8c6f4498-f4mmz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-f4mmz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-f4mmz,UID:b3f0ef51-c676-40bb-a69b-2807e0601110,ResourceVersion:4278,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc0030a19e0 0xc0030a19e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030a1a60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030a1a80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.674: INFO: Pod "nginx-deployment-7b8c6f4498-flvrj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-flvrj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-flvrj,UID:b623e21d-24e6-4c0a-b355-d2edb6745503,ResourceVersion:4103,Generation:0,CreationTimestamp:2019-06-23 17:49:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc0030a1c17 0xc0030a1c18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030a1ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030a1d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:01 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.102,PodIP:10.40.0.4,StartTime:2019-06-23 17:49:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-23 17:49:02 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ff5fdef9ebb983f5548a6e3d0781f234d77e56b35d86967fa078fd239b4a4b04}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.674: INFO: Pod "nginx-deployment-7b8c6f4498-gsv4n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gsv4n,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-gsv4n,UID:4342bcc1-1449-4228-8b42-ab2def173631,ResourceVersion:4319,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc0030a1e70 0xc0030a1e71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030a1f40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030a1f90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.11,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.674: INFO: Pod "nginx-deployment-7b8c6f4498-j8p9x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-j8p9x,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-j8p9x,UID:194c60d2-12e5-4b0b-9ab8-b83078afd9db,ResourceVersion:4220,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc0031620c7 0xc0031620c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003162170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031621e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.674: INFO: Pod "nginx-deployment-7b8c6f4498-jghfm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jghfm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-jghfm,UID:4a625848-1c86-4aec-905c-194747c9424e,ResourceVersion:4127,Generation:0,CreationTimestamp:2019-06-23 17:49:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc003162347 0xc003162348}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003162420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003162440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:01 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:10.38.0.4,StartTime:2019-06-23 17:49:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-23 17:49:04 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4a33a2b35b159f4a6205920df2933410fbb1ea9f9715dccaf54021a445165e96}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.674: INFO: Pod "nginx-deployment-7b8c6f4498-s2pjj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-s2pjj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-s2pjj,UID:9d062192-80fa-4ddb-a03e-01886e92f143,ResourceVersion:4266,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc003162580 0xc003162581}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003162640} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003162660}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.102,PodIP:,StartTime:2019-06-23 17:49:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.675: INFO: Pod "nginx-deployment-7b8c6f4498-smqnr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-smqnr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-smqnr,UID:d27b0f9c-b204-42a3-8c7c-c3ae63612728,ResourceVersion:4272,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc003162797 0xc003162798}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031628b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031628d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.11,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.675: INFO: Pod "nginx-deployment-7b8c6f4498-tv52x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tv52x,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-tv52x,UID:c243a71f-ddaf-431d-ac01-2e0fdd2ff6de,ResourceVersion:4113,Generation:0,CreationTimestamp:2019-06-23 17:49:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc003162a37 0xc003162a38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003162c50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003162cc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:01 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.11,PodIP:10.32.0.5,StartTime:2019-06-23 17:49:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-23 17:49:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c880dcae830f2564f7c8f328b5f6ea65a68fa2206ed437af014a28fbe17ef641}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.675: INFO: Pod "nginx-deployment-7b8c6f4498-vczdr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vczdr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-vczdr,UID:676f4e11-3415-4c55-b798-e4a69352a731,ResourceVersion:4316,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc003162e80 0xc003162e81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003162f40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003162f60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.676: INFO: Pod "nginx-deployment-7b8c6f4498-xs6vb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xs6vb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-xs6vb,UID:eac48c4d-edfe-4023-bc25-c9ea20bd83d5,ResourceVersion:4262,Generation:0,CreationTimestamp:2019-06-23 17:49:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc003163097 0xc003163098}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003163110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003163130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-06-23 17:49:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 23 17:49:08.676: INFO: Pod "nginx-deployment-7b8c6f4498-zd9xp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zd9xp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-208,SelfLink:/api/v1/namespaces/deployment-208/pods/nginx-deployment-7b8c6f4498-zd9xp,UID:e24649b8-90a4-4a18-b51f-a89123ca66f7,ResourceVersion:4116,Generation:0,CreationTimestamp:2019-06-23 17:49:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 58424ecc-81ef-4296-a39e-b2571541c688 0xc003163377 0xc003163378}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt98d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt98d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt98d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controlplane-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003163510} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003163530}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 17:49:01 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.11,PodIP:10.32.0.6,StartTime:2019-06-23 17:49:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-23 17:49:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9750cb7fd5a697c492f10bbbae94a86a036e83ef6eaae5c6c8ef9be576f2eb09}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:49:08.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-208" for this suite.
Jun 23 17:49:16.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:49:16.824: INFO: namespace deployment-208 deletion completed in 8.132972041s

• [SLOW TEST:16.420 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:49:16.825: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 23 17:49:16.869: INFO: Waiting up to 5m0s for pod "pod-58bdc993-d267-4783-9bb0-6ec33e0c4d9c" in namespace "emptydir-4904" to be "success or failure"
Jun 23 17:49:16.876: INFO: Pod "pod-58bdc993-d267-4783-9bb0-6ec33e0c4d9c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.984743ms
Jun 23 17:49:18.882: INFO: Pod "pod-58bdc993-d267-4783-9bb0-6ec33e0c4d9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012935208s
Jun 23 17:49:20.890: INFO: Pod "pod-58bdc993-d267-4783-9bb0-6ec33e0c4d9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020601407s
STEP: Saw pod success
Jun 23 17:49:20.890: INFO: Pod "pod-58bdc993-d267-4783-9bb0-6ec33e0c4d9c" satisfied condition "success or failure"
Jun 23 17:49:20.894: INFO: Trying to get logs from node worker-1 pod pod-58bdc993-d267-4783-9bb0-6ec33e0c4d9c container test-container: <nil>
STEP: delete the pod
Jun 23 17:49:20.933: INFO: Waiting for pod pod-58bdc993-d267-4783-9bb0-6ec33e0c4d9c to disappear
Jun 23 17:49:20.952: INFO: Pod pod-58bdc993-d267-4783-9bb0-6ec33e0c4d9c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:49:20.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4904" for this suite.
Jun 23 17:49:26.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:49:27.052: INFO: namespace emptydir-4904 deletion completed in 6.093035596s

• [SLOW TEST:10.227 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:49:27.052: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 23 17:49:37.116: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2171 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 17:49:37.116: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 17:49:37.230: INFO: Exec stderr: ""
Jun 23 17:49:37.230: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2171 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 17:49:37.230: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 17:49:37.325: INFO: Exec stderr: ""
Jun 23 17:49:37.326: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2171 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 17:49:37.326: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 17:49:37.397: INFO: Exec stderr: ""
Jun 23 17:49:37.397: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2171 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 17:49:37.397: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 17:49:37.472: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 23 17:49:37.472: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2171 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 17:49:37.472: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 17:49:37.557: INFO: Exec stderr: ""
Jun 23 17:49:37.557: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2171 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 17:49:37.557: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 17:49:37.635: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 23 17:49:37.635: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2171 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 17:49:37.635: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 17:49:37.716: INFO: Exec stderr: ""
Jun 23 17:49:37.716: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2171 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 17:49:37.716: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 17:49:37.796: INFO: Exec stderr: ""
Jun 23 17:49:37.796: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2171 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 17:49:37.796: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 17:49:37.877: INFO: Exec stderr: ""
Jun 23 17:49:37.877: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2171 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 17:49:37.877: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 17:49:37.950: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:49:37.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2171" for this suite.
Jun 23 17:50:21.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:50:22.017: INFO: namespace e2e-kubelet-etc-hosts-2171 deletion completed in 44.064942566s

• [SLOW TEST:54.965 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:50:22.017: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 23 17:50:22.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7613'
Jun 23 17:50:22.727: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 23 17:50:22.727: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jun 23 17:50:22.742: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-gr2qh]
Jun 23 17:50:22.742: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-gr2qh" in namespace "kubectl-7613" to be "running and ready"
Jun 23 17:50:22.745: INFO: Pod "e2e-test-nginx-rc-gr2qh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.065907ms
Jun 23 17:50:24.753: INFO: Pod "e2e-test-nginx-rc-gr2qh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010732133s
Jun 23 17:50:26.762: INFO: Pod "e2e-test-nginx-rc-gr2qh": Phase="Running", Reason="", readiness=true. Elapsed: 4.020618637s
Jun 23 17:50:26.762: INFO: Pod "e2e-test-nginx-rc-gr2qh" satisfied condition "running and ready"
Jun 23 17:50:26.762: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-gr2qh]
Jun 23 17:50:26.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 logs rc/e2e-test-nginx-rc --namespace=kubectl-7613'
Jun 23 17:50:26.864: INFO: stderr: ""
Jun 23 17:50:26.864: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Jun 23 17:50:26.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete rc e2e-test-nginx-rc --namespace=kubectl-7613'
Jun 23 17:50:26.918: INFO: stderr: ""
Jun 23 17:50:26.918: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:50:26.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7613" for this suite.
Jun 23 17:50:32.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:50:33.013: INFO: namespace kubectl-7613 deletion completed in 6.09284783s

• [SLOW TEST:10.996 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:50:33.015: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Jun 23 17:50:33.039: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-309105263 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:50:33.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2699" for this suite.
Jun 23 17:50:39.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:50:39.195: INFO: namespace kubectl-2699 deletion completed in 6.101742089s

• [SLOW TEST:6.180 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:50:39.195: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-7b197801-98ff-4c0f-983d-1da7a7dc58d1
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:50:39.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6648" for this suite.
Jun 23 17:50:45.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:50:45.296: INFO: namespace secrets-6648 deletion completed in 6.071510612s

• [SLOW TEST:6.101 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:50:45.297: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jun 23 17:50:45.386: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 23 17:50:45.393: INFO: Waiting for terminating namespaces to be deleted...
Jun 23 17:50:45.394: INFO: 
Logging pods the kubelet thinks is on node controlplane-1 before test
Jun 23 17:50:45.412: INFO: etcd-controlplane-1 from kube-system started at 2019-06-23 17:35:42 +0000 UTC (1 container statuses recorded)
Jun 23 17:50:45.412: INFO: 	Container etcd ready: true, restart count 0
Jun 23 17:50:45.412: INFO: coredns-5c98db65d4-2c474 from kube-system started at 2019-06-23 17:36:39 +0000 UTC (1 container statuses recorded)
Jun 23 17:50:45.412: INFO: 	Container coredns ready: true, restart count 0
Jun 23 17:50:45.412: INFO: kube-scheduler-controlplane-1 from kube-system started at 2019-06-23 17:35:42 +0000 UTC (1 container statuses recorded)
Jun 23 17:50:45.412: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 23 17:50:45.412: INFO: sonobuoy-systemd-logs-daemon-set-97428795f2c54d48-9t4bv from heptio-sonobuoy started at 2019-06-23 17:37:16 +0000 UTC (2 container statuses recorded)
Jun 23 17:50:45.412: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 23 17:50:45.412: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 23 17:50:45.412: INFO: kube-controller-manager-controlplane-1 from kube-system started at 2019-06-23 17:35:42 +0000 UTC (1 container statuses recorded)
Jun 23 17:50:45.412: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 23 17:50:45.412: INFO: kube-proxy-g5w8t from kube-system started at 2019-06-23 17:36:08 +0000 UTC (1 container statuses recorded)
Jun 23 17:50:45.412: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 23 17:50:45.412: INFO: coredns-5c98db65d4-x58jr from kube-system started at 2019-06-23 17:36:39 +0000 UTC (1 container statuses recorded)
Jun 23 17:50:45.412: INFO: 	Container coredns ready: true, restart count 0
Jun 23 17:50:45.412: INFO: kube-apiserver-controlplane-1 from kube-system started at 2019-06-23 17:35:42 +0000 UTC (1 container statuses recorded)
Jun 23 17:50:45.412: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 23 17:50:45.412: INFO: weave-net-2ljj6 from kube-system started at 2019-06-23 17:36:08 +0000 UTC (2 container statuses recorded)
Jun 23 17:50:45.412: INFO: 	Container weave ready: true, restart count 0
Jun 23 17:50:45.412: INFO: 	Container weave-npc ready: true, restart count 0
Jun 23 17:50:45.412: INFO: 
Logging pods the kubelet thinks is on node worker-1 before test
Jun 23 17:50:45.416: INFO: kube-proxy-vmf86 from kube-system started at 2019-06-23 17:36:12 +0000 UTC (1 container statuses recorded)
Jun 23 17:50:45.416: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 23 17:50:45.416: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-23 17:37:03 +0000 UTC (1 container statuses recorded)
Jun 23 17:50:45.416: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 23 17:50:45.416: INFO: weave-net-rh6sc from kube-system started at 2019-06-23 17:36:12 +0000 UTC (2 container statuses recorded)
Jun 23 17:50:45.416: INFO: 	Container weave ready: true, restart count 0
Jun 23 17:50:45.416: INFO: 	Container weave-npc ready: true, restart count 0
Jun 23 17:50:45.417: INFO: sonobuoy-systemd-logs-daemon-set-97428795f2c54d48-twnth from heptio-sonobuoy started at 2019-06-23 17:37:16 +0000 UTC (2 container statuses recorded)
Jun 23 17:50:45.417: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 23 17:50:45.417: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 23 17:50:45.417: INFO: 
Logging pods the kubelet thinks is on node worker-2 before test
Jun 23 17:50:45.420: INFO: sonobuoy-e2e-job-25634377b43945a3 from heptio-sonobuoy started at 2019-06-23 17:37:15 +0000 UTC (2 container statuses recorded)
Jun 23 17:50:45.420: INFO: 	Container e2e ready: true, restart count 0
Jun 23 17:50:45.420: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 23 17:50:45.420: INFO: kube-proxy-djmbj from kube-system started at 2019-06-23 17:36:11 +0000 UTC (1 container statuses recorded)
Jun 23 17:50:45.420: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 23 17:50:45.420: INFO: sonobuoy-systemd-logs-daemon-set-97428795f2c54d48-v5ldn from heptio-sonobuoy started at 2019-06-23 17:37:15 +0000 UTC (2 container statuses recorded)
Jun 23 17:50:45.420: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 23 17:50:45.420: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 23 17:50:45.420: INFO: weave-net-6pxvd from kube-system started at 2019-06-23 17:36:11 +0000 UTC (2 container statuses recorded)
Jun 23 17:50:45.420: INFO: 	Container weave ready: true, restart count 0
Jun 23 17:50:45.420: INFO: 	Container weave-npc ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5d9eb4b3-f2e3-401c-9673-11ec219b375a 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-5d9eb4b3-f2e3-401c-9673-11ec219b375a off the node worker-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5d9eb4b3-f2e3-401c-9673-11ec219b375a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:50:53.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5878" for this suite.
Jun 23 17:51:01.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:51:01.591: INFO: namespace sched-pred-5878 deletion completed in 8.055443313s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:16.294 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:51:01.592: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:51:25.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2382" for this suite.
Jun 23 17:51:31.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:51:31.781: INFO: namespace namespaces-2382 deletion completed in 6.081460977s
STEP: Destroying namespace "nsdeletetest-7762" for this suite.
Jun 23 17:51:31.783: INFO: Namespace nsdeletetest-7762 was already deleted
STEP: Destroying namespace "nsdeletetest-6630" for this suite.
Jun 23 17:51:37.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:51:37.846: INFO: namespace nsdeletetest-6630 deletion completed in 6.062854337s

• [SLOW TEST:36.254 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:51:37.849: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Jun 23 17:51:37.875: INFO: Waiting up to 5m0s for pod "client-containers-3f5b7948-5fae-4654-a5d1-aedda84f7909" in namespace "containers-6344" to be "success or failure"
Jun 23 17:51:37.882: INFO: Pod "client-containers-3f5b7948-5fae-4654-a5d1-aedda84f7909": Phase="Pending", Reason="", readiness=false. Elapsed: 6.366811ms
Jun 23 17:51:39.920: INFO: Pod "client-containers-3f5b7948-5fae-4654-a5d1-aedda84f7909": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04463728s
Jun 23 17:51:41.926: INFO: Pod "client-containers-3f5b7948-5fae-4654-a5d1-aedda84f7909": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050475385s
STEP: Saw pod success
Jun 23 17:51:41.926: INFO: Pod "client-containers-3f5b7948-5fae-4654-a5d1-aedda84f7909" satisfied condition "success or failure"
Jun 23 17:51:41.933: INFO: Trying to get logs from node worker-1 pod client-containers-3f5b7948-5fae-4654-a5d1-aedda84f7909 container test-container: <nil>
STEP: delete the pod
Jun 23 17:51:41.962: INFO: Waiting for pod client-containers-3f5b7948-5fae-4654-a5d1-aedda84f7909 to disappear
Jun 23 17:51:41.963: INFO: Pod client-containers-3f5b7948-5fae-4654-a5d1-aedda84f7909 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:51:41.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6344" for this suite.
Jun 23 17:51:47.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:51:48.052: INFO: namespace containers-6344 deletion completed in 6.086833734s

• [SLOW TEST:10.203 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:51:48.052: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-f3d34b2d-831f-4c48-814f-29feab4c1b44
STEP: Creating a pod to test consume configMaps
Jun 23 17:51:48.148: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ba8e945e-7203-4a44-9c00-6ab7469c201c" in namespace "projected-7420" to be "success or failure"
Jun 23 17:51:48.155: INFO: Pod "pod-projected-configmaps-ba8e945e-7203-4a44-9c00-6ab7469c201c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.101817ms
Jun 23 17:51:50.162: INFO: Pod "pod-projected-configmaps-ba8e945e-7203-4a44-9c00-6ab7469c201c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013417765s
Jun 23 17:51:52.172: INFO: Pod "pod-projected-configmaps-ba8e945e-7203-4a44-9c00-6ab7469c201c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023876499s
STEP: Saw pod success
Jun 23 17:51:52.173: INFO: Pod "pod-projected-configmaps-ba8e945e-7203-4a44-9c00-6ab7469c201c" satisfied condition "success or failure"
Jun 23 17:51:52.180: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-ba8e945e-7203-4a44-9c00-6ab7469c201c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 23 17:51:52.216: INFO: Waiting for pod pod-projected-configmaps-ba8e945e-7203-4a44-9c00-6ab7469c201c to disappear
Jun 23 17:51:52.220: INFO: Pod pod-projected-configmaps-ba8e945e-7203-4a44-9c00-6ab7469c201c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:51:52.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7420" for this suite.
Jun 23 17:51:58.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:51:58.297: INFO: namespace projected-7420 deletion completed in 6.072519359s

• [SLOW TEST:10.244 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:51:58.301: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 23 17:51:58.333: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1945,SelfLink:/api/v1/namespaces/watch-1945/configmaps/e2e-watch-test-label-changed,UID:af42ef67-3d2e-414c-bcfa-56257aaa1160,ResourceVersion:5115,Generation:0,CreationTimestamp:2019-06-23 17:51:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 23 17:51:58.333: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1945,SelfLink:/api/v1/namespaces/watch-1945/configmaps/e2e-watch-test-label-changed,UID:af42ef67-3d2e-414c-bcfa-56257aaa1160,ResourceVersion:5116,Generation:0,CreationTimestamp:2019-06-23 17:51:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 23 17:51:58.333: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1945,SelfLink:/api/v1/namespaces/watch-1945/configmaps/e2e-watch-test-label-changed,UID:af42ef67-3d2e-414c-bcfa-56257aaa1160,ResourceVersion:5117,Generation:0,CreationTimestamp:2019-06-23 17:51:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 23 17:52:08.363: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1945,SelfLink:/api/v1/namespaces/watch-1945/configmaps/e2e-watch-test-label-changed,UID:af42ef67-3d2e-414c-bcfa-56257aaa1160,ResourceVersion:5133,Generation:0,CreationTimestamp:2019-06-23 17:51:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 23 17:52:08.363: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1945,SelfLink:/api/v1/namespaces/watch-1945/configmaps/e2e-watch-test-label-changed,UID:af42ef67-3d2e-414c-bcfa-56257aaa1160,ResourceVersion:5134,Generation:0,CreationTimestamp:2019-06-23 17:51:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jun 23 17:52:08.363: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1945,SelfLink:/api/v1/namespaces/watch-1945/configmaps/e2e-watch-test-label-changed,UID:af42ef67-3d2e-414c-bcfa-56257aaa1160,ResourceVersion:5135,Generation:0,CreationTimestamp:2019-06-23 17:51:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:52:08.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1945" for this suite.
Jun 23 17:52:14.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:52:14.432: INFO: namespace watch-1945 deletion completed in 6.066257915s

• [SLOW TEST:16.131 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:52:14.433: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 23 17:52:14.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6532'
Jun 23 17:52:14.527: INFO: stderr: ""
Jun 23 17:52:14.527: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Jun 23 17:52:14.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete pods e2e-test-nginx-pod --namespace=kubectl-6532'
Jun 23 17:52:21.247: INFO: stderr: ""
Jun 23 17:52:21.247: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:52:21.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6532" for this suite.
Jun 23 17:52:27.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:52:27.351: INFO: namespace kubectl-6532 deletion completed in 6.100776683s

• [SLOW TEST:12.918 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:52:27.352: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-2327/configmap-test-0208bc84-5447-4e4f-9a25-699bbea92990
STEP: Creating a pod to test consume configMaps
Jun 23 17:52:27.384: INFO: Waiting up to 5m0s for pod "pod-configmaps-ab91cd3d-9d16-4d1b-be60-884ce7183621" in namespace "configmap-2327" to be "success or failure"
Jun 23 17:52:27.389: INFO: Pod "pod-configmaps-ab91cd3d-9d16-4d1b-be60-884ce7183621": Phase="Pending", Reason="", readiness=false. Elapsed: 5.166962ms
Jun 23 17:52:29.392: INFO: Pod "pod-configmaps-ab91cd3d-9d16-4d1b-be60-884ce7183621": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007322174s
STEP: Saw pod success
Jun 23 17:52:29.392: INFO: Pod "pod-configmaps-ab91cd3d-9d16-4d1b-be60-884ce7183621" satisfied condition "success or failure"
Jun 23 17:52:29.393: INFO: Trying to get logs from node worker-1 pod pod-configmaps-ab91cd3d-9d16-4d1b-be60-884ce7183621 container env-test: <nil>
STEP: delete the pod
Jun 23 17:52:29.409: INFO: Waiting for pod pod-configmaps-ab91cd3d-9d16-4d1b-be60-884ce7183621 to disappear
Jun 23 17:52:29.411: INFO: Pod pod-configmaps-ab91cd3d-9d16-4d1b-be60-884ce7183621 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:52:29.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2327" for this suite.
Jun 23 17:52:35.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:52:35.536: INFO: namespace configmap-2327 deletion completed in 6.122030518s

• [SLOW TEST:8.184 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:52:35.537: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 23 17:52:35.562: INFO: Waiting up to 5m0s for pod "pod-99169ce9-df75-4b59-9ed9-39719547e783" in namespace "emptydir-6971" to be "success or failure"
Jun 23 17:52:35.569: INFO: Pod "pod-99169ce9-df75-4b59-9ed9-39719547e783": Phase="Pending", Reason="", readiness=false. Elapsed: 7.183308ms
Jun 23 17:52:37.572: INFO: Pod "pod-99169ce9-df75-4b59-9ed9-39719547e783": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009748452s
Jun 23 17:52:39.578: INFO: Pod "pod-99169ce9-df75-4b59-9ed9-39719547e783": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015964502s
STEP: Saw pod success
Jun 23 17:52:39.578: INFO: Pod "pod-99169ce9-df75-4b59-9ed9-39719547e783" satisfied condition "success or failure"
Jun 23 17:52:39.583: INFO: Trying to get logs from node worker-1 pod pod-99169ce9-df75-4b59-9ed9-39719547e783 container test-container: <nil>
STEP: delete the pod
Jun 23 17:52:39.618: INFO: Waiting for pod pod-99169ce9-df75-4b59-9ed9-39719547e783 to disappear
Jun 23 17:52:39.620: INFO: Pod pod-99169ce9-df75-4b59-9ed9-39719547e783 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:52:39.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6971" for this suite.
Jun 23 17:52:45.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:52:45.719: INFO: namespace emptydir-6971 deletion completed in 6.096921151s

• [SLOW TEST:10.183 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:52:45.720: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jun 23 17:52:49.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec pod-sharedvolume-974f9848-06f3-44ce-80a0-e68c2651cfde -c busybox-main-container --namespace=emptydir-5965 -- cat /usr/share/volumeshare/shareddata.txt'
Jun 23 17:52:49.923: INFO: stderr: ""
Jun 23 17:52:49.923: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:52:49.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5965" for this suite.
Jun 23 17:52:55.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:52:56.016: INFO: namespace emptydir-5965 deletion completed in 6.090605323s

• [SLOW TEST:10.296 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:52:56.017: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 23 17:52:56.044: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5978,SelfLink:/api/v1/namespaces/watch-5978/configmaps/e2e-watch-test-configmap-a,UID:c72846e5-47b1-47f9-b384-830af87507b2,ResourceVersion:5306,Generation:0,CreationTimestamp:2019-06-23 17:52:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 23 17:52:56.044: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5978,SelfLink:/api/v1/namespaces/watch-5978/configmaps/e2e-watch-test-configmap-a,UID:c72846e5-47b1-47f9-b384-830af87507b2,ResourceVersion:5306,Generation:0,CreationTimestamp:2019-06-23 17:52:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 23 17:53:06.059: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5978,SelfLink:/api/v1/namespaces/watch-5978/configmaps/e2e-watch-test-configmap-a,UID:c72846e5-47b1-47f9-b384-830af87507b2,ResourceVersion:5321,Generation:0,CreationTimestamp:2019-06-23 17:52:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 23 17:53:06.059: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5978,SelfLink:/api/v1/namespaces/watch-5978/configmaps/e2e-watch-test-configmap-a,UID:c72846e5-47b1-47f9-b384-830af87507b2,ResourceVersion:5321,Generation:0,CreationTimestamp:2019-06-23 17:52:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 23 17:53:16.074: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5978,SelfLink:/api/v1/namespaces/watch-5978/configmaps/e2e-watch-test-configmap-a,UID:c72846e5-47b1-47f9-b384-830af87507b2,ResourceVersion:5336,Generation:0,CreationTimestamp:2019-06-23 17:52:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 23 17:53:16.074: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5978,SelfLink:/api/v1/namespaces/watch-5978/configmaps/e2e-watch-test-configmap-a,UID:c72846e5-47b1-47f9-b384-830af87507b2,ResourceVersion:5336,Generation:0,CreationTimestamp:2019-06-23 17:52:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 23 17:53:26.086: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5978,SelfLink:/api/v1/namespaces/watch-5978/configmaps/e2e-watch-test-configmap-a,UID:c72846e5-47b1-47f9-b384-830af87507b2,ResourceVersion:5352,Generation:0,CreationTimestamp:2019-06-23 17:52:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 23 17:53:26.086: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5978,SelfLink:/api/v1/namespaces/watch-5978/configmaps/e2e-watch-test-configmap-a,UID:c72846e5-47b1-47f9-b384-830af87507b2,ResourceVersion:5352,Generation:0,CreationTimestamp:2019-06-23 17:52:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 23 17:53:36.100: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5978,SelfLink:/api/v1/namespaces/watch-5978/configmaps/e2e-watch-test-configmap-b,UID:331a7ef1-1588-41de-89db-6c40562e858a,ResourceVersion:5367,Generation:0,CreationTimestamp:2019-06-23 17:53:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 23 17:53:36.100: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5978,SelfLink:/api/v1/namespaces/watch-5978/configmaps/e2e-watch-test-configmap-b,UID:331a7ef1-1588-41de-89db-6c40562e858a,ResourceVersion:5367,Generation:0,CreationTimestamp:2019-06-23 17:53:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 23 17:53:46.112: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5978,SelfLink:/api/v1/namespaces/watch-5978/configmaps/e2e-watch-test-configmap-b,UID:331a7ef1-1588-41de-89db-6c40562e858a,ResourceVersion:5384,Generation:0,CreationTimestamp:2019-06-23 17:53:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 23 17:53:46.113: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5978,SelfLink:/api/v1/namespaces/watch-5978/configmaps/e2e-watch-test-configmap-b,UID:331a7ef1-1588-41de-89db-6c40562e858a,ResourceVersion:5384,Generation:0,CreationTimestamp:2019-06-23 17:53:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:53:56.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5978" for this suite.
Jun 23 17:54:02.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:54:02.193: INFO: namespace watch-5978 deletion completed in 6.07630706s

• [SLOW TEST:66.176 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:54:02.193: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 23 17:54:02.237: INFO: Number of nodes with available pods: 0
Jun 23 17:54:02.238: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 17:54:03.243: INFO: Number of nodes with available pods: 0
Jun 23 17:54:03.243: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 17:54:04.242: INFO: Number of nodes with available pods: 2
Jun 23 17:54:04.242: INFO: Node worker-1 is running more than one daemon pod
Jun 23 17:54:05.243: INFO: Number of nodes with available pods: 3
Jun 23 17:54:05.243: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 23 17:54:05.262: INFO: Number of nodes with available pods: 2
Jun 23 17:54:05.262: INFO: Node worker-1 is running more than one daemon pod
Jun 23 17:54:06.276: INFO: Number of nodes with available pods: 2
Jun 23 17:54:06.276: INFO: Node worker-1 is running more than one daemon pod
Jun 23 17:54:07.273: INFO: Number of nodes with available pods: 2
Jun 23 17:54:07.273: INFO: Node worker-1 is running more than one daemon pod
Jun 23 17:54:08.276: INFO: Number of nodes with available pods: 2
Jun 23 17:54:08.277: INFO: Node worker-1 is running more than one daemon pod
Jun 23 17:54:09.271: INFO: Number of nodes with available pods: 2
Jun 23 17:54:09.271: INFO: Node worker-1 is running more than one daemon pod
Jun 23 17:54:10.277: INFO: Number of nodes with available pods: 2
Jun 23 17:54:10.277: INFO: Node worker-1 is running more than one daemon pod
Jun 23 17:54:11.270: INFO: Number of nodes with available pods: 2
Jun 23 17:54:11.270: INFO: Node worker-1 is running more than one daemon pod
Jun 23 17:54:12.296: INFO: Number of nodes with available pods: 2
Jun 23 17:54:12.296: INFO: Node worker-1 is running more than one daemon pod
Jun 23 17:54:13.266: INFO: Number of nodes with available pods: 2
Jun 23 17:54:13.266: INFO: Node worker-1 is running more than one daemon pod
Jun 23 17:54:14.292: INFO: Number of nodes with available pods: 3
Jun 23 17:54:14.292: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8159, will wait for the garbage collector to delete the pods
Jun 23 17:54:14.371: INFO: Deleting DaemonSet.extensions daemon-set took: 14.085709ms
Jun 23 17:54:14.674: INFO: Terminating DaemonSet.extensions daemon-set pods took: 303.205608ms
Jun 23 17:54:21.280: INFO: Number of nodes with available pods: 0
Jun 23 17:54:21.280: INFO: Number of running nodes: 0, number of available pods: 0
Jun 23 17:54:21.286: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8159/daemonsets","resourceVersion":"5510"},"items":null}

Jun 23 17:54:21.291: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8159/pods","resourceVersion":"5510"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:54:21.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8159" for this suite.
Jun 23 17:54:27.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:54:27.420: INFO: namespace daemonsets-8159 deletion completed in 6.102941051s

• [SLOW TEST:25.226 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:54:27.421: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-aba150f9-7eb5-47ea-ab16-029c57ca6025
STEP: Creating configMap with name cm-test-opt-upd-cdb72844-709d-4a00-ad0c-76effda24086
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-aba150f9-7eb5-47ea-ab16-029c57ca6025
STEP: Updating configmap cm-test-opt-upd-cdb72844-709d-4a00-ad0c-76effda24086
STEP: Creating configMap with name cm-test-opt-create-af14fed2-d691-443f-b189-e492b6a900fb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:55:34.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-374" for this suite.
Jun 23 17:55:56.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:55:56.264: INFO: namespace configmap-374 deletion completed in 22.124723301s

• [SLOW TEST:88.843 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:55:56.264: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-4ee6535e-be7b-4244-a772-b900f4c515bc
STEP: Creating configMap with name cm-test-opt-upd-7db0555e-1a7d-4238-87d6-3535ac5d8104
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4ee6535e-be7b-4244-a772-b900f4c515bc
STEP: Updating configmap cm-test-opt-upd-7db0555e-1a7d-4238-87d6-3535ac5d8104
STEP: Creating configMap with name cm-test-opt-create-be768943-686b-4357-a61c-8667832a205f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:56:00.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6724" for this suite.
Jun 23 17:56:12.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:56:12.576: INFO: namespace projected-6724 deletion completed in 12.0707902s

• [SLOW TEST:16.312 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:56:12.579: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 23 17:56:12.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-633'
Jun 23 17:56:12.659: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 23 17:56:12.659: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Jun 23 17:56:14.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete deployment e2e-test-nginx-deployment --namespace=kubectl-633'
Jun 23 17:56:14.740: INFO: stderr: ""
Jun 23 17:56:14.740: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:56:14.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-633" for this suite.
Jun 23 17:56:36.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:56:36.877: INFO: namespace kubectl-633 deletion completed in 22.133561841s

• [SLOW TEST:24.299 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:56:36.878: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9920.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9920.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9920.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9920.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9920.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9920.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 23 17:57:02.950: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:02.954: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:02.957: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.dns-9920.svc.cluster.local from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:02.960: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:02.962: INFO: Unable to read jessie_udp@PodARecord from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:02.964: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:02.964: INFO: Lookups using dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_hosts@dns-querier-1.dns-test-service.dns-9920.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun 23 17:57:07.979: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.dns-9920.svc.cluster.local from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:07.981: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:07.983: INFO: Unable to read jessie_udp@PodARecord from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:07.985: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:07.985: INFO: Lookups using dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa failed for: [jessie_hosts@dns-querier-1.dns-test-service.dns-9920.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun 23 17:57:13.004: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:13.008: INFO: Unable to read jessie_udp@PodARecord from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:13.011: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:13.011: INFO: Lookups using dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa failed for: [jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun 23 17:57:18.014: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:18.021: INFO: Unable to read jessie_udp@PodARecord from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:18.025: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa: the server could not find the requested resource (get pods dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa)
Jun 23 17:57:18.025: INFO: Lookups using dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa failed for: [jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun 23 17:57:23.018: INFO: DNS probes using dns-9920/dns-test-00de9b85-ee74-4e16-a8c4-830891d534fa succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:57:23.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9920" for this suite.
Jun 23 17:57:29.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:57:29.139: INFO: namespace dns-9920 deletion completed in 6.095057471s

• [SLOW TEST:52.262 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:57:29.140: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Jun 23 17:57:29.166: INFO: Waiting up to 5m0s for pod "client-containers-b2daa523-57e4-4b38-a0e9-63a0af28a966" in namespace "containers-5470" to be "success or failure"
Jun 23 17:57:29.171: INFO: Pod "client-containers-b2daa523-57e4-4b38-a0e9-63a0af28a966": Phase="Pending", Reason="", readiness=false. Elapsed: 4.303462ms
Jun 23 17:57:31.174: INFO: Pod "client-containers-b2daa523-57e4-4b38-a0e9-63a0af28a966": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007525467s
STEP: Saw pod success
Jun 23 17:57:31.174: INFO: Pod "client-containers-b2daa523-57e4-4b38-a0e9-63a0af28a966" satisfied condition "success or failure"
Jun 23 17:57:31.176: INFO: Trying to get logs from node worker-1 pod client-containers-b2daa523-57e4-4b38-a0e9-63a0af28a966 container test-container: <nil>
STEP: delete the pod
Jun 23 17:57:31.192: INFO: Waiting for pod client-containers-b2daa523-57e4-4b38-a0e9-63a0af28a966 to disappear
Jun 23 17:57:31.193: INFO: Pod client-containers-b2daa523-57e4-4b38-a0e9-63a0af28a966 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:57:31.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5470" for this suite.
Jun 23 17:57:37.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:57:37.307: INFO: namespace containers-5470 deletion completed in 6.110432578s

• [SLOW TEST:8.167 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:57:37.307: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 23 17:57:37.335: INFO: Waiting up to 5m0s for pod "downward-api-63cb6124-6086-434f-9a7d-0af081a0f6fc" in namespace "downward-api-5237" to be "success or failure"
Jun 23 17:57:37.340: INFO: Pod "downward-api-63cb6124-6086-434f-9a7d-0af081a0f6fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.309574ms
Jun 23 17:57:39.342: INFO: Pod "downward-api-63cb6124-6086-434f-9a7d-0af081a0f6fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006659981s
STEP: Saw pod success
Jun 23 17:57:39.343: INFO: Pod "downward-api-63cb6124-6086-434f-9a7d-0af081a0f6fc" satisfied condition "success or failure"
Jun 23 17:57:39.345: INFO: Trying to get logs from node worker-1 pod downward-api-63cb6124-6086-434f-9a7d-0af081a0f6fc container dapi-container: <nil>
STEP: delete the pod
Jun 23 17:57:39.359: INFO: Waiting for pod downward-api-63cb6124-6086-434f-9a7d-0af081a0f6fc to disappear
Jun 23 17:57:39.363: INFO: Pod downward-api-63cb6124-6086-434f-9a7d-0af081a0f6fc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 17:57:39.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5237" for this suite.
Jun 23 17:57:45.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 17:57:45.461: INFO: namespace downward-api-5237 deletion completed in 6.096389877s

• [SLOW TEST:8.155 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 17:57:45.462: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-2324c213-5bf3-466b-9c29-a638f1bf5566 in namespace container-probe-9265
Jun 23 17:57:47.502: INFO: Started pod test-webserver-2324c213-5bf3-466b-9c29-a638f1bf5566 in namespace container-probe-9265
STEP: checking the pod's current state and verifying that restartCount is present
Jun 23 17:57:47.503: INFO: Initial restart count of pod test-webserver-2324c213-5bf3-466b-9c29-a638f1bf5566 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:01:48.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9265" for this suite.
Jun 23 18:01:54.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:01:54.578: INFO: namespace container-probe-9265 deletion completed in 6.098548418s

• [SLOW TEST:249.116 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:01:54.579: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0623 18:02:04.700795      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 23 18:02:04.701: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:02:04.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8541" for this suite.
Jun 23 18:02:10.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:02:10.828: INFO: namespace gc-8541 deletion completed in 6.117958564s

• [SLOW TEST:16.250 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:02:10.830: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-d36175af-7168-4b7a-92dc-38d1b2e6935d
STEP: Creating a pod to test consume secrets
Jun 23 18:02:10.857: INFO: Waiting up to 5m0s for pod "pod-secrets-7f20418c-29ea-4846-9377-ac1ee618d7ee" in namespace "secrets-7316" to be "success or failure"
Jun 23 18:02:10.860: INFO: Pod "pod-secrets-7f20418c-29ea-4846-9377-ac1ee618d7ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.547701ms
Jun 23 18:02:12.866: INFO: Pod "pod-secrets-7f20418c-29ea-4846-9377-ac1ee618d7ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00854192s
Jun 23 18:02:14.874: INFO: Pod "pod-secrets-7f20418c-29ea-4846-9377-ac1ee618d7ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016073489s
STEP: Saw pod success
Jun 23 18:02:14.874: INFO: Pod "pod-secrets-7f20418c-29ea-4846-9377-ac1ee618d7ee" satisfied condition "success or failure"
Jun 23 18:02:14.879: INFO: Trying to get logs from node worker-1 pod pod-secrets-7f20418c-29ea-4846-9377-ac1ee618d7ee container secret-env-test: <nil>
STEP: delete the pod
Jun 23 18:02:14.925: INFO: Waiting for pod pod-secrets-7f20418c-29ea-4846-9377-ac1ee618d7ee to disappear
Jun 23 18:02:14.927: INFO: Pod pod-secrets-7f20418c-29ea-4846-9377-ac1ee618d7ee no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:02:14.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7316" for this suite.
Jun 23 18:02:20.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:02:21.034: INFO: namespace secrets-7316 deletion completed in 6.103687845s

• [SLOW TEST:10.204 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:02:21.034: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 23 18:02:21.060: INFO: Waiting up to 5m0s for pod "pod-afc707ee-8669-4ad8-807a-6ba433e0c48e" in namespace "emptydir-6369" to be "success or failure"
Jun 23 18:02:21.065: INFO: Pod "pod-afc707ee-8669-4ad8-807a-6ba433e0c48e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.404698ms
Jun 23 18:02:23.072: INFO: Pod "pod-afc707ee-8669-4ad8-807a-6ba433e0c48e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012524314s
Jun 23 18:02:25.079: INFO: Pod "pod-afc707ee-8669-4ad8-807a-6ba433e0c48e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019437278s
STEP: Saw pod success
Jun 23 18:02:25.080: INFO: Pod "pod-afc707ee-8669-4ad8-807a-6ba433e0c48e" satisfied condition "success or failure"
Jun 23 18:02:25.085: INFO: Trying to get logs from node worker-1 pod pod-afc707ee-8669-4ad8-807a-6ba433e0c48e container test-container: <nil>
STEP: delete the pod
Jun 23 18:02:25.129: INFO: Waiting for pod pod-afc707ee-8669-4ad8-807a-6ba433e0c48e to disappear
Jun 23 18:02:25.132: INFO: Pod pod-afc707ee-8669-4ad8-807a-6ba433e0c48e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:02:25.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6369" for this suite.
Jun 23 18:02:31.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:02:31.198: INFO: namespace emptydir-6369 deletion completed in 6.063376274s

• [SLOW TEST:10.164 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:02:31.198: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 23 18:02:31.455: INFO: Pod name wrapped-volume-race-602f2b05-354c-421d-9370-2a1721bf485c: Found 3 pods out of 5
Jun 23 18:02:36.459: INFO: Pod name wrapped-volume-race-602f2b05-354c-421d-9370-2a1721bf485c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-602f2b05-354c-421d-9370-2a1721bf485c in namespace emptydir-wrapper-5857, will wait for the garbage collector to delete the pods
Jun 23 18:02:52.573: INFO: Deleting ReplicationController wrapped-volume-race-602f2b05-354c-421d-9370-2a1721bf485c took: 6.779362ms
Jun 23 18:02:52.873: INFO: Terminating ReplicationController wrapped-volume-race-602f2b05-354c-421d-9370-2a1721bf485c pods took: 300.712746ms
STEP: Creating RC which spawns configmap-volume pods
Jun 23 18:03:32.601: INFO: Pod name wrapped-volume-race-f726db3f-011e-4d2d-9f50-5ece7a4aff38: Found 0 pods out of 5
Jun 23 18:03:37.631: INFO: Pod name wrapped-volume-race-f726db3f-011e-4d2d-9f50-5ece7a4aff38: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f726db3f-011e-4d2d-9f50-5ece7a4aff38 in namespace emptydir-wrapper-5857, will wait for the garbage collector to delete the pods
Jun 23 18:03:49.783: INFO: Deleting ReplicationController wrapped-volume-race-f726db3f-011e-4d2d-9f50-5ece7a4aff38 took: 22.343323ms
Jun 23 18:03:50.084: INFO: Terminating ReplicationController wrapped-volume-race-f726db3f-011e-4d2d-9f50-5ece7a4aff38 pods took: 300.977579ms
STEP: Creating RC which spawns configmap-volume pods
Jun 23 18:04:31.598: INFO: Pod name wrapped-volume-race-65485cee-697e-4d3d-adcc-6729ff365462: Found 0 pods out of 5
Jun 23 18:04:36.609: INFO: Pod name wrapped-volume-race-65485cee-697e-4d3d-adcc-6729ff365462: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-65485cee-697e-4d3d-adcc-6729ff365462 in namespace emptydir-wrapper-5857, will wait for the garbage collector to delete the pods
Jun 23 18:04:48.709: INFO: Deleting ReplicationController wrapped-volume-race-65485cee-697e-4d3d-adcc-6729ff365462 took: 10.006704ms
Jun 23 18:04:49.015: INFO: Terminating ReplicationController wrapped-volume-race-65485cee-697e-4d3d-adcc-6729ff365462 pods took: 305.800865ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:05:31.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5857" for this suite.
Jun 23 18:05:37.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:05:37.855: INFO: namespace emptydir-wrapper-5857 deletion completed in 6.092401117s

• [SLOW TEST:186.657 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:05:37.856: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-29fd
STEP: Creating a pod to test atomic-volume-subpath
Jun 23 18:05:37.884: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-29fd" in namespace "subpath-5258" to be "success or failure"
Jun 23 18:05:37.887: INFO: Pod "pod-subpath-test-projected-29fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.949107ms
Jun 23 18:05:39.894: INFO: Pod "pod-subpath-test-projected-29fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009782777s
Jun 23 18:05:41.901: INFO: Pod "pod-subpath-test-projected-29fd": Phase="Running", Reason="", readiness=true. Elapsed: 4.016498733s
Jun 23 18:05:43.907: INFO: Pod "pod-subpath-test-projected-29fd": Phase="Running", Reason="", readiness=true. Elapsed: 6.022853169s
Jun 23 18:05:45.912: INFO: Pod "pod-subpath-test-projected-29fd": Phase="Running", Reason="", readiness=true. Elapsed: 8.028283797s
Jun 23 18:05:47.919: INFO: Pod "pod-subpath-test-projected-29fd": Phase="Running", Reason="", readiness=true. Elapsed: 10.035182034s
Jun 23 18:05:49.926: INFO: Pod "pod-subpath-test-projected-29fd": Phase="Running", Reason="", readiness=true. Elapsed: 12.041343842s
Jun 23 18:05:51.934: INFO: Pod "pod-subpath-test-projected-29fd": Phase="Running", Reason="", readiness=true. Elapsed: 14.049551176s
Jun 23 18:05:53.942: INFO: Pod "pod-subpath-test-projected-29fd": Phase="Running", Reason="", readiness=true. Elapsed: 16.057915882s
Jun 23 18:05:55.950: INFO: Pod "pod-subpath-test-projected-29fd": Phase="Running", Reason="", readiness=true. Elapsed: 18.066034015s
Jun 23 18:05:57.958: INFO: Pod "pod-subpath-test-projected-29fd": Phase="Running", Reason="", readiness=true. Elapsed: 20.073834898s
Jun 23 18:05:59.964: INFO: Pod "pod-subpath-test-projected-29fd": Phase="Running", Reason="", readiness=true. Elapsed: 22.080291257s
Jun 23 18:06:01.970: INFO: Pod "pod-subpath-test-projected-29fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.086015079s
STEP: Saw pod success
Jun 23 18:06:01.970: INFO: Pod "pod-subpath-test-projected-29fd" satisfied condition "success or failure"
Jun 23 18:06:01.975: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-projected-29fd container test-container-subpath-projected-29fd: <nil>
STEP: delete the pod
Jun 23 18:06:02.023: INFO: Waiting for pod pod-subpath-test-projected-29fd to disappear
Jun 23 18:06:02.027: INFO: Pod pod-subpath-test-projected-29fd no longer exists
STEP: Deleting pod pod-subpath-test-projected-29fd
Jun 23 18:06:02.027: INFO: Deleting pod "pod-subpath-test-projected-29fd" in namespace "subpath-5258"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:06:02.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5258" for this suite.
Jun 23 18:06:08.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:06:08.131: INFO: namespace subpath-5258 deletion completed in 6.098414491s

• [SLOW TEST:30.276 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:06:08.132: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0623 18:06:18.195621      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 23 18:06:18.195: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:06:18.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1785" for this suite.
Jun 23 18:06:24.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:06:24.282: INFO: namespace gc-1785 deletion completed in 6.07971034s

• [SLOW TEST:16.150 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:06:24.282: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 18:06:24.307: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f84b76e8-c563-4e25-9e04-547232879d36" in namespace "downward-api-64" to be "success or failure"
Jun 23 18:06:24.313: INFO: Pod "downwardapi-volume-f84b76e8-c563-4e25-9e04-547232879d36": Phase="Pending", Reason="", readiness=false. Elapsed: 5.911095ms
Jun 23 18:06:26.321: INFO: Pod "downwardapi-volume-f84b76e8-c563-4e25-9e04-547232879d36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014323422s
STEP: Saw pod success
Jun 23 18:06:26.322: INFO: Pod "downwardapi-volume-f84b76e8-c563-4e25-9e04-547232879d36" satisfied condition "success or failure"
Jun 23 18:06:26.325: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-f84b76e8-c563-4e25-9e04-547232879d36 container client-container: <nil>
STEP: delete the pod
Jun 23 18:06:26.343: INFO: Waiting for pod downwardapi-volume-f84b76e8-c563-4e25-9e04-547232879d36 to disappear
Jun 23 18:06:26.345: INFO: Pod downwardapi-volume-f84b76e8-c563-4e25-9e04-547232879d36 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:06:26.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-64" for this suite.
Jun 23 18:06:32.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:06:32.448: INFO: namespace downward-api-64 deletion completed in 6.10176513s

• [SLOW TEST:8.166 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:06:32.450: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-2e2eb37f-b752-4cc3-ba5b-b7fda3c35e64 in namespace container-probe-2113
Jun 23 18:06:36.532: INFO: Started pod liveness-2e2eb37f-b752-4cc3-ba5b-b7fda3c35e64 in namespace container-probe-2113
STEP: checking the pod's current state and verifying that restartCount is present
Jun 23 18:06:36.534: INFO: Initial restart count of pod liveness-2e2eb37f-b752-4cc3-ba5b-b7fda3c35e64 is 0
Jun 23 18:06:50.583: INFO: Restart count of pod container-probe-2113/liveness-2e2eb37f-b752-4cc3-ba5b-b7fda3c35e64 is now 1 (14.049163883s elapsed)
Jun 23 18:07:10.653: INFO: Restart count of pod container-probe-2113/liveness-2e2eb37f-b752-4cc3-ba5b-b7fda3c35e64 is now 2 (34.119046656s elapsed)
Jun 23 18:07:30.707: INFO: Restart count of pod container-probe-2113/liveness-2e2eb37f-b752-4cc3-ba5b-b7fda3c35e64 is now 3 (54.173423753s elapsed)
Jun 23 18:07:50.781: INFO: Restart count of pod container-probe-2113/liveness-2e2eb37f-b752-4cc3-ba5b-b7fda3c35e64 is now 4 (1m14.247782422s elapsed)
Jun 23 18:08:51.148: INFO: Restart count of pod container-probe-2113/liveness-2e2eb37f-b752-4cc3-ba5b-b7fda3c35e64 is now 5 (2m14.613931958s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:08:51.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2113" for this suite.
Jun 23 18:08:57.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:08:57.276: INFO: namespace container-probe-2113 deletion completed in 6.092180648s

• [SLOW TEST:144.826 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:08:57.277: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-4914e4dc-b8b5-4fd1-95ba-9a1f6a2f5b63
STEP: Creating a pod to test consume secrets
Jun 23 18:08:57.308: INFO: Waiting up to 5m0s for pod "pod-secrets-c77da95a-d297-416c-9fc7-fb7858c7147e" in namespace "secrets-1916" to be "success or failure"
Jun 23 18:08:57.317: INFO: Pod "pod-secrets-c77da95a-d297-416c-9fc7-fb7858c7147e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.10744ms
Jun 23 18:08:59.327: INFO: Pod "pod-secrets-c77da95a-d297-416c-9fc7-fb7858c7147e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018875728s
Jun 23 18:09:01.336: INFO: Pod "pod-secrets-c77da95a-d297-416c-9fc7-fb7858c7147e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027504645s
STEP: Saw pod success
Jun 23 18:09:01.336: INFO: Pod "pod-secrets-c77da95a-d297-416c-9fc7-fb7858c7147e" satisfied condition "success or failure"
Jun 23 18:09:01.351: INFO: Trying to get logs from node worker-1 pod pod-secrets-c77da95a-d297-416c-9fc7-fb7858c7147e container secret-volume-test: <nil>
STEP: delete the pod
Jun 23 18:09:01.381: INFO: Waiting for pod pod-secrets-c77da95a-d297-416c-9fc7-fb7858c7147e to disappear
Jun 23 18:09:01.383: INFO: Pod pod-secrets-c77da95a-d297-416c-9fc7-fb7858c7147e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:09:01.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1916" for this suite.
Jun 23 18:09:07.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:09:07.476: INFO: namespace secrets-1916 deletion completed in 6.090239002s

• [SLOW TEST:10.199 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:09:07.476: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 18:09:07.501: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6d77e8a-5970-4f72-bbd9-3b6f17aec2ac" in namespace "projected-1882" to be "success or failure"
Jun 23 18:09:07.507: INFO: Pod "downwardapi-volume-b6d77e8a-5970-4f72-bbd9-3b6f17aec2ac": Phase="Pending", Reason="", readiness=false. Elapsed: 5.965004ms
Jun 23 18:09:09.513: INFO: Pod "downwardapi-volume-b6d77e8a-5970-4f72-bbd9-3b6f17aec2ac": Phase="Running", Reason="", readiness=true. Elapsed: 2.011964204s
Jun 23 18:09:11.519: INFO: Pod "downwardapi-volume-b6d77e8a-5970-4f72-bbd9-3b6f17aec2ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018150486s
STEP: Saw pod success
Jun 23 18:09:11.519: INFO: Pod "downwardapi-volume-b6d77e8a-5970-4f72-bbd9-3b6f17aec2ac" satisfied condition "success or failure"
Jun 23 18:09:11.525: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-b6d77e8a-5970-4f72-bbd9-3b6f17aec2ac container client-container: <nil>
STEP: delete the pod
Jun 23 18:09:11.538: INFO: Waiting for pod downwardapi-volume-b6d77e8a-5970-4f72-bbd9-3b6f17aec2ac to disappear
Jun 23 18:09:11.539: INFO: Pod downwardapi-volume-b6d77e8a-5970-4f72-bbd9-3b6f17aec2ac no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:09:11.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1882" for this suite.
Jun 23 18:09:17.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:09:17.624: INFO: namespace projected-1882 deletion completed in 6.082492898s

• [SLOW TEST:10.148 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:09:17.624: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 23 18:09:17.658: INFO: Waiting up to 5m0s for pod "pod-86651b61-dfbb-41bf-8a8d-5f07436ca402" in namespace "emptydir-8643" to be "success or failure"
Jun 23 18:09:17.663: INFO: Pod "pod-86651b61-dfbb-41bf-8a8d-5f07436ca402": Phase="Pending", Reason="", readiness=false. Elapsed: 5.556419ms
Jun 23 18:09:19.669: INFO: Pod "pod-86651b61-dfbb-41bf-8a8d-5f07436ca402": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010769448s
Jun 23 18:09:21.672: INFO: Pod "pod-86651b61-dfbb-41bf-8a8d-5f07436ca402": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014447572s
STEP: Saw pod success
Jun 23 18:09:21.672: INFO: Pod "pod-86651b61-dfbb-41bf-8a8d-5f07436ca402" satisfied condition "success or failure"
Jun 23 18:09:21.678: INFO: Trying to get logs from node worker-1 pod pod-86651b61-dfbb-41bf-8a8d-5f07436ca402 container test-container: <nil>
STEP: delete the pod
Jun 23 18:09:21.696: INFO: Waiting for pod pod-86651b61-dfbb-41bf-8a8d-5f07436ca402 to disappear
Jun 23 18:09:21.697: INFO: Pod pod-86651b61-dfbb-41bf-8a8d-5f07436ca402 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:09:21.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8643" for this suite.
Jun 23 18:09:27.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:09:27.794: INFO: namespace emptydir-8643 deletion completed in 6.09455074s

• [SLOW TEST:10.170 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:09:27.794: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Jun 23 18:09:27.821: INFO: Waiting up to 5m0s for pod "client-containers-b193df32-1d67-4533-8287-747a4b6c23a6" in namespace "containers-1896" to be "success or failure"
Jun 23 18:09:27.830: INFO: Pod "client-containers-b193df32-1d67-4533-8287-747a4b6c23a6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.213839ms
Jun 23 18:09:29.834: INFO: Pod "client-containers-b193df32-1d67-4533-8287-747a4b6c23a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01323098s
STEP: Saw pod success
Jun 23 18:09:29.834: INFO: Pod "client-containers-b193df32-1d67-4533-8287-747a4b6c23a6" satisfied condition "success or failure"
Jun 23 18:09:29.837: INFO: Trying to get logs from node worker-1 pod client-containers-b193df32-1d67-4533-8287-747a4b6c23a6 container test-container: <nil>
STEP: delete the pod
Jun 23 18:09:29.862: INFO: Waiting for pod client-containers-b193df32-1d67-4533-8287-747a4b6c23a6 to disappear
Jun 23 18:09:29.864: INFO: Pod client-containers-b193df32-1d67-4533-8287-747a4b6c23a6 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:09:29.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1896" for this suite.
Jun 23 18:09:35.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:09:35.964: INFO: namespace containers-1896 deletion completed in 6.098655835s

• [SLOW TEST:8.170 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:09:35.965: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:09:36.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1072" for this suite.
Jun 23 18:09:42.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:09:42.096: INFO: namespace kubelet-test-1072 deletion completed in 6.090961886s

• [SLOW TEST:6.132 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:09:42.097: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:09:44.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6404" for this suite.
Jun 23 18:09:50.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:09:50.256: INFO: namespace emptydir-wrapper-6404 deletion completed in 6.087173709s

• [SLOW TEST:8.160 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:09:50.257: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 23 18:09:54.839: INFO: Successfully updated pod "annotationupdatebd826af2-d07c-4576-a5f4-a55b9486eae9"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:09:56.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-513" for this suite.
Jun 23 18:10:18.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:10:18.979: INFO: namespace downward-api-513 deletion completed in 22.101660384s

• [SLOW TEST:28.722 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:10:18.980: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-e711263f-179c-4aa2-a1b8-aab711ee9621 in namespace container-probe-5886
Jun 23 18:10:21.016: INFO: Started pod liveness-e711263f-179c-4aa2-a1b8-aab711ee9621 in namespace container-probe-5886
STEP: checking the pod's current state and verifying that restartCount is present
Jun 23 18:10:21.023: INFO: Initial restart count of pod liveness-e711263f-179c-4aa2-a1b8-aab711ee9621 is 0
Jun 23 18:10:41.138: INFO: Restart count of pod container-probe-5886/liveness-e711263f-179c-4aa2-a1b8-aab711ee9621 is now 1 (20.11524878s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:10:41.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5886" for this suite.
Jun 23 18:10:47.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:10:47.254: INFO: namespace container-probe-5886 deletion completed in 6.088935325s

• [SLOW TEST:28.275 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:10:47.256: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jun 23 18:10:47.279: INFO: namespace kubectl-434
Jun 23 18:10:47.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 create -f - --namespace=kubectl-434'
Jun 23 18:10:47.915: INFO: stderr: ""
Jun 23 18:10:47.915: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 23 18:10:48.921: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 18:10:48.921: INFO: Found 0 / 1
Jun 23 18:10:49.920: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 18:10:49.920: INFO: Found 0 / 1
Jun 23 18:10:50.922: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 18:10:50.922: INFO: Found 0 / 1
Jun 23 18:10:51.922: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 18:10:51.922: INFO: Found 1 / 1
Jun 23 18:10:51.922: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 23 18:10:51.928: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 18:10:51.928: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 23 18:10:51.928: INFO: wait on redis-master startup in kubectl-434 
Jun 23 18:10:51.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 logs redis-master-d6654 redis-master --namespace=kubectl-434'
Jun 23 18:10:52.019: INFO: stderr: ""
Jun 23 18:10:52.019: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 23 Jun 18:10:51.451 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Jun 18:10:51.451 # Server started, Redis version 3.2.12\n1:M 23 Jun 18:10:51.451 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Jun 18:10:51.451 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jun 23 18:10:52.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-434'
Jun 23 18:10:52.083: INFO: stderr: ""
Jun 23 18:10:52.083: INFO: stdout: "service/rm2 exposed\n"
Jun 23 18:10:52.089: INFO: Service rm2 in namespace kubectl-434 found.
STEP: exposing service
Jun 23 18:10:54.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-434'
Jun 23 18:10:54.201: INFO: stderr: ""
Jun 23 18:10:54.201: INFO: stdout: "service/rm3 exposed\n"
Jun 23 18:10:54.204: INFO: Service rm3 in namespace kubectl-434 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:10:56.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-434" for this suite.
Jun 23 18:11:14.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:11:14.311: INFO: namespace kubectl-434 deletion completed in 18.08959191s

• [SLOW TEST:27.055 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:11:14.311: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-274
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-274
STEP: Deleting pre-stop pod
Jun 23 18:11:29.416: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:11:29.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-274" for this suite.
Jun 23 18:12:07.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:12:07.559: INFO: namespace prestop-274 deletion completed in 38.118599555s

• [SLOW TEST:53.248 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:12:07.559: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-26d82a39-428a-472d-bdd3-e569f110b540
STEP: Creating secret with name s-test-opt-upd-3359a817-0dab-41cc-91e1-d0c1b6b73046
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-26d82a39-428a-472d-bdd3-e569f110b540
STEP: Updating secret s-test-opt-upd-3359a817-0dab-41cc-91e1-d0c1b6b73046
STEP: Creating secret with name s-test-opt-create-37db9326-fd9c-4a29-ac40-6eac008f1569
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:12:11.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2490" for this suite.
Jun 23 18:12:33.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:12:33.825: INFO: namespace secrets-2490 deletion completed in 22.102715125s

• [SLOW TEST:26.266 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:12:33.825: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jun 23 18:12:33.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 create -f - --namespace=kubectl-6763'
Jun 23 18:12:33.956: INFO: stderr: ""
Jun 23 18:12:33.956: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 23 18:12:33.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6763'
Jun 23 18:12:34.017: INFO: stderr: ""
Jun 23 18:12:34.017: INFO: stdout: "update-demo-nautilus-jt6m8 update-demo-nautilus-rrqbl "
Jun 23 18:12:34.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-jt6m8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Jun 23 18:12:34.064: INFO: stderr: ""
Jun 23 18:12:34.064: INFO: stdout: ""
Jun 23 18:12:34.064: INFO: update-demo-nautilus-jt6m8 is created but not running
Jun 23 18:12:39.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6763'
Jun 23 18:12:39.151: INFO: stderr: ""
Jun 23 18:12:39.151: INFO: stdout: "update-demo-nautilus-jt6m8 update-demo-nautilus-rrqbl "
Jun 23 18:12:39.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-jt6m8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Jun 23 18:12:39.212: INFO: stderr: ""
Jun 23 18:12:39.212: INFO: stdout: "true"
Jun 23 18:12:39.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-jt6m8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Jun 23 18:12:39.270: INFO: stderr: ""
Jun 23 18:12:39.270: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 23 18:12:39.270: INFO: validating pod update-demo-nautilus-jt6m8
Jun 23 18:12:39.273: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 23 18:12:39.273: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 23 18:12:39.273: INFO: update-demo-nautilus-jt6m8 is verified up and running
Jun 23 18:12:39.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-rrqbl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Jun 23 18:12:39.319: INFO: stderr: ""
Jun 23 18:12:39.319: INFO: stdout: "true"
Jun 23 18:12:39.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-rrqbl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Jun 23 18:12:39.366: INFO: stderr: ""
Jun 23 18:12:39.366: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 23 18:12:39.366: INFO: validating pod update-demo-nautilus-rrqbl
Jun 23 18:12:39.369: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 23 18:12:39.369: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 23 18:12:39.369: INFO: update-demo-nautilus-rrqbl is verified up and running
STEP: scaling down the replication controller
Jun 23 18:12:39.371: INFO: scanned /root for discovery docs: <nil>
Jun 23 18:12:39.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-6763'
Jun 23 18:12:40.460: INFO: stderr: ""
Jun 23 18:12:40.460: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 23 18:12:40.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6763'
Jun 23 18:12:40.530: INFO: stderr: ""
Jun 23 18:12:40.530: INFO: stdout: "update-demo-nautilus-jt6m8 update-demo-nautilus-rrqbl "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 23 18:12:45.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6763'
Jun 23 18:12:45.612: INFO: stderr: ""
Jun 23 18:12:45.612: INFO: stdout: "update-demo-nautilus-jt6m8 update-demo-nautilus-rrqbl "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 23 18:12:50.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6763'
Jun 23 18:12:50.675: INFO: stderr: ""
Jun 23 18:12:50.675: INFO: stdout: "update-demo-nautilus-jt6m8 update-demo-nautilus-rrqbl "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 23 18:12:55.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6763'
Jun 23 18:12:55.759: INFO: stderr: ""
Jun 23 18:12:55.759: INFO: stdout: "update-demo-nautilus-rrqbl "
Jun 23 18:12:55.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-rrqbl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Jun 23 18:12:55.811: INFO: stderr: ""
Jun 23 18:12:55.811: INFO: stdout: "true"
Jun 23 18:12:55.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-rrqbl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Jun 23 18:12:55.877: INFO: stderr: ""
Jun 23 18:12:55.877: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 23 18:12:55.878: INFO: validating pod update-demo-nautilus-rrqbl
Jun 23 18:12:55.880: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 23 18:12:55.880: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 23 18:12:55.880: INFO: update-demo-nautilus-rrqbl is verified up and running
STEP: scaling up the replication controller
Jun 23 18:12:55.881: INFO: scanned /root for discovery docs: <nil>
Jun 23 18:12:55.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-6763'
Jun 23 18:12:56.962: INFO: stderr: ""
Jun 23 18:12:56.962: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 23 18:12:56.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6763'
Jun 23 18:12:57.023: INFO: stderr: ""
Jun 23 18:12:57.023: INFO: stdout: "update-demo-nautilus-9ndvt update-demo-nautilus-rrqbl "
Jun 23 18:12:57.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-9ndvt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Jun 23 18:12:57.085: INFO: stderr: ""
Jun 23 18:12:57.085: INFO: stdout: ""
Jun 23 18:12:57.085: INFO: update-demo-nautilus-9ndvt is created but not running
Jun 23 18:13:02.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6763'
Jun 23 18:13:02.182: INFO: stderr: ""
Jun 23 18:13:02.182: INFO: stdout: "update-demo-nautilus-9ndvt update-demo-nautilus-rrqbl "
Jun 23 18:13:02.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-9ndvt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Jun 23 18:13:02.234: INFO: stderr: ""
Jun 23 18:13:02.234: INFO: stdout: "true"
Jun 23 18:13:02.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-9ndvt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Jun 23 18:13:02.295: INFO: stderr: ""
Jun 23 18:13:02.295: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 23 18:13:02.295: INFO: validating pod update-demo-nautilus-9ndvt
Jun 23 18:13:02.299: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 23 18:13:02.299: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 23 18:13:02.299: INFO: update-demo-nautilus-9ndvt is verified up and running
Jun 23 18:13:02.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-rrqbl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Jun 23 18:13:02.351: INFO: stderr: ""
Jun 23 18:13:02.351: INFO: stdout: "true"
Jun 23 18:13:02.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-rrqbl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Jun 23 18:13:02.404: INFO: stderr: ""
Jun 23 18:13:02.404: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 23 18:13:02.404: INFO: validating pod update-demo-nautilus-rrqbl
Jun 23 18:13:02.407: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 23 18:13:02.407: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 23 18:13:02.407: INFO: update-demo-nautilus-rrqbl is verified up and running
STEP: using delete to clean up resources
Jun 23 18:13:02.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete --grace-period=0 --force -f - --namespace=kubectl-6763'
Jun 23 18:13:02.458: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 23 18:13:02.458: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 23 18:13:02.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6763'
Jun 23 18:13:02.514: INFO: stderr: "No resources found.\n"
Jun 23 18:13:02.514: INFO: stdout: ""
Jun 23 18:13:02.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -l name=update-demo --namespace=kubectl-6763 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 23 18:13:02.564: INFO: stderr: ""
Jun 23 18:13:02.564: INFO: stdout: "update-demo-nautilus-9ndvt\nupdate-demo-nautilus-rrqbl\n"
Jun 23 18:13:03.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6763'
Jun 23 18:13:03.124: INFO: stderr: "No resources found.\n"
Jun 23 18:13:03.124: INFO: stdout: ""
Jun 23 18:13:03.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -l name=update-demo --namespace=kubectl-6763 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 23 18:13:03.174: INFO: stderr: ""
Jun 23 18:13:03.174: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:13:03.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6763" for this suite.
Jun 23 18:13:25.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:13:25.240: INFO: namespace kubectl-6763 deletion completed in 22.064895092s

• [SLOW TEST:51.415 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:13:25.241: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-eadba712-278d-4a56-a6be-d335b6541e32
STEP: Creating a pod to test consume configMaps
Jun 23 18:13:25.268: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-25cbe6e3-4fe8-4ea9-b0ab-0e2292765230" in namespace "projected-8427" to be "success or failure"
Jun 23 18:13:25.273: INFO: Pod "pod-projected-configmaps-25cbe6e3-4fe8-4ea9-b0ab-0e2292765230": Phase="Pending", Reason="", readiness=false. Elapsed: 4.627296ms
Jun 23 18:13:27.279: INFO: Pod "pod-projected-configmaps-25cbe6e3-4fe8-4ea9-b0ab-0e2292765230": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010945803s
STEP: Saw pod success
Jun 23 18:13:27.279: INFO: Pod "pod-projected-configmaps-25cbe6e3-4fe8-4ea9-b0ab-0e2292765230" satisfied condition "success or failure"
Jun 23 18:13:27.285: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-25cbe6e3-4fe8-4ea9-b0ab-0e2292765230 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 23 18:13:27.324: INFO: Waiting for pod pod-projected-configmaps-25cbe6e3-4fe8-4ea9-b0ab-0e2292765230 to disappear
Jun 23 18:13:27.327: INFO: Pod pod-projected-configmaps-25cbe6e3-4fe8-4ea9-b0ab-0e2292765230 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:13:27.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8427" for this suite.
Jun 23 18:13:33.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:13:33.426: INFO: namespace projected-8427 deletion completed in 6.095881336s

• [SLOW TEST:8.185 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:13:33.426: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 23 18:13:35.474: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:13:35.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6497" for this suite.
Jun 23 18:13:41.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:13:41.591: INFO: namespace container-runtime-6497 deletion completed in 6.07658939s

• [SLOW TEST:8.165 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:13:41.592: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4386
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jun 23 18:13:41.629: INFO: Found 0 stateful pods, waiting for 3
Jun 23 18:13:51.636: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 23 18:13:51.636: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 23 18:13:51.636: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 23 18:13:51.681: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 23 18:14:01.716: INFO: Updating stateful set ss2
Jun 23 18:14:01.739: INFO: Waiting for Pod statefulset-4386/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Jun 23 18:14:11.826: INFO: Found 2 stateful pods, waiting for 3
Jun 23 18:14:21.833: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 23 18:14:21.833: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 23 18:14:21.833: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 23 18:14:21.876: INFO: Updating stateful set ss2
Jun 23 18:14:21.891: INFO: Waiting for Pod statefulset-4386/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 23 18:14:31.941: INFO: Updating stateful set ss2
Jun 23 18:14:31.956: INFO: Waiting for StatefulSet statefulset-4386/ss2 to complete update
Jun 23 18:14:31.956: INFO: Waiting for Pod statefulset-4386/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 23 18:14:41.961: INFO: Deleting all statefulset in ns statefulset-4386
Jun 23 18:14:41.963: INFO: Scaling statefulset ss2 to 0
Jun 23 18:15:01.983: INFO: Waiting for statefulset status.replicas updated to 0
Jun 23 18:15:01.988: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:15:02.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4386" for this suite.
Jun 23 18:15:08.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:15:08.121: INFO: namespace statefulset-4386 deletion completed in 6.098444874s

• [SLOW TEST:86.529 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:15:08.123: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 23 18:15:08.145: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:15:13.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3507" for this suite.
Jun 23 18:15:35.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:15:35.275: INFO: namespace init-container-3507 deletion completed in 22.102635703s

• [SLOW TEST:27.152 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:15:35.275: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-f1de7a42-2c04-4e00-a2b8-403db0308e26
STEP: Creating a pod to test consume configMaps
Jun 23 18:15:35.303: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0ba0ba62-a6f3-48fa-bc52-b190080742db" in namespace "projected-2546" to be "success or failure"
Jun 23 18:15:35.306: INFO: Pod "pod-projected-configmaps-0ba0ba62-a6f3-48fa-bc52-b190080742db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.527507ms
Jun 23 18:15:37.313: INFO: Pod "pod-projected-configmaps-0ba0ba62-a6f3-48fa-bc52-b190080742db": Phase="Running", Reason="", readiness=true. Elapsed: 2.01017971s
Jun 23 18:15:39.322: INFO: Pod "pod-projected-configmaps-0ba0ba62-a6f3-48fa-bc52-b190080742db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018782392s
STEP: Saw pod success
Jun 23 18:15:39.322: INFO: Pod "pod-projected-configmaps-0ba0ba62-a6f3-48fa-bc52-b190080742db" satisfied condition "success or failure"
Jun 23 18:15:39.328: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-0ba0ba62-a6f3-48fa-bc52-b190080742db container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 23 18:15:39.362: INFO: Waiting for pod pod-projected-configmaps-0ba0ba62-a6f3-48fa-bc52-b190080742db to disappear
Jun 23 18:15:39.364: INFO: Pod pod-projected-configmaps-0ba0ba62-a6f3-48fa-bc52-b190080742db no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:15:39.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2546" for this suite.
Jun 23 18:15:45.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:15:45.506: INFO: namespace projected-2546 deletion completed in 6.140459844s

• [SLOW TEST:10.231 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:15:45.507: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8866
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 23 18:15:45.527: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 23 18:16:09.713: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.38.0.3:8080/dial?request=hostName&protocol=udp&host=10.32.0.4&port=8081&tries=1'] Namespace:pod-network-test-8866 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 18:16:09.713: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 18:16:09.855: INFO: Waiting for endpoints: map[]
Jun 23 18:16:09.857: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.38.0.3:8080/dial?request=hostName&protocol=udp&host=10.38.0.2&port=8081&tries=1'] Namespace:pod-network-test-8866 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 18:16:09.858: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 18:16:09.944: INFO: Waiting for endpoints: map[]
Jun 23 18:16:09.946: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.38.0.3:8080/dial?request=hostName&protocol=udp&host=10.40.0.2&port=8081&tries=1'] Namespace:pod-network-test-8866 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 18:16:09.947: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 18:16:10.051: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:16:10.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8866" for this suite.
Jun 23 18:16:32.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:16:32.115: INFO: namespace pod-network-test-8866 deletion completed in 22.061366122s

• [SLOW TEST:46.608 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:16:32.115: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 18:16:32.145: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 23 18:16:32.155: INFO: Number of nodes with available pods: 0
Jun 23 18:16:32.155: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 18:16:33.162: INFO: Number of nodes with available pods: 0
Jun 23 18:16:33.162: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 18:16:34.160: INFO: Number of nodes with available pods: 3
Jun 23 18:16:34.160: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 23 18:16:34.180: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:34.180: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:34.180: INFO: Wrong image for pod: daemon-set-zz276. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:35.197: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:35.198: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:35.198: INFO: Wrong image for pod: daemon-set-zz276. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:36.195: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:36.195: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:36.195: INFO: Wrong image for pod: daemon-set-zz276. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:37.207: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:37.207: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:37.207: INFO: Wrong image for pod: daemon-set-zz276. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:37.207: INFO: Pod daemon-set-zz276 is not available
Jun 23 18:16:38.194: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:38.194: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:38.194: INFO: Wrong image for pod: daemon-set-zz276. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:38.194: INFO: Pod daemon-set-zz276 is not available
Jun 23 18:16:39.194: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:39.194: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:39.194: INFO: Wrong image for pod: daemon-set-zz276. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:39.194: INFO: Pod daemon-set-zz276 is not available
Jun 23 18:16:40.196: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:40.196: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:40.196: INFO: Wrong image for pod: daemon-set-zz276. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:40.196: INFO: Pod daemon-set-zz276 is not available
Jun 23 18:16:41.197: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:41.197: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:41.197: INFO: Wrong image for pod: daemon-set-zz276. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:41.197: INFO: Pod daemon-set-zz276 is not available
Jun 23 18:16:42.195: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:42.195: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:42.195: INFO: Pod daemon-set-qspvm is not available
Jun 23 18:16:43.196: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:43.196: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:43.196: INFO: Pod daemon-set-qspvm is not available
Jun 23 18:16:44.202: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:44.202: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:44.202: INFO: Pod daemon-set-qspvm is not available
Jun 23 18:16:45.196: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:45.196: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:45.197: INFO: Pod daemon-set-qspvm is not available
Jun 23 18:16:46.199: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:46.199: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:47.197: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:47.197: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:47.197: INFO: Pod daemon-set-dqp8v is not available
Jun 23 18:16:48.195: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:48.195: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:48.195: INFO: Pod daemon-set-dqp8v is not available
Jun 23 18:16:49.195: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:49.195: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:49.195: INFO: Pod daemon-set-dqp8v is not available
Jun 23 18:16:50.196: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:50.196: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:50.196: INFO: Pod daemon-set-dqp8v is not available
Jun 23 18:16:51.197: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:51.197: INFO: Wrong image for pod: daemon-set-dqp8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:51.197: INFO: Pod daemon-set-dqp8v is not available
Jun 23 18:16:52.196: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:52.197: INFO: Pod daemon-set-kk9sx is not available
Jun 23 18:16:53.194: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:53.194: INFO: Pod daemon-set-kk9sx is not available
Jun 23 18:16:54.192: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:55.200: INFO: Wrong image for pod: daemon-set-2rn9b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 23 18:16:55.201: INFO: Pod daemon-set-2rn9b is not available
Jun 23 18:16:56.196: INFO: Pod daemon-set-j4fv5 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 23 18:16:56.232: INFO: Number of nodes with available pods: 2
Jun 23 18:16:56.232: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 18:16:57.239: INFO: Number of nodes with available pods: 2
Jun 23 18:16:57.239: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 18:16:58.247: INFO: Number of nodes with available pods: 2
Jun 23 18:16:58.247: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 18:16:59.246: INFO: Number of nodes with available pods: 3
Jun 23 18:16:59.246: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5137, will wait for the garbage collector to delete the pods
Jun 23 18:16:59.314: INFO: Deleting DaemonSet.extensions daemon-set took: 9.78716ms
Jun 23 18:16:59.614: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.390193ms
Jun 23 18:17:12.026: INFO: Number of nodes with available pods: 0
Jun 23 18:17:12.027: INFO: Number of running nodes: 0, number of available pods: 0
Jun 23 18:17:12.032: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5137/daemonsets","resourceVersion":"10017"},"items":null}

Jun 23 18:17:12.037: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5137/pods","resourceVersion":"10017"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:17:12.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5137" for this suite.
Jun 23 18:17:18.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:17:18.152: INFO: namespace daemonsets-5137 deletion completed in 6.098793941s

• [SLOW TEST:46.037 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:17:18.152: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jun 23 18:17:18.173: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Jun 23 18:17:18.746: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 23 18:17:20.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:22.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:24.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:26.825: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:28.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:30.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:32.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:34.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:36.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:38.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:40.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:42.831: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:44.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:46.829: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:48.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:50.856: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:52.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:54.825: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:56.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:17:58.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:18:00.825: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:18:02.829: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:18:04.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:18:06.822: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:18:08.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:18:10.832: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:18:12.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696910639, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:18:23.296: INFO: Waited 8.458660277s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:18:23.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8249" for this suite.
Jun 23 18:18:29.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:18:29.805: INFO: namespace aggregator-8249 deletion completed in 6.155753246s

• [SLOW TEST:71.653 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:18:29.805: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Jun 23 18:18:29.830: INFO: Waiting up to 5m0s for pod "client-containers-53495feb-6fc8-4fb5-8a40-d0899f1f94ee" in namespace "containers-9567" to be "success or failure"
Jun 23 18:18:29.833: INFO: Pod "client-containers-53495feb-6fc8-4fb5-8a40-d0899f1f94ee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.361151ms
Jun 23 18:18:31.842: INFO: Pod "client-containers-53495feb-6fc8-4fb5-8a40-d0899f1f94ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011574662s
STEP: Saw pod success
Jun 23 18:18:31.842: INFO: Pod "client-containers-53495feb-6fc8-4fb5-8a40-d0899f1f94ee" satisfied condition "success or failure"
Jun 23 18:18:31.846: INFO: Trying to get logs from node worker-1 pod client-containers-53495feb-6fc8-4fb5-8a40-d0899f1f94ee container test-container: <nil>
STEP: delete the pod
Jun 23 18:18:31.867: INFO: Waiting for pod client-containers-53495feb-6fc8-4fb5-8a40-d0899f1f94ee to disappear
Jun 23 18:18:31.870: INFO: Pod client-containers-53495feb-6fc8-4fb5-8a40-d0899f1f94ee no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:18:31.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9567" for this suite.
Jun 23 18:18:37.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:18:37.990: INFO: namespace containers-9567 deletion completed in 6.117303858s

• [SLOW TEST:8.185 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:18:37.990: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 18:18:38.010: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:18:40.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1980" for this suite.
Jun 23 18:19:18.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:19:18.174: INFO: namespace pods-1980 deletion completed in 38.100972292s

• [SLOW TEST:40.184 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:19:18.177: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8542
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 23 18:19:18.257: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 23 18:19:44.421: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.38.0.2 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8542 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 18:19:44.421: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 18:19:45.602: INFO: Found all expected endpoints: [netserver-0]
Jun 23 18:19:45.604: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.40.0.2 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8542 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 18:19:45.604: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 18:19:46.685: INFO: Found all expected endpoints: [netserver-1]
Jun 23 18:19:46.687: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.32.0.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8542 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 18:19:46.687: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 18:19:47.791: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:19:47.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8542" for this suite.
Jun 23 18:20:09.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:20:09.889: INFO: namespace pod-network-test-8542 deletion completed in 22.095415299s

• [SLOW TEST:51.713 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:20:09.891: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 23 18:20:11.931: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:20:11.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8543" for this suite.
Jun 23 18:20:17.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:20:18.013: INFO: namespace container-runtime-8543 deletion completed in 6.067797234s

• [SLOW TEST:8.122 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:20:18.016: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-71d50e15-9f73-4371-91a0-d9382f9107c5 in namespace container-probe-8561
Jun 23 18:20:22.049: INFO: Started pod busybox-71d50e15-9f73-4371-91a0-d9382f9107c5 in namespace container-probe-8561
STEP: checking the pod's current state and verifying that restartCount is present
Jun 23 18:20:22.056: INFO: Initial restart count of pod busybox-71d50e15-9f73-4371-91a0-d9382f9107c5 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:24:22.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8561" for this suite.
Jun 23 18:24:29.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:24:29.065: INFO: namespace container-probe-8561 deletion completed in 6.068246914s

• [SLOW TEST:251.049 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:24:29.065: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jun 23 18:24:29.088: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 23 18:24:29.091: INFO: Waiting for terminating namespaces to be deleted...
Jun 23 18:24:29.093: INFO: 
Logging pods the kubelet thinks is on node controlplane-1 before test
Jun 23 18:24:29.097: INFO: kube-controller-manager-controlplane-1 from kube-system started at 2019-06-23 17:35:42 +0000 UTC (1 container statuses recorded)
Jun 23 18:24:29.097: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 23 18:24:29.097: INFO: kube-proxy-g5w8t from kube-system started at 2019-06-23 17:36:08 +0000 UTC (1 container statuses recorded)
Jun 23 18:24:29.097: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 23 18:24:29.097: INFO: coredns-5c98db65d4-x58jr from kube-system started at 2019-06-23 17:36:39 +0000 UTC (1 container statuses recorded)
Jun 23 18:24:29.097: INFO: 	Container coredns ready: true, restart count 0
Jun 23 18:24:29.097: INFO: kube-apiserver-controlplane-1 from kube-system started at 2019-06-23 17:35:42 +0000 UTC (1 container statuses recorded)
Jun 23 18:24:29.097: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 23 18:24:29.097: INFO: weave-net-2ljj6 from kube-system started at 2019-06-23 17:36:08 +0000 UTC (2 container statuses recorded)
Jun 23 18:24:29.097: INFO: 	Container weave ready: true, restart count 0
Jun 23 18:24:29.097: INFO: 	Container weave-npc ready: true, restart count 0
Jun 23 18:24:29.097: INFO: etcd-controlplane-1 from kube-system started at 2019-06-23 17:35:42 +0000 UTC (1 container statuses recorded)
Jun 23 18:24:29.097: INFO: 	Container etcd ready: true, restart count 0
Jun 23 18:24:29.097: INFO: coredns-5c98db65d4-2c474 from kube-system started at 2019-06-23 17:36:39 +0000 UTC (1 container statuses recorded)
Jun 23 18:24:29.097: INFO: 	Container coredns ready: true, restart count 0
Jun 23 18:24:29.097: INFO: kube-scheduler-controlplane-1 from kube-system started at 2019-06-23 17:35:42 +0000 UTC (1 container statuses recorded)
Jun 23 18:24:29.097: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 23 18:24:29.097: INFO: sonobuoy-systemd-logs-daemon-set-97428795f2c54d48-9t4bv from heptio-sonobuoy started at 2019-06-23 17:37:16 +0000 UTC (2 container statuses recorded)
Jun 23 18:24:29.097: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 23 18:24:29.097: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 23 18:24:29.097: INFO: 
Logging pods the kubelet thinks is on node worker-1 before test
Jun 23 18:24:29.101: INFO: kube-proxy-vmf86 from kube-system started at 2019-06-23 17:36:12 +0000 UTC (1 container statuses recorded)
Jun 23 18:24:29.101: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 23 18:24:29.101: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-23 17:37:03 +0000 UTC (1 container statuses recorded)
Jun 23 18:24:29.101: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 23 18:24:29.101: INFO: weave-net-rh6sc from kube-system started at 2019-06-23 17:36:12 +0000 UTC (2 container statuses recorded)
Jun 23 18:24:29.101: INFO: 	Container weave ready: true, restart count 0
Jun 23 18:24:29.101: INFO: 	Container weave-npc ready: true, restart count 0
Jun 23 18:24:29.101: INFO: sonobuoy-systemd-logs-daemon-set-97428795f2c54d48-twnth from heptio-sonobuoy started at 2019-06-23 17:37:16 +0000 UTC (2 container statuses recorded)
Jun 23 18:24:29.101: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 23 18:24:29.101: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 23 18:24:29.101: INFO: 
Logging pods the kubelet thinks is on node worker-2 before test
Jun 23 18:24:29.105: INFO: kube-proxy-djmbj from kube-system started at 2019-06-23 17:36:11 +0000 UTC (1 container statuses recorded)
Jun 23 18:24:29.105: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 23 18:24:29.105: INFO: sonobuoy-e2e-job-25634377b43945a3 from heptio-sonobuoy started at 2019-06-23 17:37:15 +0000 UTC (2 container statuses recorded)
Jun 23 18:24:29.105: INFO: 	Container e2e ready: true, restart count 0
Jun 23 18:24:29.105: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 23 18:24:29.105: INFO: weave-net-6pxvd from kube-system started at 2019-06-23 17:36:11 +0000 UTC (2 container statuses recorded)
Jun 23 18:24:29.105: INFO: 	Container weave ready: true, restart count 0
Jun 23 18:24:29.105: INFO: 	Container weave-npc ready: true, restart count 0
Jun 23 18:24:29.105: INFO: sonobuoy-systemd-logs-daemon-set-97428795f2c54d48-v5ldn from heptio-sonobuoy started at 2019-06-23 17:37:15 +0000 UTC (2 container statuses recorded)
Jun 23 18:24:29.106: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 23 18:24:29.106: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15aae70e4b8d8808], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:24:30.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9090" for this suite.
Jun 23 18:24:36.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:24:36.205: INFO: namespace sched-pred-9090 deletion completed in 6.07586602s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.140 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:24:36.205: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 23 18:24:36.231: INFO: Waiting up to 5m0s for pod "pod-7e5e1f7f-739b-45c0-8fd1-98274aa00872" in namespace "emptydir-9895" to be "success or failure"
Jun 23 18:24:36.235: INFO: Pod "pod-7e5e1f7f-739b-45c0-8fd1-98274aa00872": Phase="Pending", Reason="", readiness=false. Elapsed: 3.849736ms
Jun 23 18:24:38.241: INFO: Pod "pod-7e5e1f7f-739b-45c0-8fd1-98274aa00872": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009715841s
STEP: Saw pod success
Jun 23 18:24:38.241: INFO: Pod "pod-7e5e1f7f-739b-45c0-8fd1-98274aa00872" satisfied condition "success or failure"
Jun 23 18:24:38.244: INFO: Trying to get logs from node worker-1 pod pod-7e5e1f7f-739b-45c0-8fd1-98274aa00872 container test-container: <nil>
STEP: delete the pod
Jun 23 18:24:38.261: INFO: Waiting for pod pod-7e5e1f7f-739b-45c0-8fd1-98274aa00872 to disappear
Jun 23 18:24:38.263: INFO: Pod pod-7e5e1f7f-739b-45c0-8fd1-98274aa00872 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:24:38.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9895" for this suite.
Jun 23 18:24:44.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:24:44.342: INFO: namespace emptydir-9895 deletion completed in 6.07566972s

• [SLOW TEST:8.137 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:24:44.342: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-0b309e27-706c-48cc-8604-b1bda1c4eda9
STEP: Creating a pod to test consume secrets
Jun 23 18:24:44.376: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-23466149-3793-4620-9888-12626b0fd3ab" in namespace "projected-6393" to be "success or failure"
Jun 23 18:24:44.381: INFO: Pod "pod-projected-secrets-23466149-3793-4620-9888-12626b0fd3ab": Phase="Pending", Reason="", readiness=false. Elapsed: 5.182014ms
Jun 23 18:24:46.388: INFO: Pod "pod-projected-secrets-23466149-3793-4620-9888-12626b0fd3ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011494847s
Jun 23 18:24:48.394: INFO: Pod "pod-projected-secrets-23466149-3793-4620-9888-12626b0fd3ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017638533s
STEP: Saw pod success
Jun 23 18:24:48.394: INFO: Pod "pod-projected-secrets-23466149-3793-4620-9888-12626b0fd3ab" satisfied condition "success or failure"
Jun 23 18:24:48.400: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-23466149-3793-4620-9888-12626b0fd3ab container secret-volume-test: <nil>
STEP: delete the pod
Jun 23 18:24:48.442: INFO: Waiting for pod pod-projected-secrets-23466149-3793-4620-9888-12626b0fd3ab to disappear
Jun 23 18:24:48.444: INFO: Pod pod-projected-secrets-23466149-3793-4620-9888-12626b0fd3ab no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:24:48.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6393" for this suite.
Jun 23 18:24:54.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:24:54.542: INFO: namespace projected-6393 deletion completed in 6.093161286s

• [SLOW TEST:10.200 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:24:54.542: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-5db732a0-490b-4df9-91eb-e701633849bd
STEP: Creating a pod to test consume secrets
Jun 23 18:24:54.593: INFO: Waiting up to 5m0s for pod "pod-secrets-bcfe9af3-179d-4b2e-a52d-d7ca4b39b4ba" in namespace "secrets-7557" to be "success or failure"
Jun 23 18:24:54.595: INFO: Pod "pod-secrets-bcfe9af3-179d-4b2e-a52d-d7ca4b39b4ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1.907556ms
Jun 23 18:24:56.602: INFO: Pod "pod-secrets-bcfe9af3-179d-4b2e-a52d-d7ca4b39b4ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008360919s
Jun 23 18:24:58.613: INFO: Pod "pod-secrets-bcfe9af3-179d-4b2e-a52d-d7ca4b39b4ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019916318s
STEP: Saw pod success
Jun 23 18:24:58.614: INFO: Pod "pod-secrets-bcfe9af3-179d-4b2e-a52d-d7ca4b39b4ba" satisfied condition "success or failure"
Jun 23 18:24:58.620: INFO: Trying to get logs from node worker-1 pod pod-secrets-bcfe9af3-179d-4b2e-a52d-d7ca4b39b4ba container secret-volume-test: <nil>
STEP: delete the pod
Jun 23 18:24:58.662: INFO: Waiting for pod pod-secrets-bcfe9af3-179d-4b2e-a52d-d7ca4b39b4ba to disappear
Jun 23 18:24:58.671: INFO: Pod pod-secrets-bcfe9af3-179d-4b2e-a52d-d7ca4b39b4ba no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:24:58.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7557" for this suite.
Jun 23 18:25:04.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:25:04.777: INFO: namespace secrets-7557 deletion completed in 6.100309623s
STEP: Destroying namespace "secret-namespace-1187" for this suite.
Jun 23 18:25:10.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:25:10.904: INFO: namespace secret-namespace-1187 deletion completed in 6.127488903s

• [SLOW TEST:16.362 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:25:10.905: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-8e792095-6002-4a40-8a29-8224e234dd18
STEP: Creating a pod to test consume secrets
Jun 23 18:25:10.931: INFO: Waiting up to 5m0s for pod "pod-secrets-623e7f3e-c211-4b72-be6f-1e06fbf13d3c" in namespace "secrets-4413" to be "success or failure"
Jun 23 18:25:10.937: INFO: Pod "pod-secrets-623e7f3e-c211-4b72-be6f-1e06fbf13d3c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.405618ms
Jun 23 18:25:12.945: INFO: Pod "pod-secrets-623e7f3e-c211-4b72-be6f-1e06fbf13d3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013306965s
Jun 23 18:25:14.952: INFO: Pod "pod-secrets-623e7f3e-c211-4b72-be6f-1e06fbf13d3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020641199s
STEP: Saw pod success
Jun 23 18:25:14.952: INFO: Pod "pod-secrets-623e7f3e-c211-4b72-be6f-1e06fbf13d3c" satisfied condition "success or failure"
Jun 23 18:25:14.957: INFO: Trying to get logs from node worker-1 pod pod-secrets-623e7f3e-c211-4b72-be6f-1e06fbf13d3c container secret-volume-test: <nil>
STEP: delete the pod
Jun 23 18:25:14.994: INFO: Waiting for pod pod-secrets-623e7f3e-c211-4b72-be6f-1e06fbf13d3c to disappear
Jun 23 18:25:15.001: INFO: Pod pod-secrets-623e7f3e-c211-4b72-be6f-1e06fbf13d3c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:25:15.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4413" for this suite.
Jun 23 18:25:21.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:25:21.101: INFO: namespace secrets-4413 deletion completed in 6.095749666s

• [SLOW TEST:10.196 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:25:21.102: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:25:26.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3808" for this suite.
Jun 23 18:25:32.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:25:32.802: INFO: namespace watch-3808 deletion completed in 6.191307206s

• [SLOW TEST:11.700 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:25:32.802: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6675
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 23 18:25:32.825: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 23 18:25:58.970: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.0.3:8080/dial?request=hostName&protocol=http&host=10.40.0.2&port=8080&tries=1'] Namespace:pod-network-test-6675 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 18:25:58.970: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 18:25:59.075: INFO: Waiting for endpoints: map[]
Jun 23 18:25:59.077: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.0.3:8080/dial?request=hostName&protocol=http&host=10.32.0.4&port=8080&tries=1'] Namespace:pod-network-test-6675 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 18:25:59.077: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 18:25:59.162: INFO: Waiting for endpoints: map[]
Jun 23 18:25:59.164: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.0.3:8080/dial?request=hostName&protocol=http&host=10.38.0.2&port=8080&tries=1'] Namespace:pod-network-test-6675 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 18:25:59.164: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 18:25:59.244: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:25:59.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6675" for this suite.
Jun 23 18:26:13.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:26:13.306: INFO: namespace pod-network-test-6675 deletion completed in 14.060084781s

• [SLOW TEST:40.504 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:26:13.307: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 18:26:13.333: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1ad6f3a-4144-4385-9349-400429a0291b" in namespace "downward-api-7088" to be "success or failure"
Jun 23 18:26:13.341: INFO: Pod "downwardapi-volume-f1ad6f3a-4144-4385-9349-400429a0291b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.910539ms
Jun 23 18:26:15.344: INFO: Pod "downwardapi-volume-f1ad6f3a-4144-4385-9349-400429a0291b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010939065s
STEP: Saw pod success
Jun 23 18:26:15.344: INFO: Pod "downwardapi-volume-f1ad6f3a-4144-4385-9349-400429a0291b" satisfied condition "success or failure"
Jun 23 18:26:15.345: INFO: Trying to get logs from node worker-2 pod downwardapi-volume-f1ad6f3a-4144-4385-9349-400429a0291b container client-container: <nil>
STEP: delete the pod
Jun 23 18:26:15.359: INFO: Waiting for pod downwardapi-volume-f1ad6f3a-4144-4385-9349-400429a0291b to disappear
Jun 23 18:26:15.361: INFO: Pod downwardapi-volume-f1ad6f3a-4144-4385-9349-400429a0291b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:26:15.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7088" for this suite.
Jun 23 18:26:21.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:26:21.430: INFO: namespace downward-api-7088 deletion completed in 6.065835379s

• [SLOW TEST:8.123 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:26:21.430: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 23 18:26:23.999: INFO: Successfully updated pod "labelsupdatec327fda9-64ad-4c9d-8065-ea47352252f4"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:26:26.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4074" for this suite.
Jun 23 18:26:48.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:26:48.147: INFO: namespace downward-api-4074 deletion completed in 22.107091319s

• [SLOW TEST:26.717 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:26:48.149: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 23 18:26:52.722: INFO: Successfully updated pod "labelsupdatee8ae4d04-77d1-4058-aa5d-9d7be7880c97"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:26:54.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6240" for this suite.
Jun 23 18:27:16.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:27:16.849: INFO: namespace projected-6240 deletion completed in 22.094439034s

• [SLOW TEST:28.701 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:27:16.853: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 18:27:20.934: INFO: Waiting up to 5m0s for pod "client-envvars-a3265090-bb6a-4775-abcc-6d8af4f53bd1" in namespace "pods-9374" to be "success or failure"
Jun 23 18:27:20.948: INFO: Pod "client-envvars-a3265090-bb6a-4775-abcc-6d8af4f53bd1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.397069ms
Jun 23 18:27:22.954: INFO: Pod "client-envvars-a3265090-bb6a-4775-abcc-6d8af4f53bd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02013938s
Jun 23 18:27:24.961: INFO: Pod "client-envvars-a3265090-bb6a-4775-abcc-6d8af4f53bd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026842337s
STEP: Saw pod success
Jun 23 18:27:24.961: INFO: Pod "client-envvars-a3265090-bb6a-4775-abcc-6d8af4f53bd1" satisfied condition "success or failure"
Jun 23 18:27:24.967: INFO: Trying to get logs from node worker-2 pod client-envvars-a3265090-bb6a-4775-abcc-6d8af4f53bd1 container env3cont: <nil>
STEP: delete the pod
Jun 23 18:27:24.998: INFO: Waiting for pod client-envvars-a3265090-bb6a-4775-abcc-6d8af4f53bd1 to disappear
Jun 23 18:27:25.001: INFO: Pod client-envvars-a3265090-bb6a-4775-abcc-6d8af4f53bd1 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:27:25.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9374" for this suite.
Jun 23 18:28:15.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:28:15.073: INFO: namespace pods-9374 deletion completed in 50.067193242s

• [SLOW TEST:58.220 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:28:15.073: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-75473b90-4b15-4992-9e92-2d0658c8b0c1
STEP: Creating secret with name secret-projected-all-test-volume-1ea0e118-5e46-40a2-ac7f-6e9f60699715
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 23 18:28:15.108: INFO: Waiting up to 5m0s for pod "projected-volume-20ebdb6a-83a4-41a9-9f7e-389f3412042f" in namespace "projected-5903" to be "success or failure"
Jun 23 18:28:15.112: INFO: Pod "projected-volume-20ebdb6a-83a4-41a9-9f7e-389f3412042f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.361507ms
Jun 23 18:28:17.116: INFO: Pod "projected-volume-20ebdb6a-83a4-41a9-9f7e-389f3412042f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00732437s
STEP: Saw pod success
Jun 23 18:28:17.116: INFO: Pod "projected-volume-20ebdb6a-83a4-41a9-9f7e-389f3412042f" satisfied condition "success or failure"
Jun 23 18:28:17.117: INFO: Trying to get logs from node worker-1 pod projected-volume-20ebdb6a-83a4-41a9-9f7e-389f3412042f container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 23 18:28:17.134: INFO: Waiting for pod projected-volume-20ebdb6a-83a4-41a9-9f7e-389f3412042f to disappear
Jun 23 18:28:17.139: INFO: Pod projected-volume-20ebdb6a-83a4-41a9-9f7e-389f3412042f no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:28:17.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5903" for this suite.
Jun 23 18:28:23.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:28:23.209: INFO: namespace projected-5903 deletion completed in 6.064581384s

• [SLOW TEST:8.136 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:28:23.209: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 23 18:28:23.300: INFO: PodSpec: initContainers in spec.initContainers
Jun 23 18:29:05.074: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-0c7dfd92-9d59-4d7a-b7b3-86cae552f13f", GenerateName:"", Namespace:"init-container-5039", SelfLink:"/api/v1/namespaces/init-container-5039/pods/pod-init-0c7dfd92-9d59-4d7a-b7b3-86cae552f13f", UID:"e04c290e-d5e8-4047-af02-4713d394c4d2", ResourceVersion:"11860", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63696911303, loc:(*time.Location)(0x80bb5c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"300283407"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-8z5cx", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002760cc0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8z5cx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8z5cx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8z5cx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002c4f988), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001cea5a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c4fa10)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c4fa30)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002c4fa38), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002c4fa3c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696911303, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696911303, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696911303, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696911303, loc:(*time.Location)(0x80bb5c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.5.101", PodIP:"10.38.0.2", StartTime:(*v1.Time)(0xc001deebc0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002d2d7a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002d2d810)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://b8c8e53b80aca4d86a450eceddde01b9d8708231ed0285d56de6658c1ace2976"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001deec00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001deebe0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:29:05.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5039" for this suite.
Jun 23 18:29:27.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:29:27.182: INFO: namespace init-container-5039 deletion completed in 22.101622179s

• [SLOW TEST:63.973 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:29:27.183: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 18:29:27.216: INFO: Waiting up to 5m0s for pod "downwardapi-volume-185963b3-2f13-4b28-afb4-02e85db9ec4f" in namespace "projected-3692" to be "success or failure"
Jun 23 18:29:27.229: INFO: Pod "downwardapi-volume-185963b3-2f13-4b28-afb4-02e85db9ec4f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.892605ms
Jun 23 18:29:29.235: INFO: Pod "downwardapi-volume-185963b3-2f13-4b28-afb4-02e85db9ec4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018777791s
Jun 23 18:29:31.238: INFO: Pod "downwardapi-volume-185963b3-2f13-4b28-afb4-02e85db9ec4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022213626s
STEP: Saw pod success
Jun 23 18:29:31.238: INFO: Pod "downwardapi-volume-185963b3-2f13-4b28-afb4-02e85db9ec4f" satisfied condition "success or failure"
Jun 23 18:29:31.240: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-185963b3-2f13-4b28-afb4-02e85db9ec4f container client-container: <nil>
STEP: delete the pod
Jun 23 18:29:31.254: INFO: Waiting for pod downwardapi-volume-185963b3-2f13-4b28-afb4-02e85db9ec4f to disappear
Jun 23 18:29:31.255: INFO: Pod downwardapi-volume-185963b3-2f13-4b28-afb4-02e85db9ec4f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:29:31.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3692" for this suite.
Jun 23 18:29:37.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:29:37.335: INFO: namespace projected-3692 deletion completed in 6.077366634s

• [SLOW TEST:10.153 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:29:37.337: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 23 18:32:25.462: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 23 18:32:25.465: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 23 18:32:27.469: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 23 18:32:27.476: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 23 18:32:29.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 23 18:32:29.472: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 23 18:32:31.465: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 23 18:32:31.468: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 23 18:32:33.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 23 18:32:33.473: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 23 18:32:35.467: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 23 18:32:35.474: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 23 18:32:37.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 23 18:32:37.477: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 23 18:32:39.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 23 18:32:39.472: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 23 18:32:41.476: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 23 18:32:41.482: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 23 18:32:43.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 23 18:32:43.473: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 23 18:32:45.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 23 18:32:45.474: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 23 18:32:47.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 23 18:32:47.467: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 23 18:32:49.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 23 18:32:49.473: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 23 18:32:51.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 23 18:32:51.472: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 23 18:32:53.466: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 23 18:32:53.472: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:32:53.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3516" for this suite.
Jun 23 18:33:15.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:33:15.596: INFO: namespace container-lifecycle-hook-3516 deletion completed in 22.117015792s

• [SLOW TEST:218.259 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:33:15.597: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 23 18:33:23.672: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 23 18:33:23.676: INFO: Pod pod-with-prestop-http-hook still exists
Jun 23 18:33:25.677: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 23 18:33:25.684: INFO: Pod pod-with-prestop-http-hook still exists
Jun 23 18:33:27.677: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 23 18:33:27.683: INFO: Pod pod-with-prestop-http-hook still exists
Jun 23 18:33:29.677: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 23 18:33:29.685: INFO: Pod pod-with-prestop-http-hook still exists
Jun 23 18:33:31.678: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 23 18:33:31.692: INFO: Pod pod-with-prestop-http-hook still exists
Jun 23 18:33:33.676: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 23 18:33:33.683: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:33:33.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9487" for this suite.
Jun 23 18:33:55.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:33:55.778: INFO: namespace container-lifecycle-hook-9487 deletion completed in 22.072889181s

• [SLOW TEST:40.181 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:33:55.779: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-2772
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2772 to expose endpoints map[]
Jun 23 18:33:55.812: INFO: successfully validated that service endpoint-test2 in namespace services-2772 exposes endpoints map[] (3.048001ms elapsed)
STEP: Creating pod pod1 in namespace services-2772
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2772 to expose endpoints map[pod1:[80]]
Jun 23 18:33:57.839: INFO: successfully validated that service endpoint-test2 in namespace services-2772 exposes endpoints map[pod1:[80]] (2.018749517s elapsed)
STEP: Creating pod pod2 in namespace services-2772
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2772 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 23 18:34:00.911: INFO: successfully validated that service endpoint-test2 in namespace services-2772 exposes endpoints map[pod1:[80] pod2:[80]] (3.066846741s elapsed)
STEP: Deleting pod pod1 in namespace services-2772
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2772 to expose endpoints map[pod2:[80]]
Jun 23 18:34:00.945: INFO: successfully validated that service endpoint-test2 in namespace services-2772 exposes endpoints map[pod2:[80]] (23.960153ms elapsed)
STEP: Deleting pod pod2 in namespace services-2772
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2772 to expose endpoints map[]
Jun 23 18:34:00.958: INFO: successfully validated that service endpoint-test2 in namespace services-2772 exposes endpoints map[] (3.192934ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:34:00.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2772" for this suite.
Jun 23 18:34:22.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:34:23.049: INFO: namespace services-2772 deletion completed in 22.071046591s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:27.270 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:34:23.049: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 23 18:34:23.161: INFO: Waiting up to 5m0s for pod "pod-134cf7b2-f7d4-4c3d-8000-3a1d2e96741c" in namespace "emptydir-1109" to be "success or failure"
Jun 23 18:34:23.169: INFO: Pod "pod-134cf7b2-f7d4-4c3d-8000-3a1d2e96741c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.528652ms
Jun 23 18:34:25.171: INFO: Pod "pod-134cf7b2-f7d4-4c3d-8000-3a1d2e96741c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010226848s
STEP: Saw pod success
Jun 23 18:34:25.171: INFO: Pod "pod-134cf7b2-f7d4-4c3d-8000-3a1d2e96741c" satisfied condition "success or failure"
Jun 23 18:34:25.173: INFO: Trying to get logs from node worker-1 pod pod-134cf7b2-f7d4-4c3d-8000-3a1d2e96741c container test-container: <nil>
STEP: delete the pod
Jun 23 18:34:25.189: INFO: Waiting for pod pod-134cf7b2-f7d4-4c3d-8000-3a1d2e96741c to disappear
Jun 23 18:34:25.191: INFO: Pod pod-134cf7b2-f7d4-4c3d-8000-3a1d2e96741c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:34:25.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1109" for this suite.
Jun 23 18:34:31.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:34:31.290: INFO: namespace emptydir-1109 deletion completed in 6.096861579s

• [SLOW TEST:8.241 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:34:31.291: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 23 18:34:34.362: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:34:34.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4802" for this suite.
Jun 23 18:34:40.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:34:40.470: INFO: namespace container-runtime-4802 deletion completed in 6.066069567s

• [SLOW TEST:9.179 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:34:40.471: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:34:40.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-87" for this suite.
Jun 23 18:34:46.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:34:46.659: INFO: namespace services-87 deletion completed in 6.093755804s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.188 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:34:46.659: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Jun 23 18:34:46.684: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1423" to be "success or failure"
Jun 23 18:34:46.689: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.445715ms
Jun 23 18:34:48.691: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006430544s
STEP: Saw pod success
Jun 23 18:34:48.691: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jun 23 18:34:48.692: INFO: Trying to get logs from node worker-2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jun 23 18:34:48.707: INFO: Waiting for pod pod-host-path-test to disappear
Jun 23 18:34:48.708: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:34:48.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1423" for this suite.
Jun 23 18:34:54.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:34:54.796: INFO: namespace hostpath-1423 deletion completed in 6.086168502s

• [SLOW TEST:8.137 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:34:54.802: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 18:34:54.827: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0924d03-bb89-4f08-97f4-0821ca6f2f7b" in namespace "downward-api-4449" to be "success or failure"
Jun 23 18:34:54.834: INFO: Pod "downwardapi-volume-b0924d03-bb89-4f08-97f4-0821ca6f2f7b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.892775ms
Jun 23 18:34:56.841: INFO: Pod "downwardapi-volume-b0924d03-bb89-4f08-97f4-0821ca6f2f7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013752064s
STEP: Saw pod success
Jun 23 18:34:56.841: INFO: Pod "downwardapi-volume-b0924d03-bb89-4f08-97f4-0821ca6f2f7b" satisfied condition "success or failure"
Jun 23 18:34:56.844: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-b0924d03-bb89-4f08-97f4-0821ca6f2f7b container client-container: <nil>
STEP: delete the pod
Jun 23 18:34:56.862: INFO: Waiting for pod downwardapi-volume-b0924d03-bb89-4f08-97f4-0821ca6f2f7b to disappear
Jun 23 18:34:56.864: INFO: Pod downwardapi-volume-b0924d03-bb89-4f08-97f4-0821ca6f2f7b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:34:56.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4449" for this suite.
Jun 23 18:35:02.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:35:02.920: INFO: namespace downward-api-4449 deletion completed in 6.054134682s

• [SLOW TEST:8.119 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:35:02.922: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 23 18:35:02.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-9365'
Jun 23 18:35:03.548: INFO: stderr: ""
Jun 23 18:35:03.548: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jun 23 18:35:08.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pod e2e-test-nginx-pod --namespace=kubectl-9365 -o json'
Jun 23 18:35:08.659: INFO: stderr: ""
Jun 23 18:35:08.659: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-06-23T18:35:03Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-9365\",\n        \"resourceVersion\": \"12681\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9365/pods/e2e-test-nginx-pod\",\n        \"uid\": \"8c056e82-ce02-414d-89bf-49142eb6dab9\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-cfqlw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-cfqlw\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-cfqlw\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-23T18:35:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-23T18:35:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-23T18:35:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-23T18:35:03Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://596d9c6d7ae7d857fffe5cd50ce96eb1aa5e40309ab4c5858191820046aeb1a7\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-06-23T18:35:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.5.101\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.38.0.2\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-06-23T18:35:03Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 23 18:35:08.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 replace -f - --namespace=kubectl-9365'
Jun 23 18:35:08.817: INFO: stderr: ""
Jun 23 18:35:08.817: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Jun 23 18:35:08.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete pods e2e-test-nginx-pod --namespace=kubectl-9365'
Jun 23 18:35:10.455: INFO: stderr: ""
Jun 23 18:35:10.455: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:35:10.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9365" for this suite.
Jun 23 18:35:16.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:35:16.547: INFO: namespace kubectl-9365 deletion completed in 6.088007545s

• [SLOW TEST:13.625 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:35:16.547: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0623 18:35:56.611572      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 23 18:35:56.611: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:35:56.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7376" for this suite.
Jun 23 18:36:02.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:36:02.724: INFO: namespace gc-7376 deletion completed in 6.106511275s

• [SLOW TEST:46.177 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:36:02.724: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Jun 23 18:36:04.773: INFO: Pod pod-hostip-4e3ba234-74d9-43cd-8822-6e2579d50d9c has hostIP: 192.168.5.101
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:36:04.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7534" for this suite.
Jun 23 18:36:26.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:36:26.873: INFO: namespace pods-7534 deletion completed in 22.091757558s

• [SLOW TEST:24.149 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:36:26.874: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:36:31.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2180" for this suite.
Jun 23 18:36:53.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:36:54.009: INFO: namespace replication-controller-2180 deletion completed in 22.064535549s

• [SLOW TEST:27.136 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:36:54.010: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-25727ccc-1b07-4720-b8ec-5e2f28d1a338
STEP: Creating a pod to test consume configMaps
Jun 23 18:36:54.038: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d03c8eee-9c7b-485f-aa58-b88512889175" in namespace "projected-7563" to be "success or failure"
Jun 23 18:36:54.041: INFO: Pod "pod-projected-configmaps-d03c8eee-9c7b-485f-aa58-b88512889175": Phase="Pending", Reason="", readiness=false. Elapsed: 3.595078ms
Jun 23 18:36:56.048: INFO: Pod "pod-projected-configmaps-d03c8eee-9c7b-485f-aa58-b88512889175": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010469174s
Jun 23 18:36:58.054: INFO: Pod "pod-projected-configmaps-d03c8eee-9c7b-485f-aa58-b88512889175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01619248s
STEP: Saw pod success
Jun 23 18:36:58.054: INFO: Pod "pod-projected-configmaps-d03c8eee-9c7b-485f-aa58-b88512889175" satisfied condition "success or failure"
Jun 23 18:36:58.060: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-d03c8eee-9c7b-485f-aa58-b88512889175 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 23 18:36:58.088: INFO: Waiting for pod pod-projected-configmaps-d03c8eee-9c7b-485f-aa58-b88512889175 to disappear
Jun 23 18:36:58.090: INFO: Pod pod-projected-configmaps-d03c8eee-9c7b-485f-aa58-b88512889175 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:36:58.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7563" for this suite.
Jun 23 18:37:04.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:37:04.158: INFO: namespace projected-7563 deletion completed in 6.066046638s

• [SLOW TEST:10.148 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:37:04.158: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9558
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 23 18:37:04.179: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 23 18:37:26.339: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.32.0.4:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9558 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 18:37:26.339: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 18:37:26.462: INFO: Found all expected endpoints: [netserver-0]
Jun 23 18:37:26.464: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.38.0.2:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9558 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 18:37:26.464: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 18:37:26.541: INFO: Found all expected endpoints: [netserver-1]
Jun 23 18:37:26.543: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.40.0.2:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9558 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 23 18:37:26.543: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
Jun 23 18:37:26.619: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:37:26.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9558" for this suite.
Jun 23 18:37:48.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:37:48.719: INFO: namespace pod-network-test-9558 deletion completed in 22.098252269s

• [SLOW TEST:44.561 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:37:48.721: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 23 18:37:56.837: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 23 18:37:56.841: INFO: Pod pod-with-poststart-http-hook still exists
Jun 23 18:37:58.842: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 23 18:37:58.850: INFO: Pod pod-with-poststart-http-hook still exists
Jun 23 18:38:00.842: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 23 18:38:00.849: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:38:00.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9681" for this suite.
Jun 23 18:38:22.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:38:22.965: INFO: namespace container-lifecycle-hook-9681 deletion completed in 22.107709849s

• [SLOW TEST:34.244 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:38:22.966: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 18:38:23.007: INFO: (0) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 16.617662ms)
Jun 23 18:38:23.010: INFO: (1) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.536144ms)
Jun 23 18:38:23.012: INFO: (2) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.611516ms)
Jun 23 18:38:23.013: INFO: (3) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.625081ms)
Jun 23 18:38:23.015: INFO: (4) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.755504ms)
Jun 23 18:38:23.017: INFO: (5) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.879791ms)
Jun 23 18:38:23.019: INFO: (6) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.686086ms)
Jun 23 18:38:23.021: INFO: (7) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.748071ms)
Jun 23 18:38:23.023: INFO: (8) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.872098ms)
Jun 23 18:38:23.024: INFO: (9) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.67135ms)
Jun 23 18:38:23.027: INFO: (10) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.652248ms)
Jun 23 18:38:23.029: INFO: (11) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.011589ms)
Jun 23 18:38:23.031: INFO: (12) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.864715ms)
Jun 23 18:38:23.033: INFO: (13) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.775298ms)
Jun 23 18:38:23.035: INFO: (14) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.880411ms)
Jun 23 18:38:23.037: INFO: (15) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.849358ms)
Jun 23 18:38:23.039: INFO: (16) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.925594ms)
Jun 23 18:38:23.041: INFO: (17) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.913575ms)
Jun 23 18:38:23.043: INFO: (18) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.146036ms)
Jun 23 18:38:23.045: INFO: (19) /api/v1/nodes/controlplane-1:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.968695ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:38:23.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7077" for this suite.
Jun 23 18:38:29.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:38:29.139: INFO: namespace proxy-7077 deletion completed in 6.091855959s

• [SLOW TEST:6.173 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:38:29.139: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-52n2
STEP: Creating a pod to test atomic-volume-subpath
Jun 23 18:38:29.170: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-52n2" in namespace "subpath-2668" to be "success or failure"
Jun 23 18:38:29.175: INFO: Pod "pod-subpath-test-configmap-52n2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.909757ms
Jun 23 18:38:31.181: INFO: Pod "pod-subpath-test-configmap-52n2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01108133s
Jun 23 18:38:33.185: INFO: Pod "pod-subpath-test-configmap-52n2": Phase="Running", Reason="", readiness=true. Elapsed: 4.014884482s
Jun 23 18:38:35.191: INFO: Pod "pod-subpath-test-configmap-52n2": Phase="Running", Reason="", readiness=true. Elapsed: 6.020957996s
Jun 23 18:38:37.201: INFO: Pod "pod-subpath-test-configmap-52n2": Phase="Running", Reason="", readiness=true. Elapsed: 8.030949151s
Jun 23 18:38:39.206: INFO: Pod "pod-subpath-test-configmap-52n2": Phase="Running", Reason="", readiness=true. Elapsed: 10.035919503s
Jun 23 18:38:41.210: INFO: Pod "pod-subpath-test-configmap-52n2": Phase="Running", Reason="", readiness=true. Elapsed: 12.040693479s
Jun 23 18:38:43.213: INFO: Pod "pod-subpath-test-configmap-52n2": Phase="Running", Reason="", readiness=true. Elapsed: 14.043473078s
Jun 23 18:38:45.220: INFO: Pod "pod-subpath-test-configmap-52n2": Phase="Running", Reason="", readiness=true. Elapsed: 16.050714689s
Jun 23 18:38:47.235: INFO: Pod "pod-subpath-test-configmap-52n2": Phase="Running", Reason="", readiness=true. Elapsed: 18.064930156s
Jun 23 18:38:49.239: INFO: Pod "pod-subpath-test-configmap-52n2": Phase="Running", Reason="", readiness=true. Elapsed: 20.069168658s
Jun 23 18:38:51.248: INFO: Pod "pod-subpath-test-configmap-52n2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.078009735s
STEP: Saw pod success
Jun 23 18:38:51.248: INFO: Pod "pod-subpath-test-configmap-52n2" satisfied condition "success or failure"
Jun 23 18:38:51.253: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-configmap-52n2 container test-container-subpath-configmap-52n2: <nil>
STEP: delete the pod
Jun 23 18:38:51.298: INFO: Waiting for pod pod-subpath-test-configmap-52n2 to disappear
Jun 23 18:38:51.303: INFO: Pod pod-subpath-test-configmap-52n2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-52n2
Jun 23 18:38:51.303: INFO: Deleting pod "pod-subpath-test-configmap-52n2" in namespace "subpath-2668"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:38:51.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2668" for this suite.
Jun 23 18:38:57.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:38:57.376: INFO: namespace subpath-2668 deletion completed in 6.068121734s

• [SLOW TEST:28.237 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:38:57.376: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-d49e1155-e6fb-4c48-9801-10e4f6cc9365
STEP: Creating secret with name s-test-opt-upd-2601b512-5c39-4e53-b0d5-0de9c882ce72
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d49e1155-e6fb-4c48-9801-10e4f6cc9365
STEP: Updating secret s-test-opt-upd-2601b512-5c39-4e53-b0d5-0de9c882ce72
STEP: Creating secret with name s-test-opt-create-93fa305f-d2da-44b0-a664-b37d7da70984
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:39:01.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9366" for this suite.
Jun 23 18:39:23.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:39:23.690: INFO: namespace projected-9366 deletion completed in 22.097105169s

• [SLOW TEST:26.314 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:39:23.691: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-7a3a5f10-3942-4483-8869-28c2dc6da77e
STEP: Creating a pod to test consume secrets
Jun 23 18:39:23.719: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c617e7f0-f137-452d-8240-2406ec36ba2d" in namespace "projected-836" to be "success or failure"
Jun 23 18:39:23.723: INFO: Pod "pod-projected-secrets-c617e7f0-f137-452d-8240-2406ec36ba2d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.607803ms
Jun 23 18:39:25.731: INFO: Pod "pod-projected-secrets-c617e7f0-f137-452d-8240-2406ec36ba2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012440626s
Jun 23 18:39:27.741: INFO: Pod "pod-projected-secrets-c617e7f0-f137-452d-8240-2406ec36ba2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022504531s
STEP: Saw pod success
Jun 23 18:39:27.741: INFO: Pod "pod-projected-secrets-c617e7f0-f137-452d-8240-2406ec36ba2d" satisfied condition "success or failure"
Jun 23 18:39:27.747: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-c617e7f0-f137-452d-8240-2406ec36ba2d container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 23 18:39:27.789: INFO: Waiting for pod pod-projected-secrets-c617e7f0-f137-452d-8240-2406ec36ba2d to disappear
Jun 23 18:39:27.792: INFO: Pod pod-projected-secrets-c617e7f0-f137-452d-8240-2406ec36ba2d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:39:27.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-836" for this suite.
Jun 23 18:39:33.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:39:33.898: INFO: namespace projected-836 deletion completed in 6.102931563s

• [SLOW TEST:10.208 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:39:33.899: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-e9a371ff-c83e-4429-bb65-06ba8ea15919
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-e9a371ff-c83e-4429-bb65-06ba8ea15919
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:41:00.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-940" for this suite.
Jun 23 18:41:14.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:41:15.048: INFO: namespace projected-940 deletion completed in 14.120538719s

• [SLOW TEST:101.149 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:41:15.050: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-a3136a4e-7dae-473b-9608-4ab2db83dbbd
STEP: Creating a pod to test consume configMaps
Jun 23 18:41:15.078: INFO: Waiting up to 5m0s for pod "pod-configmaps-c4fe8467-02c1-41de-b533-a3d83a7618c6" in namespace "configmap-1521" to be "success or failure"
Jun 23 18:41:15.079: INFO: Pod "pod-configmaps-c4fe8467-02c1-41de-b533-a3d83a7618c6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.463274ms
Jun 23 18:41:17.086: INFO: Pod "pod-configmaps-c4fe8467-02c1-41de-b533-a3d83a7618c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007642059s
Jun 23 18:41:19.093: INFO: Pod "pod-configmaps-c4fe8467-02c1-41de-b533-a3d83a7618c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014800631s
STEP: Saw pod success
Jun 23 18:41:19.093: INFO: Pod "pod-configmaps-c4fe8467-02c1-41de-b533-a3d83a7618c6" satisfied condition "success or failure"
Jun 23 18:41:19.098: INFO: Trying to get logs from node worker-1 pod pod-configmaps-c4fe8467-02c1-41de-b533-a3d83a7618c6 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 23 18:41:19.143: INFO: Waiting for pod pod-configmaps-c4fe8467-02c1-41de-b533-a3d83a7618c6 to disappear
Jun 23 18:41:19.146: INFO: Pod pod-configmaps-c4fe8467-02c1-41de-b533-a3d83a7618c6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:41:19.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1521" for this suite.
Jun 23 18:41:25.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:41:25.220: INFO: namespace configmap-1521 deletion completed in 6.069805916s

• [SLOW TEST:10.170 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:41:25.220: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-dcb79dbe-a2cd-4346-9714-6ae321b120c0 in namespace container-probe-6583
Jun 23 18:41:29.264: INFO: Started pod busybox-dcb79dbe-a2cd-4346-9714-6ae321b120c0 in namespace container-probe-6583
STEP: checking the pod's current state and verifying that restartCount is present
Jun 23 18:41:29.269: INFO: Initial restart count of pod busybox-dcb79dbe-a2cd-4346-9714-6ae321b120c0 is 0
Jun 23 18:42:15.584: INFO: Restart count of pod container-probe-6583/busybox-dcb79dbe-a2cd-4346-9714-6ae321b120c0 is now 1 (46.314651458s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:42:15.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6583" for this suite.
Jun 23 18:42:21.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:42:21.710: INFO: namespace container-probe-6583 deletion completed in 6.08834759s

• [SLOW TEST:56.490 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:42:21.710: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-dfff84e2-4a0c-49ec-adc9-d507e0cc704a
STEP: Creating a pod to test consume configMaps
Jun 23 18:42:21.737: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-162b1b05-e352-499a-aee2-928b51c62624" in namespace "projected-2728" to be "success or failure"
Jun 23 18:42:21.740: INFO: Pod "pod-projected-configmaps-162b1b05-e352-499a-aee2-928b51c62624": Phase="Pending", Reason="", readiness=false. Elapsed: 2.503243ms
Jun 23 18:42:23.745: INFO: Pod "pod-projected-configmaps-162b1b05-e352-499a-aee2-928b51c62624": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007906304s
Jun 23 18:42:25.752: INFO: Pod "pod-projected-configmaps-162b1b05-e352-499a-aee2-928b51c62624": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015061667s
STEP: Saw pod success
Jun 23 18:42:25.753: INFO: Pod "pod-projected-configmaps-162b1b05-e352-499a-aee2-928b51c62624" satisfied condition "success or failure"
Jun 23 18:42:25.759: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-162b1b05-e352-499a-aee2-928b51c62624 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 23 18:42:25.797: INFO: Waiting for pod pod-projected-configmaps-162b1b05-e352-499a-aee2-928b51c62624 to disappear
Jun 23 18:42:25.801: INFO: Pod pod-projected-configmaps-162b1b05-e352-499a-aee2-928b51c62624 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:42:25.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2728" for this suite.
Jun 23 18:42:31.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:42:31.872: INFO: namespace projected-2728 deletion completed in 6.067659415s

• [SLOW TEST:10.161 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:42:31.872: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 18:42:31.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c50260c-a48f-4d64-b693-e9dd38fdbd25" in namespace "projected-4598" to be "success or failure"
Jun 23 18:42:31.901: INFO: Pod "downwardapi-volume-5c50260c-a48f-4d64-b693-e9dd38fdbd25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.904276ms
Jun 23 18:42:33.908: INFO: Pod "downwardapi-volume-5c50260c-a48f-4d64-b693-e9dd38fdbd25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009607654s
Jun 23 18:42:35.916: INFO: Pod "downwardapi-volume-5c50260c-a48f-4d64-b693-e9dd38fdbd25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017802355s
STEP: Saw pod success
Jun 23 18:42:35.916: INFO: Pod "downwardapi-volume-5c50260c-a48f-4d64-b693-e9dd38fdbd25" satisfied condition "success or failure"
Jun 23 18:42:35.922: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-5c50260c-a48f-4d64-b693-e9dd38fdbd25 container client-container: <nil>
STEP: delete the pod
Jun 23 18:42:35.957: INFO: Waiting for pod downwardapi-volume-5c50260c-a48f-4d64-b693-e9dd38fdbd25 to disappear
Jun 23 18:42:35.962: INFO: Pod downwardapi-volume-5c50260c-a48f-4d64-b693-e9dd38fdbd25 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:42:35.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4598" for this suite.
Jun 23 18:42:41.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:42:42.028: INFO: namespace projected-4598 deletion completed in 6.061227925s

• [SLOW TEST:10.156 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:42:42.028: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:42:46.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8735" for this suite.
Jun 23 18:42:52.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:42:52.165: INFO: namespace kubelet-test-8735 deletion completed in 6.089278366s

• [SLOW TEST:10.137 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:42:52.166: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-0b83249f-9679-4b79-8fd6-f5f7121a041c
STEP: Creating a pod to test consume configMaps
Jun 23 18:42:52.193: INFO: Waiting up to 5m0s for pod "pod-configmaps-7fceb517-8d8a-48ae-9116-f20ce439f794" in namespace "configmap-696" to be "success or failure"
Jun 23 18:42:52.206: INFO: Pod "pod-configmaps-7fceb517-8d8a-48ae-9116-f20ce439f794": Phase="Pending", Reason="", readiness=false. Elapsed: 12.80136ms
Jun 23 18:42:54.213: INFO: Pod "pod-configmaps-7fceb517-8d8a-48ae-9116-f20ce439f794": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019886187s
STEP: Saw pod success
Jun 23 18:42:54.213: INFO: Pod "pod-configmaps-7fceb517-8d8a-48ae-9116-f20ce439f794" satisfied condition "success or failure"
Jun 23 18:42:54.220: INFO: Trying to get logs from node worker-1 pod pod-configmaps-7fceb517-8d8a-48ae-9116-f20ce439f794 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 23 18:42:54.258: INFO: Waiting for pod pod-configmaps-7fceb517-8d8a-48ae-9116-f20ce439f794 to disappear
Jun 23 18:42:54.261: INFO: Pod pod-configmaps-7fceb517-8d8a-48ae-9116-f20ce439f794 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:42:54.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-696" for this suite.
Jun 23 18:43:00.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:43:00.335: INFO: namespace configmap-696 deletion completed in 6.069682159s

• [SLOW TEST:8.170 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:43:00.337: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 23 18:43:00.375: INFO: Number of nodes with available pods: 0
Jun 23 18:43:00.376: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 18:43:01.393: INFO: Number of nodes with available pods: 0
Jun 23 18:43:01.393: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 18:43:02.395: INFO: Number of nodes with available pods: 0
Jun 23 18:43:02.395: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 18:43:03.396: INFO: Number of nodes with available pods: 2
Jun 23 18:43:03.396: INFO: Node worker-1 is running more than one daemon pod
Jun 23 18:43:04.394: INFO: Number of nodes with available pods: 3
Jun 23 18:43:04.394: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 23 18:43:04.436: INFO: Number of nodes with available pods: 2
Jun 23 18:43:04.436: INFO: Node worker-1 is running more than one daemon pod
Jun 23 18:43:05.440: INFO: Number of nodes with available pods: 2
Jun 23 18:43:05.440: INFO: Node worker-1 is running more than one daemon pod
Jun 23 18:43:06.441: INFO: Number of nodes with available pods: 2
Jun 23 18:43:06.441: INFO: Node worker-1 is running more than one daemon pod
Jun 23 18:43:07.451: INFO: Number of nodes with available pods: 3
Jun 23 18:43:07.451: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7227, will wait for the garbage collector to delete the pods
Jun 23 18:43:07.536: INFO: Deleting DaemonSet.extensions daemon-set took: 18.251109ms
Jun 23 18:43:07.839: INFO: Terminating DaemonSet.extensions daemon-set pods took: 303.311041ms
Jun 23 18:43:22.352: INFO: Number of nodes with available pods: 0
Jun 23 18:43:22.352: INFO: Number of running nodes: 0, number of available pods: 0
Jun 23 18:43:22.357: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7227/daemonsets","resourceVersion":"14229"},"items":null}

Jun 23 18:43:22.363: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7227/pods","resourceVersion":"14229"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:43:22.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7227" for this suite.
Jun 23 18:43:28.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:43:28.487: INFO: namespace daemonsets-7227 deletion completed in 6.09290431s

• [SLOW TEST:28.150 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:43:28.487: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jun 23 18:43:32.538: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-309105263 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun 23 18:43:37.631: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:43:37.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1868" for this suite.
Jun 23 18:43:43.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:43:43.751: INFO: namespace pods-1868 deletion completed in 6.103755017s

• [SLOW TEST:15.264 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:43:43.753: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-505.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-505.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-505.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 23 18:43:47.812: INFO: File wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local from pod  dns-505/dns-test-998bf74a-2250-4326-b5d6-8bbeda76df20 contains '' instead of 'foo.example.com.'
Jun 23 18:43:47.815: INFO: File jessie_udp@dns-test-service-3.dns-505.svc.cluster.local from pod  dns-505/dns-test-998bf74a-2250-4326-b5d6-8bbeda76df20 contains '' instead of 'foo.example.com.'
Jun 23 18:43:47.815: INFO: Lookups using dns-505/dns-test-998bf74a-2250-4326-b5d6-8bbeda76df20 failed for: [wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local jessie_udp@dns-test-service-3.dns-505.svc.cluster.local]

Jun 23 18:43:52.835: INFO: DNS probes using dns-test-998bf74a-2250-4326-b5d6-8bbeda76df20 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-505.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-505.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-505.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 23 18:43:56.920: INFO: File wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local from pod  dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 contains '' instead of 'bar.example.com.'
Jun 23 18:43:56.927: INFO: File jessie_udp@dns-test-service-3.dns-505.svc.cluster.local from pod  dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 contains '' instead of 'bar.example.com.'
Jun 23 18:43:56.927: INFO: Lookups using dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 failed for: [wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local jessie_udp@dns-test-service-3.dns-505.svc.cluster.local]

Jun 23 18:44:01.930: INFO: File wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local from pod  dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 23 18:44:01.932: INFO: File jessie_udp@dns-test-service-3.dns-505.svc.cluster.local from pod  dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 23 18:44:01.932: INFO: Lookups using dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 failed for: [wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local jessie_udp@dns-test-service-3.dns-505.svc.cluster.local]

Jun 23 18:44:06.936: INFO: File wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local from pod  dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 23 18:44:06.944: INFO: File jessie_udp@dns-test-service-3.dns-505.svc.cluster.local from pod  dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 23 18:44:06.944: INFO: Lookups using dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 failed for: [wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local jessie_udp@dns-test-service-3.dns-505.svc.cluster.local]

Jun 23 18:44:11.937: INFO: File wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local from pod  dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 23 18:44:11.947: INFO: File jessie_udp@dns-test-service-3.dns-505.svc.cluster.local from pod  dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 23 18:44:11.947: INFO: Lookups using dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 failed for: [wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local jessie_udp@dns-test-service-3.dns-505.svc.cluster.local]

Jun 23 18:44:16.937: INFO: File wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local from pod  dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 23 18:44:16.946: INFO: File jessie_udp@dns-test-service-3.dns-505.svc.cluster.local from pod  dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 23 18:44:16.946: INFO: Lookups using dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 failed for: [wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local jessie_udp@dns-test-service-3.dns-505.svc.cluster.local]

Jun 23 18:44:21.934: INFO: File jessie_udp@dns-test-service-3.dns-505.svc.cluster.local from pod  dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 contains '' instead of 'bar.example.com.'
Jun 23 18:44:21.934: INFO: Lookups using dns-505/dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 failed for: [jessie_udp@dns-test-service-3.dns-505.svc.cluster.local]

Jun 23 18:44:26.948: INFO: DNS probes using dns-test-e07fa186-4d96-4aa4-a660-2e29d7456703 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-505.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-505.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-505.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 23 18:44:29.016: INFO: File wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local from pod  dns-505/dns-test-2265dcec-facb-4588-9283-df0253151421 contains '' instead of '10.106.7.74'
Jun 23 18:44:29.019: INFO: File jessie_udp@dns-test-service-3.dns-505.svc.cluster.local from pod  dns-505/dns-test-2265dcec-facb-4588-9283-df0253151421 contains '' instead of '10.106.7.74'
Jun 23 18:44:29.019: INFO: Lookups using dns-505/dns-test-2265dcec-facb-4588-9283-df0253151421 failed for: [wheezy_udp@dns-test-service-3.dns-505.svc.cluster.local jessie_udp@dns-test-service-3.dns-505.svc.cluster.local]

Jun 23 18:44:34.057: INFO: DNS probes using dns-test-2265dcec-facb-4588-9283-df0253151421 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:44:34.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-505" for this suite.
Jun 23 18:44:40.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:44:40.185: INFO: namespace dns-505 deletion completed in 6.083405118s

• [SLOW TEST:56.432 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:44:40.185: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:44:40.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9635" for this suite.
Jun 23 18:45:02.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:45:02.358: INFO: namespace pods-9635 deletion completed in 22.139600648s

• [SLOW TEST:22.172 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:45:02.359: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-995c54a9-e37b-4260-a722-2252ce770b7c
STEP: Creating a pod to test consume secrets
Jun 23 18:45:02.400: INFO: Waiting up to 5m0s for pod "pod-secrets-29fcbe6e-da47-4c38-90bd-268c911f03fb" in namespace "secrets-2010" to be "success or failure"
Jun 23 18:45:02.402: INFO: Pod "pod-secrets-29fcbe6e-da47-4c38-90bd-268c911f03fb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.929028ms
Jun 23 18:45:04.408: INFO: Pod "pod-secrets-29fcbe6e-da47-4c38-90bd-268c911f03fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007450108s
Jun 23 18:45:06.416: INFO: Pod "pod-secrets-29fcbe6e-da47-4c38-90bd-268c911f03fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016096666s
STEP: Saw pod success
Jun 23 18:45:06.416: INFO: Pod "pod-secrets-29fcbe6e-da47-4c38-90bd-268c911f03fb" satisfied condition "success or failure"
Jun 23 18:45:06.420: INFO: Trying to get logs from node worker-1 pod pod-secrets-29fcbe6e-da47-4c38-90bd-268c911f03fb container secret-volume-test: <nil>
STEP: delete the pod
Jun 23 18:45:06.440: INFO: Waiting for pod pod-secrets-29fcbe6e-da47-4c38-90bd-268c911f03fb to disappear
Jun 23 18:45:06.442: INFO: Pod pod-secrets-29fcbe6e-da47-4c38-90bd-268c911f03fb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:45:06.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2010" for this suite.
Jun 23 18:45:12.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:45:12.523: INFO: namespace secrets-2010 deletion completed in 6.068352218s

• [SLOW TEST:10.164 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:45:12.523: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4426.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4426.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4426.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4426.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4426.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4426.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4426.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4426.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4426.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4426.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4426.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4426.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4426.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 201.139.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.139.201_udp@PTR;check="$$(dig +tcp +noall +answer +search 201.139.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.139.201_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4426.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4426.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4426.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4426.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4426.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4426.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4426.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4426.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4426.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4426.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4426.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4426.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4426.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 201.139.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.139.201_udp@PTR;check="$$(dig +tcp +noall +answer +search 201.139.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.139.201_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 23 18:45:14.595: INFO: Unable to read wheezy_udp@dns-test-service.dns-4426.svc.cluster.local from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.599: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4426.svc.cluster.local from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.604: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4426.svc.cluster.local from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.606: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4426.svc.cluster.local from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.610: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-4426.svc.cluster.local from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.612: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-4426.svc.cluster.local from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.615: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.618: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.621: INFO: Unable to read 10.100.139.201_udp@PTR from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.623: INFO: Unable to read 10.100.139.201_tcp@PTR from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.626: INFO: Unable to read jessie_udp@dns-test-service.dns-4426.svc.cluster.local from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.628: INFO: Unable to read jessie_tcp@dns-test-service.dns-4426.svc.cluster.local from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.630: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4426.svc.cluster.local from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.632: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4426.svc.cluster.local from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.634: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-4426.svc.cluster.local from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.636: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-4426.svc.cluster.local from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.638: INFO: Unable to read jessie_udp@PodARecord from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.640: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.642: INFO: Unable to read 10.100.139.201_udp@PTR from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.644: INFO: Unable to read 10.100.139.201_tcp@PTR from pod dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248: the server could not find the requested resource (get pods dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248)
Jun 23 18:45:14.644: INFO: Lookups using dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248 failed for: [wheezy_udp@dns-test-service.dns-4426.svc.cluster.local wheezy_tcp@dns-test-service.dns-4426.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4426.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4426.svc.cluster.local wheezy_udp@_http._tcp.test-service-2.dns-4426.svc.cluster.local wheezy_tcp@_http._tcp.test-service-2.dns-4426.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.100.139.201_udp@PTR 10.100.139.201_tcp@PTR jessie_udp@dns-test-service.dns-4426.svc.cluster.local jessie_tcp@dns-test-service.dns-4426.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4426.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4426.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-4426.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-4426.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord 10.100.139.201_udp@PTR 10.100.139.201_tcp@PTR]

Jun 23 18:45:19.729: INFO: DNS probes using dns-4426/dns-test-edce3720-5df2-4b7b-8800-3658cb0d6248 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:45:19.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4426" for this suite.
Jun 23 18:45:25.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:45:25.892: INFO: namespace dns-4426 deletion completed in 6.08801229s

• [SLOW TEST:13.369 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:45:25.895: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 18:45:25.922: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41a085ff-8f56-4fea-bdfc-1bb61e6920ca" in namespace "downward-api-8585" to be "success or failure"
Jun 23 18:45:25.929: INFO: Pod "downwardapi-volume-41a085ff-8f56-4fea-bdfc-1bb61e6920ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.884085ms
Jun 23 18:45:27.935: INFO: Pod "downwardapi-volume-41a085ff-8f56-4fea-bdfc-1bb61e6920ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013348198s
STEP: Saw pod success
Jun 23 18:45:27.935: INFO: Pod "downwardapi-volume-41a085ff-8f56-4fea-bdfc-1bb61e6920ca" satisfied condition "success or failure"
Jun 23 18:45:27.942: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-41a085ff-8f56-4fea-bdfc-1bb61e6920ca container client-container: <nil>
STEP: delete the pod
Jun 23 18:45:27.968: INFO: Waiting for pod downwardapi-volume-41a085ff-8f56-4fea-bdfc-1bb61e6920ca to disappear
Jun 23 18:45:27.970: INFO: Pod downwardapi-volume-41a085ff-8f56-4fea-bdfc-1bb61e6920ca no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:45:27.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8585" for this suite.
Jun 23 18:45:33.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:45:34.035: INFO: namespace downward-api-8585 deletion completed in 6.061657131s

• [SLOW TEST:8.140 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:45:34.036: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-8956c221-b658-49c7-a5bd-56854289cc99
STEP: Creating a pod to test consume secrets
Jun 23 18:45:34.063: INFO: Waiting up to 5m0s for pod "pod-secrets-662adc2c-53a7-4e8a-8783-859e66bb0af9" in namespace "secrets-2221" to be "success or failure"
Jun 23 18:45:34.065: INFO: Pod "pod-secrets-662adc2c-53a7-4e8a-8783-859e66bb0af9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.231674ms
Jun 23 18:45:36.067: INFO: Pod "pod-secrets-662adc2c-53a7-4e8a-8783-859e66bb0af9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004281833s
STEP: Saw pod success
Jun 23 18:45:36.068: INFO: Pod "pod-secrets-662adc2c-53a7-4e8a-8783-859e66bb0af9" satisfied condition "success or failure"
Jun 23 18:45:36.069: INFO: Trying to get logs from node worker-1 pod pod-secrets-662adc2c-53a7-4e8a-8783-859e66bb0af9 container secret-volume-test: <nil>
STEP: delete the pod
Jun 23 18:45:36.081: INFO: Waiting for pod pod-secrets-662adc2c-53a7-4e8a-8783-859e66bb0af9 to disappear
Jun 23 18:45:36.083: INFO: Pod pod-secrets-662adc2c-53a7-4e8a-8783-859e66bb0af9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:45:36.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2221" for this suite.
Jun 23 18:45:42.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:45:42.158: INFO: namespace secrets-2221 deletion completed in 6.071761356s

• [SLOW TEST:8.122 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:45:42.158: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 18:45:42.238: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a9621af9-9f74-4203-88af-669ac31a8ba2" in namespace "downward-api-9146" to be "success or failure"
Jun 23 18:45:42.248: INFO: Pod "downwardapi-volume-a9621af9-9f74-4203-88af-669ac31a8ba2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.722752ms
Jun 23 18:45:44.255: INFO: Pod "downwardapi-volume-a9621af9-9f74-4203-88af-669ac31a8ba2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016047529s
STEP: Saw pod success
Jun 23 18:45:44.255: INFO: Pod "downwardapi-volume-a9621af9-9f74-4203-88af-669ac31a8ba2" satisfied condition "success or failure"
Jun 23 18:45:44.260: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-a9621af9-9f74-4203-88af-669ac31a8ba2 container client-container: <nil>
STEP: delete the pod
Jun 23 18:45:44.299: INFO: Waiting for pod downwardapi-volume-a9621af9-9f74-4203-88af-669ac31a8ba2 to disappear
Jun 23 18:45:44.302: INFO: Pod downwardapi-volume-a9621af9-9f74-4203-88af-669ac31a8ba2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:45:44.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9146" for this suite.
Jun 23 18:45:50.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:45:50.411: INFO: namespace downward-api-9146 deletion completed in 6.103763863s

• [SLOW TEST:8.253 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:45:50.411: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 18:45:50.436: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7bb95f95-1622-4657-969a-ffe09f71e561" in namespace "projected-4592" to be "success or failure"
Jun 23 18:45:50.438: INFO: Pod "downwardapi-volume-7bb95f95-1622-4657-969a-ffe09f71e561": Phase="Pending", Reason="", readiness=false. Elapsed: 1.952828ms
Jun 23 18:45:52.441: INFO: Pod "downwardapi-volume-7bb95f95-1622-4657-969a-ffe09f71e561": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005303397s
Jun 23 18:45:54.444: INFO: Pod "downwardapi-volume-7bb95f95-1622-4657-969a-ffe09f71e561": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007838644s
STEP: Saw pod success
Jun 23 18:45:54.444: INFO: Pod "downwardapi-volume-7bb95f95-1622-4657-969a-ffe09f71e561" satisfied condition "success or failure"
Jun 23 18:45:54.445: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-7bb95f95-1622-4657-969a-ffe09f71e561 container client-container: <nil>
STEP: delete the pod
Jun 23 18:45:54.458: INFO: Waiting for pod downwardapi-volume-7bb95f95-1622-4657-969a-ffe09f71e561 to disappear
Jun 23 18:45:54.460: INFO: Pod downwardapi-volume-7bb95f95-1622-4657-969a-ffe09f71e561 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:45:54.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4592" for this suite.
Jun 23 18:46:00.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:46:00.555: INFO: namespace projected-4592 deletion completed in 6.093637292s

• [SLOW TEST:10.144 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:46:00.556: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-62503bd3-5e6d-4594-826d-a44c96ff17ad
STEP: Creating a pod to test consume secrets
Jun 23 18:46:00.587: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3df1a2c0-ec7c-4d9f-b121-1676e94e2f7e" in namespace "projected-7162" to be "success or failure"
Jun 23 18:46:00.597: INFO: Pod "pod-projected-secrets-3df1a2c0-ec7c-4d9f-b121-1676e94e2f7e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042891ms
Jun 23 18:46:02.604: INFO: Pod "pod-projected-secrets-3df1a2c0-ec7c-4d9f-b121-1676e94e2f7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016706328s
Jun 23 18:46:04.611: INFO: Pod "pod-projected-secrets-3df1a2c0-ec7c-4d9f-b121-1676e94e2f7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02330534s
STEP: Saw pod success
Jun 23 18:46:04.611: INFO: Pod "pod-projected-secrets-3df1a2c0-ec7c-4d9f-b121-1676e94e2f7e" satisfied condition "success or failure"
Jun 23 18:46:04.617: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-3df1a2c0-ec7c-4d9f-b121-1676e94e2f7e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 23 18:46:04.651: INFO: Waiting for pod pod-projected-secrets-3df1a2c0-ec7c-4d9f-b121-1676e94e2f7e to disappear
Jun 23 18:46:04.653: INFO: Pod pod-projected-secrets-3df1a2c0-ec7c-4d9f-b121-1676e94e2f7e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:46:04.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7162" for this suite.
Jun 23 18:46:10.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:46:10.775: INFO: namespace projected-7162 deletion completed in 6.11965308s

• [SLOW TEST:10.219 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:46:10.775: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 23 18:46:10.853: INFO: Waiting up to 5m0s for pod "pod-95dc30dd-f489-422c-9262-e0eab06fce58" in namespace "emptydir-2620" to be "success or failure"
Jun 23 18:46:10.860: INFO: Pod "pod-95dc30dd-f489-422c-9262-e0eab06fce58": Phase="Pending", Reason="", readiness=false. Elapsed: 7.315319ms
Jun 23 18:46:12.866: INFO: Pod "pod-95dc30dd-f489-422c-9262-e0eab06fce58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012991211s
Jun 23 18:46:14.869: INFO: Pod "pod-95dc30dd-f489-422c-9262-e0eab06fce58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016704427s
STEP: Saw pod success
Jun 23 18:46:14.869: INFO: Pod "pod-95dc30dd-f489-422c-9262-e0eab06fce58" satisfied condition "success or failure"
Jun 23 18:46:14.871: INFO: Trying to get logs from node worker-1 pod pod-95dc30dd-f489-422c-9262-e0eab06fce58 container test-container: <nil>
STEP: delete the pod
Jun 23 18:46:14.887: INFO: Waiting for pod pod-95dc30dd-f489-422c-9262-e0eab06fce58 to disappear
Jun 23 18:46:14.888: INFO: Pod pod-95dc30dd-f489-422c-9262-e0eab06fce58 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:46:14.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2620" for this suite.
Jun 23 18:46:20.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:46:20.985: INFO: namespace emptydir-2620 deletion completed in 6.093883973s

• [SLOW TEST:10.209 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:46:20.985: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-52cccd6f-76ec-4b79-a896-3e29c42643a1
STEP: Creating a pod to test consume configMaps
Jun 23 18:46:21.013: INFO: Waiting up to 5m0s for pod "pod-configmaps-4ec25b2e-b016-4230-9c61-be1e889a3132" in namespace "configmap-5806" to be "success or failure"
Jun 23 18:46:21.019: INFO: Pod "pod-configmaps-4ec25b2e-b016-4230-9c61-be1e889a3132": Phase="Pending", Reason="", readiness=false. Elapsed: 5.435862ms
Jun 23 18:46:23.026: INFO: Pod "pod-configmaps-4ec25b2e-b016-4230-9c61-be1e889a3132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012490017s
Jun 23 18:46:25.030: INFO: Pod "pod-configmaps-4ec25b2e-b016-4230-9c61-be1e889a3132": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017143432s
STEP: Saw pod success
Jun 23 18:46:25.031: INFO: Pod "pod-configmaps-4ec25b2e-b016-4230-9c61-be1e889a3132" satisfied condition "success or failure"
Jun 23 18:46:25.034: INFO: Trying to get logs from node worker-1 pod pod-configmaps-4ec25b2e-b016-4230-9c61-be1e889a3132 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 23 18:46:25.065: INFO: Waiting for pod pod-configmaps-4ec25b2e-b016-4230-9c61-be1e889a3132 to disappear
Jun 23 18:46:25.068: INFO: Pod pod-configmaps-4ec25b2e-b016-4230-9c61-be1e889a3132 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:46:25.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5806" for this suite.
Jun 23 18:46:31.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:46:31.161: INFO: namespace configmap-5806 deletion completed in 6.088306631s

• [SLOW TEST:10.176 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:46:31.162: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 23 18:46:35.266: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-64018cb1-6f43-4213-add6-dd63551067f0,GenerateName:,Namespace:events-6403,SelfLink:/api/v1/namespaces/events-6403/pods/send-events-64018cb1-6f43-4213-add6-dd63551067f0,UID:f1c320e4-0bba-460e-9154-4304030a1832,ResourceVersion:14958,Generation:0,CreationTimestamp:2019-06-23 18:46:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 236855113,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mxsvg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mxsvg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-mxsvg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c81ad0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c81af0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:46:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:46:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:46:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:46:31 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:10.38.0.2,StartTime:2019-06-23 18:46:31 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-06-23 18:46:32 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://0e5bd0f7640b2311ddfb1a9d6d5da327260ae7388a9388e2ef9df2d9ad1e76dd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jun 23 18:46:37.270: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 23 18:46:39.281: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:46:39.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6403" for this suite.
Jun 23 18:47:23.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:47:23.377: INFO: namespace events-6403 deletion completed in 44.089852141s

• [SLOW TEST:52.215 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:47:23.378: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 18:47:23.404: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f3d59f8-9eab-42fe-bc36-dbc4f718a985" in namespace "downward-api-7487" to be "success or failure"
Jun 23 18:47:23.406: INFO: Pod "downwardapi-volume-4f3d59f8-9eab-42fe-bc36-dbc4f718a985": Phase="Pending", Reason="", readiness=false. Elapsed: 1.526662ms
Jun 23 18:47:25.412: INFO: Pod "downwardapi-volume-4f3d59f8-9eab-42fe-bc36-dbc4f718a985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007659596s
Jun 23 18:47:27.419: INFO: Pod "downwardapi-volume-4f3d59f8-9eab-42fe-bc36-dbc4f718a985": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01492891s
STEP: Saw pod success
Jun 23 18:47:27.419: INFO: Pod "downwardapi-volume-4f3d59f8-9eab-42fe-bc36-dbc4f718a985" satisfied condition "success or failure"
Jun 23 18:47:27.426: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-4f3d59f8-9eab-42fe-bc36-dbc4f718a985 container client-container: <nil>
STEP: delete the pod
Jun 23 18:47:27.450: INFO: Waiting for pod downwardapi-volume-4f3d59f8-9eab-42fe-bc36-dbc4f718a985 to disappear
Jun 23 18:47:27.452: INFO: Pod downwardapi-volume-4f3d59f8-9eab-42fe-bc36-dbc4f718a985 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:47:27.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7487" for this suite.
Jun 23 18:47:33.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:47:33.586: INFO: namespace downward-api-7487 deletion completed in 6.132442787s

• [SLOW TEST:10.209 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:47:33.587: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 18:47:33.613: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47ccfd79-55f9-405a-bee3-61d670a3f05d" in namespace "projected-9219" to be "success or failure"
Jun 23 18:47:33.621: INFO: Pod "downwardapi-volume-47ccfd79-55f9-405a-bee3-61d670a3f05d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.278413ms
Jun 23 18:47:35.628: INFO: Pod "downwardapi-volume-47ccfd79-55f9-405a-bee3-61d670a3f05d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014491639s
Jun 23 18:47:37.634: INFO: Pod "downwardapi-volume-47ccfd79-55f9-405a-bee3-61d670a3f05d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020440519s
STEP: Saw pod success
Jun 23 18:47:37.634: INFO: Pod "downwardapi-volume-47ccfd79-55f9-405a-bee3-61d670a3f05d" satisfied condition "success or failure"
Jun 23 18:47:37.640: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-47ccfd79-55f9-405a-bee3-61d670a3f05d container client-container: <nil>
STEP: delete the pod
Jun 23 18:47:37.676: INFO: Waiting for pod downwardapi-volume-47ccfd79-55f9-405a-bee3-61d670a3f05d to disappear
Jun 23 18:47:37.681: INFO: Pod downwardapi-volume-47ccfd79-55f9-405a-bee3-61d670a3f05d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:47:37.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9219" for this suite.
Jun 23 18:47:43.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:47:43.753: INFO: namespace projected-9219 deletion completed in 6.068988633s

• [SLOW TEST:10.166 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:47:43.753: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 18:47:43.779: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 23 18:47:48.787: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 23 18:47:48.787: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 23 18:47:50.793: INFO: Creating deployment "test-rollover-deployment"
Jun 23 18:47:50.809: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 23 18:47:52.821: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 23 18:47:52.834: INFO: Ensure that both replica sets have 1 created replica
Jun 23 18:47:52.847: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 23 18:47:52.860: INFO: Updating deployment test-rollover-deployment
Jun 23 18:47:52.860: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 23 18:47:54.872: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 23 18:47:54.885: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 23 18:47:54.896: INFO: all replica sets need to contain the pod-template-hash label
Jun 23 18:47:54.897: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912472, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:47:56.909: INFO: all replica sets need to contain the pod-template-hash label
Jun 23 18:47:56.909: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912475, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:47:58.910: INFO: all replica sets need to contain the pod-template-hash label
Jun 23 18:47:58.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912475, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:48:00.910: INFO: all replica sets need to contain the pod-template-hash label
Jun 23 18:48:00.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912475, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:48:02.908: INFO: all replica sets need to contain the pod-template-hash label
Jun 23 18:48:02.909: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912475, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:48:04.909: INFO: all replica sets need to contain the pod-template-hash label
Jun 23 18:48:04.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912475, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696912470, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:48:06.908: INFO: 
Jun 23 18:48:06.909: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 23 18:48:06.932: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-359,SelfLink:/apis/apps/v1/namespaces/deployment-359/deployments/test-rollover-deployment,UID:94cad5f3-1ace-4421-b3a9-e33045bef038,ResourceVersion:15225,Generation:2,CreationTimestamp:2019-06-23 18:47:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-23 18:47:50 +0000 UTC 2019-06-23 18:47:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-23 18:48:05 +0000 UTC 2019-06-23 18:47:50 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 23 18:48:06.936: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-359,SelfLink:/apis/apps/v1/namespaces/deployment-359/replicasets/test-rollover-deployment-854595fc44,UID:fc255b1b-ad29-436c-b250-a1a732d7d381,ResourceVersion:15214,Generation:2,CreationTimestamp:2019-06-23 18:47:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 94cad5f3-1ace-4421-b3a9-e33045bef038 0xc00307f4b7 0xc00307f4b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 23 18:48:06.936: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 23 18:48:06.937: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-359,SelfLink:/apis/apps/v1/namespaces/deployment-359/replicasets/test-rollover-controller,UID:788e425d-e124-4e3c-80e7-d0ddeb0a8f54,ResourceVersion:15223,Generation:2,CreationTimestamp:2019-06-23 18:47:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 94cad5f3-1ace-4421-b3a9-e33045bef038 0xc00307f327 0xc00307f328}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 23 18:48:06.937: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-359,SelfLink:/apis/apps/v1/namespaces/deployment-359/replicasets/test-rollover-deployment-9b8b997cf,UID:25ec3746-f887-4ca5-9ca3-417908df0e82,ResourceVersion:15187,Generation:2,CreationTimestamp:2019-06-23 18:47:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 94cad5f3-1ace-4421-b3a9-e33045bef038 0xc00307f580 0xc00307f581}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 23 18:48:06.940: INFO: Pod "test-rollover-deployment-854595fc44-9cmw9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-9cmw9,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-359,SelfLink:/api/v1/namespaces/deployment-359/pods/test-rollover-deployment-854595fc44-9cmw9,UID:ea1f7b23-1a16-4f2b-b492-abde4bf6b2b9,ResourceVersion:15197,Generation:0,CreationTimestamp:2019-06-23 18:47:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 fc255b1b-ad29-436c-b250-a1a732d7d381 0xc002a6c347 0xc002a6c348}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8rftk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8rftk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-8rftk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a6c3c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a6c3e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:47:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:47:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:47:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:47:52 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:10.38.0.3,StartTime:2019-06-23 18:47:52 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-23 18:47:54 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://7825a15abc42d720257668c83cb8eccff26214839e7f54cd0c990d9f55280a36}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:48:06.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-359" for this suite.
Jun 23 18:48:12.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:48:13.043: INFO: namespace deployment-359 deletion completed in 6.099079333s

• [SLOW TEST:29.290 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:48:13.043: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6095, will wait for the garbage collector to delete the pods
Jun 23 18:48:17.223: INFO: Deleting Job.batch foo took: 10.473768ms
Jun 23 18:48:17.344: INFO: Terminating Job.batch foo pods took: 120.349197ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:48:52.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6095" for this suite.
Jun 23 18:48:58.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:48:58.439: INFO: namespace job-6095 deletion completed in 6.090212282s

• [SLOW TEST:45.396 seconds]
[sig-apps] Job
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:48:58.442: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 23 18:48:58.464: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun 23 18:49:07.570: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:49:07.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1487" for this suite.
Jun 23 18:49:13.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:49:13.676: INFO: namespace pods-1487 deletion completed in 6.092886326s

• [SLOW TEST:15.234 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:49:13.677: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9426.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9426.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 23 18:49:17.733: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-9426/dns-test-62863bee-cc8e-4981-ba54-dff89493471d: the server could not find the requested resource (get pods dns-test-62863bee-cc8e-4981-ba54-dff89493471d)
Jun 23 18:49:17.742: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-9426/dns-test-62863bee-cc8e-4981-ba54-dff89493471d: the server could not find the requested resource (get pods dns-test-62863bee-cc8e-4981-ba54-dff89493471d)
Jun 23 18:49:17.748: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9426/dns-test-62863bee-cc8e-4981-ba54-dff89493471d: the server could not find the requested resource (get pods dns-test-62863bee-cc8e-4981-ba54-dff89493471d)
Jun 23 18:49:17.752: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9426/dns-test-62863bee-cc8e-4981-ba54-dff89493471d: the server could not find the requested resource (get pods dns-test-62863bee-cc8e-4981-ba54-dff89493471d)
Jun 23 18:49:17.756: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-9426/dns-test-62863bee-cc8e-4981-ba54-dff89493471d: the server could not find the requested resource (get pods dns-test-62863bee-cc8e-4981-ba54-dff89493471d)
Jun 23 18:49:17.759: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-9426/dns-test-62863bee-cc8e-4981-ba54-dff89493471d: the server could not find the requested resource (get pods dns-test-62863bee-cc8e-4981-ba54-dff89493471d)
Jun 23 18:49:17.763: INFO: Unable to read jessie_udp@PodARecord from pod dns-9426/dns-test-62863bee-cc8e-4981-ba54-dff89493471d: the server could not find the requested resource (get pods dns-test-62863bee-cc8e-4981-ba54-dff89493471d)
Jun 23 18:49:17.766: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9426/dns-test-62863bee-cc8e-4981-ba54-dff89493471d: the server could not find the requested resource (get pods dns-test-62863bee-cc8e-4981-ba54-dff89493471d)
Jun 23 18:49:17.766: INFO: Lookups using dns-9426/dns-test-62863bee-cc8e-4981-ba54-dff89493471d failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun 23 18:49:22.823: INFO: DNS probes using dns-9426/dns-test-62863bee-cc8e-4981-ba54-dff89493471d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:49:22.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9426" for this suite.
Jun 23 18:49:28.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:49:28.917: INFO: namespace dns-9426 deletion completed in 6.070185424s

• [SLOW TEST:15.241 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:49:28.919: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:50:28.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7017" for this suite.
Jun 23 18:50:50.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:50:51.063: INFO: namespace container-probe-7017 deletion completed in 22.102769479s

• [SLOW TEST:82.144 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:50:51.063: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jun 23 18:50:51.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 create -f - --namespace=kubectl-7822'
Jun 23 18:50:51.733: INFO: stderr: ""
Jun 23 18:50:51.733: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 23 18:50:51.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7822'
Jun 23 18:50:51.820: INFO: stderr: ""
Jun 23 18:50:51.820: INFO: stdout: "update-demo-nautilus-fht7m update-demo-nautilus-jk4bd "
Jun 23 18:50:51.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-fht7m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7822'
Jun 23 18:50:51.871: INFO: stderr: ""
Jun 23 18:50:51.871: INFO: stdout: ""
Jun 23 18:50:51.871: INFO: update-demo-nautilus-fht7m is created but not running
Jun 23 18:50:56.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7822'
Jun 23 18:50:56.938: INFO: stderr: ""
Jun 23 18:50:56.938: INFO: stdout: "update-demo-nautilus-fht7m update-demo-nautilus-jk4bd "
Jun 23 18:50:56.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-fht7m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7822'
Jun 23 18:50:56.988: INFO: stderr: ""
Jun 23 18:50:56.988: INFO: stdout: "true"
Jun 23 18:50:56.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-fht7m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7822'
Jun 23 18:50:57.033: INFO: stderr: ""
Jun 23 18:50:57.033: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 23 18:50:57.033: INFO: validating pod update-demo-nautilus-fht7m
Jun 23 18:50:57.037: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 23 18:50:57.037: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 23 18:50:57.037: INFO: update-demo-nautilus-fht7m is verified up and running
Jun 23 18:50:57.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-jk4bd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7822'
Jun 23 18:50:57.085: INFO: stderr: ""
Jun 23 18:50:57.085: INFO: stdout: "true"
Jun 23 18:50:57.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-jk4bd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7822'
Jun 23 18:50:57.131: INFO: stderr: ""
Jun 23 18:50:57.131: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 23 18:50:57.131: INFO: validating pod update-demo-nautilus-jk4bd
Jun 23 18:50:57.135: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 23 18:50:57.135: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 23 18:50:57.135: INFO: update-demo-nautilus-jk4bd is verified up and running
STEP: using delete to clean up resources
Jun 23 18:50:57.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete --grace-period=0 --force -f - --namespace=kubectl-7822'
Jun 23 18:50:57.194: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 23 18:50:57.195: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 23 18:50:57.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7822'
Jun 23 18:50:57.270: INFO: stderr: "No resources found.\n"
Jun 23 18:50:57.270: INFO: stdout: ""
Jun 23 18:50:57.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -l name=update-demo --namespace=kubectl-7822 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 23 18:50:57.326: INFO: stderr: ""
Jun 23 18:50:57.326: INFO: stdout: "update-demo-nautilus-fht7m\nupdate-demo-nautilus-jk4bd\n"
Jun 23 18:50:57.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7822'
Jun 23 18:50:57.920: INFO: stderr: "No resources found.\n"
Jun 23 18:50:57.920: INFO: stdout: ""
Jun 23 18:50:57.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -l name=update-demo --namespace=kubectl-7822 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 23 18:50:57.990: INFO: stderr: ""
Jun 23 18:50:57.990: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:50:57.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7822" for this suite.
Jun 23 18:51:04.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:51:04.070: INFO: namespace kubectl-7822 deletion completed in 6.07808084s

• [SLOW TEST:13.007 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:51:04.071: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:51:08.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9890" for this suite.
Jun 23 18:51:46.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:51:46.217: INFO: namespace kubelet-test-9890 deletion completed in 38.084238409s

• [SLOW TEST:42.147 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:51:46.218: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:51:52.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6991" for this suite.
Jun 23 18:51:58.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:51:58.496: INFO: namespace namespaces-6991 deletion completed in 6.179247971s
STEP: Destroying namespace "nsdeletetest-2751" for this suite.
Jun 23 18:51:58.497: INFO: Namespace nsdeletetest-2751 was already deleted
STEP: Destroying namespace "nsdeletetest-8581" for this suite.
Jun 23 18:52:04.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:52:04.603: INFO: namespace nsdeletetest-8581 deletion completed in 6.105736224s

• [SLOW TEST:18.385 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:52:04.604: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jun 23 18:52:05.188: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0623 18:52:05.188619      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:52:05.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6694" for this suite.
Jun 23 18:52:11.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:52:11.275: INFO: namespace gc-6694 deletion completed in 6.079312119s

• [SLOW TEST:6.671 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:52:11.275: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 23 18:52:11.309: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6884,SelfLink:/api/v1/namespaces/watch-6884/configmaps/e2e-watch-test-watch-closed,UID:8428a7ff-5550-4d45-8828-9dba3dcf7325,ResourceVersion:15923,Generation:0,CreationTimestamp:2019-06-23 18:52:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 23 18:52:11.310: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6884,SelfLink:/api/v1/namespaces/watch-6884/configmaps/e2e-watch-test-watch-closed,UID:8428a7ff-5550-4d45-8828-9dba3dcf7325,ResourceVersion:15924,Generation:0,CreationTimestamp:2019-06-23 18:52:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 23 18:52:11.316: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6884,SelfLink:/api/v1/namespaces/watch-6884/configmaps/e2e-watch-test-watch-closed,UID:8428a7ff-5550-4d45-8828-9dba3dcf7325,ResourceVersion:15925,Generation:0,CreationTimestamp:2019-06-23 18:52:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 23 18:52:11.316: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6884,SelfLink:/api/v1/namespaces/watch-6884/configmaps/e2e-watch-test-watch-closed,UID:8428a7ff-5550-4d45-8828-9dba3dcf7325,ResourceVersion:15926,Generation:0,CreationTimestamp:2019-06-23 18:52:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:52:11.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6884" for this suite.
Jun 23 18:52:17.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:52:17.382: INFO: namespace watch-6884 deletion completed in 6.064713652s

• [SLOW TEST:6.107 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:52:17.383: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-5056/configmap-test-c07ef247-05e1-4c19-b30d-9eca9f989774
STEP: Creating a pod to test consume configMaps
Jun 23 18:52:17.409: INFO: Waiting up to 5m0s for pod "pod-configmaps-dc6cf2a4-7d32-482c-b818-23faec13c6d3" in namespace "configmap-5056" to be "success or failure"
Jun 23 18:52:17.416: INFO: Pod "pod-configmaps-dc6cf2a4-7d32-482c-b818-23faec13c6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.314644ms
Jun 23 18:52:19.456: INFO: Pod "pod-configmaps-dc6cf2a4-7d32-482c-b818-23faec13c6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04661216s
Jun 23 18:52:21.462: INFO: Pod "pod-configmaps-dc6cf2a4-7d32-482c-b818-23faec13c6d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052578283s
STEP: Saw pod success
Jun 23 18:52:21.463: INFO: Pod "pod-configmaps-dc6cf2a4-7d32-482c-b818-23faec13c6d3" satisfied condition "success or failure"
Jun 23 18:52:21.469: INFO: Trying to get logs from node worker-1 pod pod-configmaps-dc6cf2a4-7d32-482c-b818-23faec13c6d3 container env-test: <nil>
STEP: delete the pod
Jun 23 18:52:21.507: INFO: Waiting for pod pod-configmaps-dc6cf2a4-7d32-482c-b818-23faec13c6d3 to disappear
Jun 23 18:52:21.510: INFO: Pod pod-configmaps-dc6cf2a4-7d32-482c-b818-23faec13c6d3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:52:21.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5056" for this suite.
Jun 23 18:52:27.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:52:27.620: INFO: namespace configmap-5056 deletion completed in 6.1077636s

• [SLOW TEST:10.237 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:52:27.621: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 18:52:27.653: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 23 18:52:31.669: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 23 18:52:35.718: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-8268,SelfLink:/apis/apps/v1/namespaces/deployment-8268/deployments/test-cleanup-deployment,UID:7a595959-5676-4ce5-8c72-ae066e8bb733,ResourceVersion:16035,Generation:1,CreationTimestamp:2019-06-23 18:52:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-23 18:52:31 +0000 UTC 2019-06-23 18:52:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-23 18:52:34 +0000 UTC 2019-06-23 18:52:31 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 23 18:52:35.725: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-8268,SelfLink:/apis/apps/v1/namespaces/deployment-8268/replicasets/test-cleanup-deployment-55bbcbc84c,UID:1b9f0f1e-2875-4762-9114-63d9bd3db0ef,ResourceVersion:16024,Generation:1,CreationTimestamp:2019-06-23 18:52:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 7a595959-5676-4ce5-8c72-ae066e8bb733 0xc002d46bf7 0xc002d46bf8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 23 18:52:35.732: INFO: Pod "test-cleanup-deployment-55bbcbc84c-8zf5r" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-8zf5r,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-8268,SelfLink:/api/v1/namespaces/deployment-8268/pods/test-cleanup-deployment-55bbcbc84c-8zf5r,UID:8c7aed43-d301-42f9-b9d2-eb2850d21633,ResourceVersion:16023,Generation:0,CreationTimestamp:2019-06-23 18:52:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 1b9f0f1e-2875-4762-9114-63d9bd3db0ef 0xc002d47207 0xc002d47208}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gw58l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gw58l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-gw58l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d47280} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d472a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:52:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:52:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:52:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:52:31 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.102,PodIP:10.40.0.2,StartTime:2019-06-23 18:52:31 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-23 18:52:33 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://bbf92776482d8359e3734ac3c0f92b635cb7c779be9ec076df38505ccc3fa998}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:52:35.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8268" for this suite.
Jun 23 18:52:41.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:52:41.839: INFO: namespace deployment-8268 deletion completed in 6.099193753s

• [SLOW TEST:14.218 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:52:41.840: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Jun 23 18:52:41.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 cluster-info'
Jun 23 18:52:41.919: INFO: stderr: ""
Jun 23 18:52:41.919: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:52:41.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3221" for this suite.
Jun 23 18:52:47.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:52:47.987: INFO: namespace kubectl-3221 deletion completed in 6.065984175s

• [SLOW TEST:6.148 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:52:47.988: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jun 23 18:52:48.008: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 23 18:52:48.012: INFO: Waiting for terminating namespaces to be deleted...
Jun 23 18:52:48.013: INFO: 
Logging pods the kubelet thinks is on node controlplane-1 before test
Jun 23 18:52:48.018: INFO: kube-scheduler-controlplane-1 from kube-system started at 2019-06-23 17:35:42 +0000 UTC (1 container statuses recorded)
Jun 23 18:52:48.018: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 23 18:52:48.018: INFO: sonobuoy-systemd-logs-daemon-set-97428795f2c54d48-9t4bv from heptio-sonobuoy started at 2019-06-23 17:37:16 +0000 UTC (2 container statuses recorded)
Jun 23 18:52:48.018: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 23 18:52:48.018: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 23 18:52:48.018: INFO: kube-controller-manager-controlplane-1 from kube-system started at 2019-06-23 17:35:42 +0000 UTC (1 container statuses recorded)
Jun 23 18:52:48.018: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 23 18:52:48.018: INFO: kube-proxy-g5w8t from kube-system started at 2019-06-23 17:36:08 +0000 UTC (1 container statuses recorded)
Jun 23 18:52:48.018: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 23 18:52:48.018: INFO: coredns-5c98db65d4-x58jr from kube-system started at 2019-06-23 17:36:39 +0000 UTC (1 container statuses recorded)
Jun 23 18:52:48.018: INFO: 	Container coredns ready: true, restart count 0
Jun 23 18:52:48.018: INFO: kube-apiserver-controlplane-1 from kube-system started at 2019-06-23 17:35:42 +0000 UTC (1 container statuses recorded)
Jun 23 18:52:48.018: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 23 18:52:48.018: INFO: weave-net-2ljj6 from kube-system started at 2019-06-23 17:36:08 +0000 UTC (2 container statuses recorded)
Jun 23 18:52:48.018: INFO: 	Container weave ready: true, restart count 0
Jun 23 18:52:48.018: INFO: 	Container weave-npc ready: true, restart count 0
Jun 23 18:52:48.018: INFO: etcd-controlplane-1 from kube-system started at 2019-06-23 17:35:42 +0000 UTC (1 container statuses recorded)
Jun 23 18:52:48.018: INFO: 	Container etcd ready: true, restart count 0
Jun 23 18:52:48.018: INFO: coredns-5c98db65d4-2c474 from kube-system started at 2019-06-23 17:36:39 +0000 UTC (1 container statuses recorded)
Jun 23 18:52:48.018: INFO: 	Container coredns ready: true, restart count 0
Jun 23 18:52:48.018: INFO: 
Logging pods the kubelet thinks is on node worker-1 before test
Jun 23 18:52:48.021: INFO: kube-proxy-vmf86 from kube-system started at 2019-06-23 17:36:12 +0000 UTC (1 container statuses recorded)
Jun 23 18:52:48.021: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 23 18:52:48.021: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-23 17:37:03 +0000 UTC (1 container statuses recorded)
Jun 23 18:52:48.021: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 23 18:52:48.021: INFO: weave-net-rh6sc from kube-system started at 2019-06-23 17:36:12 +0000 UTC (2 container statuses recorded)
Jun 23 18:52:48.021: INFO: 	Container weave ready: true, restart count 0
Jun 23 18:52:48.021: INFO: 	Container weave-npc ready: true, restart count 0
Jun 23 18:52:48.021: INFO: sonobuoy-systemd-logs-daemon-set-97428795f2c54d48-twnth from heptio-sonobuoy started at 2019-06-23 17:37:16 +0000 UTC (2 container statuses recorded)
Jun 23 18:52:48.021: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 23 18:52:48.021: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 23 18:52:48.021: INFO: 
Logging pods the kubelet thinks is on node worker-2 before test
Jun 23 18:52:48.025: INFO: kube-proxy-djmbj from kube-system started at 2019-06-23 17:36:11 +0000 UTC (1 container statuses recorded)
Jun 23 18:52:48.025: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 23 18:52:48.025: INFO: sonobuoy-e2e-job-25634377b43945a3 from heptio-sonobuoy started at 2019-06-23 17:37:15 +0000 UTC (2 container statuses recorded)
Jun 23 18:52:48.025: INFO: 	Container e2e ready: true, restart count 0
Jun 23 18:52:48.025: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 23 18:52:48.025: INFO: weave-net-6pxvd from kube-system started at 2019-06-23 17:36:11 +0000 UTC (2 container statuses recorded)
Jun 23 18:52:48.025: INFO: 	Container weave ready: true, restart count 0
Jun 23 18:52:48.025: INFO: 	Container weave-npc ready: true, restart count 0
Jun 23 18:52:48.025: INFO: sonobuoy-systemd-logs-daemon-set-97428795f2c54d48-v5ldn from heptio-sonobuoy started at 2019-06-23 17:37:15 +0000 UTC (2 container statuses recorded)
Jun 23 18:52:48.026: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 23 18:52:48.026: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node controlplane-1
STEP: verifying the node has the label node worker-1
STEP: verifying the node has the label node worker-2
Jun 23 18:52:48.050: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-1
Jun 23 18:52:48.050: INFO: Pod sonobuoy-e2e-job-25634377b43945a3 requesting resource cpu=0m on Node worker-2
Jun 23 18:52:48.051: INFO: Pod sonobuoy-systemd-logs-daemon-set-97428795f2c54d48-9t4bv requesting resource cpu=0m on Node controlplane-1
Jun 23 18:52:48.051: INFO: Pod sonobuoy-systemd-logs-daemon-set-97428795f2c54d48-twnth requesting resource cpu=0m on Node worker-1
Jun 23 18:52:48.051: INFO: Pod sonobuoy-systemd-logs-daemon-set-97428795f2c54d48-v5ldn requesting resource cpu=0m on Node worker-2
Jun 23 18:52:48.051: INFO: Pod coredns-5c98db65d4-2c474 requesting resource cpu=100m on Node controlplane-1
Jun 23 18:52:48.051: INFO: Pod coredns-5c98db65d4-x58jr requesting resource cpu=100m on Node controlplane-1
Jun 23 18:52:48.051: INFO: Pod etcd-controlplane-1 requesting resource cpu=0m on Node controlplane-1
Jun 23 18:52:48.051: INFO: Pod kube-apiserver-controlplane-1 requesting resource cpu=250m on Node controlplane-1
Jun 23 18:52:48.051: INFO: Pod kube-controller-manager-controlplane-1 requesting resource cpu=200m on Node controlplane-1
Jun 23 18:52:48.051: INFO: Pod kube-proxy-djmbj requesting resource cpu=0m on Node worker-2
Jun 23 18:52:48.051: INFO: Pod kube-proxy-g5w8t requesting resource cpu=0m on Node controlplane-1
Jun 23 18:52:48.051: INFO: Pod kube-proxy-vmf86 requesting resource cpu=0m on Node worker-1
Jun 23 18:52:48.051: INFO: Pod kube-scheduler-controlplane-1 requesting resource cpu=100m on Node controlplane-1
Jun 23 18:52:48.051: INFO: Pod weave-net-2ljj6 requesting resource cpu=20m on Node controlplane-1
Jun 23 18:52:48.051: INFO: Pod weave-net-6pxvd requesting resource cpu=20m on Node worker-2
Jun 23 18:52:48.051: INFO: Pod weave-net-rh6sc requesting resource cpu=20m on Node worker-1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-199c63a9-ac16-460a-91bb-f4ce89936147.15aae899ccaad0c5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1716/filler-pod-199c63a9-ac16-460a-91bb-f4ce89936147 to worker-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-199c63a9-ac16-460a-91bb-f4ce89936147.15aae89a11cf5376], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-199c63a9-ac16-460a-91bb-f4ce89936147.15aae89a169ddc0b], Reason = [Created], Message = [Created container filler-pod-199c63a9-ac16-460a-91bb-f4ce89936147]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-199c63a9-ac16-460a-91bb-f4ce89936147.15aae89a257b4e7f], Reason = [Started], Message = [Started container filler-pod-199c63a9-ac16-460a-91bb-f4ce89936147]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-323a4044-3a40-4c7b-8f04-0c2036887e75.15aae899cbebc9d9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1716/filler-pod-323a4044-3a40-4c7b-8f04-0c2036887e75 to worker-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-323a4044-3a40-4c7b-8f04-0c2036887e75.15aae89a07ea5f85], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-323a4044-3a40-4c7b-8f04-0c2036887e75.15aae89a0b6e3a72], Reason = [Created], Message = [Created container filler-pod-323a4044-3a40-4c7b-8f04-0c2036887e75]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-323a4044-3a40-4c7b-8f04-0c2036887e75.15aae89a19ec539d], Reason = [Started], Message = [Started container filler-pod-323a4044-3a40-4c7b-8f04-0c2036887e75]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60cf5f31-edcf-4374-bfb6-6a02988b1d1c.15aae899cb56c6cd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1716/filler-pod-60cf5f31-edcf-4374-bfb6-6a02988b1d1c to controlplane-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60cf5f31-edcf-4374-bfb6-6a02988b1d1c.15aae89a0812b749], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60cf5f31-edcf-4374-bfb6-6a02988b1d1c.15aae89a0ba39073], Reason = [Created], Message = [Created container filler-pod-60cf5f31-edcf-4374-bfb6-6a02988b1d1c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60cf5f31-edcf-4374-bfb6-6a02988b1d1c.15aae89a195f66b8], Reason = [Started], Message = [Started container filler-pod-60cf5f31-edcf-4374-bfb6-6a02988b1d1c]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15aae89abd33d6d9], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node controlplane-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:52:53.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1716" for this suite.
Jun 23 18:52:59.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:52:59.253: INFO: namespace sched-pred-1716 deletion completed in 6.059798214s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:11.266 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:52:59.253: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jun 23 18:52:59.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 create -f - --namespace=kubectl-5405'
Jun 23 18:52:59.374: INFO: stderr: ""
Jun 23 18:52:59.374: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 23 18:53:00.379: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 18:53:00.379: INFO: Found 0 / 1
Jun 23 18:53:01.380: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 18:53:01.380: INFO: Found 1 / 1
Jun 23 18:53:01.380: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 23 18:53:01.387: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 18:53:01.387: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 23 18:53:01.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 patch pod redis-master-wwdfh --namespace=kubectl-5405 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 23 18:53:01.471: INFO: stderr: ""
Jun 23 18:53:01.471: INFO: stdout: "pod/redis-master-wwdfh patched\n"
STEP: checking annotations
Jun 23 18:53:01.474: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 18:53:01.474: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:53:01.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5405" for this suite.
Jun 23 18:53:23.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:53:23.570: INFO: namespace kubectl-5405 deletion completed in 22.094708934s

• [SLOW TEST:24.317 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:53:23.573: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Jun 23 18:53:23.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 api-versions'
Jun 23 18:53:23.655: INFO: stderr: ""
Jun 23 18:53:23.655: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:53:23.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-669" for this suite.
Jun 23 18:53:29.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:53:29.756: INFO: namespace kubectl-669 deletion completed in 6.098548737s

• [SLOW TEST:6.183 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:53:29.756: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Jun 23 18:53:29.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 create -f - --namespace=kubectl-3079'
Jun 23 18:53:29.886: INFO: stderr: ""
Jun 23 18:53:29.886: INFO: stdout: "pod/pause created\n"
Jun 23 18:53:29.886: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 23 18:53:29.886: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3079" to be "running and ready"
Jun 23 18:53:29.889: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.295829ms
Jun 23 18:53:31.906: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020041646s
Jun 23 18:53:33.913: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.026595339s
Jun 23 18:53:33.913: INFO: Pod "pause" satisfied condition "running and ready"
Jun 23 18:53:33.913: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 23 18:53:33.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 label pods pause testing-label=testing-label-value --namespace=kubectl-3079'
Jun 23 18:53:33.967: INFO: stderr: ""
Jun 23 18:53:33.968: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 23 18:53:33.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pod pause -L testing-label --namespace=kubectl-3079'
Jun 23 18:53:34.021: INFO: stderr: ""
Jun 23 18:53:34.021: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 23 18:53:34.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 label pods pause testing-label- --namespace=kubectl-3079'
Jun 23 18:53:34.083: INFO: stderr: ""
Jun 23 18:53:34.083: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 23 18:53:34.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pod pause -L testing-label --namespace=kubectl-3079'
Jun 23 18:53:34.139: INFO: stderr: ""
Jun 23 18:53:34.139: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Jun 23 18:53:34.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete --grace-period=0 --force -f - --namespace=kubectl-3079'
Jun 23 18:53:34.198: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 23 18:53:34.198: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 23 18:53:34.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get rc,svc -l name=pause --no-headers --namespace=kubectl-3079'
Jun 23 18:53:34.251: INFO: stderr: "No resources found.\n"
Jun 23 18:53:34.251: INFO: stdout: ""
Jun 23 18:53:34.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -l name=pause --namespace=kubectl-3079 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 23 18:53:34.303: INFO: stderr: ""
Jun 23 18:53:34.303: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:53:34.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3079" for this suite.
Jun 23 18:53:40.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:53:40.376: INFO: namespace kubectl-3079 deletion completed in 6.070562775s

• [SLOW TEST:10.620 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:53:40.376: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 18:53:40.407: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bebf8667-54bf-417f-9b7d-6616a94e4997" in namespace "downward-api-670" to be "success or failure"
Jun 23 18:53:40.414: INFO: Pod "downwardapi-volume-bebf8667-54bf-417f-9b7d-6616a94e4997": Phase="Pending", Reason="", readiness=false. Elapsed: 6.134488ms
Jun 23 18:53:42.419: INFO: Pod "downwardapi-volume-bebf8667-54bf-417f-9b7d-6616a94e4997": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011961979s
Jun 23 18:53:44.425: INFO: Pod "downwardapi-volume-bebf8667-54bf-417f-9b7d-6616a94e4997": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017268944s
STEP: Saw pod success
Jun 23 18:53:44.425: INFO: Pod "downwardapi-volume-bebf8667-54bf-417f-9b7d-6616a94e4997" satisfied condition "success or failure"
Jun 23 18:53:44.431: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-bebf8667-54bf-417f-9b7d-6616a94e4997 container client-container: <nil>
STEP: delete the pod
Jun 23 18:53:44.454: INFO: Waiting for pod downwardapi-volume-bebf8667-54bf-417f-9b7d-6616a94e4997 to disappear
Jun 23 18:53:44.458: INFO: Pod downwardapi-volume-bebf8667-54bf-417f-9b7d-6616a94e4997 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:53:44.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-670" for this suite.
Jun 23 18:53:50.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:53:50.527: INFO: namespace downward-api-670 deletion completed in 6.065770475s

• [SLOW TEST:10.150 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:53:50.528: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 23 18:53:50.569: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-8898,SelfLink:/api/v1/namespaces/watch-8898/configmaps/e2e-watch-test-resource-version,UID:973d5bef-32f1-48ea-b19e-55b15706ac2d,ResourceVersion:16344,Generation:0,CreationTimestamp:2019-06-23 18:53:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 23 18:53:50.569: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-8898,SelfLink:/api/v1/namespaces/watch-8898/configmaps/e2e-watch-test-resource-version,UID:973d5bef-32f1-48ea-b19e-55b15706ac2d,ResourceVersion:16345,Generation:0,CreationTimestamp:2019-06-23 18:53:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:53:50.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8898" for this suite.
Jun 23 18:53:56.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:53:56.636: INFO: namespace watch-8898 deletion completed in 6.064250194s

• [SLOW TEST:6.108 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:53:56.636: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Jun 23 18:53:56.661: INFO: Waiting up to 5m0s for pod "var-expansion-b4f36593-df2b-459b-a163-84b95b76b8c3" in namespace "var-expansion-1104" to be "success or failure"
Jun 23 18:53:56.664: INFO: Pod "var-expansion-b4f36593-df2b-459b-a163-84b95b76b8c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.757675ms
Jun 23 18:53:58.671: INFO: Pod "var-expansion-b4f36593-df2b-459b-a163-84b95b76b8c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009654117s
STEP: Saw pod success
Jun 23 18:53:58.671: INFO: Pod "var-expansion-b4f36593-df2b-459b-a163-84b95b76b8c3" satisfied condition "success or failure"
Jun 23 18:53:58.673: INFO: Trying to get logs from node worker-1 pod var-expansion-b4f36593-df2b-459b-a163-84b95b76b8c3 container dapi-container: <nil>
STEP: delete the pod
Jun 23 18:53:58.703: INFO: Waiting for pod var-expansion-b4f36593-df2b-459b-a163-84b95b76b8c3 to disappear
Jun 23 18:53:58.707: INFO: Pod var-expansion-b4f36593-df2b-459b-a163-84b95b76b8c3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:53:58.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1104" for this suite.
Jun 23 18:54:04.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:54:04.814: INFO: namespace var-expansion-1104 deletion completed in 6.103144056s

• [SLOW TEST:8.178 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:54:04.815: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-0d4ec38b-3968-4a99-8ed4-5ceb63e86452
STEP: Creating a pod to test consume configMaps
Jun 23 18:54:04.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-e2a06e98-a3de-4f20-b5d7-38e69ee9d026" in namespace "configmap-8271" to be "success or failure"
Jun 23 18:54:04.852: INFO: Pod "pod-configmaps-e2a06e98-a3de-4f20-b5d7-38e69ee9d026": Phase="Pending", Reason="", readiness=false. Elapsed: 5.389263ms
Jun 23 18:54:06.858: INFO: Pod "pod-configmaps-e2a06e98-a3de-4f20-b5d7-38e69ee9d026": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011375229s
STEP: Saw pod success
Jun 23 18:54:06.858: INFO: Pod "pod-configmaps-e2a06e98-a3de-4f20-b5d7-38e69ee9d026" satisfied condition "success or failure"
Jun 23 18:54:06.863: INFO: Trying to get logs from node worker-1 pod pod-configmaps-e2a06e98-a3de-4f20-b5d7-38e69ee9d026 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 23 18:54:06.901: INFO: Waiting for pod pod-configmaps-e2a06e98-a3de-4f20-b5d7-38e69ee9d026 to disappear
Jun 23 18:54:06.905: INFO: Pod pod-configmaps-e2a06e98-a3de-4f20-b5d7-38e69ee9d026 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:54:06.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8271" for this suite.
Jun 23 18:54:12.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:54:12.974: INFO: namespace configmap-8271 deletion completed in 6.066451652s

• [SLOW TEST:8.159 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:54:12.975: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 18:54:12.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 version'
Jun 23 18:54:13.048: INFO: stderr: ""
Jun 23 18:54:13.048: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:40:16Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:32:14Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:54:13.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-904" for this suite.
Jun 23 18:54:19.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:54:19.147: INFO: namespace kubectl-904 deletion completed in 6.096520843s

• [SLOW TEST:6.172 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:54:19.147: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 23 18:54:19.174: INFO: Waiting up to 5m0s for pod "pod-d63716c7-d616-4d62-a151-603138086aea" in namespace "emptydir-2170" to be "success or failure"
Jun 23 18:54:19.182: INFO: Pod "pod-d63716c7-d616-4d62-a151-603138086aea": Phase="Pending", Reason="", readiness=false. Elapsed: 8.691518ms
Jun 23 18:54:21.195: INFO: Pod "pod-d63716c7-d616-4d62-a151-603138086aea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021226738s
Jun 23 18:54:23.202: INFO: Pod "pod-d63716c7-d616-4d62-a151-603138086aea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028626534s
STEP: Saw pod success
Jun 23 18:54:23.203: INFO: Pod "pod-d63716c7-d616-4d62-a151-603138086aea" satisfied condition "success or failure"
Jun 23 18:54:23.210: INFO: Trying to get logs from node worker-1 pod pod-d63716c7-d616-4d62-a151-603138086aea container test-container: <nil>
STEP: delete the pod
Jun 23 18:54:23.237: INFO: Waiting for pod pod-d63716c7-d616-4d62-a151-603138086aea to disappear
Jun 23 18:54:23.238: INFO: Pod pod-d63716c7-d616-4d62-a151-603138086aea no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:54:23.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2170" for this suite.
Jun 23 18:54:29.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:54:29.342: INFO: namespace emptydir-2170 deletion completed in 6.101880357s

• [SLOW TEST:10.195 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:54:29.343: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 18:54:29.405: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"37c7d259-7efc-4b70-8352-efc83586f931", Controller:(*bool)(0xc00296a34a), BlockOwnerDeletion:(*bool)(0xc00296a34b)}}
Jun 23 18:54:29.412: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a90a7ff7-bac5-4007-ac35-b306d7c01a90", Controller:(*bool)(0xc0014a17e6), BlockOwnerDeletion:(*bool)(0xc0014a17e7)}}
Jun 23 18:54:29.416: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"5bea81c3-32e2-42ab-ac2a-925d7b6dcc5b", Controller:(*bool)(0xc00296a556), BlockOwnerDeletion:(*bool)(0xc00296a557)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:54:34.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9236" for this suite.
Jun 23 18:54:40.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:54:40.565: INFO: namespace gc-9236 deletion completed in 6.098038375s

• [SLOW TEST:11.223 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:54:40.568: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-b6d71552-385f-45da-8d24-3efba9fbfdc5
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:54:42.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4100" for this suite.
Jun 23 18:55:04.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:55:04.750: INFO: namespace configmap-4100 deletion completed in 22.123608159s

• [SLOW TEST:24.182 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:55:04.751: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Jun 23 18:55:04.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 --namespace=kubectl-9243 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jun 23 18:55:06.928: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jun 23 18:55:06.928: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:55:08.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9243" for this suite.
Jun 23 18:55:14.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:55:15.047: INFO: namespace kubectl-9243 deletion completed in 6.097710999s

• [SLOW TEST:10.297 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:55:15.048: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Jun 23 18:55:15.071: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-309105263 proxy --unix-socket=/tmp/kubectl-proxy-unix168248662/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:55:15.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6642" for this suite.
Jun 23 18:55:21.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:55:21.204: INFO: namespace kubectl-6642 deletion completed in 6.09313109s

• [SLOW TEST:6.156 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:55:21.204: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Jun 23 18:55:21.759: INFO: created pod pod-service-account-defaultsa
Jun 23 18:55:21.759: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 23 18:55:21.782: INFO: created pod pod-service-account-mountsa
Jun 23 18:55:21.782: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 23 18:55:21.791: INFO: created pod pod-service-account-nomountsa
Jun 23 18:55:21.791: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 23 18:55:21.797: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 23 18:55:21.797: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 23 18:55:21.807: INFO: created pod pod-service-account-mountsa-mountspec
Jun 23 18:55:21.807: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 23 18:55:21.823: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 23 18:55:21.823: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 23 18:55:21.829: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 23 18:55:21.829: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 23 18:55:21.839: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 23 18:55:21.839: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 23 18:55:21.844: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 23 18:55:21.844: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:55:21.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2930" for this suite.
Jun 23 18:55:27.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:55:27.964: INFO: namespace svcaccounts-2930 deletion completed in 6.112769096s

• [SLOW TEST:6.760 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:55:27.965: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 18:55:27.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 create -f - --namespace=kubectl-4094'
Jun 23 18:55:28.087: INFO: stderr: ""
Jun 23 18:55:28.087: INFO: stdout: "replicationcontroller/redis-master created\n"
Jun 23 18:55:28.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 create -f - --namespace=kubectl-4094'
Jun 23 18:55:28.200: INFO: stderr: ""
Jun 23 18:55:28.200: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 23 18:55:29.205: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 18:55:29.205: INFO: Found 0 / 1
Jun 23 18:55:30.207: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 18:55:30.207: INFO: Found 1 / 1
Jun 23 18:55:30.207: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 23 18:55:30.213: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 18:55:30.213: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 23 18:55:30.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 describe pod redis-master-hcdcp --namespace=kubectl-4094'
Jun 23 18:55:30.318: INFO: stderr: ""
Jun 23 18:55:30.318: INFO: stdout: "Name:           redis-master-hcdcp\nNamespace:      kubectl-4094\nPriority:       0\nNode:           worker-1/192.168.5.101\nStart Time:     Sun, 23 Jun 2019 18:55:28 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             10.38.0.2\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://c66033edb93b40bb21e2cd15ffa2e2b12dbeac7d9175c0f6a168d1c213df2403\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 23 Jun 2019 18:55:29 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-x4vwl (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-x4vwl:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-x4vwl\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-4094/redis-master-hcdcp to worker-1\n  Normal  Pulled     1s    kubelet, worker-1  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, worker-1  Created container redis-master\n  Normal  Started    1s    kubelet, worker-1  Started container redis-master\n"
Jun 23 18:55:30.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 describe rc redis-master --namespace=kubectl-4094'
Jun 23 18:55:30.396: INFO: stderr: ""
Jun 23 18:55:30.396: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4094\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-hcdcp\n"
Jun 23 18:55:30.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 describe service redis-master --namespace=kubectl-4094'
Jun 23 18:55:30.458: INFO: stderr: ""
Jun 23 18:55:30.458: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4094\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.97.241.76\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.38.0.2:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 23 18:55:30.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 describe node controlplane-1'
Jun 23 18:55:30.524: INFO: stderr: ""
Jun 23 18:55:30.524: INFO: stdout: "Name:               controlplane-1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=controlplane-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 23 Jun 2019 17:35:48 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sun, 23 Jun 2019 17:36:42 +0000   Sun, 23 Jun 2019 17:36:42 +0000   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Sun, 23 Jun 2019 18:55:23 +0000   Sun, 23 Jun 2019 17:35:43 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sun, 23 Jun 2019 18:55:23 +0000   Sun, 23 Jun 2019 17:35:43 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sun, 23 Jun 2019 18:55:23 +0000   Sun, 23 Jun 2019 17:35:43 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sun, 23 Jun 2019 18:55:23 +0000   Sun, 23 Jun 2019 17:36:38 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.5.11\n  Hostname:    controlplane-1\nCapacity:\n cpu:                2\n ephemeral-storage:  64800356Ki\n hugepages-2Mi:      0\n memory:             2041132Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  59720007991\n hugepages-2Mi:      0\n memory:             1938732Ki\n pods:               110\nSystem Info:\n Machine ID:                 d52bfe167857422cbc96c9ad8a48733d\n System UUID:                80722441-0A96-4A27-95A8-9169888A3F02\n Boot ID:                    2f981621-8df3-4614-8af4-1593305cc19c\n Kernel Version:             4.15.0-29-generic\n OS Image:                   Ubuntu 18.04.1 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.5\n Kubelet Version:            v1.15.0\n Kube-Proxy Version:         v1.15.0\nPodCIDR:                     10.96.0.0/24\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-97428795f2c54d48-9t4bv    0 (0%)        0 (0%)      0 (0%)           0 (0%)         78m\n  kube-system                coredns-5c98db65d4-2c474                                   100m (5%)     0 (0%)      70Mi (3%)        170Mi (8%)     79m\n  kube-system                coredns-5c98db65d4-x58jr                                   100m (5%)     0 (0%)      70Mi (3%)        170Mi (8%)     79m\n  kube-system                etcd-controlplane-1                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         78m\n  kube-system                kube-apiserver-controlplane-1                              250m (12%)    0 (0%)      0 (0%)           0 (0%)         78m\n  kube-system                kube-controller-manager-controlplane-1                     200m (10%)    0 (0%)      0 (0%)           0 (0%)         78m\n  kube-system                kube-proxy-g5w8t                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                kube-scheduler-controlplane-1                              100m (5%)     0 (0%)      0 (0%)           0 (0%)         78m\n  kube-system                weave-net-2ljj6                                            20m (1%)      0 (0%)      0 (0%)           0 (0%)         79m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                770m (38%)  0 (0%)\n  memory             140Mi (7%)  340Mi (17%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Jun 23 18:55:30.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 describe namespace kubectl-4094'
Jun 23 18:55:30.581: INFO: stderr: ""
Jun 23 18:55:30.581: INFO: stdout: "Name:         kubectl-4094\nLabels:       e2e-framework=kubectl\n              e2e-run=049fc179-ac49-4759-ad65-b2168bdb0223\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:55:30.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4094" for this suite.
Jun 23 18:55:52.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:55:52.689: INFO: namespace kubectl-4094 deletion completed in 22.105984378s

• [SLOW TEST:24.724 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:55:52.689: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 23 18:55:52.714: INFO: Waiting up to 5m0s for pod "downward-api-a0939db5-acac-4b1d-bba9-df5500a9beb9" in namespace "downward-api-5260" to be "success or failure"
Jun 23 18:55:52.717: INFO: Pod "downward-api-a0939db5-acac-4b1d-bba9-df5500a9beb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.211518ms
Jun 23 18:55:54.780: INFO: Pod "downward-api-a0939db5-acac-4b1d-bba9-df5500a9beb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065624982s
Jun 23 18:55:56.787: INFO: Pod "downward-api-a0939db5-acac-4b1d-bba9-df5500a9beb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072042608s
STEP: Saw pod success
Jun 23 18:55:56.787: INFO: Pod "downward-api-a0939db5-acac-4b1d-bba9-df5500a9beb9" satisfied condition "success or failure"
Jun 23 18:55:56.791: INFO: Trying to get logs from node worker-2 pod downward-api-a0939db5-acac-4b1d-bba9-df5500a9beb9 container dapi-container: <nil>
STEP: delete the pod
Jun 23 18:55:56.829: INFO: Waiting for pod downward-api-a0939db5-acac-4b1d-bba9-df5500a9beb9 to disappear
Jun 23 18:55:56.831: INFO: Pod downward-api-a0939db5-acac-4b1d-bba9-df5500a9beb9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:55:56.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5260" for this suite.
Jun 23 18:56:02.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:56:02.899: INFO: namespace downward-api-5260 deletion completed in 6.065248407s

• [SLOW TEST:10.210 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:56:02.900: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3406
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-3406
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3406
Jun 23 18:56:02.935: INFO: Found 0 stateful pods, waiting for 1
Jun 23 18:56:12.942: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 23 18:56:12.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-3406 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 23 18:56:13.092: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 23 18:56:13.092: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 23 18:56:13.092: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 23 18:56:13.094: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 23 18:56:23.101: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 23 18:56:23.101: INFO: Waiting for statefulset status.replicas updated to 0
Jun 23 18:56:23.141: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jun 23 18:56:23.141: INFO: ss-0  worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  }]
Jun 23 18:56:23.141: INFO: ss-1            Pending         []
Jun 23 18:56:23.141: INFO: 
Jun 23 18:56:23.141: INFO: StatefulSet ss has not reached scale 3, at 2
Jun 23 18:56:24.144: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.979104333s
Jun 23 18:56:25.152: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.976465965s
Jun 23 18:56:26.167: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960955566s
Jun 23 18:56:27.174: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.953679822s
Jun 23 18:56:28.226: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.898518403s
Jun 23 18:56:29.229: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.893281294s
Jun 23 18:56:30.241: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.886547093s
Jun 23 18:56:31.247: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.879397256s
Jun 23 18:56:32.252: INFO: Verifying statefulset ss doesn't scale past 3 for another 872.50835ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3406
Jun 23 18:56:33.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-3406 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 23 18:56:33.424: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 23 18:56:33.424: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 23 18:56:33.424: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 23 18:56:33.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-3406 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 23 18:56:33.595: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 23 18:56:33.595: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 23 18:56:33.595: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 23 18:56:33.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-3406 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 23 18:56:33.747: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 23 18:56:33.747: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 23 18:56:33.747: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 23 18:56:33.749: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 23 18:56:33.749: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 23 18:56:33.749: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 23 18:56:33.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-3406 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 23 18:56:33.883: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 23 18:56:33.883: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 23 18:56:33.883: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 23 18:56:33.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-3406 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 23 18:56:34.052: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 23 18:56:34.052: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 23 18:56:34.052: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 23 18:56:34.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-3406 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 23 18:56:34.228: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 23 18:56:34.228: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 23 18:56:34.228: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 23 18:56:34.228: INFO: Waiting for statefulset status.replicas updated to 0
Jun 23 18:56:34.230: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun 23 18:56:44.241: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 23 18:56:44.241: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 23 18:56:44.241: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 23 18:56:44.257: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun 23 18:56:44.257: INFO: ss-0  worker-1        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  }]
Jun 23 18:56:44.257: INFO: ss-1  worker-2        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  }]
Jun 23 18:56:44.257: INFO: ss-2  controlplane-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  }]
Jun 23 18:56:44.257: INFO: 
Jun 23 18:56:44.257: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 23 18:56:45.267: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun 23 18:56:45.267: INFO: ss-0  worker-1        Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  }]
Jun 23 18:56:45.267: INFO: ss-1  worker-2        Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  }]
Jun 23 18:56:45.267: INFO: ss-2  controlplane-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  }]
Jun 23 18:56:45.267: INFO: 
Jun 23 18:56:45.267: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 23 18:56:46.275: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun 23 18:56:46.276: INFO: ss-0  worker-1        Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  }]
Jun 23 18:56:46.276: INFO: ss-1  worker-2        Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  }]
Jun 23 18:56:46.276: INFO: ss-2  controlplane-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  }]
Jun 23 18:56:46.276: INFO: 
Jun 23 18:56:46.276: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 23 18:56:47.283: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jun 23 18:56:47.283: INFO: ss-0  worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  }]
Jun 23 18:56:47.283: INFO: ss-1  worker-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  }]
Jun 23 18:56:47.283: INFO: 
Jun 23 18:56:47.283: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 23 18:56:48.291: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jun 23 18:56:48.291: INFO: ss-0  worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  }]
Jun 23 18:56:48.291: INFO: ss-1  worker-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  }]
Jun 23 18:56:48.291: INFO: 
Jun 23 18:56:48.291: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 23 18:56:49.298: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jun 23 18:56:49.298: INFO: ss-0  worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  }]
Jun 23 18:56:49.298: INFO: ss-1  worker-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  }]
Jun 23 18:56:49.298: INFO: 
Jun 23 18:56:49.298: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 23 18:56:50.300: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jun 23 18:56:50.300: INFO: ss-0  worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  }]
Jun 23 18:56:50.300: INFO: ss-1  worker-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:23 +0000 UTC  }]
Jun 23 18:56:50.300: INFO: 
Jun 23 18:56:50.300: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 23 18:56:51.308: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jun 23 18:56:51.308: INFO: ss-0  worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  }]
Jun 23 18:56:51.308: INFO: 
Jun 23 18:56:51.308: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 23 18:56:52.315: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jun 23 18:56:52.316: INFO: ss-0  worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:56:02 +0000 UTC  }]
Jun 23 18:56:52.316: INFO: 
Jun 23 18:56:52.316: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 23 18:56:53.343: INFO: Verifying statefulset ss doesn't scale past 0 for another 910.992204ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3406
Jun 23 18:56:54.351: INFO: Scaling statefulset ss to 0
Jun 23 18:56:54.369: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 23 18:56:54.375: INFO: Deleting all statefulset in ns statefulset-3406
Jun 23 18:56:54.381: INFO: Scaling statefulset ss to 0
Jun 23 18:56:54.397: INFO: Waiting for statefulset status.replicas updated to 0
Jun 23 18:56:54.399: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:56:54.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3406" for this suite.
Jun 23 18:57:00.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:57:00.493: INFO: namespace statefulset-3406 deletion completed in 6.079160773s

• [SLOW TEST:57.593 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:57:00.495: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-1d1b78a2-eca8-4cce-b651-56eafcbe91e5
STEP: Creating a pod to test consume configMaps
Jun 23 18:57:00.523: INFO: Waiting up to 5m0s for pod "pod-configmaps-9884f70a-bd73-4d09-b983-83fe33d65a30" in namespace "configmap-5227" to be "success or failure"
Jun 23 18:57:00.531: INFO: Pod "pod-configmaps-9884f70a-bd73-4d09-b983-83fe33d65a30": Phase="Pending", Reason="", readiness=false. Elapsed: 7.506646ms
Jun 23 18:57:02.537: INFO: Pod "pod-configmaps-9884f70a-bd73-4d09-b983-83fe33d65a30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01396267s
Jun 23 18:57:04.544: INFO: Pod "pod-configmaps-9884f70a-bd73-4d09-b983-83fe33d65a30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020781261s
STEP: Saw pod success
Jun 23 18:57:04.544: INFO: Pod "pod-configmaps-9884f70a-bd73-4d09-b983-83fe33d65a30" satisfied condition "success or failure"
Jun 23 18:57:04.551: INFO: Trying to get logs from node worker-1 pod pod-configmaps-9884f70a-bd73-4d09-b983-83fe33d65a30 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 23 18:57:04.593: INFO: Waiting for pod pod-configmaps-9884f70a-bd73-4d09-b983-83fe33d65a30 to disappear
Jun 23 18:57:04.596: INFO: Pod pod-configmaps-9884f70a-bd73-4d09-b983-83fe33d65a30 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:57:04.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5227" for this suite.
Jun 23 18:57:10.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:57:10.669: INFO: namespace configmap-5227 deletion completed in 6.068219112s

• [SLOW TEST:10.174 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:57:10.669: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 23 18:57:10.694: INFO: Waiting up to 5m0s for pod "pod-d027b895-ac9b-4a03-9c2d-d661d8cd0969" in namespace "emptydir-7927" to be "success or failure"
Jun 23 18:57:10.699: INFO: Pod "pod-d027b895-ac9b-4a03-9c2d-d661d8cd0969": Phase="Pending", Reason="", readiness=false. Elapsed: 4.402985ms
Jun 23 18:57:12.706: INFO: Pod "pod-d027b895-ac9b-4a03-9c2d-d661d8cd0969": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011095088s
Jun 23 18:57:14.711: INFO: Pod "pod-d027b895-ac9b-4a03-9c2d-d661d8cd0969": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01697004s
STEP: Saw pod success
Jun 23 18:57:14.712: INFO: Pod "pod-d027b895-ac9b-4a03-9c2d-d661d8cd0969" satisfied condition "success or failure"
Jun 23 18:57:14.717: INFO: Trying to get logs from node worker-1 pod pod-d027b895-ac9b-4a03-9c2d-d661d8cd0969 container test-container: <nil>
STEP: delete the pod
Jun 23 18:57:14.754: INFO: Waiting for pod pod-d027b895-ac9b-4a03-9c2d-d661d8cd0969 to disappear
Jun 23 18:57:14.758: INFO: Pod pod-d027b895-ac9b-4a03-9c2d-d661d8cd0969 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:57:14.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7927" for this suite.
Jun 23 18:57:20.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:57:20.827: INFO: namespace emptydir-7927 deletion completed in 6.065114446s

• [SLOW TEST:10.159 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:57:20.828: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 18:57:20.847: INFO: Creating deployment "test-recreate-deployment"
Jun 23 18:57:20.851: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 23 18:57:20.856: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 23 18:57:22.863: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 23 18:57:22.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696913040, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696913040, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696913040, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696913040, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 18:57:24.872: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 23 18:57:24.877: INFO: Updating deployment test-recreate-deployment
Jun 23 18:57:24.877: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 23 18:57:24.949: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-388,SelfLink:/apis/apps/v1/namespaces/deployment-388/deployments/test-recreate-deployment,UID:ae57e58a-1073-4622-831e-721da8409f29,ResourceVersion:17270,Generation:2,CreationTimestamp:2019-06-23 18:57:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-06-23 18:57:24 +0000 UTC 2019-06-23 18:57:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-06-23 18:57:24 +0000 UTC 2019-06-23 18:57:20 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jun 23 18:57:24.951: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-388,SelfLink:/apis/apps/v1/namespaces/deployment-388/replicasets/test-recreate-deployment-5c8c9cc69d,UID:f6cd17b7-4c69-4529-8058-8ccdff59d063,ResourceVersion:17268,Generation:1,CreationTimestamp:2019-06-23 18:57:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ae57e58a-1073-4622-831e-721da8409f29 0xc002b807c7 0xc002b807c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 23 18:57:24.951: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 23 18:57:24.951: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-388,SelfLink:/apis/apps/v1/namespaces/deployment-388/replicasets/test-recreate-deployment-6df85df6b9,UID:f097c8dc-5146-4ede-845b-ddfbafdc32e4,ResourceVersion:17258,Generation:2,CreationTimestamp:2019-06-23 18:57:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ae57e58a-1073-4622-831e-721da8409f29 0xc002b80897 0xc002b80898}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 23 18:57:24.953: INFO: Pod "test-recreate-deployment-5c8c9cc69d-xfqww" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-xfqww,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-388,SelfLink:/api/v1/namespaces/deployment-388/pods/test-recreate-deployment-5c8c9cc69d-xfqww,UID:3d3b9f7d-35d0-4012-a9d1-89e3a15189b4,ResourceVersion:17269,Generation:0,CreationTimestamp:2019-06-23 18:57:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d f6cd17b7-4c69-4529-8058-8ccdff59d063 0xc002b811b7 0xc002b811b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n5q54 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n5q54,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n5q54 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b81230} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b81250}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:57:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:57:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:57:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 18:57:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:,StartTime:2019-06-23 18:57:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:57:24.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-388" for this suite.
Jun 23 18:57:30.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:57:31.021: INFO: namespace deployment-388 deletion completed in 6.066074147s

• [SLOW TEST:10.193 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:57:31.021: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 23 18:57:35.629: INFO: Successfully updated pod "annotationupdate13735d37-6d59-44b7-86b9-c40d9bd84a49"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:57:37.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3846" for this suite.
Jun 23 18:57:59.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:57:59.784: INFO: namespace projected-3846 deletion completed in 22.102531789s

• [SLOW TEST:28.763 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:57:59.784: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 23 18:58:04.339: INFO: Successfully updated pod "pod-update-activedeadlineseconds-52c78ded-12b3-4245-a19c-0384e9f39ece"
Jun 23 18:58:04.339: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-52c78ded-12b3-4245-a19c-0384e9f39ece" in namespace "pods-6288" to be "terminated due to deadline exceeded"
Jun 23 18:58:04.343: INFO: Pod "pod-update-activedeadlineseconds-52c78ded-12b3-4245-a19c-0384e9f39ece": Phase="Running", Reason="", readiness=true. Elapsed: 3.97641ms
Jun 23 18:58:06.350: INFO: Pod "pod-update-activedeadlineseconds-52c78ded-12b3-4245-a19c-0384e9f39ece": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.010293282s
Jun 23 18:58:06.350: INFO: Pod "pod-update-activedeadlineseconds-52c78ded-12b3-4245-a19c-0384e9f39ece" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:58:06.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6288" for this suite.
Jun 23 18:58:12.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:58:12.443: INFO: namespace pods-6288 deletion completed in 6.08569587s

• [SLOW TEST:12.659 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:58:12.443: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 23 18:58:14.996: INFO: Successfully updated pod "pod-update-cc658731-eb5b-4907-a2d2-ce5a656178b4"
STEP: verifying the updated pod is in kubernetes
Jun 23 18:58:15.008: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:58:15.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4010" for this suite.
Jun 23 18:58:37.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:58:37.085: INFO: namespace pods-4010 deletion completed in 22.069655218s

• [SLOW TEST:24.642 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:58:37.086: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 23 18:58:43.159: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0623 18:58:43.159844      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:58:43.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4645" for this suite.
Jun 23 18:58:49.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:58:49.274: INFO: namespace gc-4645 deletion completed in 6.105859592s

• [SLOW TEST:12.188 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:58:49.274: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-a0b0447f-9d67-4ca9-ad29-0555951dff95
STEP: Creating a pod to test consume secrets
Jun 23 18:58:49.306: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ef7afca-d176-42d9-912c-bc9023c6047a" in namespace "projected-6046" to be "success or failure"
Jun 23 18:58:49.316: INFO: Pod "pod-projected-secrets-6ef7afca-d176-42d9-912c-bc9023c6047a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.698246ms
Jun 23 18:58:51.322: INFO: Pod "pod-projected-secrets-6ef7afca-d176-42d9-912c-bc9023c6047a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016008487s
STEP: Saw pod success
Jun 23 18:58:51.322: INFO: Pod "pod-projected-secrets-6ef7afca-d176-42d9-912c-bc9023c6047a" satisfied condition "success or failure"
Jun 23 18:58:51.324: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-6ef7afca-d176-42d9-912c-bc9023c6047a container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 23 18:58:51.340: INFO: Waiting for pod pod-projected-secrets-6ef7afca-d176-42d9-912c-bc9023c6047a to disappear
Jun 23 18:58:51.341: INFO: Pod pod-projected-secrets-6ef7afca-d176-42d9-912c-bc9023c6047a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:58:51.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6046" for this suite.
Jun 23 18:58:57.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:58:57.408: INFO: namespace projected-6046 deletion completed in 6.063452484s

• [SLOW TEST:8.133 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:58:57.408: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 18:58:57.428: INFO: Creating ReplicaSet my-hostname-basic-92c93b2c-266a-4a10-9055-e57965b7348e
Jun 23 18:58:57.440: INFO: Pod name my-hostname-basic-92c93b2c-266a-4a10-9055-e57965b7348e: Found 0 pods out of 1
Jun 23 18:59:02.454: INFO: Pod name my-hostname-basic-92c93b2c-266a-4a10-9055-e57965b7348e: Found 1 pods out of 1
Jun 23 18:59:02.454: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-92c93b2c-266a-4a10-9055-e57965b7348e" is running
Jun 23 18:59:02.459: INFO: Pod "my-hostname-basic-92c93b2c-266a-4a10-9055-e57965b7348e-7z7m2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-23 18:58:57 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-23 18:58:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-23 18:58:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-23 18:58:57 +0000 UTC Reason: Message:}])
Jun 23 18:59:02.459: INFO: Trying to dial the pod
Jun 23 18:59:07.481: INFO: Controller my-hostname-basic-92c93b2c-266a-4a10-9055-e57965b7348e: Got expected result from replica 1 [my-hostname-basic-92c93b2c-266a-4a10-9055-e57965b7348e-7z7m2]: "my-hostname-basic-92c93b2c-266a-4a10-9055-e57965b7348e-7z7m2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:59:07.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-977" for this suite.
Jun 23 18:59:13.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:59:13.547: INFO: namespace replicaset-977 deletion completed in 6.063143971s

• [SLOW TEST:16.139 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:59:13.548: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 23 18:59:13.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-4464'
Jun 23 18:59:13.625: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 23 18:59:13.625: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jun 23 18:59:13.634: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jun 23 18:59:13.637: INFO: scanned /root for discovery docs: <nil>
Jun 23 18:59:13.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-4464'
Jun 23 18:59:29.557: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 23 18:59:29.557: INFO: stdout: "Created e2e-test-nginx-rc-9ebb108aca73b4813ef4ad7a765a7aef\nScaling up e2e-test-nginx-rc-9ebb108aca73b4813ef4ad7a765a7aef from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-9ebb108aca73b4813ef4ad7a765a7aef up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-9ebb108aca73b4813ef4ad7a765a7aef to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jun 23 18:59:29.557: INFO: stdout: "Created e2e-test-nginx-rc-9ebb108aca73b4813ef4ad7a765a7aef\nScaling up e2e-test-nginx-rc-9ebb108aca73b4813ef4ad7a765a7aef from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-9ebb108aca73b4813ef4ad7a765a7aef up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-9ebb108aca73b4813ef4ad7a765a7aef to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jun 23 18:59:29.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-4464'
Jun 23 18:59:29.629: INFO: stderr: ""
Jun 23 18:59:29.629: INFO: stdout: "e2e-test-nginx-rc-9ebb108aca73b4813ef4ad7a765a7aef-82jff "
Jun 23 18:59:29.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods e2e-test-nginx-rc-9ebb108aca73b4813ef4ad7a765a7aef-82jff -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4464'
Jun 23 18:59:29.679: INFO: stderr: ""
Jun 23 18:59:29.679: INFO: stdout: "true"
Jun 23 18:59:29.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods e2e-test-nginx-rc-9ebb108aca73b4813ef4ad7a765a7aef-82jff -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4464'
Jun 23 18:59:29.725: INFO: stderr: ""
Jun 23 18:59:29.725: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jun 23 18:59:29.725: INFO: e2e-test-nginx-rc-9ebb108aca73b4813ef4ad7a765a7aef-82jff is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Jun 23 18:59:29.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete rc e2e-test-nginx-rc --namespace=kubectl-4464'
Jun 23 18:59:29.793: INFO: stderr: ""
Jun 23 18:59:29.794: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:59:29.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4464" for this suite.
Jun 23 18:59:35.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:59:35.892: INFO: namespace kubectl-4464 deletion completed in 6.096213644s

• [SLOW TEST:22.344 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:59:35.893: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-60ed7cd4-b855-4916-bb3b-0ee6e1fbe0dc
STEP: Creating a pod to test consume secrets
Jun 23 18:59:35.924: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a56f9dc9-60dc-4f78-9738-8c5e76b74d98" in namespace "projected-722" to be "success or failure"
Jun 23 18:59:35.927: INFO: Pod "pod-projected-secrets-a56f9dc9-60dc-4f78-9738-8c5e76b74d98": Phase="Pending", Reason="", readiness=false. Elapsed: 3.007563ms
Jun 23 18:59:37.938: INFO: Pod "pod-projected-secrets-a56f9dc9-60dc-4f78-9738-8c5e76b74d98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014602599s
Jun 23 18:59:39.957: INFO: Pod "pod-projected-secrets-a56f9dc9-60dc-4f78-9738-8c5e76b74d98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033758647s
STEP: Saw pod success
Jun 23 18:59:39.957: INFO: Pod "pod-projected-secrets-a56f9dc9-60dc-4f78-9738-8c5e76b74d98" satisfied condition "success or failure"
Jun 23 18:59:39.963: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-a56f9dc9-60dc-4f78-9738-8c5e76b74d98 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 23 18:59:40.020: INFO: Waiting for pod pod-projected-secrets-a56f9dc9-60dc-4f78-9738-8c5e76b74d98 to disappear
Jun 23 18:59:40.023: INFO: Pod pod-projected-secrets-a56f9dc9-60dc-4f78-9738-8c5e76b74d98 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:59:40.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-722" for this suite.
Jun 23 18:59:46.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 18:59:46.112: INFO: namespace projected-722 deletion completed in 6.08609208s

• [SLOW TEST:10.220 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 18:59:46.112: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-f26sw in namespace proxy-3513
I0623 18:59:46.147814      20 runners.go:180] Created replication controller with name: proxy-service-f26sw, namespace: proxy-3513, replica count: 1
I0623 18:59:47.198854      20 runners.go:180] proxy-service-f26sw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0623 18:59:48.199229      20 runners.go:180] proxy-service-f26sw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0623 18:59:49.199881      20 runners.go:180] proxy-service-f26sw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0623 18:59:50.200271      20 runners.go:180] proxy-service-f26sw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0623 18:59:51.200516      20 runners.go:180] proxy-service-f26sw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0623 18:59:52.204545      20 runners.go:180] proxy-service-f26sw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0623 18:59:53.206874      20 runners.go:180] proxy-service-f26sw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0623 18:59:54.211126      20 runners.go:180] proxy-service-f26sw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0623 18:59:55.211477      20 runners.go:180] proxy-service-f26sw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0623 18:59:56.213335      20 runners.go:180] proxy-service-f26sw Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 23 18:59:56.219: INFO: setup took 10.086842713s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 23 18:59:56.234: INFO: (0) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 14.218638ms)
Jun 23 18:59:56.236: INFO: (0) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 16.051644ms)
Jun 23 18:59:56.236: INFO: (0) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 16.092195ms)
Jun 23 18:59:56.236: INFO: (0) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 16.029708ms)
Jun 23 18:59:56.236: INFO: (0) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 16.91158ms)
Jun 23 18:59:56.236: INFO: (0) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 16.59047ms)
Jun 23 18:59:56.236: INFO: (0) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 17.114532ms)
Jun 23 18:59:56.238: INFO: (0) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 18.296066ms)
Jun 23 18:59:56.238: INFO: (0) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 17.980767ms)
Jun 23 18:59:56.238: INFO: (0) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 17.94508ms)
Jun 23 18:59:56.238: INFO: (0) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 18.3216ms)
Jun 23 18:59:56.246: INFO: (0) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 26.117101ms)
Jun 23 18:59:56.247: INFO: (0) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 26.478184ms)
Jun 23 18:59:56.247: INFO: (0) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 26.800719ms)
Jun 23 18:59:56.247: INFO: (0) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 26.913044ms)
Jun 23 18:59:56.247: INFO: (0) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 27.581881ms)
Jun 23 18:59:56.252: INFO: (1) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 4.595477ms)
Jun 23 18:59:56.252: INFO: (1) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 4.816348ms)
Jun 23 18:59:56.252: INFO: (1) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 5.127059ms)
Jun 23 18:59:56.253: INFO: (1) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 5.14662ms)
Jun 23 18:59:56.253: INFO: (1) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 5.170796ms)
Jun 23 18:59:56.253: INFO: (1) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 5.175255ms)
Jun 23 18:59:56.253: INFO: (1) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 5.748592ms)
Jun 23 18:59:56.254: INFO: (1) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 6.48451ms)
Jun 23 18:59:56.254: INFO: (1) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 6.555293ms)
Jun 23 18:59:56.254: INFO: (1) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 6.596962ms)
Jun 23 18:59:56.254: INFO: (1) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 6.866858ms)
Jun 23 18:59:56.255: INFO: (1) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 7.273072ms)
Jun 23 18:59:56.256: INFO: (1) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 8.994155ms)
Jun 23 18:59:56.257: INFO: (1) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 9.417997ms)
Jun 23 18:59:56.257: INFO: (1) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 9.412751ms)
Jun 23 18:59:56.258: INFO: (1) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 10.315948ms)
Jun 23 18:59:56.263: INFO: (2) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 5.237415ms)
Jun 23 18:59:56.263: INFO: (2) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 5.541766ms)
Jun 23 18:59:56.265: INFO: (2) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 6.784967ms)
Jun 23 18:59:56.268: INFO: (2) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 9.687872ms)
Jun 23 18:59:56.268: INFO: (2) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 9.530313ms)
Jun 23 18:59:56.268: INFO: (2) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 9.691002ms)
Jun 23 18:59:56.268: INFO: (2) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 9.626933ms)
Jun 23 18:59:56.268: INFO: (2) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 10.065696ms)
Jun 23 18:59:56.268: INFO: (2) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 10.04544ms)
Jun 23 18:59:56.268: INFO: (2) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 9.810052ms)
Jun 23 18:59:56.268: INFO: (2) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 9.976762ms)
Jun 23 18:59:56.268: INFO: (2) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 10.310996ms)
Jun 23 18:59:56.268: INFO: (2) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 10.382354ms)
Jun 23 18:59:56.268: INFO: (2) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 10.380611ms)
Jun 23 18:59:56.268: INFO: (2) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 10.469153ms)
Jun 23 18:59:56.269: INFO: (2) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 10.561729ms)
Jun 23 18:59:56.275: INFO: (3) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 6.469604ms)
Jun 23 18:59:56.277: INFO: (3) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 8.380501ms)
Jun 23 18:59:56.278: INFO: (3) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 8.732584ms)
Jun 23 18:59:56.278: INFO: (3) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 9.2593ms)
Jun 23 18:59:56.278: INFO: (3) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 9.58054ms)
Jun 23 18:59:56.278: INFO: (3) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 9.482684ms)
Jun 23 18:59:56.278: INFO: (3) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 9.365912ms)
Jun 23 18:59:56.279: INFO: (3) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 9.702744ms)
Jun 23 18:59:56.279: INFO: (3) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 9.698732ms)
Jun 23 18:59:56.280: INFO: (3) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 11.229384ms)
Jun 23 18:59:56.281: INFO: (3) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 12.123671ms)
Jun 23 18:59:56.281: INFO: (3) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 11.826055ms)
Jun 23 18:59:56.281: INFO: (3) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 12.275409ms)
Jun 23 18:59:56.281: INFO: (3) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 12.070766ms)
Jun 23 18:59:56.281: INFO: (3) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 12.05373ms)
Jun 23 18:59:56.281: INFO: (3) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 12.382373ms)
Jun 23 18:59:56.290: INFO: (4) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 8.27851ms)
Jun 23 18:59:56.290: INFO: (4) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 8.053714ms)
Jun 23 18:59:56.290: INFO: (4) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 8.905552ms)
Jun 23 18:59:56.291: INFO: (4) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 8.565589ms)
Jun 23 18:59:56.291: INFO: (4) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 8.621455ms)
Jun 23 18:59:56.291: INFO: (4) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 9.3047ms)
Jun 23 18:59:56.291: INFO: (4) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 8.793008ms)
Jun 23 18:59:56.291: INFO: (4) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 9.050485ms)
Jun 23 18:59:56.293: INFO: (4) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 11.133374ms)
Jun 23 18:59:56.293: INFO: (4) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 11.422396ms)
Jun 23 18:59:56.293: INFO: (4) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 11.83466ms)
Jun 23 18:59:56.293: INFO: (4) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 11.669208ms)
Jun 23 18:59:56.294: INFO: (4) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 11.990143ms)
Jun 23 18:59:56.294: INFO: (4) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 12.003273ms)
Jun 23 18:59:56.294: INFO: (4) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 12.214521ms)
Jun 23 18:59:56.294: INFO: (4) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 11.880641ms)
Jun 23 18:59:56.299: INFO: (5) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 4.525149ms)
Jun 23 18:59:56.300: INFO: (5) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 5.706129ms)
Jun 23 18:59:56.300: INFO: (5) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 5.015917ms)
Jun 23 18:59:56.300: INFO: (5) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 5.187354ms)
Jun 23 18:59:56.300: INFO: (5) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 4.790868ms)
Jun 23 18:59:56.300: INFO: (5) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 5.933372ms)
Jun 23 18:59:56.301: INFO: (5) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 6.402411ms)
Jun 23 18:59:56.301: INFO: (5) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 6.461269ms)
Jun 23 18:59:56.301: INFO: (5) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 6.640029ms)
Jun 23 18:59:56.302: INFO: (5) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 6.798563ms)
Jun 23 18:59:56.302: INFO: (5) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 7.359423ms)
Jun 23 18:59:56.303: INFO: (5) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 7.788011ms)
Jun 23 18:59:56.303: INFO: (5) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 9.226218ms)
Jun 23 18:59:56.303: INFO: (5) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 8.556444ms)
Jun 23 18:59:56.304: INFO: (5) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 9.315753ms)
Jun 23 18:59:56.304: INFO: (5) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 10.019242ms)
Jun 23 18:59:56.308: INFO: (6) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 3.559428ms)
Jun 23 18:59:56.308: INFO: (6) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 3.540991ms)
Jun 23 18:59:56.310: INFO: (6) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 5.589843ms)
Jun 23 18:59:56.310: INFO: (6) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 5.402771ms)
Jun 23 18:59:56.310: INFO: (6) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 6.041233ms)
Jun 23 18:59:56.310: INFO: (6) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 6.140079ms)
Jun 23 18:59:56.311: INFO: (6) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 6.208845ms)
Jun 23 18:59:56.311: INFO: (6) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 6.359475ms)
Jun 23 18:59:56.311: INFO: (6) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 6.247883ms)
Jun 23 18:59:56.311: INFO: (6) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 6.28797ms)
Jun 23 18:59:56.311: INFO: (6) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 6.770797ms)
Jun 23 18:59:56.313: INFO: (6) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 8.321705ms)
Jun 23 18:59:56.313: INFO: (6) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 8.690903ms)
Jun 23 18:59:56.313: INFO: (6) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 8.893991ms)
Jun 23 18:59:56.313: INFO: (6) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 8.81974ms)
Jun 23 18:59:56.314: INFO: (6) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 9.1325ms)
Jun 23 18:59:56.317: INFO: (7) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 3.613426ms)
Jun 23 18:59:56.317: INFO: (7) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 3.805091ms)
Jun 23 18:59:56.319: INFO: (7) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 4.85575ms)
Jun 23 18:59:56.319: INFO: (7) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 4.721979ms)
Jun 23 18:59:56.322: INFO: (7) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 7.533367ms)
Jun 23 18:59:56.322: INFO: (7) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 7.771518ms)
Jun 23 18:59:56.322: INFO: (7) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 7.720573ms)
Jun 23 18:59:56.322: INFO: (7) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 7.924357ms)
Jun 23 18:59:56.322: INFO: (7) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 7.849183ms)
Jun 23 18:59:56.322: INFO: (7) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 7.727922ms)
Jun 23 18:59:56.322: INFO: (7) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 7.895665ms)
Jun 23 18:59:56.322: INFO: (7) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 8.004748ms)
Jun 23 18:59:56.323: INFO: (7) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 8.620089ms)
Jun 23 18:59:56.323: INFO: (7) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 9.016057ms)
Jun 23 18:59:56.323: INFO: (7) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 8.881008ms)
Jun 23 18:59:56.323: INFO: (7) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 9.565323ms)
Jun 23 18:59:56.331: INFO: (8) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 7.011818ms)
Jun 23 18:59:56.331: INFO: (8) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 6.020049ms)
Jun 23 18:59:56.332: INFO: (8) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 8.348616ms)
Jun 23 18:59:56.332: INFO: (8) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 8.258272ms)
Jun 23 18:59:56.332: INFO: (8) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 7.793604ms)
Jun 23 18:59:56.333: INFO: (8) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 9.102688ms)
Jun 23 18:59:56.334: INFO: (8) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 10.125192ms)
Jun 23 18:59:56.335: INFO: (8) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 11.447499ms)
Jun 23 18:59:56.335: INFO: (8) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 10.853612ms)
Jun 23 18:59:56.336: INFO: (8) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 11.594969ms)
Jun 23 18:59:56.336: INFO: (8) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 11.763564ms)
Jun 23 18:59:56.336: INFO: (8) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 12.283413ms)
Jun 23 18:59:56.336: INFO: (8) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 12.13284ms)
Jun 23 18:59:56.336: INFO: (8) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 12.102676ms)
Jun 23 18:59:56.337: INFO: (8) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 12.034128ms)
Jun 23 18:59:56.337: INFO: (8) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 12.56996ms)
Jun 23 18:59:56.344: INFO: (9) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 6.798436ms)
Jun 23 18:59:56.344: INFO: (9) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 6.054097ms)
Jun 23 18:59:56.344: INFO: (9) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 7.144702ms)
Jun 23 18:59:56.346: INFO: (9) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 8.104662ms)
Jun 23 18:59:56.346: INFO: (9) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 8.233753ms)
Jun 23 18:59:56.346: INFO: (9) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 9.014834ms)
Jun 23 18:59:56.346: INFO: (9) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 9.238212ms)
Jun 23 18:59:56.346: INFO: (9) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 8.945732ms)
Jun 23 18:59:56.347: INFO: (9) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 9.07236ms)
Jun 23 18:59:56.347: INFO: (9) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 9.856004ms)
Jun 23 18:59:56.347: INFO: (9) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 9.761223ms)
Jun 23 18:59:56.348: INFO: (9) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 10.725112ms)
Jun 23 18:59:56.348: INFO: (9) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 10.390286ms)
Jun 23 18:59:56.348: INFO: (9) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 10.831967ms)
Jun 23 18:59:56.348: INFO: (9) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 10.070229ms)
Jun 23 18:59:56.348: INFO: (9) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 10.497521ms)
Jun 23 18:59:56.352: INFO: (10) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 3.675639ms)
Jun 23 18:59:56.352: INFO: (10) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 3.604457ms)
Jun 23 18:59:56.352: INFO: (10) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 3.835968ms)
Jun 23 18:59:56.352: INFO: (10) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 3.961451ms)
Jun 23 18:59:56.352: INFO: (10) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 3.95194ms)
Jun 23 18:59:56.354: INFO: (10) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 5.980598ms)
Jun 23 18:59:56.356: INFO: (10) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 7.130176ms)
Jun 23 18:59:56.357: INFO: (10) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 8.867025ms)
Jun 23 18:59:56.357: INFO: (10) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 9.140843ms)
Jun 23 18:59:56.358: INFO: (10) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 9.238081ms)
Jun 23 18:59:56.358: INFO: (10) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 9.454514ms)
Jun 23 18:59:56.359: INFO: (10) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 10.015288ms)
Jun 23 18:59:56.359: INFO: (10) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 10.444958ms)
Jun 23 18:59:56.359: INFO: (10) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 10.706607ms)
Jun 23 18:59:56.359: INFO: (10) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 10.650739ms)
Jun 23 18:59:56.359: INFO: (10) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 10.714443ms)
Jun 23 18:59:56.365: INFO: (11) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 4.924659ms)
Jun 23 18:59:56.366: INFO: (11) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 5.984543ms)
Jun 23 18:59:56.367: INFO: (11) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 6.988433ms)
Jun 23 18:59:56.368: INFO: (11) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 7.804652ms)
Jun 23 18:59:56.368: INFO: (11) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 8.022201ms)
Jun 23 18:59:56.368: INFO: (11) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 8.624414ms)
Jun 23 18:59:56.369: INFO: (11) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 8.853887ms)
Jun 23 18:59:56.369: INFO: (11) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 8.961993ms)
Jun 23 18:59:56.369: INFO: (11) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 8.996204ms)
Jun 23 18:59:56.370: INFO: (11) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 10.058238ms)
Jun 23 18:59:56.372: INFO: (11) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 12.906351ms)
Jun 23 18:59:56.372: INFO: (11) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 12.78582ms)
Jun 23 18:59:56.373: INFO: (11) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 13.073153ms)
Jun 23 18:59:56.373: INFO: (11) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 13.112464ms)
Jun 23 18:59:56.373: INFO: (11) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 13.510461ms)
Jun 23 18:59:56.373: INFO: (11) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 13.994887ms)
Jun 23 18:59:56.378: INFO: (12) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 4.215158ms)
Jun 23 18:59:56.381: INFO: (12) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 7.807697ms)
Jun 23 18:59:56.381: INFO: (12) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 7.305306ms)
Jun 23 18:59:56.381: INFO: (12) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 7.470227ms)
Jun 23 18:59:56.382: INFO: (12) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 7.998676ms)
Jun 23 18:59:56.382: INFO: (12) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 7.849107ms)
Jun 23 18:59:56.382: INFO: (12) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 8.005085ms)
Jun 23 18:59:56.387: INFO: (12) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 12.426763ms)
Jun 23 18:59:56.387: INFO: (12) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 13.251889ms)
Jun 23 18:59:56.387: INFO: (12) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 13.500024ms)
Jun 23 18:59:56.388: INFO: (12) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 13.836443ms)
Jun 23 18:59:56.388: INFO: (12) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 14.090371ms)
Jun 23 18:59:56.388: INFO: (12) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 13.986165ms)
Jun 23 18:59:56.388: INFO: (12) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 13.835596ms)
Jun 23 18:59:56.389: INFO: (12) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 15.845264ms)
Jun 23 18:59:56.389: INFO: (12) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 15.390551ms)
Jun 23 18:59:56.396: INFO: (13) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 5.644098ms)
Jun 23 18:59:56.396: INFO: (13) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 6.882657ms)
Jun 23 18:59:56.397: INFO: (13) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 7.714987ms)
Jun 23 18:59:56.397: INFO: (13) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 7.664076ms)
Jun 23 18:59:56.398: INFO: (13) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 7.945813ms)
Jun 23 18:59:56.398: INFO: (13) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 7.967052ms)
Jun 23 18:59:56.399: INFO: (13) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 9.109978ms)
Jun 23 18:59:56.400: INFO: (13) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 9.430624ms)
Jun 23 18:59:56.400: INFO: (13) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 9.681918ms)
Jun 23 18:59:56.400: INFO: (13) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 10.478478ms)
Jun 23 18:59:56.403: INFO: (13) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 12.734427ms)
Jun 23 18:59:56.403: INFO: (13) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 12.276328ms)
Jun 23 18:59:56.403: INFO: (13) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 12.842201ms)
Jun 23 18:59:56.403: INFO: (13) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 13.060566ms)
Jun 23 18:59:56.403: INFO: (13) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 12.821767ms)
Jun 23 18:59:56.403: INFO: (13) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 13.486657ms)
Jun 23 18:59:56.408: INFO: (14) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 4.930361ms)
Jun 23 18:59:56.409: INFO: (14) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 5.230491ms)
Jun 23 18:59:56.409: INFO: (14) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 5.080993ms)
Jun 23 18:59:56.410: INFO: (14) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 6.819591ms)
Jun 23 18:59:56.411: INFO: (14) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 7.079632ms)
Jun 23 18:59:56.411: INFO: (14) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 7.254363ms)
Jun 23 18:59:56.412: INFO: (14) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 8.147943ms)
Jun 23 18:59:56.412: INFO: (14) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 8.018287ms)
Jun 23 18:59:56.413: INFO: (14) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 9.113302ms)
Jun 23 18:59:56.413: INFO: (14) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 9.388606ms)
Jun 23 18:59:56.413: INFO: (14) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 9.364796ms)
Jun 23 18:59:56.414: INFO: (14) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 10.151754ms)
Jun 23 18:59:56.414: INFO: (14) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 10.795096ms)
Jun 23 18:59:56.414: INFO: (14) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 10.74201ms)
Jun 23 18:59:56.414: INFO: (14) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 10.531337ms)
Jun 23 18:59:56.414: INFO: (14) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 10.923383ms)
Jun 23 18:59:56.419: INFO: (15) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 4.795591ms)
Jun 23 18:59:56.421: INFO: (15) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 6.379714ms)
Jun 23 18:59:56.421: INFO: (15) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 6.427751ms)
Jun 23 18:59:56.421: INFO: (15) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 6.484505ms)
Jun 23 18:59:56.422: INFO: (15) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 6.974215ms)
Jun 23 18:59:56.422: INFO: (15) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 7.166485ms)
Jun 23 18:59:56.422: INFO: (15) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 7.313687ms)
Jun 23 18:59:56.422: INFO: (15) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 7.395418ms)
Jun 23 18:59:56.422: INFO: (15) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 7.347528ms)
Jun 23 18:59:56.422: INFO: (15) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 7.66061ms)
Jun 23 18:59:56.422: INFO: (15) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 7.387978ms)
Jun 23 18:59:56.425: INFO: (15) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 10.128686ms)
Jun 23 18:59:56.425: INFO: (15) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 10.08956ms)
Jun 23 18:59:56.425: INFO: (15) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 10.117875ms)
Jun 23 18:59:56.425: INFO: (15) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 10.284938ms)
Jun 23 18:59:56.425: INFO: (15) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 10.405162ms)
Jun 23 18:59:56.429: INFO: (16) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 3.408829ms)
Jun 23 18:59:56.429: INFO: (16) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 3.502967ms)
Jun 23 18:59:56.431: INFO: (16) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 6.049663ms)
Jun 23 18:59:56.431: INFO: (16) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 6.016347ms)
Jun 23 18:59:56.431: INFO: (16) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 6.044083ms)
Jun 23 18:59:56.433: INFO: (16) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 7.323987ms)
Jun 23 18:59:56.433: INFO: (16) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 7.813031ms)
Jun 23 18:59:56.434: INFO: (16) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 8.180107ms)
Jun 23 18:59:56.434: INFO: (16) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 8.279987ms)
Jun 23 18:59:56.434: INFO: (16) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 8.336512ms)
Jun 23 18:59:56.434: INFO: (16) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 8.378681ms)
Jun 23 18:59:56.434: INFO: (16) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 8.498442ms)
Jun 23 18:59:56.434: INFO: (16) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 8.620204ms)
Jun 23 18:59:56.434: INFO: (16) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 8.449527ms)
Jun 23 18:59:56.434: INFO: (16) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 8.501205ms)
Jun 23 18:59:56.435: INFO: (16) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 9.443063ms)
Jun 23 18:59:56.441: INFO: (17) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 5.72997ms)
Jun 23 18:59:56.442: INFO: (17) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 6.387415ms)
Jun 23 18:59:56.442: INFO: (17) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 7.458921ms)
Jun 23 18:59:56.442: INFO: (17) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 6.636731ms)
Jun 23 18:59:56.442: INFO: (17) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 6.989634ms)
Jun 23 18:59:56.442: INFO: (17) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 6.540502ms)
Jun 23 18:59:56.442: INFO: (17) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 7.485964ms)
Jun 23 18:59:56.443: INFO: (17) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 7.885909ms)
Jun 23 18:59:56.443: INFO: (17) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 7.544512ms)
Jun 23 18:59:56.443: INFO: (17) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 7.182653ms)
Jun 23 18:59:56.444: INFO: (17) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 8.428293ms)
Jun 23 18:59:56.444: INFO: (17) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 8.6026ms)
Jun 23 18:59:56.444: INFO: (17) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 8.23026ms)
Jun 23 18:59:56.444: INFO: (17) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 8.87768ms)
Jun 23 18:59:56.445: INFO: (17) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 9.724893ms)
Jun 23 18:59:56.445: INFO: (17) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 9.44722ms)
Jun 23 18:59:56.453: INFO: (18) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 8.003202ms)
Jun 23 18:59:56.453: INFO: (18) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 7.796935ms)
Jun 23 18:59:56.453: INFO: (18) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 8.565559ms)
Jun 23 18:59:56.454: INFO: (18) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 8.849264ms)
Jun 23 18:59:56.454: INFO: (18) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 8.922504ms)
Jun 23 18:59:56.454: INFO: (18) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 9.185108ms)
Jun 23 18:59:56.454: INFO: (18) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 9.340997ms)
Jun 23 18:59:56.454: INFO: (18) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 9.431289ms)
Jun 23 18:59:56.455: INFO: (18) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 9.779336ms)
Jun 23 18:59:56.455: INFO: (18) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 9.873374ms)
Jun 23 18:59:56.455: INFO: (18) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 9.897017ms)
Jun 23 18:59:56.455: INFO: (18) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 9.916198ms)
Jun 23 18:59:56.455: INFO: (18) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 10.011325ms)
Jun 23 18:59:56.455: INFO: (18) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 9.951883ms)
Jun 23 18:59:56.455: INFO: (18) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 10.057748ms)
Jun 23 18:59:56.455: INFO: (18) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 9.816527ms)
Jun 23 18:59:56.459: INFO: (19) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">... (200; 3.234074ms)
Jun 23 18:59:56.459: INFO: (19) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:462/proxy/: tls qux (200; 3.48182ms)
Jun 23 18:59:56.461: INFO: (19) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 5.311846ms)
Jun 23 18:59:56.461: INFO: (19) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname2/proxy/: tls qux (200; 6.22047ms)
Jun 23 18:59:56.462: INFO: (19) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 6.61981ms)
Jun 23 18:59:56.462: INFO: (19) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:443/proxy/tlsrewritem... (200; 6.549722ms)
Jun 23 18:59:56.462: INFO: (19) /api/v1/namespaces/proxy-3513/pods/https:proxy-service-f26sw-4q4bf:460/proxy/: tls baz (200; 6.188395ms)
Jun 23 18:59:56.462: INFO: (19) /api/v1/namespaces/proxy-3513/services/https:proxy-service-f26sw:tlsportname1/proxy/: tls baz (200; 7.018863ms)
Jun 23 18:59:56.462: INFO: (19) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname2/proxy/: bar (200; 6.554381ms)
Jun 23 18:59:56.463: INFO: (19) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:160/proxy/: foo (200; 7.25389ms)
Jun 23 18:59:56.463: INFO: (19) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf:1080/proxy/rewriteme">test<... (200; 6.748474ms)
Jun 23 18:59:56.463: INFO: (19) /api/v1/namespaces/proxy-3513/pods/http:proxy-service-f26sw-4q4bf:162/proxy/: bar (200; 7.140785ms)
Jun 23 18:59:56.463: INFO: (19) /api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/: <a href="/api/v1/namespaces/proxy-3513/pods/proxy-service-f26sw-4q4bf/proxy/rewriteme">test</a> (200; 7.585486ms)
Jun 23 18:59:56.464: INFO: (19) /api/v1/namespaces/proxy-3513/services/http:proxy-service-f26sw:portname1/proxy/: foo (200; 8.390883ms)
Jun 23 18:59:56.464: INFO: (19) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname1/proxy/: foo (200; 8.229115ms)
Jun 23 18:59:56.464: INFO: (19) /api/v1/namespaces/proxy-3513/services/proxy-service-f26sw:portname2/proxy/: bar (200; 8.126768ms)
STEP: deleting ReplicationController proxy-service-f26sw in namespace proxy-3513, will wait for the garbage collector to delete the pods
Jun 23 18:59:56.539: INFO: Deleting ReplicationController proxy-service-f26sw took: 16.453825ms
Jun 23 18:59:56.839: INFO: Terminating ReplicationController proxy-service-f26sw pods took: 300.308663ms
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 18:59:58.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3513" for this suite.
Jun 23 19:00:04.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:00:04.746: INFO: namespace proxy-3513 deletion completed in 6.101348514s

• [SLOW TEST:18.634 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:00:04.747: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 19:00:04.781: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2ffabf3-10e8-46ec-b36b-9ad851ebd894" in namespace "projected-3962" to be "success or failure"
Jun 23 19:00:04.785: INFO: Pod "downwardapi-volume-a2ffabf3-10e8-46ec-b36b-9ad851ebd894": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045632ms
Jun 23 19:00:06.792: INFO: Pod "downwardapi-volume-a2ffabf3-10e8-46ec-b36b-9ad851ebd894": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010697426s
Jun 23 19:00:08.799: INFO: Pod "downwardapi-volume-a2ffabf3-10e8-46ec-b36b-9ad851ebd894": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017875425s
STEP: Saw pod success
Jun 23 19:00:08.799: INFO: Pod "downwardapi-volume-a2ffabf3-10e8-46ec-b36b-9ad851ebd894" satisfied condition "success or failure"
Jun 23 19:00:08.805: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-a2ffabf3-10e8-46ec-b36b-9ad851ebd894 container client-container: <nil>
STEP: delete the pod
Jun 23 19:00:08.843: INFO: Waiting for pod downwardapi-volume-a2ffabf3-10e8-46ec-b36b-9ad851ebd894 to disappear
Jun 23 19:00:08.847: INFO: Pod downwardapi-volume-a2ffabf3-10e8-46ec-b36b-9ad851ebd894 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:00:08.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3962" for this suite.
Jun 23 19:00:14.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:00:14.945: INFO: namespace projected-3962 deletion completed in 6.095523734s

• [SLOW TEST:10.199 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:00:14.945: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 23 19:00:23.026: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 23 19:00:23.033: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 23 19:00:25.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 23 19:00:25.039: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 23 19:00:27.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 23 19:00:27.040: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 23 19:00:29.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 23 19:00:29.039: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 23 19:00:31.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 23 19:00:31.044: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 23 19:00:33.043: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 23 19:00:33.049: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 23 19:00:35.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 23 19:00:35.042: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 23 19:00:37.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 23 19:00:37.040: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 23 19:00:39.043: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 23 19:00:39.049: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 23 19:00:41.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 23 19:00:41.040: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:00:41.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6338" for this suite.
Jun 23 19:01:03.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:01:03.172: INFO: namespace container-lifecycle-hook-6338 deletion completed in 22.104123253s

• [SLOW TEST:48.226 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:01:03.172: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 19:01:03.195: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 23 19:01:03.212: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 23 19:01:05.223: INFO: Creating deployment "test-rolling-update-deployment"
Jun 23 19:01:05.237: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 23 19:01:05.250: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 23 19:01:07.260: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 23 19:01:07.263: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696913265, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696913265, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696913265, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696913265, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 23 19:01:09.271: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 23 19:01:09.290: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-6896,SelfLink:/apis/apps/v1/namespaces/deployment-6896/deployments/test-rolling-update-deployment,UID:fdd8f965-c28f-4136-9f22-9a18027ba67d,ResourceVersion:18241,Generation:1,CreationTimestamp:2019-06-23 19:01:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-23 19:01:05 +0000 UTC 2019-06-23 19:01:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-23 19:01:07 +0000 UTC 2019-06-23 19:01:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 23 19:01:09.298: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-6896,SelfLink:/apis/apps/v1/namespaces/deployment-6896/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:94a846f5-1ae9-40ef-b457-d310558f0d30,ResourceVersion:18230,Generation:1,CreationTimestamp:2019-06-23 19:01:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment fdd8f965-c28f-4136-9f22-9a18027ba67d 0xc003f4e6d7 0xc003f4e6d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 23 19:01:09.298: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 23 19:01:09.298: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-6896,SelfLink:/apis/apps/v1/namespaces/deployment-6896/replicasets/test-rolling-update-controller,UID:7ae1f9f9-4ce3-4cee-8eb9-510bb9cc624a,ResourceVersion:18240,Generation:2,CreationTimestamp:2019-06-23 19:01:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment fdd8f965-c28f-4136-9f22-9a18027ba67d 0xc003f4e607 0xc003f4e608}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 23 19:01:09.310: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-2dp8z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-2dp8z,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-6896,SelfLink:/api/v1/namespaces/deployment-6896/pods/test-rolling-update-deployment-79f6b9d75c-2dp8z,UID:d37fdc7e-fc2c-43de-b08d-61846c37cae9,ResourceVersion:18229,Generation:0,CreationTimestamp:2019-06-23 19:01:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 94a846f5-1ae9-40ef-b457-d310558f0d30 0xc003f4efd7 0xc003f4efd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vftmj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vftmj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-vftmj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003f4f050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003f4f070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 19:01:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 19:01:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 19:01:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-23 19:01:05 +0000 UTC  }],Message:,Reason:,HostIP:192.168.5.101,PodIP:10.38.0.3,StartTime:2019-06-23 19:01:05 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-23 19:01:06 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://e0c3059f70ad553685221a3e609813bbed61a2173b98e48472481e8ae9887f42}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:01:09.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6896" for this suite.
Jun 23 19:01:15.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:01:15.429: INFO: namespace deployment-6896 deletion completed in 6.113457984s

• [SLOW TEST:12.257 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:01:15.430: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 19:01:15.462: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 23 19:01:15.468: INFO: Number of nodes with available pods: 0
Jun 23 19:01:15.468: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 23 19:01:15.478: INFO: Number of nodes with available pods: 0
Jun 23 19:01:15.478: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:16.485: INFO: Number of nodes with available pods: 0
Jun 23 19:01:16.485: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:17.481: INFO: Number of nodes with available pods: 1
Jun 23 19:01:17.481: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 23 19:01:17.491: INFO: Number of nodes with available pods: 1
Jun 23 19:01:17.491: INFO: Number of running nodes: 0, number of available pods: 1
Jun 23 19:01:18.500: INFO: Number of nodes with available pods: 0
Jun 23 19:01:18.500: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 23 19:01:18.522: INFO: Number of nodes with available pods: 0
Jun 23 19:01:18.522: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:19.528: INFO: Number of nodes with available pods: 0
Jun 23 19:01:19.529: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:20.525: INFO: Number of nodes with available pods: 0
Jun 23 19:01:20.525: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:21.526: INFO: Number of nodes with available pods: 0
Jun 23 19:01:21.526: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:22.527: INFO: Number of nodes with available pods: 0
Jun 23 19:01:22.527: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:23.529: INFO: Number of nodes with available pods: 0
Jun 23 19:01:23.529: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:24.529: INFO: Number of nodes with available pods: 0
Jun 23 19:01:24.530: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:25.529: INFO: Number of nodes with available pods: 0
Jun 23 19:01:25.529: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:26.529: INFO: Number of nodes with available pods: 0
Jun 23 19:01:26.529: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:27.531: INFO: Number of nodes with available pods: 0
Jun 23 19:01:27.532: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:28.529: INFO: Number of nodes with available pods: 0
Jun 23 19:01:28.529: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:29.530: INFO: Number of nodes with available pods: 0
Jun 23 19:01:29.530: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:30.530: INFO: Number of nodes with available pods: 0
Jun 23 19:01:30.530: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:31.533: INFO: Number of nodes with available pods: 0
Jun 23 19:01:31.533: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:32.528: INFO: Number of nodes with available pods: 0
Jun 23 19:01:32.528: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:33.531: INFO: Number of nodes with available pods: 0
Jun 23 19:01:33.531: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:34.534: INFO: Number of nodes with available pods: 0
Jun 23 19:01:34.534: INFO: Node controlplane-1 is running more than one daemon pod
Jun 23 19:01:35.530: INFO: Number of nodes with available pods: 1
Jun 23 19:01:35.530: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8698, will wait for the garbage collector to delete the pods
Jun 23 19:01:35.662: INFO: Deleting DaemonSet.extensions daemon-set took: 47.818783ms
Jun 23 19:01:35.862: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.305276ms
Jun 23 19:01:38.965: INFO: Number of nodes with available pods: 0
Jun 23 19:01:38.965: INFO: Number of running nodes: 0, number of available pods: 0
Jun 23 19:01:38.968: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8698/daemonsets","resourceVersion":"18364"},"items":null}

Jun 23 19:01:38.970: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8698/pods","resourceVersion":"18364"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:01:38.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8698" for this suite.
Jun 23 19:01:44.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:01:45.068: INFO: namespace daemonsets-8698 deletion completed in 6.080957459s

• [SLOW TEST:29.639 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:01:45.069: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 19:01:45.096: INFO: (0) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.958597ms)
Jun 23 19:01:45.098: INFO: (1) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.271934ms)
Jun 23 19:01:45.100: INFO: (2) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.813587ms)
Jun 23 19:01:45.102: INFO: (3) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.140776ms)
Jun 23 19:01:45.104: INFO: (4) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.238324ms)
Jun 23 19:01:45.106: INFO: (5) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.821458ms)
Jun 23 19:01:45.108: INFO: (6) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.030079ms)
Jun 23 19:01:45.110: INFO: (7) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.069379ms)
Jun 23 19:01:45.112: INFO: (8) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.905402ms)
Jun 23 19:01:45.114: INFO: (9) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 2.073885ms)
Jun 23 19:01:45.116: INFO: (10) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.950751ms)
Jun 23 19:01:45.118: INFO: (11) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.868217ms)
Jun 23 19:01:45.120: INFO: (12) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.888646ms)
Jun 23 19:01:45.122: INFO: (13) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.640326ms)
Jun 23 19:01:45.124: INFO: (14) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.938343ms)
Jun 23 19:01:45.126: INFO: (15) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.854893ms)
Jun 23 19:01:45.127: INFO: (16) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.654493ms)
Jun 23 19:01:45.129: INFO: (17) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.756929ms)
Jun 23 19:01:45.131: INFO: (18) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.7482ms)
Jun 23 19:01:45.133: INFO: (19) /api/v1/nodes/controlplane-1/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="auth.log">auth.log</a>
<a href="btmp">btmp</a>
<a href="co... (200; 1.703587ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:01:45.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6965" for this suite.
Jun 23 19:01:51.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:01:51.227: INFO: namespace proxy-6965 deletion completed in 6.092754776s

• [SLOW TEST:6.158 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:01:51.228: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Jun 23 19:01:51.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 create -f - --namespace=kubectl-2761'
Jun 23 19:01:51.826: INFO: stderr: ""
Jun 23 19:01:51.826: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 23 19:01:51.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2761'
Jun 23 19:01:51.900: INFO: stderr: ""
Jun 23 19:01:51.900: INFO: stdout: "update-demo-nautilus-d54rg update-demo-nautilus-zdndz "
Jun 23 19:01:51.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-d54rg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2761'
Jun 23 19:01:51.960: INFO: stderr: ""
Jun 23 19:01:51.960: INFO: stdout: ""
Jun 23 19:01:51.960: INFO: update-demo-nautilus-d54rg is created but not running
Jun 23 19:01:56.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2761'
Jun 23 19:01:57.049: INFO: stderr: ""
Jun 23 19:01:57.049: INFO: stdout: "update-demo-nautilus-d54rg update-demo-nautilus-zdndz "
Jun 23 19:01:57.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-d54rg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2761'
Jun 23 19:01:57.095: INFO: stderr: ""
Jun 23 19:01:57.095: INFO: stdout: "true"
Jun 23 19:01:57.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-d54rg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2761'
Jun 23 19:01:57.146: INFO: stderr: ""
Jun 23 19:01:57.146: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 23 19:01:57.146: INFO: validating pod update-demo-nautilus-d54rg
Jun 23 19:01:57.150: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 23 19:01:57.150: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 23 19:01:57.150: INFO: update-demo-nautilus-d54rg is verified up and running
Jun 23 19:01:57.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-zdndz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2761'
Jun 23 19:01:57.211: INFO: stderr: ""
Jun 23 19:01:57.211: INFO: stdout: "true"
Jun 23 19:01:57.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-nautilus-zdndz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2761'
Jun 23 19:01:57.268: INFO: stderr: ""
Jun 23 19:01:57.268: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 23 19:01:57.268: INFO: validating pod update-demo-nautilus-zdndz
Jun 23 19:01:57.272: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 23 19:01:57.272: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 23 19:01:57.272: INFO: update-demo-nautilus-zdndz is verified up and running
STEP: rolling-update to new replication controller
Jun 23 19:01:57.272: INFO: scanned /root for discovery docs: <nil>
Jun 23 19:01:57.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2761'
Jun 23 19:02:19.657: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 23 19:02:19.657: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 23 19:02:19.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2761'
Jun 23 19:02:19.718: INFO: stderr: ""
Jun 23 19:02:19.718: INFO: stdout: "update-demo-kitten-47l84 update-demo-kitten-6z62k "
Jun 23 19:02:19.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-kitten-47l84 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2761'
Jun 23 19:02:19.767: INFO: stderr: ""
Jun 23 19:02:19.767: INFO: stdout: "true"
Jun 23 19:02:19.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-kitten-47l84 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2761'
Jun 23 19:02:19.823: INFO: stderr: ""
Jun 23 19:02:19.823: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 23 19:02:19.823: INFO: validating pod update-demo-kitten-47l84
Jun 23 19:02:19.827: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 23 19:02:19.827: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 23 19:02:19.827: INFO: update-demo-kitten-47l84 is verified up and running
Jun 23 19:02:19.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-kitten-6z62k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2761'
Jun 23 19:02:19.873: INFO: stderr: ""
Jun 23 19:02:19.873: INFO: stdout: "true"
Jun 23 19:02:19.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods update-demo-kitten-6z62k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2761'
Jun 23 19:02:19.924: INFO: stderr: ""
Jun 23 19:02:19.924: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 23 19:02:19.924: INFO: validating pod update-demo-kitten-6z62k
Jun 23 19:02:19.928: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 23 19:02:19.928: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 23 19:02:19.928: INFO: update-demo-kitten-6z62k is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:02:19.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2761" for this suite.
Jun 23 19:02:41.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:02:42.015: INFO: namespace kubectl-2761 deletion completed in 22.08546601s

• [SLOW TEST:50.788 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:02:42.017: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-37322082-783b-4aa0-849d-35fd63230cf0
STEP: Creating a pod to test consume secrets
Jun 23 19:02:42.048: INFO: Waiting up to 5m0s for pod "pod-secrets-58ab2286-1069-4e08-954a-8ec5227dd92f" in namespace "secrets-2245" to be "success or failure"
Jun 23 19:02:42.051: INFO: Pod "pod-secrets-58ab2286-1069-4e08-954a-8ec5227dd92f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.142277ms
Jun 23 19:02:44.057: INFO: Pod "pod-secrets-58ab2286-1069-4e08-954a-8ec5227dd92f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008892211s
Jun 23 19:02:46.064: INFO: Pod "pod-secrets-58ab2286-1069-4e08-954a-8ec5227dd92f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015803154s
STEP: Saw pod success
Jun 23 19:02:46.064: INFO: Pod "pod-secrets-58ab2286-1069-4e08-954a-8ec5227dd92f" satisfied condition "success or failure"
Jun 23 19:02:46.071: INFO: Trying to get logs from node worker-1 pod pod-secrets-58ab2286-1069-4e08-954a-8ec5227dd92f container secret-volume-test: <nil>
STEP: delete the pod
Jun 23 19:02:46.125: INFO: Waiting for pod pod-secrets-58ab2286-1069-4e08-954a-8ec5227dd92f to disappear
Jun 23 19:02:46.130: INFO: Pod pod-secrets-58ab2286-1069-4e08-954a-8ec5227dd92f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:02:46.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2245" for this suite.
Jun 23 19:02:52.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:02:52.231: INFO: namespace secrets-2245 deletion completed in 6.097836777s

• [SLOW TEST:10.215 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:02:52.232: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Jun 23 19:02:52.254: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jun 23 19:02:52.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 create -f - --namespace=kubectl-3419'
Jun 23 19:02:52.382: INFO: stderr: ""
Jun 23 19:02:52.382: INFO: stdout: "service/redis-slave created\n"
Jun 23 19:02:52.382: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jun 23 19:02:52.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 create -f - --namespace=kubectl-3419'
Jun 23 19:02:52.550: INFO: stderr: ""
Jun 23 19:02:52.550: INFO: stdout: "service/redis-master created\n"
Jun 23 19:02:52.550: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 23 19:02:52.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 create -f - --namespace=kubectl-3419'
Jun 23 19:02:52.669: INFO: stderr: ""
Jun 23 19:02:52.669: INFO: stdout: "service/frontend created\n"
Jun 23 19:02:52.669: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jun 23 19:02:52.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 create -f - --namespace=kubectl-3419'
Jun 23 19:02:52.795: INFO: stderr: ""
Jun 23 19:02:52.795: INFO: stdout: "deployment.apps/frontend created\n"
Jun 23 19:02:52.796: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 23 19:02:52.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 create -f - --namespace=kubectl-3419'
Jun 23 19:02:52.916: INFO: stderr: ""
Jun 23 19:02:52.916: INFO: stdout: "deployment.apps/redis-master created\n"
Jun 23 19:02:52.917: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jun 23 19:02:52.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 create -f - --namespace=kubectl-3419'
Jun 23 19:02:53.024: INFO: stderr: ""
Jun 23 19:02:53.024: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jun 23 19:02:53.024: INFO: Waiting for all frontend pods to be Running.
Jun 23 19:04:23.093: INFO: Waiting for frontend to serve content.
Jun 23 19:04:28.120: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jun 23 19:04:36.474: INFO: Trying to add a new entry to the guestbook.
Jun 23 19:04:36.485: INFO: Verifying that added entry can be retrieved.
Jun 23 19:04:37.835: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Jun 23 19:04:42.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete --grace-period=0 --force -f - --namespace=kubectl-3419'
Jun 23 19:04:42.952: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 23 19:04:42.952: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jun 23 19:04:42.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete --grace-period=0 --force -f - --namespace=kubectl-3419'
Jun 23 19:04:43.038: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 23 19:04:43.038: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 23 19:04:43.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete --grace-period=0 --force -f - --namespace=kubectl-3419'
Jun 23 19:04:43.122: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 23 19:04:43.122: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 23 19:04:43.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete --grace-period=0 --force -f - --namespace=kubectl-3419'
Jun 23 19:04:43.188: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 23 19:04:43.188: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 23 19:04:43.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete --grace-period=0 --force -f - --namespace=kubectl-3419'
Jun 23 19:04:43.256: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 23 19:04:43.256: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 23 19:04:43.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete --grace-period=0 --force -f - --namespace=kubectl-3419'
Jun 23 19:04:43.315: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 23 19:04:43.315: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:04:43.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3419" for this suite.
Jun 23 19:05:21.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:05:21.402: INFO: namespace kubectl-3419 deletion completed in 38.084440583s

• [SLOW TEST:149.170 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:05:21.402: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-9t5s
STEP: Creating a pod to test atomic-volume-subpath
Jun 23 19:05:21.431: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-9t5s" in namespace "subpath-1748" to be "success or failure"
Jun 23 19:05:21.434: INFO: Pod "pod-subpath-test-downwardapi-9t5s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.637307ms
Jun 23 19:05:23.441: INFO: Pod "pod-subpath-test-downwardapi-9t5s": Phase="Running", Reason="", readiness=true. Elapsed: 2.01037409s
Jun 23 19:05:25.487: INFO: Pod "pod-subpath-test-downwardapi-9t5s": Phase="Running", Reason="", readiness=true. Elapsed: 4.055669301s
Jun 23 19:05:27.512: INFO: Pod "pod-subpath-test-downwardapi-9t5s": Phase="Running", Reason="", readiness=true. Elapsed: 6.080726065s
Jun 23 19:05:29.525: INFO: Pod "pod-subpath-test-downwardapi-9t5s": Phase="Running", Reason="", readiness=true. Elapsed: 8.093667584s
Jun 23 19:05:31.531: INFO: Pod "pod-subpath-test-downwardapi-9t5s": Phase="Running", Reason="", readiness=true. Elapsed: 10.10051288s
Jun 23 19:05:33.537: INFO: Pod "pod-subpath-test-downwardapi-9t5s": Phase="Running", Reason="", readiness=true. Elapsed: 12.106209069s
Jun 23 19:05:35.545: INFO: Pod "pod-subpath-test-downwardapi-9t5s": Phase="Running", Reason="", readiness=true. Elapsed: 14.11442424s
Jun 23 19:05:37.552: INFO: Pod "pod-subpath-test-downwardapi-9t5s": Phase="Running", Reason="", readiness=true. Elapsed: 16.120899996s
Jun 23 19:05:39.559: INFO: Pod "pod-subpath-test-downwardapi-9t5s": Phase="Running", Reason="", readiness=true. Elapsed: 18.128111845s
Jun 23 19:05:41.574: INFO: Pod "pod-subpath-test-downwardapi-9t5s": Phase="Running", Reason="", readiness=true. Elapsed: 20.143346329s
Jun 23 19:05:43.581: INFO: Pod "pod-subpath-test-downwardapi-9t5s": Phase="Running", Reason="", readiness=true. Elapsed: 22.150212398s
Jun 23 19:05:45.588: INFO: Pod "pod-subpath-test-downwardapi-9t5s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.157522367s
STEP: Saw pod success
Jun 23 19:05:45.588: INFO: Pod "pod-subpath-test-downwardapi-9t5s" satisfied condition "success or failure"
Jun 23 19:05:45.592: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-downwardapi-9t5s container test-container-subpath-downwardapi-9t5s: <nil>
STEP: delete the pod
Jun 23 19:05:45.614: INFO: Waiting for pod pod-subpath-test-downwardapi-9t5s to disappear
Jun 23 19:05:45.618: INFO: Pod pod-subpath-test-downwardapi-9t5s no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-9t5s
Jun 23 19:05:45.619: INFO: Deleting pod "pod-subpath-test-downwardapi-9t5s" in namespace "subpath-1748"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:05:45.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1748" for this suite.
Jun 23 19:05:51.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:05:51.696: INFO: namespace subpath-1748 deletion completed in 6.072270145s

• [SLOW TEST:30.295 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:05:51.699: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 23 19:05:56.774: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:05:56.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2991" for this suite.
Jun 23 19:06:14.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:06:14.888: INFO: namespace replicaset-2991 deletion completed in 18.074272333s

• [SLOW TEST:23.189 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:06:14.889: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Jun 23 19:06:14.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 create -f - --namespace=kubectl-2158'
Jun 23 19:06:15.027: INFO: stderr: ""
Jun 23 19:06:15.027: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Jun 23 19:06:16.033: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 19:06:16.033: INFO: Found 0 / 1
Jun 23 19:06:17.039: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 19:06:17.039: INFO: Found 0 / 1
Jun 23 19:06:18.042: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 19:06:18.042: INFO: Found 1 / 1
Jun 23 19:06:18.042: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 23 19:06:18.049: INFO: Selector matched 1 pods for map[app:redis]
Jun 23 19:06:18.049: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jun 23 19:06:18.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 logs redis-master-qwwtz redis-master --namespace=kubectl-2158'
Jun 23 19:06:18.109: INFO: stderr: ""
Jun 23 19:06:18.109: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 23 Jun 19:06:16.670 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Jun 19:06:16.670 # Server started, Redis version 3.2.12\n1:M 23 Jun 19:06:16.670 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Jun 19:06:16.670 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jun 23 19:06:18.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 log redis-master-qwwtz redis-master --namespace=kubectl-2158 --tail=1'
Jun 23 19:06:18.172: INFO: stderr: ""
Jun 23 19:06:18.172: INFO: stdout: "1:M 23 Jun 19:06:16.670 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jun 23 19:06:18.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 log redis-master-qwwtz redis-master --namespace=kubectl-2158 --limit-bytes=1'
Jun 23 19:06:18.244: INFO: stderr: ""
Jun 23 19:06:18.244: INFO: stdout: " "
STEP: exposing timestamps
Jun 23 19:06:18.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 log redis-master-qwwtz redis-master --namespace=kubectl-2158 --tail=1 --timestamps'
Jun 23 19:06:18.299: INFO: stderr: ""
Jun 23 19:06:18.299: INFO: stdout: "2019-06-23T19:06:16.670560423Z 1:M 23 Jun 19:06:16.670 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jun 23 19:06:20.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 log redis-master-qwwtz redis-master --namespace=kubectl-2158 --since=1s'
Jun 23 19:06:20.892: INFO: stderr: ""
Jun 23 19:06:20.892: INFO: stdout: ""
Jun 23 19:06:20.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 log redis-master-qwwtz redis-master --namespace=kubectl-2158 --since=24h'
Jun 23 19:06:20.964: INFO: stderr: ""
Jun 23 19:06:20.964: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 23 Jun 19:06:16.670 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Jun 19:06:16.670 # Server started, Redis version 3.2.12\n1:M 23 Jun 19:06:16.670 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Jun 19:06:16.670 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Jun 23 19:06:20.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 delete --grace-period=0 --force -f - --namespace=kubectl-2158'
Jun 23 19:06:21.019: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 23 19:06:21.019: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jun 23 19:06:21.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get rc,svc -l name=nginx --no-headers --namespace=kubectl-2158'
Jun 23 19:06:21.082: INFO: stderr: "No resources found.\n"
Jun 23 19:06:21.082: INFO: stdout: ""
Jun 23 19:06:21.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 get pods -l name=nginx --namespace=kubectl-2158 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 23 19:06:21.130: INFO: stderr: ""
Jun 23 19:06:21.130: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:06:21.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2158" for this suite.
Jun 23 19:06:27.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:06:27.194: INFO: namespace kubectl-2158 deletion completed in 6.061743583s

• [SLOW TEST:12.306 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:06:27.194: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 23 19:06:27.235: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57716c62-3b3a-4040-bf37-f9dab071fb28" in namespace "projected-6208" to be "success or failure"
Jun 23 19:06:27.242: INFO: Pod "downwardapi-volume-57716c62-3b3a-4040-bf37-f9dab071fb28": Phase="Pending", Reason="", readiness=false. Elapsed: 6.288961ms
Jun 23 19:06:29.248: INFO: Pod "downwardapi-volume-57716c62-3b3a-4040-bf37-f9dab071fb28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012593693s
Jun 23 19:06:31.250: INFO: Pod "downwardapi-volume-57716c62-3b3a-4040-bf37-f9dab071fb28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015018115s
STEP: Saw pod success
Jun 23 19:06:31.251: INFO: Pod "downwardapi-volume-57716c62-3b3a-4040-bf37-f9dab071fb28" satisfied condition "success or failure"
Jun 23 19:06:31.252: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-57716c62-3b3a-4040-bf37-f9dab071fb28 container client-container: <nil>
STEP: delete the pod
Jun 23 19:06:31.268: INFO: Waiting for pod downwardapi-volume-57716c62-3b3a-4040-bf37-f9dab071fb28 to disappear
Jun 23 19:06:31.270: INFO: Pod downwardapi-volume-57716c62-3b3a-4040-bf37-f9dab071fb28 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:06:31.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6208" for this suite.
Jun 23 19:06:37.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:06:37.350: INFO: namespace projected-6208 deletion completed in 6.078120801s

• [SLOW TEST:10.156 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:06:37.351: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-4a95eca6-8767-4c9e-b931-6a91963c5522
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-4a95eca6-8767-4c9e-b931-6a91963c5522
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:08:12.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-530" for this suite.
Jun 23 19:08:34.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:08:34.442: INFO: namespace configmap-530 deletion completed in 22.064016997s

• [SLOW TEST:117.091 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:08:34.445: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 23 19:08:34.471: INFO: Waiting up to 5m0s for pod "pod-4b3d442e-c4a2-4b60-b8a9-8d95fff1d74a" in namespace "emptydir-9805" to be "success or failure"
Jun 23 19:08:34.476: INFO: Pod "pod-4b3d442e-c4a2-4b60-b8a9-8d95fff1d74a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.942354ms
Jun 23 19:08:36.483: INFO: Pod "pod-4b3d442e-c4a2-4b60-b8a9-8d95fff1d74a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011115241s
Jun 23 19:08:38.490: INFO: Pod "pod-4b3d442e-c4a2-4b60-b8a9-8d95fff1d74a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017993503s
STEP: Saw pod success
Jun 23 19:08:38.490: INFO: Pod "pod-4b3d442e-c4a2-4b60-b8a9-8d95fff1d74a" satisfied condition "success or failure"
Jun 23 19:08:38.491: INFO: Trying to get logs from node worker-1 pod pod-4b3d442e-c4a2-4b60-b8a9-8d95fff1d74a container test-container: <nil>
STEP: delete the pod
Jun 23 19:08:38.505: INFO: Waiting for pod pod-4b3d442e-c4a2-4b60-b8a9-8d95fff1d74a to disappear
Jun 23 19:08:38.507: INFO: Pod pod-4b3d442e-c4a2-4b60-b8a9-8d95fff1d74a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:08:38.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9805" for this suite.
Jun 23 19:08:44.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:08:44.574: INFO: namespace emptydir-9805 deletion completed in 6.064694227s

• [SLOW TEST:10.129 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:08:44.574: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-6fxl
STEP: Creating a pod to test atomic-volume-subpath
Jun 23 19:08:44.601: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-6fxl" in namespace "subpath-8412" to be "success or failure"
Jun 23 19:08:44.604: INFO: Pod "pod-subpath-test-secret-6fxl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.710094ms
Jun 23 19:08:46.606: INFO: Pod "pod-subpath-test-secret-6fxl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005268681s
Jun 23 19:08:48.609: INFO: Pod "pod-subpath-test-secret-6fxl": Phase="Running", Reason="", readiness=true. Elapsed: 4.007823396s
Jun 23 19:08:50.611: INFO: Pod "pod-subpath-test-secret-6fxl": Phase="Running", Reason="", readiness=true. Elapsed: 6.010646171s
Jun 23 19:08:52.617: INFO: Pod "pod-subpath-test-secret-6fxl": Phase="Running", Reason="", readiness=true. Elapsed: 8.016444859s
Jun 23 19:08:54.623: INFO: Pod "pod-subpath-test-secret-6fxl": Phase="Running", Reason="", readiness=true. Elapsed: 10.022436674s
Jun 23 19:08:56.630: INFO: Pod "pod-subpath-test-secret-6fxl": Phase="Running", Reason="", readiness=true. Elapsed: 12.029332438s
Jun 23 19:08:58.637: INFO: Pod "pod-subpath-test-secret-6fxl": Phase="Running", Reason="", readiness=true. Elapsed: 14.036383063s
Jun 23 19:09:00.644: INFO: Pod "pod-subpath-test-secret-6fxl": Phase="Running", Reason="", readiness=true. Elapsed: 16.043636432s
Jun 23 19:09:02.652: INFO: Pod "pod-subpath-test-secret-6fxl": Phase="Running", Reason="", readiness=true. Elapsed: 18.050843415s
Jun 23 19:09:04.662: INFO: Pod "pod-subpath-test-secret-6fxl": Phase="Running", Reason="", readiness=true. Elapsed: 20.060813708s
Jun 23 19:09:06.668: INFO: Pod "pod-subpath-test-secret-6fxl": Phase="Running", Reason="", readiness=true. Elapsed: 22.067654154s
Jun 23 19:09:08.676: INFO: Pod "pod-subpath-test-secret-6fxl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.075175843s
STEP: Saw pod success
Jun 23 19:09:08.676: INFO: Pod "pod-subpath-test-secret-6fxl" satisfied condition "success or failure"
Jun 23 19:09:08.681: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-secret-6fxl container test-container-subpath-secret-6fxl: <nil>
STEP: delete the pod
Jun 23 19:09:08.719: INFO: Waiting for pod pod-subpath-test-secret-6fxl to disappear
Jun 23 19:09:08.722: INFO: Pod pod-subpath-test-secret-6fxl no longer exists
STEP: Deleting pod pod-subpath-test-secret-6fxl
Jun 23 19:09:08.722: INFO: Deleting pod "pod-subpath-test-secret-6fxl" in namespace "subpath-8412"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:09:08.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8412" for this suite.
Jun 23 19:09:14.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:09:14.802: INFO: namespace subpath-8412 deletion completed in 6.074726731s

• [SLOW TEST:30.227 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:09:14.802: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-ace8af68-0217-4eb3-8f46-9199c4d9416e
Jun 23 19:09:14.828: INFO: Pod name my-hostname-basic-ace8af68-0217-4eb3-8f46-9199c4d9416e: Found 0 pods out of 1
Jun 23 19:09:19.838: INFO: Pod name my-hostname-basic-ace8af68-0217-4eb3-8f46-9199c4d9416e: Found 1 pods out of 1
Jun 23 19:09:19.838: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ace8af68-0217-4eb3-8f46-9199c4d9416e" are running
Jun 23 19:09:19.845: INFO: Pod "my-hostname-basic-ace8af68-0217-4eb3-8f46-9199c4d9416e-hrqcr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-23 19:09:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-23 19:09:16 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-23 19:09:16 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-23 19:09:14 +0000 UTC Reason: Message:}])
Jun 23 19:09:19.845: INFO: Trying to dial the pod
Jun 23 19:09:24.855: INFO: Controller my-hostname-basic-ace8af68-0217-4eb3-8f46-9199c4d9416e: Got expected result from replica 1 [my-hostname-basic-ace8af68-0217-4eb3-8f46-9199c4d9416e-hrqcr]: "my-hostname-basic-ace8af68-0217-4eb3-8f46-9199c4d9416e-hrqcr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:09:24.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4732" for this suite.
Jun 23 19:09:30.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:09:30.952: INFO: namespace replication-controller-4732 deletion completed in 6.093793176s

• [SLOW TEST:16.150 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:09:30.952: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-r967
STEP: Creating a pod to test atomic-volume-subpath
Jun 23 19:09:30.983: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-r967" in namespace "subpath-2414" to be "success or failure"
Jun 23 19:09:30.987: INFO: Pod "pod-subpath-test-configmap-r967": Phase="Pending", Reason="", readiness=false. Elapsed: 3.705702ms
Jun 23 19:09:32.992: INFO: Pod "pod-subpath-test-configmap-r967": Phase="Running", Reason="", readiness=true. Elapsed: 2.00867343s
Jun 23 19:09:35.004: INFO: Pod "pod-subpath-test-configmap-r967": Phase="Running", Reason="", readiness=true. Elapsed: 4.020305546s
Jun 23 19:09:37.018: INFO: Pod "pod-subpath-test-configmap-r967": Phase="Running", Reason="", readiness=true. Elapsed: 6.034170842s
Jun 23 19:09:39.027: INFO: Pod "pod-subpath-test-configmap-r967": Phase="Running", Reason="", readiness=true. Elapsed: 8.043776706s
Jun 23 19:09:41.035: INFO: Pod "pod-subpath-test-configmap-r967": Phase="Running", Reason="", readiness=true. Elapsed: 10.051956037s
Jun 23 19:09:43.043: INFO: Pod "pod-subpath-test-configmap-r967": Phase="Running", Reason="", readiness=true. Elapsed: 12.059785766s
Jun 23 19:09:45.051: INFO: Pod "pod-subpath-test-configmap-r967": Phase="Running", Reason="", readiness=true. Elapsed: 14.067235359s
Jun 23 19:09:47.118: INFO: Pod "pod-subpath-test-configmap-r967": Phase="Running", Reason="", readiness=true. Elapsed: 16.134292979s
Jun 23 19:09:49.126: INFO: Pod "pod-subpath-test-configmap-r967": Phase="Running", Reason="", readiness=true. Elapsed: 18.142225165s
Jun 23 19:09:51.133: INFO: Pod "pod-subpath-test-configmap-r967": Phase="Running", Reason="", readiness=true. Elapsed: 20.149991997s
Jun 23 19:09:53.148: INFO: Pod "pod-subpath-test-configmap-r967": Phase="Running", Reason="", readiness=true. Elapsed: 22.164783749s
Jun 23 19:09:55.156: INFO: Pod "pod-subpath-test-configmap-r967": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.173049637s
STEP: Saw pod success
Jun 23 19:09:55.157: INFO: Pod "pod-subpath-test-configmap-r967" satisfied condition "success or failure"
Jun 23 19:09:55.162: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-configmap-r967 container test-container-subpath-configmap-r967: <nil>
STEP: delete the pod
Jun 23 19:09:55.206: INFO: Waiting for pod pod-subpath-test-configmap-r967 to disappear
Jun 23 19:09:55.208: INFO: Pod pod-subpath-test-configmap-r967 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-r967
Jun 23 19:09:55.209: INFO: Deleting pod "pod-subpath-test-configmap-r967" in namespace "subpath-2414"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:09:55.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2414" for this suite.
Jun 23 19:10:01.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:10:01.301: INFO: namespace subpath-2414 deletion completed in 6.087903932s

• [SLOW TEST:30.349 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:10:01.303: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-8434/secret-test-c58f02bf-a9ce-48a4-8f28-044162faf453
STEP: Creating a pod to test consume secrets
Jun 23 19:10:01.334: INFO: Waiting up to 5m0s for pod "pod-configmaps-b6ec4490-922e-4159-bc2d-f4cedf9f8739" in namespace "secrets-8434" to be "success or failure"
Jun 23 19:10:01.341: INFO: Pod "pod-configmaps-b6ec4490-922e-4159-bc2d-f4cedf9f8739": Phase="Pending", Reason="", readiness=false. Elapsed: 6.763006ms
Jun 23 19:10:03.348: INFO: Pod "pod-configmaps-b6ec4490-922e-4159-bc2d-f4cedf9f8739": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013939691s
Jun 23 19:10:05.354: INFO: Pod "pod-configmaps-b6ec4490-922e-4159-bc2d-f4cedf9f8739": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020556084s
STEP: Saw pod success
Jun 23 19:10:05.355: INFO: Pod "pod-configmaps-b6ec4490-922e-4159-bc2d-f4cedf9f8739" satisfied condition "success or failure"
Jun 23 19:10:05.362: INFO: Trying to get logs from node worker-1 pod pod-configmaps-b6ec4490-922e-4159-bc2d-f4cedf9f8739 container env-test: <nil>
STEP: delete the pod
Jun 23 19:10:05.384: INFO: Waiting for pod pod-configmaps-b6ec4490-922e-4159-bc2d-f4cedf9f8739 to disappear
Jun 23 19:10:05.386: INFO: Pod pod-configmaps-b6ec4490-922e-4159-bc2d-f4cedf9f8739 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:10:05.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8434" for this suite.
Jun 23 19:10:11.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:10:11.482: INFO: namespace secrets-8434 deletion completed in 6.09380808s

• [SLOW TEST:10.179 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:10:11.482: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-9485d964-038f-4aae-8ca9-aab0610b291a
STEP: Creating a pod to test consume secrets
Jun 23 19:10:11.595: INFO: Waiting up to 5m0s for pod "pod-secrets-e47013db-6525-4cb4-9e60-da07c26edde2" in namespace "secrets-9950" to be "success or failure"
Jun 23 19:10:11.601: INFO: Pod "pod-secrets-e47013db-6525-4cb4-9e60-da07c26edde2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.302859ms
Jun 23 19:10:13.618: INFO: Pod "pod-secrets-e47013db-6525-4cb4-9e60-da07c26edde2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022680415s
Jun 23 19:10:15.625: INFO: Pod "pod-secrets-e47013db-6525-4cb4-9e60-da07c26edde2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029439732s
STEP: Saw pod success
Jun 23 19:10:15.625: INFO: Pod "pod-secrets-e47013db-6525-4cb4-9e60-da07c26edde2" satisfied condition "success or failure"
Jun 23 19:10:15.631: INFO: Trying to get logs from node worker-1 pod pod-secrets-e47013db-6525-4cb4-9e60-da07c26edde2 container secret-volume-test: <nil>
STEP: delete the pod
Jun 23 19:10:15.676: INFO: Waiting for pod pod-secrets-e47013db-6525-4cb4-9e60-da07c26edde2 to disappear
Jun 23 19:10:15.679: INFO: Pod pod-secrets-e47013db-6525-4cb4-9e60-da07c26edde2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:10:15.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9950" for this suite.
Jun 23 19:10:21.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:10:21.763: INFO: namespace secrets-9950 deletion completed in 6.078908212s

• [SLOW TEST:10.282 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:10:21.767: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-4b6bbe21-d1b3-4daa-a110-53238d33ac2f
STEP: Creating a pod to test consume secrets
Jun 23 19:10:21.794: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0071ff87-0593-45cb-a40a-e406b88cf4bd" in namespace "projected-9360" to be "success or failure"
Jun 23 19:10:21.796: INFO: Pod "pod-projected-secrets-0071ff87-0593-45cb-a40a-e406b88cf4bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.389599ms
Jun 23 19:10:23.805: INFO: Pod "pod-projected-secrets-0071ff87-0593-45cb-a40a-e406b88cf4bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011140167s
Jun 23 19:10:25.813: INFO: Pod "pod-projected-secrets-0071ff87-0593-45cb-a40a-e406b88cf4bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01857477s
STEP: Saw pod success
Jun 23 19:10:25.813: INFO: Pod "pod-projected-secrets-0071ff87-0593-45cb-a40a-e406b88cf4bd" satisfied condition "success or failure"
Jun 23 19:10:25.818: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-0071ff87-0593-45cb-a40a-e406b88cf4bd container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 23 19:10:25.865: INFO: Waiting for pod pod-projected-secrets-0071ff87-0593-45cb-a40a-e406b88cf4bd to disappear
Jun 23 19:10:25.867: INFO: Pod pod-projected-secrets-0071ff87-0593-45cb-a40a-e406b88cf4bd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:10:25.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9360" for this suite.
Jun 23 19:10:31.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:10:31.950: INFO: namespace projected-9360 deletion completed in 6.079012847s

• [SLOW TEST:10.184 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:10:31.952: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:10:36.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2940" for this suite.
Jun 23 19:11:14.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:11:14.125: INFO: namespace kubelet-test-2940 deletion completed in 38.107834435s

• [SLOW TEST:42.173 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:11:14.125: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 23 19:11:14.152: INFO: Waiting up to 5m0s for pod "downward-api-ebed0aea-6e73-440b-81be-54ebeecb41bf" in namespace "downward-api-9583" to be "success or failure"
Jun 23 19:11:14.157: INFO: Pod "downward-api-ebed0aea-6e73-440b-81be-54ebeecb41bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.067685ms
Jun 23 19:11:16.165: INFO: Pod "downward-api-ebed0aea-6e73-440b-81be-54ebeecb41bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012901955s
Jun 23 19:11:18.171: INFO: Pod "downward-api-ebed0aea-6e73-440b-81be-54ebeecb41bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01913879s
STEP: Saw pod success
Jun 23 19:11:18.171: INFO: Pod "downward-api-ebed0aea-6e73-440b-81be-54ebeecb41bf" satisfied condition "success or failure"
Jun 23 19:11:18.177: INFO: Trying to get logs from node worker-1 pod downward-api-ebed0aea-6e73-440b-81be-54ebeecb41bf container dapi-container: <nil>
STEP: delete the pod
Jun 23 19:11:18.216: INFO: Waiting for pod downward-api-ebed0aea-6e73-440b-81be-54ebeecb41bf to disappear
Jun 23 19:11:18.218: INFO: Pod downward-api-ebed0aea-6e73-440b-81be-54ebeecb41bf no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:11:18.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9583" for this suite.
Jun 23 19:11:24.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:11:24.317: INFO: namespace downward-api-9583 deletion completed in 6.097086575s

• [SLOW TEST:10.192 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:11:24.317: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:11:50.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4617" for this suite.
Jun 23 19:11:56.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:11:56.820: INFO: namespace container-runtime-4617 deletion completed in 6.098264382s

• [SLOW TEST:32.503 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:11:56.822: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 19:11:56.841: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:11:57.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5015" for this suite.
Jun 23 19:12:03.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:12:04.090: INFO: namespace custom-resource-definition-5015 deletion completed in 6.115414928s

• [SLOW TEST:7.268 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:12:04.091: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Jun 23 19:12:04.121: INFO: Waiting up to 5m0s for pod "var-expansion-089178b1-b73f-44fc-8065-e9d5dc76240d" in namespace "var-expansion-7757" to be "success or failure"
Jun 23 19:12:04.124: INFO: Pod "var-expansion-089178b1-b73f-44fc-8065-e9d5dc76240d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.043046ms
Jun 23 19:12:06.132: INFO: Pod "var-expansion-089178b1-b73f-44fc-8065-e9d5dc76240d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010648339s
Jun 23 19:12:08.139: INFO: Pod "var-expansion-089178b1-b73f-44fc-8065-e9d5dc76240d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017257634s
STEP: Saw pod success
Jun 23 19:12:08.139: INFO: Pod "var-expansion-089178b1-b73f-44fc-8065-e9d5dc76240d" satisfied condition "success or failure"
Jun 23 19:12:08.144: INFO: Trying to get logs from node worker-1 pod var-expansion-089178b1-b73f-44fc-8065-e9d5dc76240d container dapi-container: <nil>
STEP: delete the pod
Jun 23 19:12:08.185: INFO: Waiting for pod var-expansion-089178b1-b73f-44fc-8065-e9d5dc76240d to disappear
Jun 23 19:12:08.188: INFO: Pod var-expansion-089178b1-b73f-44fc-8065-e9d5dc76240d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:12:08.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7757" for this suite.
Jun 23 19:12:14.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:12:14.256: INFO: namespace var-expansion-7757 deletion completed in 6.065281281s

• [SLOW TEST:10.166 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:12:14.257: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-da59ab1d-69f7-4e5e-858b-e87e32a0bbb0
STEP: Creating a pod to test consume configMaps
Jun 23 19:12:14.352: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6c669324-07ac-4daa-a151-8d86c405d25e" in namespace "projected-6290" to be "success or failure"
Jun 23 19:12:14.364: INFO: Pod "pod-projected-configmaps-6c669324-07ac-4daa-a151-8d86c405d25e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.409685ms
Jun 23 19:12:16.371: INFO: Pod "pod-projected-configmaps-6c669324-07ac-4daa-a151-8d86c405d25e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01922786s
Jun 23 19:12:18.379: INFO: Pod "pod-projected-configmaps-6c669324-07ac-4daa-a151-8d86c405d25e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026836487s
STEP: Saw pod success
Jun 23 19:12:18.379: INFO: Pod "pod-projected-configmaps-6c669324-07ac-4daa-a151-8d86c405d25e" satisfied condition "success or failure"
Jun 23 19:12:18.387: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-6c669324-07ac-4daa-a151-8d86c405d25e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 23 19:12:18.426: INFO: Waiting for pod pod-projected-configmaps-6c669324-07ac-4daa-a151-8d86c405d25e to disappear
Jun 23 19:12:18.429: INFO: Pod pod-projected-configmaps-6c669324-07ac-4daa-a151-8d86c405d25e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:12:18.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6290" for this suite.
Jun 23 19:12:24.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:12:24.528: INFO: namespace projected-6290 deletion completed in 6.095831276s

• [SLOW TEST:10.271 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:12:24.528: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-5e36c69e-514f-4394-a45c-3a7cc7b50d70
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:12:24.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-912" for this suite.
Jun 23 19:12:30.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:12:30.644: INFO: namespace configmap-912 deletion completed in 6.093931861s

• [SLOW TEST:6.116 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:12:30.644: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9362
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9362
STEP: Creating statefulset with conflicting port in namespace statefulset-9362
STEP: Waiting until pod test-pod will start running in namespace statefulset-9362
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9362
Jun 23 19:12:36.704: INFO: Observed stateful pod in namespace: statefulset-9362, name: ss-0, uid: 20d79daa-ec71-4e07-bc4b-8e8bad9f309b, status phase: Pending. Waiting for statefulset controller to delete.
Jun 23 19:12:36.882: INFO: Observed stateful pod in namespace: statefulset-9362, name: ss-0, uid: 20d79daa-ec71-4e07-bc4b-8e8bad9f309b, status phase: Failed. Waiting for statefulset controller to delete.
Jun 23 19:12:36.896: INFO: Observed stateful pod in namespace: statefulset-9362, name: ss-0, uid: 20d79daa-ec71-4e07-bc4b-8e8bad9f309b, status phase: Failed. Waiting for statefulset controller to delete.
Jun 23 19:12:36.906: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9362
STEP: Removing pod with conflicting port in namespace statefulset-9362
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9362 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 23 19:12:40.958: INFO: Deleting all statefulset in ns statefulset-9362
Jun 23 19:12:40.965: INFO: Scaling statefulset ss to 0
Jun 23 19:13:00.995: INFO: Waiting for statefulset status.replicas updated to 0
Jun 23 19:13:01.001: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:13:01.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9362" for this suite.
Jun 23 19:13:07.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:13:07.139: INFO: namespace statefulset-9362 deletion completed in 6.102758922s

• [SLOW TEST:36.494 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:13:07.141: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 19:13:07.162: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:13:11.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9841" for this suite.
Jun 23 19:13:55.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:13:55.395: INFO: namespace pods-9841 deletion completed in 44.100513498s

• [SLOW TEST:48.254 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:13:55.395: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 23 19:14:13.438: INFO: Container started at 2019-06-23 19:13:56 +0000 UTC, pod became ready at 2019-06-23 19:14:11 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:14:13.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8289" for this suite.
Jun 23 19:14:35.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:14:35.561: INFO: namespace container-probe-8289 deletion completed in 22.113399644s

• [SLOW TEST:40.166 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:14:35.562: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 23 19:14:35.587: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 23 19:14:40.594: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:14:40.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4167" for this suite.
Jun 23 19:14:46.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:14:46.712: INFO: namespace replication-controller-4167 deletion completed in 6.092756579s

• [SLOW TEST:11.150 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:14:46.713: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-8445
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jun 23 19:14:46.748: INFO: Found 0 stateful pods, waiting for 3
Jun 23 19:14:56.755: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 23 19:14:56.756: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 23 19:14:56.756: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 23 19:14:56.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-8445 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 23 19:14:57.443: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 23 19:14:57.443: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 23 19:14:57.443: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 23 19:15:07.508: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 23 19:15:07.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-8445 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 23 19:15:07.656: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 23 19:15:07.656: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 23 19:15:07.656: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

STEP: Rolling back to a previous revision
Jun 23 19:15:27.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-8445 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 23 19:15:27.857: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 23 19:15:27.857: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 23 19:15:27.857: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 23 19:15:37.897: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 23 19:15:47.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-309105263 exec --namespace=statefulset-8445 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 23 19:15:48.118: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 23 19:15:48.118: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 23 19:15:48.118: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 23 19:16:08.148: INFO: Waiting for StatefulSet statefulset-8445/ss2 to complete update
Jun 23 19:16:08.148: INFO: Waiting for Pod statefulset-8445/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 23 19:16:18.164: INFO: Deleting all statefulset in ns statefulset-8445
Jun 23 19:16:18.170: INFO: Scaling statefulset ss2 to 0
Jun 23 19:16:48.201: INFO: Waiting for statefulset status.replicas updated to 0
Jun 23 19:16:48.207: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:16:48.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8445" for this suite.
Jun 23 19:16:54.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:16:54.352: INFO: namespace statefulset-8445 deletion completed in 6.099141363s

• [SLOW TEST:127.640 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 23 19:16:54.353: INFO: >>> kubeConfig: /tmp/kubeconfig-309105263
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 23 19:16:54.384: INFO: Waiting up to 5m0s for pod "pod-25ec2626-5096-4aa8-be6b-cd332faca98d" in namespace "emptydir-4943" to be "success or failure"
Jun 23 19:16:54.394: INFO: Pod "pod-25ec2626-5096-4aa8-be6b-cd332faca98d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.836218ms
Jun 23 19:16:56.399: INFO: Pod "pod-25ec2626-5096-4aa8-be6b-cd332faca98d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015058951s
Jun 23 19:16:58.412: INFO: Pod "pod-25ec2626-5096-4aa8-be6b-cd332faca98d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02774927s
STEP: Saw pod success
Jun 23 19:16:58.412: INFO: Pod "pod-25ec2626-5096-4aa8-be6b-cd332faca98d" satisfied condition "success or failure"
Jun 23 19:16:58.416: INFO: Trying to get logs from node worker-1 pod pod-25ec2626-5096-4aa8-be6b-cd332faca98d container test-container: <nil>
STEP: delete the pod
Jun 23 19:16:58.436: INFO: Waiting for pod pod-25ec2626-5096-4aa8-be6b-cd332faca98d to disappear
Jun 23 19:16:58.439: INFO: Pod pod-25ec2626-5096-4aa8-be6b-cd332faca98d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 23 19:16:58.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4943" for this suite.
Jun 23 19:17:04.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 23 19:17:04.512: INFO: namespace emptydir-4943 deletion completed in 6.071051471s

• [SLOW TEST:10.160 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSJun 23 19:17:04.513: INFO: Running AfterSuite actions on all nodes
Jun 23 19:17:04.513: INFO: Running AfterSuite actions on node 1
Jun 23 19:17:04.513: INFO: Skipping dumping logs from cluster

Ran 215 of 4411 Specs in 5883.927 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4196 Skipped
PASS

Ginkgo ran 1 suite in 1h38m4.859312641s
Test Suite Passed
