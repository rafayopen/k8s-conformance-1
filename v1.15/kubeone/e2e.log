I0705 08:52:11.318106      15 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-272745911
I0705 08:52:11.318402      15 e2e.go:241] Starting e2e run "a081c77f-0181-45a1-9f18-51db69e6f5af" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1562316729 - Will randomize all specs
Will run 215 of 4411 specs

Jul  5 08:52:11.594: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 08:52:11.606: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul  5 08:52:11.657: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul  5 08:52:11.683: INFO: 29 / 29 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul  5 08:52:11.683: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Jul  5 08:52:11.683: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul  5 08:52:11.690: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Jul  5 08:52:11.690: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul  5 08:52:11.690: INFO: e2e test version: v1.15.0
Jul  5 08:52:11.691: INFO: kube-apiserver version: v1.15.0
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:52:11.691: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-lifecycle-hook
Jul  5 08:52:11.718: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul  5 08:52:19.771: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  5 08:52:19.778: INFO: Pod pod-with-prestop-http-hook still exists
Jul  5 08:52:21.778: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  5 08:52:21.781: INFO: Pod pod-with-prestop-http-hook still exists
Jul  5 08:52:23.778: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  5 08:52:23.781: INFO: Pod pod-with-prestop-http-hook still exists
Jul  5 08:52:25.778: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  5 08:52:25.781: INFO: Pod pod-with-prestop-http-hook still exists
Jul  5 08:52:27.778: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  5 08:52:27.781: INFO: Pod pod-with-prestop-http-hook still exists
Jul  5 08:52:29.778: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  5 08:52:29.781: INFO: Pod pod-with-prestop-http-hook still exists
Jul  5 08:52:31.778: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  5 08:52:31.781: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:52:31.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7564" for this suite.
Jul  5 08:52:53.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:52:53.900: INFO: namespace container-lifecycle-hook-7564 deletion completed in 22.100164109s

• [SLOW TEST:42.209 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:52:53.901: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  5 08:52:58.958: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:52:58.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6067" for this suite.
Jul  5 08:53:04.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:53:05.077: INFO: namespace container-runtime-6067 deletion completed in 6.101750826s

• [SLOW TEST:11.176 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:53:05.078: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-1dc45275-34a5-45e1-ac85-e915a5bc806e
STEP: Creating a pod to test consume secrets
Jul  5 08:53:05.118: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0f8bf0a4-4bca-4ba5-8fed-2e428eab00ab" in namespace "projected-137" to be "success or failure"
Jul  5 08:53:05.122: INFO: Pod "pod-projected-secrets-0f8bf0a4-4bca-4ba5-8fed-2e428eab00ab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.581378ms
Jul  5 08:53:07.125: INFO: Pod "pod-projected-secrets-0f8bf0a4-4bca-4ba5-8fed-2e428eab00ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006525727s
Jul  5 08:53:09.128: INFO: Pod "pod-projected-secrets-0f8bf0a4-4bca-4ba5-8fed-2e428eab00ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009626012s
STEP: Saw pod success
Jul  5 08:53:09.128: INFO: Pod "pod-projected-secrets-0f8bf0a4-4bca-4ba5-8fed-2e428eab00ab" satisfied condition "success or failure"
Jul  5 08:53:09.130: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-projected-secrets-0f8bf0a4-4bca-4ba5-8fed-2e428eab00ab container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 08:53:09.156: INFO: Waiting for pod pod-projected-secrets-0f8bf0a4-4bca-4ba5-8fed-2e428eab00ab to disappear
Jul  5 08:53:09.158: INFO: Pod pod-projected-secrets-0f8bf0a4-4bca-4ba5-8fed-2e428eab00ab no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:53:09.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-137" for this suite.
Jul  5 08:53:15.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:53:15.328: INFO: namespace projected-137 deletion completed in 6.166346484s

• [SLOW TEST:10.250 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:53:15.328: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  5 08:53:15.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2263'
Jul  5 08:53:15.604: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  5 08:53:15.604: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Jul  5 08:53:15.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete jobs e2e-test-nginx-job --namespace=kubectl-2263'
Jul  5 08:53:15.719: INFO: stderr: ""
Jul  5 08:53:15.719: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:53:15.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2263" for this suite.
Jul  5 08:53:21.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:53:21.840: INFO: namespace kubectl-2263 deletion completed in 6.117350741s

• [SLOW TEST:6.512 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:53:21.840: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-52899a14-1199-4b89-aa79-09154c76cabe
STEP: Creating secret with name s-test-opt-upd-1761b145-7757-41e6-92c3-69da974efbe6
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-52899a14-1199-4b89-aa79-09154c76cabe
STEP: Updating secret s-test-opt-upd-1761b145-7757-41e6-92c3-69da974efbe6
STEP: Creating secret with name s-test-opt-create-094d35b1-e68b-4b78-a423-771fd60bda99
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:54:42.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5615" for this suite.
Jul  5 08:55:04.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:55:04.364: INFO: namespace secrets-5615 deletion completed in 22.102506583s

• [SLOW TEST:102.523 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:55:04.364: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-c3fbd254-3508-44ee-8918-2b0229e9810f
STEP: Creating a pod to test consume secrets
Jul  5 08:55:04.415: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0ed9556b-2435-4416-9051-2d61e91d4d67" in namespace "projected-8783" to be "success or failure"
Jul  5 08:55:04.419: INFO: Pod "pod-projected-secrets-0ed9556b-2435-4416-9051-2d61e91d4d67": Phase="Pending", Reason="", readiness=false. Elapsed: 3.997872ms
Jul  5 08:55:06.422: INFO: Pod "pod-projected-secrets-0ed9556b-2435-4416-9051-2d61e91d4d67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007401335s
Jul  5 08:55:08.426: INFO: Pod "pod-projected-secrets-0ed9556b-2435-4416-9051-2d61e91d4d67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011077834s
STEP: Saw pod success
Jul  5 08:55:08.426: INFO: Pod "pod-projected-secrets-0ed9556b-2435-4416-9051-2d61e91d4d67" satisfied condition "success or failure"
Jul  5 08:55:08.428: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-projected-secrets-0ed9556b-2435-4416-9051-2d61e91d4d67 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  5 08:55:08.450: INFO: Waiting for pod pod-projected-secrets-0ed9556b-2435-4416-9051-2d61e91d4d67 to disappear
Jul  5 08:55:08.452: INFO: Pod pod-projected-secrets-0ed9556b-2435-4416-9051-2d61e91d4d67 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:55:08.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8783" for this suite.
Jul  5 08:55:14.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:55:14.565: INFO: namespace projected-8783 deletion completed in 6.110355462s

• [SLOW TEST:10.201 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:55:14.566: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 08:55:14.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 create -f - --namespace=kubectl-4210'
Jul  5 08:55:14.819: INFO: stderr: ""
Jul  5 08:55:14.819: INFO: stdout: "replicationcontroller/redis-master created\n"
Jul  5 08:55:14.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 create -f - --namespace=kubectl-4210'
Jul  5 08:55:15.058: INFO: stderr: ""
Jul  5 08:55:15.058: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul  5 08:55:16.061: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 08:55:16.061: INFO: Found 0 / 1
Jul  5 08:55:17.061: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 08:55:17.061: INFO: Found 0 / 1
Jul  5 08:55:18.061: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 08:55:18.061: INFO: Found 1 / 1
Jul  5 08:55:18.061: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  5 08:55:18.063: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 08:55:18.063: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  5 08:55:18.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 describe pod redis-master-w67kw --namespace=kubectl-4210'
Jul  5 08:55:18.151: INFO: stderr: ""
Jul  5 08:55:18.151: INFO: stdout: "Name:           redis-master-w67kw\nNamespace:      kubectl-4210\nPriority:       0\nNode:           ip-172-31-1-212.eu-west-3.compute.internal/172.31.1.212\nStart Time:     Fri, 05 Jul 2019 08:55:14 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 10.244.4.6/32\nStatus:         Running\nIP:             10.244.4.6\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://98045000b30236e5fde2dcd528b7d1f53fc89643e4f268836a9dd074fe6d02e3\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 05 Jul 2019 08:55:17 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jnnpz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-jnnpz:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-jnnpz\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                 Message\n  ----    ------     ----  ----                                                 -------\n  Normal  Scheduled  4s    default-scheduler                                    Successfully assigned kubectl-4210/redis-master-w67kw to ip-172-31-1-212.eu-west-3.compute.internal\n  Normal  Pulling    3s    kubelet, ip-172-31-1-212.eu-west-3.compute.internal  Pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     1s    kubelet, ip-172-31-1-212.eu-west-3.compute.internal  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    1s    kubelet, ip-172-31-1-212.eu-west-3.compute.internal  Created container redis-master\n  Normal  Started    1s    kubelet, ip-172-31-1-212.eu-west-3.compute.internal  Started container redis-master\n"
Jul  5 08:55:18.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 describe rc redis-master --namespace=kubectl-4210'
Jul  5 08:55:18.241: INFO: stderr: ""
Jul  5 08:55:18.241: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4210\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-w67kw\n"
Jul  5 08:55:18.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 describe service redis-master --namespace=kubectl-4210'
Jul  5 08:55:18.328: INFO: stderr: ""
Jul  5 08:55:18.328: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4210\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.109.34.224\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.4.6:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul  5 08:55:18.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 describe node ip-172-31-1-212.eu-west-3.compute.internal'
Jul  5 08:55:18.436: INFO: stderr: ""
Jul  5 08:55:18.436: INFO: stdout: "Name:               ip-172-31-1-212.eu-west-3.compute.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-west-3\n                    failure-domain.beta.kubernetes.io/zone=eu-west-3a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-1-212\n                    kubernetes.io/os=linux\n                    machine-controller/owned-by=e303aac9-5e58-44f6-9b10-09f4631b808d\n                    workerset=conformance-15-1-pool1\nAnnotations:        cluster.k8s.io/machine: kube-system/conformance-15-1-pool1-654fdd95cb-6xl4j\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"ce:dc:77:3f:6d:a5\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.1.212\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 05 Jul 2019 08:50:50 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 05 Jul 2019 08:54:20 +0000   Fri, 05 Jul 2019 08:50:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 05 Jul 2019 08:54:20 +0000   Fri, 05 Jul 2019 08:50:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 05 Jul 2019 08:54:20 +0000   Fri, 05 Jul 2019 08:50:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 05 Jul 2019 08:54:20 +0000   Fri, 05 Jul 2019 08:51:00 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.31.1.212\n  ExternalIP:   35.180.232.126\n  InternalDNS:  ip-172-31-1-212.eu-west-3.compute.internal\n  Hostname:     ip-172-31-1-212.eu-west-3.compute.internal\n  ExternalDNS:  ec2-35-180-232-126.eu-west-3.compute.amazonaws.com\nCapacity:\n attachable-volumes-aws-ebs:  25\n cpu:                         2\n ephemeral-storage:           50758604Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      3978592Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  25\n cpu:                         1800m\n ephemeral-storage:           44631645721\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      3671392Ki\n pods:                        110\nSystem Info:\n Machine ID:                 ec2f2debc1c801e5a8ab8d0b5e106b5c\n System UUID:                EC2F2DEB-C1C8-01E5-A8AB-8D0B5E106B5C\n Boot ID:                    fbc216cd-6de3-4e74-97a4-8a6b26d1a1bc\n Kernel Version:             4.15.0-1043-aws\n OS Image:                   Ubuntu 18.04.2 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.2\n Kubelet Version:            v1.15.0\n Kube-Proxy Version:         v1.15.0\nPodCIDR:                     10.244.4.0/24\nProviderID:                  aws:///eu-west-3a/i-03e39e8eedf9f99b9\nNon-terminated Pods:         (5 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m34s\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-e9d729729fbf4cf5-rfc95    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m29s\n  kube-system                canal-5kgvk                                                250m (13%)    0 (0%)      0 (0%)           0 (0%)         4m28s\n  kube-system                kube-proxy-zpgcv                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m28s\n  kubectl-4210               redis-master-w67kw                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         250m (13%)  0 (0%)\n  memory                      0 (0%)      0 (0%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:\n  Type    Reason                   Age                    From                                                    Message\n  ----    ------                   ----                   ----                                                    -------\n  Normal  Starting                 4m29s                  kubelet, ip-172-31-1-212.eu-west-3.compute.internal     Starting kubelet.\n  Normal  NodeHasSufficientMemory  4m28s (x2 over 4m28s)  kubelet, ip-172-31-1-212.eu-west-3.compute.internal     Node ip-172-31-1-212.eu-west-3.compute.internal status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    4m28s (x2 over 4m28s)  kubelet, ip-172-31-1-212.eu-west-3.compute.internal     Node ip-172-31-1-212.eu-west-3.compute.internal status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     4m28s (x2 over 4m28s)  kubelet, ip-172-31-1-212.eu-west-3.compute.internal     Node ip-172-31-1-212.eu-west-3.compute.internal status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  4m28s                  kubelet, ip-172-31-1-212.eu-west-3.compute.internal     Updated Node Allocatable limit across pods\n  Normal  Starting                 4m19s                  kube-proxy, ip-172-31-1-212.eu-west-3.compute.internal  Starting kube-proxy.\n  Normal  NodeReady                4m18s                  kubelet, ip-172-31-1-212.eu-west-3.compute.internal     Node ip-172-31-1-212.eu-west-3.compute.internal status is now: NodeReady\n"
Jul  5 08:55:18.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 describe namespace kubectl-4210'
Jul  5 08:55:18.528: INFO: stderr: ""
Jul  5 08:55:18.528: INFO: stdout: "Name:         kubectl-4210\nLabels:       e2e-framework=kubectl\n              e2e-run=a081c77f-0181-45a1-9f18-51db69e6f5af\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:55:18.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4210" for this suite.
Jul  5 08:55:40.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:55:40.644: INFO: namespace kubectl-4210 deletion completed in 22.109547629s

• [SLOW TEST:26.078 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:55:40.644: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4095.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4095.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4095.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4095.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4095.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4095.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4095.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4095.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4095.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4095.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4095.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 233.7.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.7.233_udp@PTR;check="$$(dig +tcp +noall +answer +search 233.7.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.7.233_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4095.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4095.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4095.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4095.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4095.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4095.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4095.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4095.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4095.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4095.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4095.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 233.7.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.7.233_udp@PTR;check="$$(dig +tcp +noall +answer +search 233.7.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.7.233_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  5 08:55:50.723: INFO: Unable to read wheezy_udp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:50.726: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:50.729: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:50.732: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:50.757: INFO: Unable to read jessie_udp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:50.760: INFO: Unable to read jessie_tcp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:50.763: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:50.766: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:50.783: INFO: Lookups using dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7 failed for: [wheezy_udp@dns-test-service.dns-4095.svc.cluster.local wheezy_tcp@dns-test-service.dns-4095.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local jessie_udp@dns-test-service.dns-4095.svc.cluster.local jessie_tcp@dns-test-service.dns-4095.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local]

Jul  5 08:55:55.787: INFO: Unable to read wheezy_udp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:55.790: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:55.792: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:55.795: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:55.812: INFO: Unable to read jessie_udp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:55.814: INFO: Unable to read jessie_tcp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:55.817: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:55.819: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:55:55.839: INFO: Lookups using dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7 failed for: [wheezy_udp@dns-test-service.dns-4095.svc.cluster.local wheezy_tcp@dns-test-service.dns-4095.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local jessie_udp@dns-test-service.dns-4095.svc.cluster.local jessie_tcp@dns-test-service.dns-4095.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local]

Jul  5 08:56:00.786: INFO: Unable to read wheezy_udp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:00.789: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:00.792: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:00.795: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:00.823: INFO: Unable to read jessie_udp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:00.826: INFO: Unable to read jessie_tcp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:00.828: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:00.831: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:00.846: INFO: Lookups using dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7 failed for: [wheezy_udp@dns-test-service.dns-4095.svc.cluster.local wheezy_tcp@dns-test-service.dns-4095.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local jessie_udp@dns-test-service.dns-4095.svc.cluster.local jessie_tcp@dns-test-service.dns-4095.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local]

Jul  5 08:56:05.787: INFO: Unable to read wheezy_udp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:05.790: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:05.797: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:05.800: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:05.821: INFO: Unable to read jessie_udp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:05.824: INFO: Unable to read jessie_tcp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:05.827: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:05.829: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:05.848: INFO: Lookups using dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7 failed for: [wheezy_udp@dns-test-service.dns-4095.svc.cluster.local wheezy_tcp@dns-test-service.dns-4095.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local jessie_udp@dns-test-service.dns-4095.svc.cluster.local jessie_tcp@dns-test-service.dns-4095.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local]

Jul  5 08:56:10.786: INFO: Unable to read wheezy_udp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:10.789: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:10.792: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:10.794: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:10.811: INFO: Unable to read jessie_udp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:10.814: INFO: Unable to read jessie_tcp@dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:10.816: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:10.818: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local from pod dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7: the server could not find the requested resource (get pods dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7)
Jul  5 08:56:10.833: INFO: Lookups using dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7 failed for: [wheezy_udp@dns-test-service.dns-4095.svc.cluster.local wheezy_tcp@dns-test-service.dns-4095.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local jessie_udp@dns-test-service.dns-4095.svc.cluster.local jessie_tcp@dns-test-service.dns-4095.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4095.svc.cluster.local]

Jul  5 08:56:15.834: INFO: DNS probes using dns-4095/dns-test-39a62c55-dd65-4eea-be93-e3979b28c6d7 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:56:15.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4095" for this suite.
Jul  5 08:56:21.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:56:22.025: INFO: namespace dns-4095 deletion completed in 6.110085269s

• [SLOW TEST:41.381 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:56:22.025: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Jul  5 08:56:22.059: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6406" to be "success or failure"
Jul  5 08:56:22.063: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.573291ms
Jul  5 08:56:24.066: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006911628s
Jul  5 08:56:26.070: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010445055s
STEP: Saw pod success
Jul  5 08:56:26.070: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul  5 08:56:26.072: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul  5 08:56:26.090: INFO: Waiting for pod pod-host-path-test to disappear
Jul  5 08:56:26.093: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:56:26.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6406" for this suite.
Jul  5 08:56:32.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:56:32.186: INFO: namespace hostpath-6406 deletion completed in 6.090561729s

• [SLOW TEST:10.161 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:56:32.188: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-a52ff113-432e-4832-a273-081186ca9e47
STEP: Creating a pod to test consume configMaps
Jul  5 08:56:32.224: INFO: Waiting up to 5m0s for pod "pod-configmaps-a1ca69e1-7a91-4955-b3c4-304a64cc52d1" in namespace "configmap-1881" to be "success or failure"
Jul  5 08:56:32.229: INFO: Pod "pod-configmaps-a1ca69e1-7a91-4955-b3c4-304a64cc52d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.666015ms
Jul  5 08:56:34.237: INFO: Pod "pod-configmaps-a1ca69e1-7a91-4955-b3c4-304a64cc52d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012380806s
STEP: Saw pod success
Jul  5 08:56:34.237: INFO: Pod "pod-configmaps-a1ca69e1-7a91-4955-b3c4-304a64cc52d1" satisfied condition "success or failure"
Jul  5 08:56:34.241: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-configmaps-a1ca69e1-7a91-4955-b3c4-304a64cc52d1 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 08:56:34.261: INFO: Waiting for pod pod-configmaps-a1ca69e1-7a91-4955-b3c4-304a64cc52d1 to disappear
Jul  5 08:56:34.264: INFO: Pod pod-configmaps-a1ca69e1-7a91-4955-b3c4-304a64cc52d1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:56:34.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1881" for this suite.
Jul  5 08:56:40.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:56:40.366: INFO: namespace configmap-1881 deletion completed in 6.098964973s

• [SLOW TEST:8.179 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:56:40.367: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 08:56:40.399: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9c36c82-f24c-4a2a-b243-16b84775a748" in namespace "projected-2751" to be "success or failure"
Jul  5 08:56:40.403: INFO: Pod "downwardapi-volume-f9c36c82-f24c-4a2a-b243-16b84775a748": Phase="Pending", Reason="", readiness=false. Elapsed: 3.521897ms
Jul  5 08:56:42.406: INFO: Pod "downwardapi-volume-f9c36c82-f24c-4a2a-b243-16b84775a748": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00670706s
Jul  5 08:56:44.434: INFO: Pod "downwardapi-volume-f9c36c82-f24c-4a2a-b243-16b84775a748": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034578168s
STEP: Saw pod success
Jul  5 08:56:44.434: INFO: Pod "downwardapi-volume-f9c36c82-f24c-4a2a-b243-16b84775a748" satisfied condition "success or failure"
Jul  5 08:56:44.450: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod downwardapi-volume-f9c36c82-f24c-4a2a-b243-16b84775a748 container client-container: <nil>
STEP: delete the pod
Jul  5 08:56:44.469: INFO: Waiting for pod downwardapi-volume-f9c36c82-f24c-4a2a-b243-16b84775a748 to disappear
Jul  5 08:56:44.473: INFO: Pod downwardapi-volume-f9c36c82-f24c-4a2a-b243-16b84775a748 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:56:44.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2751" for this suite.
Jul  5 08:56:50.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:56:50.591: INFO: namespace projected-2751 deletion completed in 6.114072207s

• [SLOW TEST:10.224 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:56:50.591: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul  5 08:56:53.144: INFO: Successfully updated pod "pod-update-4fb747d9-4a12-4b0d-9452-f03a2e55e8f8"
STEP: verifying the updated pod is in kubernetes
Jul  5 08:56:53.149: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:56:53.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4460" for this suite.
Jul  5 08:57:15.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:57:15.251: INFO: namespace pods-4460 deletion completed in 22.099149848s

• [SLOW TEST:24.660 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:57:15.252: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 08:57:15.278: INFO: Creating ReplicaSet my-hostname-basic-7fa7e6ea-4303-491e-8f4e-f372cb788b55
Jul  5 08:57:15.286: INFO: Pod name my-hostname-basic-7fa7e6ea-4303-491e-8f4e-f372cb788b55: Found 0 pods out of 1
Jul  5 08:57:20.289: INFO: Pod name my-hostname-basic-7fa7e6ea-4303-491e-8f4e-f372cb788b55: Found 1 pods out of 1
Jul  5 08:57:20.289: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-7fa7e6ea-4303-491e-8f4e-f372cb788b55" is running
Jul  5 08:57:20.292: INFO: Pod "my-hostname-basic-7fa7e6ea-4303-491e-8f4e-f372cb788b55-6c75k" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-05 08:57:15 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-05 08:57:18 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-05 08:57:18 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-05 08:57:15 +0000 UTC Reason: Message:}])
Jul  5 08:57:20.292: INFO: Trying to dial the pod
Jul  5 08:57:25.300: INFO: Controller my-hostname-basic-7fa7e6ea-4303-491e-8f4e-f372cb788b55: Got expected result from replica 1 [my-hostname-basic-7fa7e6ea-4303-491e-8f4e-f372cb788b55-6c75k]: "my-hostname-basic-7fa7e6ea-4303-491e-8f4e-f372cb788b55-6c75k", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:57:25.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9380" for this suite.
Jul  5 08:57:31.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:57:31.398: INFO: namespace replicaset-9380 deletion completed in 6.09412205s

• [SLOW TEST:16.146 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:57:31.398: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 08:57:31.424: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:57:33.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4982" for this suite.
Jul  5 08:58:21.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:58:21.694: INFO: namespace pods-4982 deletion completed in 48.095432229s

• [SLOW TEST:50.296 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:58:21.695: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-cb0331d7-dda5-4b10-aa56-231bae6b95ca
STEP: Creating a pod to test consume configMaps
Jul  5 08:58:21.735: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8e4329e2-18b0-4cd3-9262-bf73330e98b8" in namespace "projected-2180" to be "success or failure"
Jul  5 08:58:21.741: INFO: Pod "pod-projected-configmaps-8e4329e2-18b0-4cd3-9262-bf73330e98b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.892209ms
Jul  5 08:58:23.744: INFO: Pod "pod-projected-configmaps-8e4329e2-18b0-4cd3-9262-bf73330e98b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008942508s
STEP: Saw pod success
Jul  5 08:58:23.744: INFO: Pod "pod-projected-configmaps-8e4329e2-18b0-4cd3-9262-bf73330e98b8" satisfied condition "success or failure"
Jul  5 08:58:23.747: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-projected-configmaps-8e4329e2-18b0-4cd3-9262-bf73330e98b8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 08:58:23.767: INFO: Waiting for pod pod-projected-configmaps-8e4329e2-18b0-4cd3-9262-bf73330e98b8 to disappear
Jul  5 08:58:23.770: INFO: Pod pod-projected-configmaps-8e4329e2-18b0-4cd3-9262-bf73330e98b8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:58:23.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2180" for this suite.
Jul  5 08:58:29.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:58:29.916: INFO: namespace projected-2180 deletion completed in 6.140911933s

• [SLOW TEST:8.221 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:58:29.916: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  5 08:58:31.978: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:58:31.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5272" for this suite.
Jul  5 08:58:38.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:58:38.088: INFO: namespace container-runtime-5272 deletion completed in 6.094911492s

• [SLOW TEST:8.172 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:58:38.089: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul  5 08:58:38.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 create -f - --namespace=kubectl-5389'
Jul  5 08:58:38.329: INFO: stderr: ""
Jul  5 08:58:38.329: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  5 08:58:38.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5389'
Jul  5 08:58:38.398: INFO: stderr: ""
Jul  5 08:58:38.398: INFO: stdout: "update-demo-nautilus-fs768 update-demo-nautilus-sln59 "
Jul  5 08:58:38.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-fs768 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5389'
Jul  5 08:58:38.472: INFO: stderr: ""
Jul  5 08:58:38.472: INFO: stdout: ""
Jul  5 08:58:38.472: INFO: update-demo-nautilus-fs768 is created but not running
Jul  5 08:58:43.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5389'
Jul  5 08:58:43.541: INFO: stderr: ""
Jul  5 08:58:43.541: INFO: stdout: "update-demo-nautilus-fs768 update-demo-nautilus-sln59 "
Jul  5 08:58:43.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-fs768 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5389'
Jul  5 08:58:43.613: INFO: stderr: ""
Jul  5 08:58:43.613: INFO: stdout: "true"
Jul  5 08:58:43.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-fs768 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5389'
Jul  5 08:58:43.678: INFO: stderr: ""
Jul  5 08:58:43.678: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  5 08:58:43.678: INFO: validating pod update-demo-nautilus-fs768
Jul  5 08:58:43.681: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 08:58:43.682: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 08:58:43.682: INFO: update-demo-nautilus-fs768 is verified up and running
Jul  5 08:58:43.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-sln59 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5389'
Jul  5 08:58:43.749: INFO: stderr: ""
Jul  5 08:58:43.749: INFO: stdout: "true"
Jul  5 08:58:43.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-sln59 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5389'
Jul  5 08:58:43.818: INFO: stderr: ""
Jul  5 08:58:43.818: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  5 08:58:43.818: INFO: validating pod update-demo-nautilus-sln59
Jul  5 08:58:43.822: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 08:58:43.822: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 08:58:43.822: INFO: update-demo-nautilus-sln59 is verified up and running
STEP: scaling down the replication controller
Jul  5 08:58:43.824: INFO: scanned /root for discovery docs: <nil>
Jul  5 08:58:43.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5389'
Jul  5 08:58:44.934: INFO: stderr: ""
Jul  5 08:58:44.934: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  5 08:58:44.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5389'
Jul  5 08:58:45.004: INFO: stderr: ""
Jul  5 08:58:45.004: INFO: stdout: "update-demo-nautilus-fs768 update-demo-nautilus-sln59 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul  5 08:58:50.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5389'
Jul  5 08:58:50.081: INFO: stderr: ""
Jul  5 08:58:50.081: INFO: stdout: "update-demo-nautilus-sln59 "
Jul  5 08:58:50.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-sln59 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5389'
Jul  5 08:58:50.154: INFO: stderr: ""
Jul  5 08:58:50.154: INFO: stdout: "true"
Jul  5 08:58:50.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-sln59 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5389'
Jul  5 08:58:50.232: INFO: stderr: ""
Jul  5 08:58:50.232: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  5 08:58:50.232: INFO: validating pod update-demo-nautilus-sln59
Jul  5 08:58:50.235: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 08:58:50.235: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 08:58:50.235: INFO: update-demo-nautilus-sln59 is verified up and running
STEP: scaling up the replication controller
Jul  5 08:58:50.237: INFO: scanned /root for discovery docs: <nil>
Jul  5 08:58:50.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5389'
Jul  5 08:58:51.346: INFO: stderr: ""
Jul  5 08:58:51.346: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  5 08:58:51.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5389'
Jul  5 08:58:51.439: INFO: stderr: ""
Jul  5 08:58:51.439: INFO: stdout: "update-demo-nautilus-8mxt4 update-demo-nautilus-sln59 "
Jul  5 08:58:51.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-8mxt4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5389'
Jul  5 08:58:51.515: INFO: stderr: ""
Jul  5 08:58:51.516: INFO: stdout: ""
Jul  5 08:58:51.516: INFO: update-demo-nautilus-8mxt4 is created but not running
Jul  5 08:58:56.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5389'
Jul  5 08:58:56.590: INFO: stderr: ""
Jul  5 08:58:56.590: INFO: stdout: "update-demo-nautilus-8mxt4 update-demo-nautilus-sln59 "
Jul  5 08:58:56.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-8mxt4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5389'
Jul  5 08:58:56.666: INFO: stderr: ""
Jul  5 08:58:56.666: INFO: stdout: "true"
Jul  5 08:58:56.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-8mxt4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5389'
Jul  5 08:58:56.743: INFO: stderr: ""
Jul  5 08:58:56.743: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  5 08:58:56.743: INFO: validating pod update-demo-nautilus-8mxt4
Jul  5 08:58:56.747: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 08:58:56.747: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 08:58:56.747: INFO: update-demo-nautilus-8mxt4 is verified up and running
Jul  5 08:58:56.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-sln59 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5389'
Jul  5 08:58:56.815: INFO: stderr: ""
Jul  5 08:58:56.815: INFO: stdout: "true"
Jul  5 08:58:56.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-sln59 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5389'
Jul  5 08:58:56.887: INFO: stderr: ""
Jul  5 08:58:56.887: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  5 08:58:56.887: INFO: validating pod update-demo-nautilus-sln59
Jul  5 08:58:56.890: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 08:58:56.890: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 08:58:56.890: INFO: update-demo-nautilus-sln59 is verified up and running
STEP: using delete to clean up resources
Jul  5 08:58:56.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete --grace-period=0 --force -f - --namespace=kubectl-5389'
Jul  5 08:58:56.961: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 08:58:56.961: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul  5 08:58:56.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5389'
Jul  5 08:58:57.034: INFO: stderr: "No resources found.\n"
Jul  5 08:58:57.034: INFO: stdout: ""
Jul  5 08:58:57.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -l name=update-demo --namespace=kubectl-5389 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  5 08:58:57.113: INFO: stderr: ""
Jul  5 08:58:57.113: INFO: stdout: "update-demo-nautilus-8mxt4\nupdate-demo-nautilus-sln59\n"
Jul  5 08:58:57.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5389'
Jul  5 08:58:57.687: INFO: stderr: "No resources found.\n"
Jul  5 08:58:57.687: INFO: stdout: ""
Jul  5 08:58:57.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -l name=update-demo --namespace=kubectl-5389 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  5 08:58:57.762: INFO: stderr: ""
Jul  5 08:58:57.762: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:58:57.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5389" for this suite.
Jul  5 08:59:19.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:59:19.917: INFO: namespace kubectl-5389 deletion completed in 22.151540708s

• [SLOW TEST:41.827 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:59:19.917: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-ct99
STEP: Creating a pod to test atomic-volume-subpath
Jul  5 08:59:19.972: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ct99" in namespace "subpath-5655" to be "success or failure"
Jul  5 08:59:19.977: INFO: Pod "pod-subpath-test-downwardapi-ct99": Phase="Pending", Reason="", readiness=false. Elapsed: 4.965912ms
Jul  5 08:59:21.980: INFO: Pod "pod-subpath-test-downwardapi-ct99": Phase="Running", Reason="", readiness=true. Elapsed: 2.008076071s
Jul  5 08:59:23.984: INFO: Pod "pod-subpath-test-downwardapi-ct99": Phase="Running", Reason="", readiness=true. Elapsed: 4.011245823s
Jul  5 08:59:25.987: INFO: Pod "pod-subpath-test-downwardapi-ct99": Phase="Running", Reason="", readiness=true. Elapsed: 6.014343987s
Jul  5 08:59:27.990: INFO: Pod "pod-subpath-test-downwardapi-ct99": Phase="Running", Reason="", readiness=true. Elapsed: 8.017742317s
Jul  5 08:59:29.993: INFO: Pod "pod-subpath-test-downwardapi-ct99": Phase="Running", Reason="", readiness=true. Elapsed: 10.021046217s
Jul  5 08:59:31.997: INFO: Pod "pod-subpath-test-downwardapi-ct99": Phase="Running", Reason="", readiness=true. Elapsed: 12.024331015s
Jul  5 08:59:34.000: INFO: Pod "pod-subpath-test-downwardapi-ct99": Phase="Running", Reason="", readiness=true. Elapsed: 14.027616866s
Jul  5 08:59:36.003: INFO: Pod "pod-subpath-test-downwardapi-ct99": Phase="Running", Reason="", readiness=true. Elapsed: 16.030808045s
Jul  5 08:59:38.007: INFO: Pod "pod-subpath-test-downwardapi-ct99": Phase="Running", Reason="", readiness=true. Elapsed: 18.034400925s
Jul  5 08:59:40.012: INFO: Pod "pod-subpath-test-downwardapi-ct99": Phase="Running", Reason="", readiness=true. Elapsed: 20.03975486s
Jul  5 08:59:42.016: INFO: Pod "pod-subpath-test-downwardapi-ct99": Phase="Running", Reason="", readiness=true. Elapsed: 22.043233009s
Jul  5 08:59:44.019: INFO: Pod "pod-subpath-test-downwardapi-ct99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.046339249s
STEP: Saw pod success
Jul  5 08:59:44.019: INFO: Pod "pod-subpath-test-downwardapi-ct99" satisfied condition "success or failure"
Jul  5 08:59:44.021: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-subpath-test-downwardapi-ct99 container test-container-subpath-downwardapi-ct99: <nil>
STEP: delete the pod
Jul  5 08:59:44.040: INFO: Waiting for pod pod-subpath-test-downwardapi-ct99 to disappear
Jul  5 08:59:44.042: INFO: Pod pod-subpath-test-downwardapi-ct99 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-ct99
Jul  5 08:59:44.042: INFO: Deleting pod "pod-subpath-test-downwardapi-ct99" in namespace "subpath-5655"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 08:59:44.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5655" for this suite.
Jul  5 08:59:50.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 08:59:50.147: INFO: namespace subpath-5655 deletion completed in 6.098649257s

• [SLOW TEST:30.230 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 08:59:50.147: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-96b0aeff-a45b-407c-bd51-d1fbed7a8fe2
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-96b0aeff-a45b-407c-bd51-d1fbed7a8fe2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:00:58.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-637" for this suite.
Jul  5 09:01:20.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:01:20.605: INFO: namespace configmap-637 deletion completed in 22.115588492s

• [SLOW TEST:90.458 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:01:20.607: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-sj2nc in namespace proxy-4241
I0705 09:01:20.655755      15 runners.go:180] Created replication controller with name: proxy-service-sj2nc, namespace: proxy-4241, replica count: 1
I0705 09:01:21.708802      15 runners.go:180] proxy-service-sj2nc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0705 09:01:22.708954      15 runners.go:180] proxy-service-sj2nc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0705 09:01:23.709110      15 runners.go:180] proxy-service-sj2nc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0705 09:01:24.709300      15 runners.go:180] proxy-service-sj2nc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0705 09:01:25.709469      15 runners.go:180] proxy-service-sj2nc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  5 09:01:25.712: INFO: setup took 5.076776014s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul  5 09:01:25.721: INFO: (0) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 8.55505ms)
Jul  5 09:01:25.722: INFO: (0) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 10.04546ms)
Jul  5 09:01:25.722: INFO: (0) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 10.303215ms)
Jul  5 09:01:25.723: INFO: (0) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 9.7349ms)
Jul  5 09:01:25.724: INFO: (0) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 11.760536ms)
Jul  5 09:01:25.724: INFO: (0) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 11.645223ms)
Jul  5 09:01:25.725: INFO: (0) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 12.246688ms)
Jul  5 09:01:25.729: INFO: (0) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 16.590646ms)
Jul  5 09:01:25.729: INFO: (0) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 16.360714ms)
Jul  5 09:01:25.730: INFO: (0) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 17.229518ms)
Jul  5 09:01:25.734: INFO: (0) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 21.27602ms)
Jul  5 09:01:25.734: INFO: (0) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 21.602627ms)
Jul  5 09:01:25.735: INFO: (0) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 22.510328ms)
Jul  5 09:01:25.735: INFO: (0) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 22.408696ms)
Jul  5 09:01:25.735: INFO: (0) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 23.017263ms)
Jul  5 09:01:25.735: INFO: (0) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 22.971753ms)
Jul  5 09:01:25.740: INFO: (1) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 4.57146ms)
Jul  5 09:01:25.742: INFO: (1) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 6.375576ms)
Jul  5 09:01:25.742: INFO: (1) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 6.677355ms)
Jul  5 09:01:25.743: INFO: (1) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 7.109441ms)
Jul  5 09:01:25.745: INFO: (1) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 9.453413ms)
Jul  5 09:01:25.746: INFO: (1) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 9.626396ms)
Jul  5 09:01:25.746: INFO: (1) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 10.326937ms)
Jul  5 09:01:25.746: INFO: (1) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 10.376034ms)
Jul  5 09:01:25.746: INFO: (1) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 10.834131ms)
Jul  5 09:01:25.746: INFO: (1) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 10.611846ms)
Jul  5 09:01:25.747: INFO: (1) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 10.784502ms)
Jul  5 09:01:25.748: INFO: (1) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 12.38318ms)
Jul  5 09:01:25.748: INFO: (1) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 12.600814ms)
Jul  5 09:01:25.750: INFO: (1) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 13.838951ms)
Jul  5 09:01:25.750: INFO: (1) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 14.24663ms)
Jul  5 09:01:25.750: INFO: (1) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 14.454663ms)
Jul  5 09:01:25.758: INFO: (2) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 7.42449ms)
Jul  5 09:01:25.758: INFO: (2) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 6.911336ms)
Jul  5 09:01:25.759: INFO: (2) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 8.278361ms)
Jul  5 09:01:25.760: INFO: (2) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 8.638545ms)
Jul  5 09:01:25.760: INFO: (2) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 9.221718ms)
Jul  5 09:01:25.760: INFO: (2) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 9.371564ms)
Jul  5 09:01:25.760: INFO: (2) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 9.22392ms)
Jul  5 09:01:25.760: INFO: (2) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 9.459587ms)
Jul  5 09:01:25.761: INFO: (2) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 9.760064ms)
Jul  5 09:01:25.761: INFO: (2) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 10.317858ms)
Jul  5 09:01:25.761: INFO: (2) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 9.777232ms)
Jul  5 09:01:25.762: INFO: (2) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 10.914274ms)
Jul  5 09:01:25.764: INFO: (2) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 13.333749ms)
Jul  5 09:01:25.765: INFO: (2) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 13.816154ms)
Jul  5 09:01:25.765: INFO: (2) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 13.538246ms)
Jul  5 09:01:25.765: INFO: (2) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 13.686011ms)
Jul  5 09:01:25.769: INFO: (3) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 4.629028ms)
Jul  5 09:01:25.771: INFO: (3) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 5.571162ms)
Jul  5 09:01:25.771: INFO: (3) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 5.975705ms)
Jul  5 09:01:25.772: INFO: (3) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 6.691557ms)
Jul  5 09:01:25.772: INFO: (3) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 6.616692ms)
Jul  5 09:01:25.773: INFO: (3) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 7.682353ms)
Jul  5 09:01:25.773: INFO: (3) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 7.585017ms)
Jul  5 09:01:25.773: INFO: (3) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 8.201228ms)
Jul  5 09:01:25.773: INFO: (3) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 8.074296ms)
Jul  5 09:01:25.774: INFO: (3) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 8.453812ms)
Jul  5 09:01:25.776: INFO: (3) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 10.251004ms)
Jul  5 09:01:25.777: INFO: (3) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 11.92151ms)
Jul  5 09:01:25.778: INFO: (3) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 12.06455ms)
Jul  5 09:01:25.778: INFO: (3) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 12.153625ms)
Jul  5 09:01:25.778: INFO: (3) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 12.152945ms)
Jul  5 09:01:25.778: INFO: (3) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 12.725039ms)
Jul  5 09:01:25.785: INFO: (4) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 5.812557ms)
Jul  5 09:01:25.786: INFO: (4) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 7.070959ms)
Jul  5 09:01:25.786: INFO: (4) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 8.002619ms)
Jul  5 09:01:25.786: INFO: (4) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 7.695178ms)
Jul  5 09:01:25.786: INFO: (4) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 7.862595ms)
Jul  5 09:01:25.786: INFO: (4) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 8.39673ms)
Jul  5 09:01:25.787: INFO: (4) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 8.209677ms)
Jul  5 09:01:25.787: INFO: (4) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 9.007922ms)
Jul  5 09:01:25.787: INFO: (4) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 8.927087ms)
Jul  5 09:01:25.787: INFO: (4) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 8.88935ms)
Jul  5 09:01:25.788: INFO: (4) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 9.621203ms)
Jul  5 09:01:25.788: INFO: (4) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 9.737ms)
Jul  5 09:01:25.789: INFO: (4) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 10.212081ms)
Jul  5 09:01:25.790: INFO: (4) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 11.75243ms)
Jul  5 09:01:25.791: INFO: (4) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 12.969097ms)
Jul  5 09:01:25.791: INFO: (4) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 12.534127ms)
Jul  5 09:01:25.795: INFO: (5) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 3.894942ms)
Jul  5 09:01:25.800: INFO: (5) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 8.186591ms)
Jul  5 09:01:25.800: INFO: (5) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 8.196573ms)
Jul  5 09:01:25.800: INFO: (5) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 8.222075ms)
Jul  5 09:01:25.803: INFO: (5) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 11.110229ms)
Jul  5 09:01:25.803: INFO: (5) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 11.631927ms)
Jul  5 09:01:25.803: INFO: (5) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 11.19809ms)
Jul  5 09:01:25.803: INFO: (5) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 11.616375ms)
Jul  5 09:01:25.803: INFO: (5) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 11.340921ms)
Jul  5 09:01:25.803: INFO: (5) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 11.283806ms)
Jul  5 09:01:25.803: INFO: (5) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 11.725942ms)
Jul  5 09:01:25.804: INFO: (5) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 11.9898ms)
Jul  5 09:01:25.804: INFO: (5) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 12.231127ms)
Jul  5 09:01:25.804: INFO: (5) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 12.073615ms)
Jul  5 09:01:25.804: INFO: (5) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 12.402988ms)
Jul  5 09:01:25.804: INFO: (5) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 12.630781ms)
Jul  5 09:01:25.812: INFO: (6) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 7.687379ms)
Jul  5 09:01:25.812: INFO: (6) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 7.628058ms)
Jul  5 09:01:25.813: INFO: (6) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 8.144169ms)
Jul  5 09:01:25.813: INFO: (6) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 8.42132ms)
Jul  5 09:01:25.813: INFO: (6) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 8.417752ms)
Jul  5 09:01:25.813: INFO: (6) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 8.302393ms)
Jul  5 09:01:25.813: INFO: (6) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 8.555614ms)
Jul  5 09:01:25.813: INFO: (6) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 8.858958ms)
Jul  5 09:01:25.813: INFO: (6) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 9.119434ms)
Jul  5 09:01:25.814: INFO: (6) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 8.967628ms)
Jul  5 09:01:25.813: INFO: (6) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 8.959713ms)
Jul  5 09:01:25.814: INFO: (6) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 9.753475ms)
Jul  5 09:01:25.816: INFO: (6) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 10.793493ms)
Jul  5 09:01:25.816: INFO: (6) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 11.118572ms)
Jul  5 09:01:25.816: INFO: (6) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 11.082999ms)
Jul  5 09:01:25.816: INFO: (6) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 11.53772ms)
Jul  5 09:01:25.822: INFO: (7) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 5.809832ms)
Jul  5 09:01:25.822: INFO: (7) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 6.370569ms)
Jul  5 09:01:25.823: INFO: (7) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 6.052764ms)
Jul  5 09:01:25.823: INFO: (7) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 6.314908ms)
Jul  5 09:01:25.823: INFO: (7) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 6.184708ms)
Jul  5 09:01:25.823: INFO: (7) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 6.407618ms)
Jul  5 09:01:25.823: INFO: (7) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 6.071532ms)
Jul  5 09:01:25.823: INFO: (7) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 6.611477ms)
Jul  5 09:01:25.824: INFO: (7) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 7.067439ms)
Jul  5 09:01:25.824: INFO: (7) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 7.953185ms)
Jul  5 09:01:25.825: INFO: (7) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 8.928396ms)
Jul  5 09:01:25.827: INFO: (7) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 10.271841ms)
Jul  5 09:01:25.828: INFO: (7) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 10.869407ms)
Jul  5 09:01:25.828: INFO: (7) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 11.488328ms)
Jul  5 09:01:25.828: INFO: (7) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 11.381132ms)
Jul  5 09:01:25.829: INFO: (7) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 13.063349ms)
Jul  5 09:01:25.838: INFO: (8) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 7.727699ms)
Jul  5 09:01:25.838: INFO: (8) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 7.946747ms)
Jul  5 09:01:25.839: INFO: (8) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 9.598434ms)
Jul  5 09:01:25.839: INFO: (8) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 9.640311ms)
Jul  5 09:01:25.839: INFO: (8) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 9.588647ms)
Jul  5 09:01:25.839: INFO: (8) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 9.909705ms)
Jul  5 09:01:25.840: INFO: (8) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 10.146845ms)
Jul  5 09:01:25.840: INFO: (8) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 9.991101ms)
Jul  5 09:01:25.840: INFO: (8) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 10.078064ms)
Jul  5 09:01:25.840: INFO: (8) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 10.505633ms)
Jul  5 09:01:25.841: INFO: (8) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 10.80245ms)
Jul  5 09:01:25.841: INFO: (8) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 11.276474ms)
Jul  5 09:01:25.842: INFO: (8) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 12.073456ms)
Jul  5 09:01:25.842: INFO: (8) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 12.382873ms)
Jul  5 09:01:25.842: INFO: (8) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 12.577716ms)
Jul  5 09:01:25.843: INFO: (8) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 12.535297ms)
Jul  5 09:01:25.849: INFO: (9) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 6.177074ms)
Jul  5 09:01:25.850: INFO: (9) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 6.296982ms)
Jul  5 09:01:25.850: INFO: (9) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 7.30181ms)
Jul  5 09:01:25.851: INFO: (9) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 7.445759ms)
Jul  5 09:01:25.851: INFO: (9) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 7.590569ms)
Jul  5 09:01:25.852: INFO: (9) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 8.282137ms)
Jul  5 09:01:25.852: INFO: (9) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 9.546126ms)
Jul  5 09:01:25.852: INFO: (9) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 8.835154ms)
Jul  5 09:01:25.852: INFO: (9) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 9.369518ms)
Jul  5 09:01:25.852: INFO: (9) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 9.345831ms)
Jul  5 09:01:25.853: INFO: (9) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 9.824752ms)
Jul  5 09:01:25.853: INFO: (9) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 9.370545ms)
Jul  5 09:01:25.855: INFO: (9) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 12.332317ms)
Jul  5 09:01:25.856: INFO: (9) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 13.182414ms)
Jul  5 09:01:25.856: INFO: (9) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 13.382252ms)
Jul  5 09:01:25.856: INFO: (9) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 13.602489ms)
Jul  5 09:01:25.865: INFO: (10) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 7.829404ms)
Jul  5 09:01:25.865: INFO: (10) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 8.127198ms)
Jul  5 09:01:25.866: INFO: (10) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 8.586451ms)
Jul  5 09:01:25.866: INFO: (10) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 8.431267ms)
Jul  5 09:01:25.866: INFO: (10) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 8.683388ms)
Jul  5 09:01:25.867: INFO: (10) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 9.992299ms)
Jul  5 09:01:25.867: INFO: (10) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 9.760454ms)
Jul  5 09:01:25.867: INFO: (10) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 10.199331ms)
Jul  5 09:01:25.867: INFO: (10) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 10.202764ms)
Jul  5 09:01:25.867: INFO: (10) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 10.423597ms)
Jul  5 09:01:25.867: INFO: (10) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 10.032037ms)
Jul  5 09:01:25.870: INFO: (10) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 12.734802ms)
Jul  5 09:01:25.871: INFO: (10) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 14.106219ms)
Jul  5 09:01:25.871: INFO: (10) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 14.133265ms)
Jul  5 09:01:25.871: INFO: (10) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 13.993191ms)
Jul  5 09:01:25.871: INFO: (10) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 14.107816ms)
Jul  5 09:01:25.880: INFO: (11) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 8.258285ms)
Jul  5 09:01:25.880: INFO: (11) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 8.558794ms)
Jul  5 09:01:25.880: INFO: (11) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 8.651637ms)
Jul  5 09:01:25.881: INFO: (11) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 8.71087ms)
Jul  5 09:01:25.881: INFO: (11) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 9.966914ms)
Jul  5 09:01:25.882: INFO: (11) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 9.900456ms)
Jul  5 09:01:25.882: INFO: (11) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 10.00361ms)
Jul  5 09:01:25.882: INFO: (11) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 10.095101ms)
Jul  5 09:01:25.882: INFO: (11) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 10.321344ms)
Jul  5 09:01:25.883: INFO: (11) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 10.91086ms)
Jul  5 09:01:25.883: INFO: (11) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 11.343758ms)
Jul  5 09:01:25.883: INFO: (11) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 11.426862ms)
Jul  5 09:01:25.883: INFO: (11) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 11.862856ms)
Jul  5 09:01:25.883: INFO: (11) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 11.736964ms)
Jul  5 09:01:25.883: INFO: (11) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 11.371873ms)
Jul  5 09:01:25.883: INFO: (11) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 11.732899ms)
Jul  5 09:01:25.889: INFO: (12) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 5.289565ms)
Jul  5 09:01:25.890: INFO: (12) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 5.817198ms)
Jul  5 09:01:25.890: INFO: (12) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 6.05831ms)
Jul  5 09:01:25.892: INFO: (12) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 8.17396ms)
Jul  5 09:01:25.893: INFO: (12) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 8.705072ms)
Jul  5 09:01:25.893: INFO: (12) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 9.467823ms)
Jul  5 09:01:25.893: INFO: (12) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 9.437592ms)
Jul  5 09:01:25.893: INFO: (12) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 9.229575ms)
Jul  5 09:01:25.894: INFO: (12) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 9.900206ms)
Jul  5 09:01:25.894: INFO: (12) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 9.935881ms)
Jul  5 09:01:25.894: INFO: (12) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 9.896383ms)
Jul  5 09:01:25.894: INFO: (12) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 10.662856ms)
Jul  5 09:01:25.894: INFO: (12) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 10.301511ms)
Jul  5 09:01:25.896: INFO: (12) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 11.920976ms)
Jul  5 09:01:25.897: INFO: (12) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 13.183547ms)
Jul  5 09:01:25.897: INFO: (12) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 13.35644ms)
Jul  5 09:01:25.902: INFO: (13) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 4.710561ms)
Jul  5 09:01:25.905: INFO: (13) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 6.512134ms)
Jul  5 09:01:25.905: INFO: (13) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 7.596099ms)
Jul  5 09:01:25.905: INFO: (13) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 7.193249ms)
Jul  5 09:01:25.905: INFO: (13) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 6.986496ms)
Jul  5 09:01:25.905: INFO: (13) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 7.892921ms)
Jul  5 09:01:25.905: INFO: (13) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 7.470879ms)
Jul  5 09:01:25.906: INFO: (13) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 7.972792ms)
Jul  5 09:01:25.906: INFO: (13) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 7.999879ms)
Jul  5 09:01:25.906: INFO: (13) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 7.926162ms)
Jul  5 09:01:25.908: INFO: (13) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 10.568307ms)
Jul  5 09:01:25.909: INFO: (13) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 11.142958ms)
Jul  5 09:01:25.911: INFO: (13) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 12.606849ms)
Jul  5 09:01:25.911: INFO: (13) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 12.610135ms)
Jul  5 09:01:25.911: INFO: (13) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 12.728267ms)
Jul  5 09:01:25.911: INFO: (13) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 13.488058ms)
Jul  5 09:01:25.919: INFO: (14) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 7.36598ms)
Jul  5 09:01:25.920: INFO: (14) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 8.082366ms)
Jul  5 09:01:25.920: INFO: (14) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 8.546956ms)
Jul  5 09:01:25.920: INFO: (14) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 8.46796ms)
Jul  5 09:01:25.920: INFO: (14) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 8.969922ms)
Jul  5 09:01:25.921: INFO: (14) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 8.721956ms)
Jul  5 09:01:25.921: INFO: (14) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 9.242091ms)
Jul  5 09:01:25.921: INFO: (14) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 9.545896ms)
Jul  5 09:01:25.922: INFO: (14) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 10.07894ms)
Jul  5 09:01:25.922: INFO: (14) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 10.176856ms)
Jul  5 09:01:25.922: INFO: (14) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 10.147839ms)
Jul  5 09:01:25.923: INFO: (14) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 11.006421ms)
Jul  5 09:01:25.923: INFO: (14) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 11.683574ms)
Jul  5 09:01:25.923: INFO: (14) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 11.670893ms)
Jul  5 09:01:25.923: INFO: (14) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 11.527444ms)
Jul  5 09:01:25.925: INFO: (14) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 13.114702ms)
Jul  5 09:01:25.936: INFO: (15) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 10.709308ms)
Jul  5 09:01:25.936: INFO: (15) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 10.431952ms)
Jul  5 09:01:25.936: INFO: (15) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 10.430538ms)
Jul  5 09:01:25.936: INFO: (15) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 11.181739ms)
Jul  5 09:01:25.937: INFO: (15) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 11.665706ms)
Jul  5 09:01:25.937: INFO: (15) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 11.249129ms)
Jul  5 09:01:25.937: INFO: (15) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 11.531814ms)
Jul  5 09:01:25.937: INFO: (15) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 11.801028ms)
Jul  5 09:01:25.937: INFO: (15) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 11.760934ms)
Jul  5 09:01:25.937: INFO: (15) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 11.701267ms)
Jul  5 09:01:25.937: INFO: (15) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 12.096072ms)
Jul  5 09:01:25.937: INFO: (15) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 11.855096ms)
Jul  5 09:01:25.938: INFO: (15) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 12.556958ms)
Jul  5 09:01:25.938: INFO: (15) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 12.902824ms)
Jul  5 09:01:25.938: INFO: (15) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 12.94894ms)
Jul  5 09:01:25.938: INFO: (15) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 12.771425ms)
Jul  5 09:01:25.946: INFO: (16) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 6.843187ms)
Jul  5 09:01:25.946: INFO: (16) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 6.907005ms)
Jul  5 09:01:25.946: INFO: (16) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 7.169932ms)
Jul  5 09:01:25.946: INFO: (16) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 6.930289ms)
Jul  5 09:01:25.947: INFO: (16) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 8.23884ms)
Jul  5 09:01:25.947: INFO: (16) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 8.535523ms)
Jul  5 09:01:25.947: INFO: (16) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 8.713334ms)
Jul  5 09:01:25.948: INFO: (16) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 8.785302ms)
Jul  5 09:01:25.948: INFO: (16) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 8.857537ms)
Jul  5 09:01:25.948: INFO: (16) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 9.287978ms)
Jul  5 09:01:25.948: INFO: (16) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 9.435518ms)
Jul  5 09:01:25.948: INFO: (16) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 9.446789ms)
Jul  5 09:01:25.948: INFO: (16) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 9.890789ms)
Jul  5 09:01:25.948: INFO: (16) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 9.866251ms)
Jul  5 09:01:25.949: INFO: (16) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 10.056334ms)
Jul  5 09:01:25.949: INFO: (16) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 9.828096ms)
Jul  5 09:01:25.956: INFO: (17) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 6.822793ms)
Jul  5 09:01:25.956: INFO: (17) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 6.805646ms)
Jul  5 09:01:25.956: INFO: (17) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 6.926714ms)
Jul  5 09:01:25.957: INFO: (17) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 7.303369ms)
Jul  5 09:01:25.957: INFO: (17) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 7.497405ms)
Jul  5 09:01:25.957: INFO: (17) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 7.976706ms)
Jul  5 09:01:25.957: INFO: (17) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 7.580635ms)
Jul  5 09:01:25.957: INFO: (17) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 8.248205ms)
Jul  5 09:01:25.957: INFO: (17) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 8.166429ms)
Jul  5 09:01:25.957: INFO: (17) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 8.279993ms)
Jul  5 09:01:25.959: INFO: (17) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 9.877272ms)
Jul  5 09:01:25.962: INFO: (17) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 12.472935ms)
Jul  5 09:01:25.967: INFO: (17) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 18.161078ms)
Jul  5 09:01:25.967: INFO: (17) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 18.567944ms)
Jul  5 09:01:25.968: INFO: (17) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 18.593083ms)
Jul  5 09:01:25.969: INFO: (17) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 19.960437ms)
Jul  5 09:01:25.976: INFO: (18) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 6.928581ms)
Jul  5 09:01:25.979: INFO: (18) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 9.211956ms)
Jul  5 09:01:25.980: INFO: (18) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 10.458312ms)
Jul  5 09:01:25.980: INFO: (18) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 10.689007ms)
Jul  5 09:01:25.981: INFO: (18) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 11.125172ms)
Jul  5 09:01:25.981: INFO: (18) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 11.365388ms)
Jul  5 09:01:25.981: INFO: (18) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 11.912439ms)
Jul  5 09:01:25.981: INFO: (18) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 11.810549ms)
Jul  5 09:01:25.982: INFO: (18) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 12.133557ms)
Jul  5 09:01:25.982: INFO: (18) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 12.692853ms)
Jul  5 09:01:25.986: INFO: (18) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 16.720292ms)
Jul  5 09:01:25.987: INFO: (18) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 17.783991ms)
Jul  5 09:01:25.987: INFO: (18) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 18.314715ms)
Jul  5 09:01:25.988: INFO: (18) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 17.886315ms)
Jul  5 09:01:25.990: INFO: (18) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 20.694174ms)
Jul  5 09:01:25.990: INFO: (18) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 20.793322ms)
Jul  5 09:01:25.999: INFO: (19) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 8.364826ms)
Jul  5 09:01:25.999: INFO: (19) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml/proxy/rewriteme">test</a> (200; 8.470566ms)
Jul  5 09:01:25.999: INFO: (19) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:162/proxy/: bar (200; 8.569437ms)
Jul  5 09:01:25.999: INFO: (19) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">... (200; 9.00474ms)
Jul  5 09:01:26.000: INFO: (19) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:443/proxy/tlsrewritem... (200; 9.156533ms)
Jul  5 09:01:26.000: INFO: (19) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname1/proxy/: foo (200; 9.332726ms)
Jul  5 09:01:26.000: INFO: (19) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:460/proxy/: tls baz (200; 9.1331ms)
Jul  5 09:01:26.001: INFO: (19) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 10.476745ms)
Jul  5 09:01:26.001: INFO: (19) /api/v1/namespaces/proxy-4241/pods/http:proxy-service-sj2nc-dwhml:160/proxy/: foo (200; 10.316484ms)
Jul  5 09:01:26.001: INFO: (19) /api/v1/namespaces/proxy-4241/pods/https:proxy-service-sj2nc-dwhml:462/proxy/: tls qux (200; 10.674179ms)
Jul  5 09:01:26.001: INFO: (19) /api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/: <a href="/api/v1/namespaces/proxy-4241/pods/proxy-service-sj2nc-dwhml:1080/proxy/rewriteme">test<... (200; 10.656778ms)
Jul  5 09:01:26.004: INFO: (19) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname1/proxy/: tls baz (200; 12.802425ms)
Jul  5 09:01:26.005: INFO: (19) /api/v1/namespaces/proxy-4241/services/http:proxy-service-sj2nc:portname2/proxy/: bar (200; 14.139882ms)
Jul  5 09:01:26.005: INFO: (19) /api/v1/namespaces/proxy-4241/services/https:proxy-service-sj2nc:tlsportname2/proxy/: tls qux (200; 14.024259ms)
Jul  5 09:01:26.005: INFO: (19) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname1/proxy/: foo (200; 14.141673ms)
Jul  5 09:01:26.005: INFO: (19) /api/v1/namespaces/proxy-4241/services/proxy-service-sj2nc:portname2/proxy/: bar (200; 14.331386ms)
STEP: deleting ReplicationController proxy-service-sj2nc in namespace proxy-4241, will wait for the garbage collector to delete the pods
Jul  5 09:01:26.066: INFO: Deleting ReplicationController proxy-service-sj2nc took: 7.854615ms
Jul  5 09:01:26.466: INFO: Terminating ReplicationController proxy-service-sj2nc pods took: 400.312282ms
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:01:39.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4241" for this suite.
Jul  5 09:01:45.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:01:45.363: INFO: namespace proxy-4241 deletion completed in 6.093452095s

• [SLOW TEST:24.757 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:01:45.364: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 09:01:45.397: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0dbe1a2d-9217-486d-82ca-283596b2f869" in namespace "projected-9607" to be "success or failure"
Jul  5 09:01:45.400: INFO: Pod "downwardapi-volume-0dbe1a2d-9217-486d-82ca-283596b2f869": Phase="Pending", Reason="", readiness=false. Elapsed: 2.933119ms
Jul  5 09:01:47.403: INFO: Pod "downwardapi-volume-0dbe1a2d-9217-486d-82ca-283596b2f869": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005886763s
STEP: Saw pod success
Jul  5 09:01:47.403: INFO: Pod "downwardapi-volume-0dbe1a2d-9217-486d-82ca-283596b2f869" satisfied condition "success or failure"
Jul  5 09:01:47.406: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod downwardapi-volume-0dbe1a2d-9217-486d-82ca-283596b2f869 container client-container: <nil>
STEP: delete the pod
Jul  5 09:01:47.432: INFO: Waiting for pod downwardapi-volume-0dbe1a2d-9217-486d-82ca-283596b2f869 to disappear
Jul  5 09:01:47.435: INFO: Pod downwardapi-volume-0dbe1a2d-9217-486d-82ca-283596b2f869 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:01:47.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9607" for this suite.
Jul  5 09:01:53.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:01:53.532: INFO: namespace projected-9607 deletion completed in 6.09364831s

• [SLOW TEST:8.168 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:01:53.532: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-375b414c-aab1-474e-ae1f-4d9318a330af
STEP: Creating a pod to test consume secrets
Jul  5 09:01:53.570: INFO: Waiting up to 5m0s for pod "pod-secrets-7f68bda8-fa2b-4da9-8f30-2ff785a8c003" in namespace "secrets-4853" to be "success or failure"
Jul  5 09:01:53.573: INFO: Pod "pod-secrets-7f68bda8-fa2b-4da9-8f30-2ff785a8c003": Phase="Pending", Reason="", readiness=false. Elapsed: 3.091791ms
Jul  5 09:01:55.576: INFO: Pod "pod-secrets-7f68bda8-fa2b-4da9-8f30-2ff785a8c003": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006440039s
Jul  5 09:01:57.579: INFO: Pod "pod-secrets-7f68bda8-fa2b-4da9-8f30-2ff785a8c003": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009528631s
STEP: Saw pod success
Jul  5 09:01:57.579: INFO: Pod "pod-secrets-7f68bda8-fa2b-4da9-8f30-2ff785a8c003" satisfied condition "success or failure"
Jul  5 09:01:57.582: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-secrets-7f68bda8-fa2b-4da9-8f30-2ff785a8c003 container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 09:01:57.599: INFO: Waiting for pod pod-secrets-7f68bda8-fa2b-4da9-8f30-2ff785a8c003 to disappear
Jul  5 09:01:57.601: INFO: Pod pod-secrets-7f68bda8-fa2b-4da9-8f30-2ff785a8c003 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:01:57.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4853" for this suite.
Jul  5 09:02:03.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:02:03.701: INFO: namespace secrets-4853 deletion completed in 6.096760424s

• [SLOW TEST:10.169 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:02:03.702: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 09:02:03.736: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ad4f4d2-beb4-4819-8309-2087fc8cded4" in namespace "downward-api-7393" to be "success or failure"
Jul  5 09:02:03.742: INFO: Pod "downwardapi-volume-7ad4f4d2-beb4-4819-8309-2087fc8cded4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.270084ms
Jul  5 09:02:05.745: INFO: Pod "downwardapi-volume-7ad4f4d2-beb4-4819-8309-2087fc8cded4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00914467s
STEP: Saw pod success
Jul  5 09:02:05.745: INFO: Pod "downwardapi-volume-7ad4f4d2-beb4-4819-8309-2087fc8cded4" satisfied condition "success or failure"
Jul  5 09:02:05.748: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod downwardapi-volume-7ad4f4d2-beb4-4819-8309-2087fc8cded4 container client-container: <nil>
STEP: delete the pod
Jul  5 09:02:05.776: INFO: Waiting for pod downwardapi-volume-7ad4f4d2-beb4-4819-8309-2087fc8cded4 to disappear
Jul  5 09:02:05.778: INFO: Pod downwardapi-volume-7ad4f4d2-beb4-4819-8309-2087fc8cded4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:02:05.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7393" for this suite.
Jul  5 09:02:11.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:02:11.875: INFO: namespace downward-api-7393 deletion completed in 6.093848986s

• [SLOW TEST:8.174 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:02:11.875: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-21a7920d-b914-4a93-8991-c2a92144b767 in namespace container-probe-6602
Jul  5 09:02:15.916: INFO: Started pod busybox-21a7920d-b914-4a93-8991-c2a92144b767 in namespace container-probe-6602
STEP: checking the pod's current state and verifying that restartCount is present
Jul  5 09:02:15.918: INFO: Initial restart count of pod busybox-21a7920d-b914-4a93-8991-c2a92144b767 is 0
Jul  5 09:03:06.000: INFO: Restart count of pod container-probe-6602/busybox-21a7920d-b914-4a93-8991-c2a92144b767 is now 1 (50.082092347s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:03:06.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6602" for this suite.
Jul  5 09:03:12.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:03:12.111: INFO: namespace container-probe-6602 deletion completed in 6.095962861s

• [SLOW TEST:60.236 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:03:12.111: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-3927ac87-7411-4262-8326-0daf4aafa8ed
STEP: Creating configMap with name cm-test-opt-upd-6c9e532d-eef8-4396-9e61-696e18cf30e0
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-3927ac87-7411-4262-8326-0daf4aafa8ed
STEP: Updating configmap cm-test-opt-upd-6c9e532d-eef8-4396-9e61-696e18cf30e0
STEP: Creating configMap with name cm-test-opt-create-61751695-b53c-4e8e-bc54-a2f1e302d186
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:03:16.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2169" for this suite.
Jul  5 09:03:38.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:03:38.321: INFO: namespace configmap-2169 deletion completed in 22.096206954s

• [SLOW TEST:26.210 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:03:38.322: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2517
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-2517
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2517
Jul  5 09:03:38.366: INFO: Found 0 stateful pods, waiting for 1
Jul  5 09:03:48.370: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul  5 09:03:48.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  5 09:03:48.771: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  5 09:03:48.771: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  5 09:03:48.771: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  5 09:03:48.774: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul  5 09:03:58.778: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 09:03:58.778: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 09:03:58.790: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jul  5 09:03:58.790: INFO: ss-0  ip-172-31-11-115.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:38 +0000 UTC  }]
Jul  5 09:03:58.790: INFO: 
Jul  5 09:03:58.790: INFO: StatefulSet ss has not reached scale 3, at 1
Jul  5 09:03:59.821: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996622401s
Jul  5 09:04:00.824: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.965395265s
Jul  5 09:04:01.827: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.962449087s
Jul  5 09:04:02.831: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.959100868s
Jul  5 09:04:03.834: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.955835337s
Jul  5 09:04:04.838: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.952253942s
Jul  5 09:04:05.841: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.948667631s
Jul  5 09:04:06.853: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.941390954s
Jul  5 09:04:07.857: INFO: Verifying statefulset ss doesn't scale past 3 for another 933.004047ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2517
Jul  5 09:04:08.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:04:09.090: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  5 09:04:09.090: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  5 09:04:09.090: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  5 09:04:09.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:04:09.351: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul  5 09:04:09.351: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  5 09:04:09.351: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  5 09:04:09.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:04:09.684: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul  5 09:04:09.684: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  5 09:04:09.684: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  5 09:04:09.689: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 09:04:09.689: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Jul  5 09:04:19.696: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 09:04:19.696: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 09:04:19.696: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul  5 09:04:19.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  5 09:04:19.946: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  5 09:04:19.946: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  5 09:04:19.946: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  5 09:04:19.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  5 09:04:20.264: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  5 09:04:20.264: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  5 09:04:20.264: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  5 09:04:20.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  5 09:04:20.532: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  5 09:04:20.532: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  5 09:04:20.532: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  5 09:04:20.532: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 09:04:20.535: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul  5 09:04:30.542: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 09:04:30.542: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 09:04:30.542: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 09:04:30.553: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jul  5 09:04:30.553: INFO: ss-0  ip-172-31-11-115.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:38 +0000 UTC  }]
Jul  5 09:04:30.553: INFO: ss-1  ip-172-31-1-212.eu-west-3.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  }]
Jul  5 09:04:30.553: INFO: ss-2  ip-172-31-6-226.eu-west-3.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  }]
Jul  5 09:04:30.553: INFO: 
Jul  5 09:04:30.553: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  5 09:04:31.556: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jul  5 09:04:31.556: INFO: ss-0  ip-172-31-11-115.eu-west-3.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:38 +0000 UTC  }]
Jul  5 09:04:31.556: INFO: ss-1  ip-172-31-1-212.eu-west-3.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  }]
Jul  5 09:04:31.556: INFO: ss-2  ip-172-31-6-226.eu-west-3.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  }]
Jul  5 09:04:31.556: INFO: 
Jul  5 09:04:31.556: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  5 09:04:32.559: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  5 09:04:32.559: INFO: ss-2  ip-172-31-6-226.eu-west-3.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  }]
Jul  5 09:04:32.559: INFO: 
Jul  5 09:04:32.559: INFO: StatefulSet ss has not reached scale 0, at 1
Jul  5 09:04:33.563: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  5 09:04:33.563: INFO: ss-2  ip-172-31-6-226.eu-west-3.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  }]
Jul  5 09:04:33.563: INFO: 
Jul  5 09:04:33.563: INFO: StatefulSet ss has not reached scale 0, at 1
Jul  5 09:04:34.566: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  5 09:04:34.566: INFO: ss-2  ip-172-31-6-226.eu-west-3.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  }]
Jul  5 09:04:34.566: INFO: 
Jul  5 09:04:34.566: INFO: StatefulSet ss has not reached scale 0, at 1
Jul  5 09:04:35.570: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  5 09:04:35.570: INFO: ss-2  ip-172-31-6-226.eu-west-3.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  }]
Jul  5 09:04:35.570: INFO: 
Jul  5 09:04:35.570: INFO: StatefulSet ss has not reached scale 0, at 1
Jul  5 09:04:36.573: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  5 09:04:36.573: INFO: ss-2  ip-172-31-6-226.eu-west-3.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  }]
Jul  5 09:04:36.573: INFO: 
Jul  5 09:04:36.573: INFO: StatefulSet ss has not reached scale 0, at 1
Jul  5 09:04:37.577: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  5 09:04:37.577: INFO: ss-2  ip-172-31-6-226.eu-west-3.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  }]
Jul  5 09:04:37.577: INFO: 
Jul  5 09:04:37.577: INFO: StatefulSet ss has not reached scale 0, at 1
Jul  5 09:04:38.580: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  5 09:04:38.580: INFO: ss-2  ip-172-31-6-226.eu-west-3.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  }]
Jul  5 09:04:38.580: INFO: 
Jul  5 09:04:38.580: INFO: StatefulSet ss has not reached scale 0, at 1
Jul  5 09:04:39.584: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jul  5 09:04:39.584: INFO: ss-2  ip-172-31-6-226.eu-west-3.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:04:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:03:58 +0000 UTC  }]
Jul  5 09:04:39.584: INFO: 
Jul  5 09:04:39.584: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2517
Jul  5 09:04:40.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:04:40.696: INFO: rc: 1
Jul  5 09:04:40.696: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0029cf8f0 exit status 1 <nil> <nil> true [0xc001f9e410 0xc001f9e438 0xc001f9e458] [0xc001f9e410 0xc001f9e438 0xc001f9e458] [0xc001f9e430 0xc001f9e448] [0x9d17b0 0x9d17b0] 0xc001cc7bc0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Jul  5 09:04:50.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:04:50.768: INFO: rc: 1
Jul  5 09:04:50.768: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0029cfc50 exit status 1 <nil> <nil> true [0xc001f9e460 0xc001f9e4a8 0xc001f9e4e8] [0xc001f9e460 0xc001f9e4a8 0xc001f9e4e8] [0xc001f9e498 0xc001f9e4e0] [0x9d17b0 0x9d17b0] 0xc001eae540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:05:00.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:05:00.834: INFO: rc: 1
Jul  5 09:05:00.834: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002273560 exit status 1 <nil> <nil> true [0xc002a44190 0xc002a441a8 0xc002a441c0] [0xc002a44190 0xc002a441a8 0xc002a441c0] [0xc002a441a0 0xc002a441b8] [0x9d17b0 0x9d17b0] 0xc0022b0fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:05:10.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:05:10.913: INFO: rc: 1
Jul  5 09:05:10.913: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0022738c0 exit status 1 <nil> <nil> true [0xc002a441c8 0xc002a441e0 0xc002a441f8] [0xc002a441c8 0xc002a441e0 0xc002a441f8] [0xc002a441d8 0xc002a441f0] [0x9d17b0 0x9d17b0] 0xc0022b1680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:05:20.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:05:20.988: INFO: rc: 1
Jul  5 09:05:20.988: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0029cffb0 exit status 1 <nil> <nil> true [0xc001f9e4f0 0xc001f9e528 0xc001f9e550] [0xc001f9e4f0 0xc001f9e528 0xc001f9e550] [0xc001f9e518 0xc001f9e548] [0x9d17b0 0x9d17b0] 0xc001eaf320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:05:30.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:05:31.081: INFO: rc: 1
Jul  5 09:05:31.081: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a6a360 exit status 1 <nil> <nil> true [0xc001f9e558 0xc001f9e580 0xc001f9e5c0] [0xc001f9e558 0xc001f9e580 0xc001f9e5c0] [0xc001f9e568 0xc001f9e5b8] [0x9d17b0 0x9d17b0] 0xc001dec120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:05:41.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:05:41.165: INFO: rc: 1
Jul  5 09:05:41.165: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a6a6c0 exit status 1 <nil> <nil> true [0xc001f9e5c8 0xc001f9e5e8 0xc001f9e620] [0xc001f9e5c8 0xc001f9e5e8 0xc001f9e620] [0xc001f9e5d8 0xc001f9e608] [0x9d17b0 0x9d17b0] 0xc001decf00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:05:51.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:05:51.373: INFO: rc: 1
Jul  5 09:05:51.373: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a6aa20 exit status 1 <nil> <nil> true [0xc001f9e630 0xc001f9e660 0xc001f9e688] [0xc001f9e630 0xc001f9e660 0xc001f9e688] [0xc001f9e650 0xc001f9e680] [0x9d17b0 0x9d17b0] 0xc002872180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:06:01.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:06:01.553: INFO: rc: 1
Jul  5 09:06:01.553: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a6ad50 exit status 1 <nil> <nil> true [0xc001f9e690 0xc001f9e6d8 0xc001f9e710] [0xc001f9e690 0xc001f9e6d8 0xc001f9e710] [0xc001f9e6b8 0xc001f9e708] [0x9d17b0 0x9d17b0] 0xc0028727e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:06:11.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:06:11.631: INFO: rc: 1
Jul  5 09:06:11.631: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002273c80 exit status 1 <nil> <nil> true [0xc002a44200 0xc002a44218 0xc002a44230] [0xc002a44200 0xc002a44218 0xc002a44230] [0xc002a44210 0xc002a44228] [0x9d17b0 0x9d17b0] 0xc0022b1ce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:06:21.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:06:21.725: INFO: rc: 1
Jul  5 09:06:21.725: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a20300 exit status 1 <nil> <nil> true [0xc002a44008 0xc002a44038 0xc002a44050] [0xc002a44008 0xc002a44038 0xc002a44050] [0xc002a44030 0xc002a44048] [0x9d17b0 0x9d17b0] 0xc002797620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:06:31.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:06:31.796: INFO: rc: 1
Jul  5 09:06:31.796: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0029ce330 exit status 1 <nil> <nil> true [0xc001f9e000 0xc001f9e030 0xc001f9e078] [0xc001f9e000 0xc001f9e030 0xc001f9e078] [0xc001f9e020 0xc001f9e058] [0x9d17b0 0x9d17b0] 0xc0021e5440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:06:41.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:06:41.879: INFO: rc: 1
Jul  5 09:06:41.879: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a20630 exit status 1 <nil> <nil> true [0xc002a44060 0xc002a440a0 0xc002a440b8] [0xc002a44060 0xc002a440a0 0xc002a440b8] [0xc002a44088 0xc002a440b0] [0x9d17b0 0x9d17b0] 0xc001dec540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:06:51.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:06:51.951: INFO: rc: 1
Jul  5 09:06:51.951: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a20990 exit status 1 <nil> <nil> true [0xc002a440c0 0xc002a440d8 0xc002a440f0] [0xc002a440c0 0xc002a440d8 0xc002a440f0] [0xc002a440d0 0xc002a440e8] [0x9d17b0 0x9d17b0] 0xc001ded3e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:07:01.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:07:02.032: INFO: rc: 1
Jul  5 09:07:02.032: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a20d20 exit status 1 <nil> <nil> true [0xc002a440f8 0xc002a44110 0xc002a44128] [0xc002a440f8 0xc002a44110 0xc002a44128] [0xc002a44108 0xc002a44120] [0x9d17b0 0x9d17b0] 0xc001eae7e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:07:12.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:07:12.107: INFO: rc: 1
Jul  5 09:07:12.107: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a210b0 exit status 1 <nil> <nil> true [0xc002a44130 0xc002a44148 0xc002a44160] [0xc002a44130 0xc002a44148 0xc002a44160] [0xc002a44140 0xc002a44158] [0x9d17b0 0x9d17b0] 0xc001eaf620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:07:22.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:07:22.174: INFO: rc: 1
Jul  5 09:07:22.175: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0029ce6c0 exit status 1 <nil> <nil> true [0xc001f9e088 0xc001f9e0c8 0xc001f9e0f0] [0xc001f9e088 0xc001f9e0c8 0xc001f9e0f0] [0xc001f9e0c0 0xc001f9e0e0] [0x9d17b0 0x9d17b0] 0xc001cc6480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:07:32.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:07:32.280: INFO: rc: 1
Jul  5 09:07:32.280: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0029cea20 exit status 1 <nil> <nil> true [0xc001f9e100 0xc001f9e130 0xc001f9e150] [0xc001f9e100 0xc001f9e130 0xc001f9e150] [0xc001f9e110 0xc001f9e148] [0x9d17b0 0x9d17b0] 0xc001cc6c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:07:42.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:07:42.347: INFO: rc: 1
Jul  5 09:07:42.347: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0029ced80 exit status 1 <nil> <nil> true [0xc001f9e160 0xc001f9e190 0xc001f9e1d0] [0xc001f9e160 0xc001f9e190 0xc001f9e1d0] [0xc001f9e180 0xc001f9e1c8] [0x9d17b0 0x9d17b0] 0xc001cc7560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:07:52.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:07:52.417: INFO: rc: 1
Jul  5 09:07:52.417: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0029cf0e0 exit status 1 <nil> <nil> true [0xc001f9e1e0 0xc001f9e230 0xc001f9e250] [0xc001f9e1e0 0xc001f9e230 0xc001f9e250] [0xc001f9e210 0xc001f9e248] [0x9d17b0 0x9d17b0] 0xc001cc7c80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:08:02.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:08:02.489: INFO: rc: 1
Jul  5 09:08:02.489: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a21470 exit status 1 <nil> <nil> true [0xc002a44168 0xc002a44180 0xc002a44198] [0xc002a44168 0xc002a44180 0xc002a44198] [0xc002a44178 0xc002a44190] [0x9d17b0 0x9d17b0] 0xc0024faae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:08:12.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:08:12.566: INFO: rc: 1
Jul  5 09:08:12.566: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a217d0 exit status 1 <nil> <nil> true [0xc002a441a0 0xc002a441b8 0xc002a441d0] [0xc002a441a0 0xc002a441b8 0xc002a441d0] [0xc002a441b0 0xc002a441c8] [0x9d17b0 0x9d17b0] 0xc00260a300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:08:22.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:08:22.638: INFO: rc: 1
Jul  5 09:08:22.639: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0029ce300 exit status 1 <nil> <nil> true [0xc001f9e010 0xc001f9e040 0xc001f9e088] [0xc001f9e010 0xc001f9e040 0xc001f9e088] [0xc001f9e030 0xc001f9e078] [0x9d17b0 0x9d17b0] 0xc0024fbaa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:08:32.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:08:32.711: INFO: rc: 1
Jul  5 09:08:32.711: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0029ce6f0 exit status 1 <nil> <nil> true [0xc001f9e0a0 0xc001f9e0d0 0xc001f9e100] [0xc001f9e0a0 0xc001f9e0d0 0xc001f9e100] [0xc001f9e0c8 0xc001f9e0f0] [0x9d17b0 0x9d17b0] 0xc001eae960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:08:42.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:08:42.797: INFO: rc: 1
Jul  5 09:08:42.797: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a20330 exit status 1 <nil> <nil> true [0xc002a44000 0xc002a44030 0xc002a44048] [0xc002a44000 0xc002a44030 0xc002a44048] [0xc002a44028 0xc002a44040] [0x9d17b0 0x9d17b0] 0xc001decb40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:08:52.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:08:52.881: INFO: rc: 1
Jul  5 09:08:52.881: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0029cea80 exit status 1 <nil> <nil> true [0xc001f9e108 0xc001f9e140 0xc001f9e160] [0xc001f9e108 0xc001f9e140 0xc001f9e160] [0xc001f9e130 0xc001f9e150] [0x9d17b0 0x9d17b0] 0xc001eaf7a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:09:02.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:09:02.949: INFO: rc: 1
Jul  5 09:09:02.949: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0029cee10 exit status 1 <nil> <nil> true [0xc001f9e170 0xc001f9e1b0 0xc001f9e1e0] [0xc001f9e170 0xc001f9e1b0 0xc001f9e1e0] [0xc001f9e190 0xc001f9e1d0] [0x9d17b0 0x9d17b0] 0xc0021e49c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:09:12.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:09:13.024: INFO: rc: 1
Jul  5 09:09:13.024: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0029cf1a0 exit status 1 <nil> <nil> true [0xc001f9e200 0xc001f9e240 0xc001f9e260] [0xc001f9e200 0xc001f9e240 0xc001f9e260] [0xc001f9e230 0xc001f9e250] [0x9d17b0 0x9d17b0] 0xc0027962a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:09:23.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:09:23.096: INFO: rc: 1
Jul  5 09:09:23.096: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0029cf6e0 exit status 1 <nil> <nil> true [0xc001f9e270 0xc001f9e2a0 0xc001f9e2d0] [0xc001f9e270 0xc001f9e2a0 0xc001f9e2d0] [0xc001f9e298 0xc001f9e2c0] [0x9d17b0 0x9d17b0] 0xc002797bc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:09:33.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:09:33.189: INFO: rc: 1
Jul  5 09:09:33.190: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a206c0 exit status 1 <nil> <nil> true [0xc002a44050 0xc002a44088 0xc002a440b0] [0xc002a44050 0xc002a44088 0xc002a440b0] [0xc002a44080 0xc002a440a8] [0x9d17b0 0x9d17b0] 0xc001cc6000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jul  5 09:09:43.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2517 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:09:43.270: INFO: rc: 1
Jul  5 09:09:43.270: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Jul  5 09:09:43.270: INFO: Scaling statefulset ss to 0
Jul  5 09:09:43.279: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul  5 09:09:43.281: INFO: Deleting all statefulset in ns statefulset-2517
Jul  5 09:09:43.284: INFO: Scaling statefulset ss to 0
Jul  5 09:09:43.291: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 09:09:43.293: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:09:43.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2517" for this suite.
Jul  5 09:09:49.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:09:49.422: INFO: namespace statefulset-2517 deletion completed in 6.105785865s

• [SLOW TEST:371.100 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:09:49.423: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9022.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9022.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9022.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9022.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9022.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9022.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  5 09:10:01.494: INFO: DNS probes using dns-9022/dns-test-9f488463-a910-47e8-8617-6b638138c5f0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:10:01.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9022" for this suite.
Jul  5 09:10:07.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:10:07.615: INFO: namespace dns-9022 deletion completed in 6.099421685s

• [SLOW TEST:18.193 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:10:07.616: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:10:11.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-81" for this suite.
Jul  5 09:10:17.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:10:17.761: INFO: namespace kubelet-test-81 deletion completed in 6.098324845s

• [SLOW TEST:10.146 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:10:17.761: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 09:10:17.788: INFO: Creating deployment "nginx-deployment"
Jul  5 09:10:17.794: INFO: Waiting for observed generation 1
Jul  5 09:10:19.802: INFO: Waiting for all required pods to come up
Jul  5 09:10:19.807: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul  5 09:10:21.835: INFO: Waiting for deployment "nginx-deployment" to complete
Jul  5 09:10:21.840: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jul  5 09:10:21.847: INFO: Updating deployment nginx-deployment
Jul  5 09:10:21.847: INFO: Waiting for observed generation 2
Jul  5 09:10:23.853: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul  5 09:10:23.856: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul  5 09:10:23.859: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul  5 09:10:23.866: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul  5 09:10:23.866: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul  5 09:10:23.868: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul  5 09:10:23.872: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jul  5 09:10:23.872: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jul  5 09:10:23.879: INFO: Updating deployment nginx-deployment
Jul  5 09:10:23.879: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jul  5 09:10:23.885: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul  5 09:10:23.890: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul  5 09:10:23.914: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-9517,SelfLink:/apis/apps/v1/namespaces/deployment-9517/deployments/nginx-deployment,UID:0ea6d4c0-1582-4b97-a9d5-888e638660ca,ResourceVersion:5809,Generation:3,CreationTimestamp:2019-07-05 09:10:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Available True 2019-07-05 09:10:21 +0000 UTC 2019-07-05 09:10:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-05 09:10:22 +0000 UTC 2019-07-05 09:10:17 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Jul  5 09:10:23.930: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-9517,SelfLink:/apis/apps/v1/namespaces/deployment-9517/replicasets/nginx-deployment-55fb7cb77f,UID:541bf13b-d38f-4067-86cb-b1291c05ada7,ResourceVersion:5811,Generation:3,CreationTimestamp:2019-07-05 09:10:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 0ea6d4c0-1582-4b97-a9d5-888e638660ca 0xc0014bd5c7 0xc0014bd5c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  5 09:10:23.930: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jul  5 09:10:23.930: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-9517,SelfLink:/apis/apps/v1/namespaces/deployment-9517/replicasets/nginx-deployment-7b8c6f4498,UID:0231633c-cdb2-4640-bbe3-2c44d4535dd0,ResourceVersion:5810,Generation:3,CreationTimestamp:2019-07-05 09:10:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 0ea6d4c0-1582-4b97-a9d5-888e638660ca 0xc0014bd697 0xc0014bd698}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jul  5 09:10:23.966: INFO: Pod "nginx-deployment-55fb7cb77f-5qdsb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-5qdsb,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-55fb7cb77f-5qdsb,UID:417da850-5d6a-4601-b3ea-ca8e6fe2fa22,ResourceVersion:5796,Generation:0,CreationTimestamp:2019-07-05 09:10:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.5.7/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 541bf13b-d38f-4067-86cb-b1291c05ada7 0xc0016cc030 0xc0016cc031}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-226.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cc0a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cc0c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC  }],Message:,Reason:,HostIP:172.31.6.226,PodIP:,StartTime:2019-07-05 09:10:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.966: INFO: Pod "nginx-deployment-55fb7cb77f-86qh6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-86qh6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-55fb7cb77f-86qh6,UID:f552e7ae-3e94-4828-aec1-1ddcef4dfb80,ResourceVersion:5798,Generation:0,CreationTimestamp:2019-07-05 09:10:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.25/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 541bf13b-d38f-4067-86cb-b1291c05ada7 0xc0016cc1a0 0xc0016cc1a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-115.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cc210} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cc230}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC  }],Message:,Reason:,HostIP:172.31.11.115,PodIP:,StartTime:2019-07-05 09:10:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.966: INFO: Pod "nginx-deployment-55fb7cb77f-bmqmq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-bmqmq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-55fb7cb77f-bmqmq,UID:b528a441-4d98-4aa4-b3bf-6813099bd8b4,ResourceVersion:5799,Generation:0,CreationTimestamp:2019-07-05 09:10:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.20/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 541bf13b-d38f-4067-86cb-b1291c05ada7 0xc0016cc310 0xc0016cc311}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-212.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cc390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cc3b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC  }],Message:,Reason:,HostIP:172.31.1.212,PodIP:,StartTime:2019-07-05 09:10:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.966: INFO: Pod "nginx-deployment-55fb7cb77f-jhvz9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-jhvz9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-55fb7cb77f-jhvz9,UID:a211ccd2-e91b-4203-9fa1-fb0a70cb5a4e,ResourceVersion:5827,Generation:0,CreationTimestamp:2019-07-05 09:10:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 541bf13b-d38f-4067-86cb-b1291c05ada7 0xc0016cc480 0xc0016cc481}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cc4f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cc540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.966: INFO: Pod "nginx-deployment-55fb7cb77f-mjcdz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mjcdz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-55fb7cb77f-mjcdz,UID:ef6d90fd-4aa9-42d3-9d46-72edd6fb5fcd,ResourceVersion:5804,Generation:0,CreationTimestamp:2019-07-05 09:10:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.21/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 541bf13b-d38f-4067-86cb-b1291c05ada7 0xc0016cc5e7 0xc0016cc5e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-212.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cc650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cc670}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:22 +0000 UTC  }],Message:,Reason:,HostIP:172.31.1.212,PodIP:,StartTime:2019-07-05 09:10:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.967: INFO: Pod "nginx-deployment-55fb7cb77f-ppxrz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-ppxrz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-55fb7cb77f-ppxrz,UID:0318a22a-dee5-43ba-8e52-f46224e4068d,ResourceVersion:5806,Generation:0,CreationTimestamp:2019-07-05 09:10:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.26/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 541bf13b-d38f-4067-86cb-b1291c05ada7 0xc0016cc780 0xc0016cc781}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-115.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cc7f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cc810}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:22 +0000 UTC  }],Message:,Reason:,HostIP:172.31.11.115,PodIP:,StartTime:2019-07-05 09:10:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.967: INFO: Pod "nginx-deployment-55fb7cb77f-q2t6m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-q2t6m,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-55fb7cb77f-q2t6m,UID:b664798c-8ff8-4312-a3a7-668001c3db4f,ResourceVersion:5832,Generation:0,CreationTimestamp:2019-07-05 09:10:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 541bf13b-d38f-4067-86cb-b1291c05ada7 0xc0016cc8f0 0xc0016cc8f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-226.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cc960} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cc980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.967: INFO: Pod "nginx-deployment-55fb7cb77f-rmtfg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rmtfg,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-55fb7cb77f-rmtfg,UID:906bc234-3439-449d-b7fe-c371fae4ff6b,ResourceVersion:5828,Generation:0,CreationTimestamp:2019-07-05 09:10:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 541bf13b-d38f-4067-86cb-b1291c05ada7 0xc0016cca00 0xc0016cca01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-226.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cca70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cca90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.967: INFO: Pod "nginx-deployment-7b8c6f4498-2vtdx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-2vtdx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-7b8c6f4498-2vtdx,UID:550c9074-2c01-4a87-bd4a-c51c39dbf767,ResourceVersion:5716,Generation:0,CreationTimestamp:2019-07-05 09:10:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.5.6/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0231633c-cdb2-4640-bbe3-2c44d4535dd0 0xc0016ccb30 0xc0016ccb31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-226.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016ccb90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016ccbb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  }],Message:,Reason:,HostIP:172.31.6.226,PodIP:10.244.5.6,StartTime:2019-07-05 09:10:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-05 09:10:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://125e34a41d62e1d630f60811173e5117043ec119dda9c59495f4b3d8f29e0f34}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.967: INFO: Pod "nginx-deployment-7b8c6f4498-48jvb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-48jvb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-7b8c6f4498-48jvb,UID:d4d052a9-f1e0-4e55-8406-a022efcd8ec9,ResourceVersion:5726,Generation:0,CreationTimestamp:2019-07-05 09:10:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.23/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0231633c-cdb2-4640-bbe3-2c44d4535dd0 0xc0016ccca0 0xc0016ccca1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-115.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016ccd00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016ccd20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  }],Message:,Reason:,HostIP:172.31.11.115,PodIP:10.244.3.23,StartTime:2019-07-05 09:10:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-05 09:10:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://847bcb2343bb1cc1975bddaeb06189e07bdbb017cf4a1e1625c8f1fad8511ae1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.967: INFO: Pod "nginx-deployment-7b8c6f4498-4rd2p" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4rd2p,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-7b8c6f4498-4rd2p,UID:a0d1f109-4ce0-4e19-9d8f-ff3d380cf3a8,ResourceVersion:5704,Generation:0,CreationTimestamp:2019-07-05 09:10:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.18/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0231633c-cdb2-4640-bbe3-2c44d4535dd0 0xc0016cce10 0xc0016cce11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-212.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cce70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cce90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  }],Message:,Reason:,HostIP:172.31.1.212,PodIP:10.244.4.18,StartTime:2019-07-05 09:10:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-05 09:10:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8361aa18decce3de42f43b3d4b3bebd1073bd59c932b32a49a0e7adc3207da21}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.967: INFO: Pod "nginx-deployment-7b8c6f4498-5cbxg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5cbxg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-7b8c6f4498-5cbxg,UID:e36ecdfa-9a93-46f2-8e08-1c59a2dae942,ResourceVersion:5732,Generation:0,CreationTimestamp:2019-07-05 09:10:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.22/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0231633c-cdb2-4640-bbe3-2c44d4535dd0 0xc0016ccf70 0xc0016ccf71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-115.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016ccfd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016ccff0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  }],Message:,Reason:,HostIP:172.31.11.115,PodIP:10.244.3.22,StartTime:2019-07-05 09:10:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-05 09:10:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://7a5d9697cd0aba5cb5036f46f69455860a6d94cb190f915b5f8ff96a116151a5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.968: INFO: Pod "nginx-deployment-7b8c6f4498-5dr29" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5dr29,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-7b8c6f4498-5dr29,UID:b46a394e-9246-4849-908e-c0b8b86d8a91,ResourceVersion:5834,Generation:0,CreationTimestamp:2019-07-05 09:10:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0231633c-cdb2-4640-bbe3-2c44d4535dd0 0xc0016cd0c0 0xc0016cd0c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-212.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cd120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cd140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:23 +0000 UTC  }],Message:,Reason:,HostIP:172.31.1.212,PodIP:,StartTime:2019-07-05 09:10:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.968: INFO: Pod "nginx-deployment-7b8c6f4498-5m5qz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5m5qz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-7b8c6f4498-5m5qz,UID:0c594ba8-26c8-42c9-a0ab-ff1e64b62d85,ResourceVersion:5826,Generation:0,CreationTimestamp:2019-07-05 09:10:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0231633c-cdb2-4640-bbe3-2c44d4535dd0 0xc0016cd207 0xc0016cd208}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-115.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cd270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cd290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.968: INFO: Pod "nginx-deployment-7b8c6f4498-5s7sq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5s7sq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-7b8c6f4498-5s7sq,UID:6764fd2a-3f83-4c0d-bd18-7aa7bfdc13c3,ResourceVersion:5820,Generation:0,CreationTimestamp:2019-07-05 09:10:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0231633c-cdb2-4640-bbe3-2c44d4535dd0 0xc0016cd310 0xc0016cd311}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-212.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cd370} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cd390}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:23 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.968: INFO: Pod "nginx-deployment-7b8c6f4498-8mh44" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8mh44,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-7b8c6f4498-8mh44,UID:926e2684-b4a7-45c2-ab98-1f8445b6577b,ResourceVersion:5829,Generation:0,CreationTimestamp:2019-07-05 09:10:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0231633c-cdb2-4640-bbe3-2c44d4535dd0 0xc0016cd410 0xc0016cd411}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cd470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cd490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.968: INFO: Pod "nginx-deployment-7b8c6f4498-gcw7b" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gcw7b,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-7b8c6f4498-gcw7b,UID:dbc6a27d-9d5f-41a3-bdb3-3cb5b5d1f929,ResourceVersion:5719,Generation:0,CreationTimestamp:2019-07-05 09:10:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.5.5/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0231633c-cdb2-4640-bbe3-2c44d4535dd0 0xc0016cd507 0xc0016cd508}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-226.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cd570} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cd590}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  }],Message:,Reason:,HostIP:172.31.6.226,PodIP:10.244.5.5,StartTime:2019-07-05 09:10:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-05 09:10:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c489c6889ee16f42d50ab855f9476c7628575de6b33443b2139a9413fb7f79c2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.968: INFO: Pod "nginx-deployment-7b8c6f4498-hpw58" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hpw58,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-7b8c6f4498-hpw58,UID:1909685a-882b-4b35-96de-6b2accac1144,ResourceVersion:5835,Generation:0,CreationTimestamp:2019-07-05 09:10:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0231633c-cdb2-4640-bbe3-2c44d4535dd0 0xc0016cd660 0xc0016cd661}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-212.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cd6c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cd6e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.968: INFO: Pod "nginx-deployment-7b8c6f4498-ls22k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ls22k,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-7b8c6f4498-ls22k,UID:ba313769-70de-4c78-8ce4-43c9e96f357a,ResourceVersion:5710,Generation:0,CreationTimestamp:2019-07-05 09:10:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.19/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0231633c-cdb2-4640-bbe3-2c44d4535dd0 0xc0016cd770 0xc0016cd771}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-212.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cd7d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cd7f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  }],Message:,Reason:,HostIP:172.31.1.212,PodIP:10.244.4.19,StartTime:2019-07-05 09:10:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-05 09:10:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://7a49536aa8e46c37f397651fae8c5e8c1eb8dfc663c944ac0a2160eaa34b726c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.968: INFO: Pod "nginx-deployment-7b8c6f4498-nrrtq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-nrrtq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-7b8c6f4498-nrrtq,UID:dd2a986f-d91a-453b-8449-375c53bc69e4,ResourceVersion:5831,Generation:0,CreationTimestamp:2019-07-05 09:10:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0231633c-cdb2-4640-bbe3-2c44d4535dd0 0xc0016cd8c0 0xc0016cd8c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-212.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cd920} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cd940}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.968: INFO: Pod "nginx-deployment-7b8c6f4498-p2tb4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-p2tb4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-7b8c6f4498-p2tb4,UID:087e1a18-a14a-4ac6-b1bc-7b066a461a63,ResourceVersion:5836,Generation:0,CreationTimestamp:2019-07-05 09:10:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0231633c-cdb2-4640-bbe3-2c44d4535dd0 0xc0016cd9c0 0xc0016cd9c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-226.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cda20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cda40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:24 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.969: INFO: Pod "nginx-deployment-7b8c6f4498-shf9x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-shf9x,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-7b8c6f4498-shf9x,UID:9a873301-e568-448a-b015-682a87ec0dd2,ResourceVersion:5722,Generation:0,CreationTimestamp:2019-07-05 09:10:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.5.4/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0231633c-cdb2-4640-bbe3-2c44d4535dd0 0xc0016cdad0 0xc0016cdad1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-226.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cdb30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cdb50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  }],Message:,Reason:,HostIP:172.31.6.226,PodIP:10.244.5.4,StartTime:2019-07-05 09:10:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-05 09:10:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1f2de6fe690eafbc43230c1a0e367ab88d227d48e15989010cb860fdf7344a33}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  5 09:10:23.969: INFO: Pod "nginx-deployment-7b8c6f4498-v8bb8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-v8bb8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9517,SelfLink:/api/v1/namespaces/deployment-9517/pods/nginx-deployment-7b8c6f4498-v8bb8,UID:2880b49d-6056-4753-a736-0a99993b7b68,ResourceVersion:5695,Generation:0,CreationTimestamp:2019-07-05 09:10:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.21/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0231633c-cdb2-4640-bbe3-2c44d4535dd0 0xc0016cdc30 0xc0016cdc31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vw96d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vw96d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vw96d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-115.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cdc90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cdcb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:10:17 +0000 UTC  }],Message:,Reason:,HostIP:172.31.11.115,PodIP:10.244.3.21,StartTime:2019-07-05 09:10:17 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-05 09:10:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://961c3523c96186b324d45d4e5f62155933926614a4223c1a8f370fd1aef85b0f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:10:23.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9517" for this suite.
Jul  5 09:10:32.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:10:32.124: INFO: namespace deployment-9517 deletion completed in 8.135872005s

• [SLOW TEST:14.363 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:10:32.124: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Jul  5 09:10:32.160: INFO: Waiting up to 5m0s for pod "var-expansion-179d1f6f-7af6-4b03-9f81-41583cd938bd" in namespace "var-expansion-2671" to be "success or failure"
Jul  5 09:10:32.165: INFO: Pod "var-expansion-179d1f6f-7af6-4b03-9f81-41583cd938bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.969711ms
Jul  5 09:10:34.169: INFO: Pod "var-expansion-179d1f6f-7af6-4b03-9f81-41583cd938bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008498443s
STEP: Saw pod success
Jul  5 09:10:34.169: INFO: Pod "var-expansion-179d1f6f-7af6-4b03-9f81-41583cd938bd" satisfied condition "success or failure"
Jul  5 09:10:34.171: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod var-expansion-179d1f6f-7af6-4b03-9f81-41583cd938bd container dapi-container: <nil>
STEP: delete the pod
Jul  5 09:10:34.193: INFO: Waiting for pod var-expansion-179d1f6f-7af6-4b03-9f81-41583cd938bd to disappear
Jul  5 09:10:34.195: INFO: Pod var-expansion-179d1f6f-7af6-4b03-9f81-41583cd938bd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:10:34.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2671" for this suite.
Jul  5 09:10:40.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:10:40.347: INFO: namespace var-expansion-2671 deletion completed in 6.147041128s

• [SLOW TEST:8.222 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:10:40.347: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul  5 09:10:40.391: INFO: Waiting up to 5m0s for pod "pod-17fd925b-fd95-4e8b-9e56-e58cb1570d29" in namespace "emptydir-5684" to be "success or failure"
Jul  5 09:10:40.395: INFO: Pod "pod-17fd925b-fd95-4e8b-9e56-e58cb1570d29": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277932ms
Jul  5 09:10:42.398: INFO: Pod "pod-17fd925b-fd95-4e8b-9e56-e58cb1570d29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007587073s
Jul  5 09:10:44.401: INFO: Pod "pod-17fd925b-fd95-4e8b-9e56-e58cb1570d29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010420164s
STEP: Saw pod success
Jul  5 09:10:44.401: INFO: Pod "pod-17fd925b-fd95-4e8b-9e56-e58cb1570d29" satisfied condition "success or failure"
Jul  5 09:10:44.403: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-17fd925b-fd95-4e8b-9e56-e58cb1570d29 container test-container: <nil>
STEP: delete the pod
Jul  5 09:10:44.421: INFO: Waiting for pod pod-17fd925b-fd95-4e8b-9e56-e58cb1570d29 to disappear
Jul  5 09:10:44.425: INFO: Pod pod-17fd925b-fd95-4e8b-9e56-e58cb1570d29 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:10:44.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5684" for this suite.
Jul  5 09:10:50.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:10:50.530: INFO: namespace emptydir-5684 deletion completed in 6.102392868s

• [SLOW TEST:10.184 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:10:50.531: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul  5 09:10:50.567: INFO: Waiting up to 5m0s for pod "pod-19e0d1ff-b611-4e02-a9fa-ea2c895a49dd" in namespace "emptydir-7349" to be "success or failure"
Jul  5 09:10:50.571: INFO: Pod "pod-19e0d1ff-b611-4e02-a9fa-ea2c895a49dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095221ms
Jul  5 09:10:52.574: INFO: Pod "pod-19e0d1ff-b611-4e02-a9fa-ea2c895a49dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007086445s
STEP: Saw pod success
Jul  5 09:10:52.574: INFO: Pod "pod-19e0d1ff-b611-4e02-a9fa-ea2c895a49dd" satisfied condition "success or failure"
Jul  5 09:10:52.576: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-19e0d1ff-b611-4e02-a9fa-ea2c895a49dd container test-container: <nil>
STEP: delete the pod
Jul  5 09:10:52.592: INFO: Waiting for pod pod-19e0d1ff-b611-4e02-a9fa-ea2c895a49dd to disappear
Jul  5 09:10:52.594: INFO: Pod pod-19e0d1ff-b611-4e02-a9fa-ea2c895a49dd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:10:52.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7349" for this suite.
Jul  5 09:10:58.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:10:58.711: INFO: namespace emptydir-7349 deletion completed in 6.113300786s

• [SLOW TEST:8.180 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:10:58.711: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 09:10:58.771: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e4605b8c-e45c-4dbb-99c4-1e62d9731897", Controller:(*bool)(0xc0024c97ba), BlockOwnerDeletion:(*bool)(0xc0024c97bb)}}
Jul  5 09:10:58.783: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"0c118487-2d50-4a6c-b644-3e79a15ca0c7", Controller:(*bool)(0xc0032c239a), BlockOwnerDeletion:(*bool)(0xc0032c239b)}}
Jul  5 09:10:58.791: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1216fc12-d9e1-4ad7-ab90-2a056938c70e", Controller:(*bool)(0xc0024c99b6), BlockOwnerDeletion:(*bool)(0xc0024c99b7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:11:03.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3223" for this suite.
Jul  5 09:11:09.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:11:09.963: INFO: namespace gc-3223 deletion completed in 6.15905632s

• [SLOW TEST:11.252 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:11:09.964: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul  5 09:11:10.001: INFO: Waiting up to 5m0s for pod "pod-fb927542-3316-4cb4-a783-4080d525ac29" in namespace "emptydir-6796" to be "success or failure"
Jul  5 09:11:10.008: INFO: Pod "pod-fb927542-3316-4cb4-a783-4080d525ac29": Phase="Pending", Reason="", readiness=false. Elapsed: 6.294114ms
Jul  5 09:11:12.011: INFO: Pod "pod-fb927542-3316-4cb4-a783-4080d525ac29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009352106s
Jul  5 09:11:14.014: INFO: Pod "pod-fb927542-3316-4cb4-a783-4080d525ac29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01212826s
STEP: Saw pod success
Jul  5 09:11:14.014: INFO: Pod "pod-fb927542-3316-4cb4-a783-4080d525ac29" satisfied condition "success or failure"
Jul  5 09:11:14.016: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-fb927542-3316-4cb4-a783-4080d525ac29 container test-container: <nil>
STEP: delete the pod
Jul  5 09:11:14.033: INFO: Waiting for pod pod-fb927542-3316-4cb4-a783-4080d525ac29 to disappear
Jul  5 09:11:14.035: INFO: Pod pod-fb927542-3316-4cb4-a783-4080d525ac29 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:11:14.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6796" for this suite.
Jul  5 09:11:20.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:11:20.132: INFO: namespace emptydir-6796 deletion completed in 6.094243787s

• [SLOW TEST:10.168 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:11:20.132: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  5 09:11:20.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-1867'
Jul  5 09:11:20.239: INFO: stderr: ""
Jul  5 09:11:20.239: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jul  5 09:11:25.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pod e2e-test-nginx-pod --namespace=kubectl-1867 -o json'
Jul  5 09:11:25.368: INFO: stderr: ""
Jul  5 09:11:25.368: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.244.4.30/32\"\n        },\n        \"creationTimestamp\": \"2019-07-05T09:11:20Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-1867\",\n        \"resourceVersion\": \"6439\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1867/pods/e2e-test-nginx-pod\",\n        \"uid\": \"136ea521-07d8-42cd-88b9-c24952fadecb\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-fbqg4\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-1-212.eu-west-3.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-fbqg4\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-fbqg4\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-05T09:11:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-05T09:11:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-05T09:11:22Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-05T09:11:20Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://9f17431b9d625f546fa7295136f661bd63989ab82706ad6dcb3e93e2da38ddfb\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-07-05T09:11:21Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.1.212\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.4.30\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-07-05T09:11:20Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul  5 09:11:25.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 replace -f - --namespace=kubectl-1867'
Jul  5 09:11:25.590: INFO: stderr: ""
Jul  5 09:11:25.590: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Jul  5 09:11:25.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete pods e2e-test-nginx-pod --namespace=kubectl-1867'
Jul  5 09:11:40.013: INFO: stderr: ""
Jul  5 09:11:40.013: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:11:40.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1867" for this suite.
Jul  5 09:11:46.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:11:46.121: INFO: namespace kubectl-1867 deletion completed in 6.10360191s

• [SLOW TEST:25.988 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:11:46.121: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Jul  5 09:11:46.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 create -f - --namespace=kubectl-3224'
Jul  5 09:11:46.335: INFO: stderr: ""
Jul  5 09:11:46.335: INFO: stdout: "pod/pause created\n"
Jul  5 09:11:46.335: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul  5 09:11:46.335: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3224" to be "running and ready"
Jul  5 09:11:46.340: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.504131ms
Jul  5 09:11:48.343: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007397494s
Jul  5 09:11:48.343: INFO: Pod "pause" satisfied condition "running and ready"
Jul  5 09:11:48.343: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Jul  5 09:11:48.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 label pods pause testing-label=testing-label-value --namespace=kubectl-3224'
Jul  5 09:11:48.427: INFO: stderr: ""
Jul  5 09:11:48.427: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul  5 09:11:48.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pod pause -L testing-label --namespace=kubectl-3224'
Jul  5 09:11:48.499: INFO: stderr: ""
Jul  5 09:11:48.499: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul  5 09:11:48.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 label pods pause testing-label- --namespace=kubectl-3224'
Jul  5 09:11:48.591: INFO: stderr: ""
Jul  5 09:11:48.591: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul  5 09:11:48.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pod pause -L testing-label --namespace=kubectl-3224'
Jul  5 09:11:48.665: INFO: stderr: ""
Jul  5 09:11:48.665: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Jul  5 09:11:48.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete --grace-period=0 --force -f - --namespace=kubectl-3224'
Jul  5 09:11:48.758: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 09:11:48.758: INFO: stdout: "pod \"pause\" force deleted\n"
Jul  5 09:11:48.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get rc,svc -l name=pause --no-headers --namespace=kubectl-3224'
Jul  5 09:11:48.838: INFO: stderr: "No resources found.\n"
Jul  5 09:11:48.838: INFO: stdout: ""
Jul  5 09:11:48.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -l name=pause --namespace=kubectl-3224 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  5 09:11:48.904: INFO: stderr: ""
Jul  5 09:11:48.904: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:11:48.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3224" for this suite.
Jul  5 09:11:54.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:11:55.002: INFO: namespace kubectl-3224 deletion completed in 6.094338531s

• [SLOW TEST:8.881 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:11:55.003: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-277f9b79-826b-4d45-92a3-53a792dccd03 in namespace container-probe-7122
Jul  5 09:11:57.044: INFO: Started pod test-webserver-277f9b79-826b-4d45-92a3-53a792dccd03 in namespace container-probe-7122
STEP: checking the pod's current state and verifying that restartCount is present
Jul  5 09:11:57.046: INFO: Initial restart count of pod test-webserver-277f9b79-826b-4d45-92a3-53a792dccd03 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:15:57.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7122" for this suite.
Jul  5 09:16:03.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:16:03.576: INFO: namespace container-probe-7122 deletion completed in 6.101495663s

• [SLOW TEST:248.573 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:16:03.576: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul  5 09:16:11.646: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:11.648: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 09:16:13.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:13.652: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 09:16:15.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:15.652: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 09:16:17.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:17.652: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 09:16:19.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:19.652: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 09:16:21.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:21.652: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 09:16:23.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:23.654: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 09:16:25.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:25.652: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 09:16:27.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:27.652: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 09:16:29.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:29.652: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 09:16:31.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:31.652: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 09:16:33.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:33.652: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 09:16:35.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:35.652: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 09:16:37.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:37.652: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 09:16:39.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:39.652: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  5 09:16:41.649: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  5 09:16:41.652: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:16:41.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-566" for this suite.
Jul  5 09:17:03.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:17:03.755: INFO: namespace container-lifecycle-hook-566 deletion completed in 22.099885552s

• [SLOW TEST:60.179 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:17:03.755: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul  5 09:17:06.812: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:17:07.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8961" for this suite.
Jul  5 09:17:29.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:17:29.967: INFO: namespace replicaset-8961 deletion completed in 22.13645045s

• [SLOW TEST:26.211 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:17:29.967: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-b5qt
STEP: Creating a pod to test atomic-volume-subpath
Jul  5 09:17:30.006: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-b5qt" in namespace "subpath-5038" to be "success or failure"
Jul  5 09:17:30.014: INFO: Pod "pod-subpath-test-configmap-b5qt": Phase="Pending", Reason="", readiness=false. Elapsed: 7.552663ms
Jul  5 09:17:32.017: INFO: Pod "pod-subpath-test-configmap-b5qt": Phase="Running", Reason="", readiness=true. Elapsed: 2.010282783s
Jul  5 09:17:34.020: INFO: Pod "pod-subpath-test-configmap-b5qt": Phase="Running", Reason="", readiness=true. Elapsed: 4.013156514s
Jul  5 09:17:36.023: INFO: Pod "pod-subpath-test-configmap-b5qt": Phase="Running", Reason="", readiness=true. Elapsed: 6.016827175s
Jul  5 09:17:38.027: INFO: Pod "pod-subpath-test-configmap-b5qt": Phase="Running", Reason="", readiness=true. Elapsed: 8.020331927s
Jul  5 09:17:40.030: INFO: Pod "pod-subpath-test-configmap-b5qt": Phase="Running", Reason="", readiness=true. Elapsed: 10.02335712s
Jul  5 09:17:42.033: INFO: Pod "pod-subpath-test-configmap-b5qt": Phase="Running", Reason="", readiness=true. Elapsed: 12.026499069s
Jul  5 09:17:44.036: INFO: Pod "pod-subpath-test-configmap-b5qt": Phase="Running", Reason="", readiness=true. Elapsed: 14.029750774s
Jul  5 09:17:46.041: INFO: Pod "pod-subpath-test-configmap-b5qt": Phase="Running", Reason="", readiness=true. Elapsed: 16.034551845s
Jul  5 09:17:48.045: INFO: Pod "pod-subpath-test-configmap-b5qt": Phase="Running", Reason="", readiness=true. Elapsed: 18.038137706s
Jul  5 09:17:50.050: INFO: Pod "pod-subpath-test-configmap-b5qt": Phase="Running", Reason="", readiness=true. Elapsed: 20.043600319s
Jul  5 09:17:52.053: INFO: Pod "pod-subpath-test-configmap-b5qt": Phase="Running", Reason="", readiness=true. Elapsed: 22.046698159s
Jul  5 09:17:54.057: INFO: Pod "pod-subpath-test-configmap-b5qt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.050307216s
STEP: Saw pod success
Jul  5 09:17:54.057: INFO: Pod "pod-subpath-test-configmap-b5qt" satisfied condition "success or failure"
Jul  5 09:17:54.061: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-subpath-test-configmap-b5qt container test-container-subpath-configmap-b5qt: <nil>
STEP: delete the pod
Jul  5 09:17:54.081: INFO: Waiting for pod pod-subpath-test-configmap-b5qt to disappear
Jul  5 09:17:54.084: INFO: Pod pod-subpath-test-configmap-b5qt no longer exists
STEP: Deleting pod pod-subpath-test-configmap-b5qt
Jul  5 09:17:54.084: INFO: Deleting pod "pod-subpath-test-configmap-b5qt" in namespace "subpath-5038"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:17:54.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5038" for this suite.
Jul  5 09:18:00.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:18:00.187: INFO: namespace subpath-5038 deletion completed in 6.094936681s

• [SLOW TEST:30.219 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:18:00.187: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 09:18:00.221: INFO: (0) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.247299ms)
Jul  5 09:18:00.223: INFO: (1) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.874829ms)
Jul  5 09:18:00.226: INFO: (2) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.681985ms)
Jul  5 09:18:00.229: INFO: (3) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.898905ms)
Jul  5 09:18:00.232: INFO: (4) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.785201ms)
Jul  5 09:18:00.235: INFO: (5) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.731143ms)
Jul  5 09:18:00.238: INFO: (6) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.811273ms)
Jul  5 09:18:00.240: INFO: (7) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.689562ms)
Jul  5 09:18:00.243: INFO: (8) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.586572ms)
Jul  5 09:18:00.245: INFO: (9) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.620714ms)
Jul  5 09:18:00.248: INFO: (10) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.656742ms)
Jul  5 09:18:00.251: INFO: (11) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.659614ms)
Jul  5 09:18:00.257: INFO: (12) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 6.491091ms)
Jul  5 09:18:00.260: INFO: (13) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.8196ms)
Jul  5 09:18:00.263: INFO: (14) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.800195ms)
Jul  5 09:18:00.267: INFO: (15) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.220746ms)
Jul  5 09:18:00.270: INFO: (16) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.580233ms)
Jul  5 09:18:00.272: INFO: (17) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.463977ms)
Jul  5 09:18:00.275: INFO: (18) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.704306ms)
Jul  5 09:18:00.278: INFO: (19) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.68209ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:18:00.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1170" for this suite.
Jul  5 09:18:06.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:18:06.374: INFO: namespace proxy-1170 deletion completed in 6.092979877s

• [SLOW TEST:6.187 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:18:06.374: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:18:12.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3063" for this suite.
Jul  5 09:18:18.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:18:18.585: INFO: namespace namespaces-3063 deletion completed in 6.099294103s
STEP: Destroying namespace "nsdeletetest-5279" for this suite.
Jul  5 09:18:18.587: INFO: Namespace nsdeletetest-5279 was already deleted
STEP: Destroying namespace "nsdeletetest-6422" for this suite.
Jul  5 09:18:24.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:18:24.689: INFO: namespace nsdeletetest-6422 deletion completed in 6.101795012s

• [SLOW TEST:18.315 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:18:24.689: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-ddbab491-bc33-423f-b7f6-b1709661b1fd in namespace container-probe-1039
Jul  5 09:18:26.737: INFO: Started pod busybox-ddbab491-bc33-423f-b7f6-b1709661b1fd in namespace container-probe-1039
STEP: checking the pod's current state and verifying that restartCount is present
Jul  5 09:18:26.740: INFO: Initial restart count of pod busybox-ddbab491-bc33-423f-b7f6-b1709661b1fd is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:22:27.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1039" for this suite.
Jul  5 09:22:33.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:22:33.280: INFO: namespace container-probe-1039 deletion completed in 6.09489612s

• [SLOW TEST:248.591 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:22:33.281: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul  5 09:22:39.354: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  5 09:22:39.356: INFO: Pod pod-with-poststart-http-hook still exists
Jul  5 09:22:41.357: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  5 09:22:41.360: INFO: Pod pod-with-poststart-http-hook still exists
Jul  5 09:22:43.357: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  5 09:22:43.359: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:22:43.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3981" for this suite.
Jul  5 09:23:05.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:23:05.460: INFO: namespace container-lifecycle-hook-3981 deletion completed in 22.097424689s

• [SLOW TEST:32.179 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:23:05.460: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-1cef53c6-0609-47bc-bcd5-8315301cba15
STEP: Creating a pod to test consume secrets
Jul  5 09:23:05.503: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1daef0ec-c29d-4a74-bb3a-345235339907" in namespace "projected-7727" to be "success or failure"
Jul  5 09:23:05.506: INFO: Pod "pod-projected-secrets-1daef0ec-c29d-4a74-bb3a-345235339907": Phase="Pending", Reason="", readiness=false. Elapsed: 3.160787ms
Jul  5 09:23:07.509: INFO: Pod "pod-projected-secrets-1daef0ec-c29d-4a74-bb3a-345235339907": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006163254s
STEP: Saw pod success
Jul  5 09:23:07.509: INFO: Pod "pod-projected-secrets-1daef0ec-c29d-4a74-bb3a-345235339907" satisfied condition "success or failure"
Jul  5 09:23:07.512: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-projected-secrets-1daef0ec-c29d-4a74-bb3a-345235339907 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  5 09:23:07.548: INFO: Waiting for pod pod-projected-secrets-1daef0ec-c29d-4a74-bb3a-345235339907 to disappear
Jul  5 09:23:07.551: INFO: Pod pod-projected-secrets-1daef0ec-c29d-4a74-bb3a-345235339907 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:23:07.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7727" for this suite.
Jul  5 09:23:13.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:23:13.649: INFO: namespace projected-7727 deletion completed in 6.094548018s

• [SLOW TEST:8.189 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:23:13.650: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul  5 09:23:13.682: INFO: Waiting up to 5m0s for pod "pod-a7b0fde6-7ce1-4ed1-9b64-881be92b286e" in namespace "emptydir-5776" to be "success or failure"
Jul  5 09:23:13.685: INFO: Pod "pod-a7b0fde6-7ce1-4ed1-9b64-881be92b286e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.399713ms
Jul  5 09:23:15.688: INFO: Pod "pod-a7b0fde6-7ce1-4ed1-9b64-881be92b286e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006296245s
STEP: Saw pod success
Jul  5 09:23:15.688: INFO: Pod "pod-a7b0fde6-7ce1-4ed1-9b64-881be92b286e" satisfied condition "success or failure"
Jul  5 09:23:15.691: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-a7b0fde6-7ce1-4ed1-9b64-881be92b286e container test-container: <nil>
STEP: delete the pod
Jul  5 09:23:15.716: INFO: Waiting for pod pod-a7b0fde6-7ce1-4ed1-9b64-881be92b286e to disappear
Jul  5 09:23:15.718: INFO: Pod pod-a7b0fde6-7ce1-4ed1-9b64-881be92b286e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:23:15.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5776" for this suite.
Jul  5 09:23:21.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:23:21.817: INFO: namespace emptydir-5776 deletion completed in 6.095753782s

• [SLOW TEST:8.167 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:23:21.817: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Jul  5 09:23:21.850: INFO: Waiting up to 5m0s for pod "var-expansion-1b6c6aee-87fb-41f0-b516-5c9203d3517d" in namespace "var-expansion-2001" to be "success or failure"
Jul  5 09:23:21.853: INFO: Pod "var-expansion-1b6c6aee-87fb-41f0-b516-5c9203d3517d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.655146ms
Jul  5 09:23:23.856: INFO: Pod "var-expansion-1b6c6aee-87fb-41f0-b516-5c9203d3517d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005280618s
STEP: Saw pod success
Jul  5 09:23:23.856: INFO: Pod "var-expansion-1b6c6aee-87fb-41f0-b516-5c9203d3517d" satisfied condition "success or failure"
Jul  5 09:23:23.858: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod var-expansion-1b6c6aee-87fb-41f0-b516-5c9203d3517d container dapi-container: <nil>
STEP: delete the pod
Jul  5 09:23:23.877: INFO: Waiting for pod var-expansion-1b6c6aee-87fb-41f0-b516-5c9203d3517d to disappear
Jul  5 09:23:23.879: INFO: Pod var-expansion-1b6c6aee-87fb-41f0-b516-5c9203d3517d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:23:23.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2001" for this suite.
Jul  5 09:23:29.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:23:30.042: INFO: namespace var-expansion-2001 deletion completed in 6.159912996s

• [SLOW TEST:8.225 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:23:30.044: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 09:23:30.102: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul  5 09:23:30.114: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:30.114: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:30.114: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:30.118: INFO: Number of nodes with available pods: 0
Jul  5 09:23:30.118: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:23:31.122: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:31.122: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:31.122: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:31.125: INFO: Number of nodes with available pods: 0
Jul  5 09:23:31.125: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:23:32.122: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:32.122: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:32.122: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:32.125: INFO: Number of nodes with available pods: 1
Jul  5 09:23:32.125: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:23:33.122: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:33.122: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:33.122: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:33.125: INFO: Number of nodes with available pods: 3
Jul  5 09:23:33.125: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul  5 09:23:33.166: INFO: Wrong image for pod: daemon-set-jcfvd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:33.166: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:33.166: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:33.172: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:33.172: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:33.172: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:34.175: INFO: Wrong image for pod: daemon-set-jcfvd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:34.175: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:34.175: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:34.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:34.179: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:34.179: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:35.176: INFO: Wrong image for pod: daemon-set-jcfvd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:35.176: INFO: Pod daemon-set-jcfvd is not available
Jul  5 09:23:35.176: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:35.176: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:35.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:35.180: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:35.180: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:36.175: INFO: Pod daemon-set-9lfwb is not available
Jul  5 09:23:36.175: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:36.175: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:36.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:36.180: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:36.180: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:37.176: INFO: Pod daemon-set-9lfwb is not available
Jul  5 09:23:37.176: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:37.176: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:37.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:37.179: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:37.179: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:38.175: INFO: Pod daemon-set-9lfwb is not available
Jul  5 09:23:38.175: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:38.175: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:38.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:38.179: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:38.179: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:39.175: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:39.175: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:39.178: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:39.178: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:39.178: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:40.182: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:40.182: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:40.182: INFO: Pod daemon-set-shhzp is not available
Jul  5 09:23:40.185: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:40.185: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:40.185: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:41.182: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:41.182: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:41.182: INFO: Pod daemon-set-shhzp is not available
Jul  5 09:23:41.186: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:41.186: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:41.186: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:42.176: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:42.176: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:42.176: INFO: Pod daemon-set-shhzp is not available
Jul  5 09:23:42.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:42.179: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:42.179: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:43.175: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:43.175: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:43.175: INFO: Pod daemon-set-shhzp is not available
Jul  5 09:23:43.178: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:43.179: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:43.179: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:44.177: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:44.177: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:44.177: INFO: Pod daemon-set-shhzp is not available
Jul  5 09:23:44.180: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:44.181: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:44.181: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:45.175: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:45.175: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:45.175: INFO: Pod daemon-set-shhzp is not available
Jul  5 09:23:45.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:45.179: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:45.179: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:46.182: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:46.182: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:46.182: INFO: Pod daemon-set-shhzp is not available
Jul  5 09:23:46.187: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:46.187: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:46.187: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:47.177: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:47.177: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:47.177: INFO: Pod daemon-set-shhzp is not available
Jul  5 09:23:47.180: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:47.180: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:47.180: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:48.175: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:48.175: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:48.175: INFO: Pod daemon-set-shhzp is not available
Jul  5 09:23:48.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:48.179: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:48.179: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:49.186: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:49.186: INFO: Wrong image for pod: daemon-set-shhzp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:49.186: INFO: Pod daemon-set-shhzp is not available
Jul  5 09:23:49.189: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:49.189: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:49.189: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:50.184: INFO: Pod daemon-set-bznpf is not available
Jul  5 09:23:50.184: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:50.187: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:50.188: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:50.188: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:51.175: INFO: Pod daemon-set-bznpf is not available
Jul  5 09:23:51.175: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:51.184: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:51.185: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:51.185: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:52.175: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:52.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:52.179: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:52.179: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:53.175: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:53.178: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:53.178: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:53.178: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:54.176: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:54.176: INFO: Pod daemon-set-psblw is not available
Jul  5 09:23:54.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:54.179: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:54.179: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:55.175: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:55.175: INFO: Pod daemon-set-psblw is not available
Jul  5 09:23:55.178: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:55.178: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:55.178: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:56.175: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:56.175: INFO: Pod daemon-set-psblw is not available
Jul  5 09:23:56.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:56.179: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:56.179: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:57.180: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:57.180: INFO: Pod daemon-set-psblw is not available
Jul  5 09:23:57.184: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:57.184: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:57.184: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:58.176: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:58.176: INFO: Pod daemon-set-psblw is not available
Jul  5 09:23:58.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:58.179: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:58.179: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:59.175: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:23:59.175: INFO: Pod daemon-set-psblw is not available
Jul  5 09:23:59.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:59.179: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:23:59.179: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:00.175: INFO: Wrong image for pod: daemon-set-psblw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  5 09:24:00.175: INFO: Pod daemon-set-psblw is not available
Jul  5 09:24:00.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:00.179: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:00.179: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:01.176: INFO: Pod daemon-set-49mc2 is not available
Jul  5 09:24:01.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:01.179: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:01.179: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jul  5 09:24:01.183: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:01.183: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:01.183: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:01.185: INFO: Number of nodes with available pods: 2
Jul  5 09:24:01.185: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:24:02.194: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:02.194: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:02.194: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:02.202: INFO: Number of nodes with available pods: 2
Jul  5 09:24:02.203: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:24:03.189: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:03.189: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:03.189: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:03.192: INFO: Number of nodes with available pods: 2
Jul  5 09:24:03.192: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:24:04.190: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:04.190: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:04.190: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:04.193: INFO: Number of nodes with available pods: 2
Jul  5 09:24:04.193: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:24:05.189: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:05.189: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:05.189: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:24:05.192: INFO: Number of nodes with available pods: 3
Jul  5 09:24:05.192: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-696, will wait for the garbage collector to delete the pods
Jul  5 09:24:05.266: INFO: Deleting DaemonSet.extensions daemon-set took: 8.806172ms
Jul  5 09:24:05.666: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.16873ms
Jul  5 09:24:19.274: INFO: Number of nodes with available pods: 0
Jul  5 09:24:19.274: INFO: Number of running nodes: 0, number of available pods: 0
Jul  5 09:24:19.278: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-696/daemonsets","resourceVersion":"8877"},"items":null}

Jul  5 09:24:19.281: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-696/pods","resourceVersion":"8877"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:24:19.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-696" for this suite.
Jul  5 09:24:25.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:24:25.414: INFO: namespace daemonsets-696 deletion completed in 6.107783361s

• [SLOW TEST:55.371 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:24:25.415: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-3819
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3819
STEP: Deleting pre-stop pod
Jul  5 09:24:38.480: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:24:38.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3819" for this suite.
Jul  5 09:25:16.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:25:16.690: INFO: namespace prestop-3819 deletion completed in 38.197684082s

• [SLOW TEST:51.276 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:25:16.691: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul  5 09:25:16.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 create -f - --namespace=kubectl-613'
Jul  5 09:25:17.097: INFO: stderr: ""
Jul  5 09:25:17.097: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul  5 09:25:18.101: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 09:25:18.101: INFO: Found 0 / 1
Jul  5 09:25:19.101: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 09:25:19.101: INFO: Found 1 / 1
Jul  5 09:25:19.101: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul  5 09:25:19.103: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 09:25:19.103: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  5 09:25:19.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 patch pod redis-master-6q7ww --namespace=kubectl-613 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul  5 09:25:19.186: INFO: stderr: ""
Jul  5 09:25:19.186: INFO: stdout: "pod/redis-master-6q7ww patched\n"
STEP: checking annotations
Jul  5 09:25:19.189: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 09:25:19.189: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:25:19.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-613" for this suite.
Jul  5 09:25:41.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:25:41.288: INFO: namespace kubectl-613 deletion completed in 22.095185105s

• [SLOW TEST:24.597 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:25:41.288: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:25:41.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1158" for this suite.
Jul  5 09:26:03.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:26:03.429: INFO: namespace pods-1158 deletion completed in 22.093187926s

• [SLOW TEST:22.141 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:26:03.430: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 09:26:03.463: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c762c05a-1f6d-4b61-8960-940da41cb858" in namespace "downward-api-8862" to be "success or failure"
Jul  5 09:26:03.466: INFO: Pod "downwardapi-volume-c762c05a-1f6d-4b61-8960-940da41cb858": Phase="Pending", Reason="", readiness=false. Elapsed: 2.71974ms
Jul  5 09:26:05.469: INFO: Pod "downwardapi-volume-c762c05a-1f6d-4b61-8960-940da41cb858": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005653397s
STEP: Saw pod success
Jul  5 09:26:05.469: INFO: Pod "downwardapi-volume-c762c05a-1f6d-4b61-8960-940da41cb858" satisfied condition "success or failure"
Jul  5 09:26:05.471: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod downwardapi-volume-c762c05a-1f6d-4b61-8960-940da41cb858 container client-container: <nil>
STEP: delete the pod
Jul  5 09:26:05.491: INFO: Waiting for pod downwardapi-volume-c762c05a-1f6d-4b61-8960-940da41cb858 to disappear
Jul  5 09:26:05.493: INFO: Pod downwardapi-volume-c762c05a-1f6d-4b61-8960-940da41cb858 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:26:05.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8862" for this suite.
Jul  5 09:26:11.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:26:11.606: INFO: namespace downward-api-8862 deletion completed in 6.10964396s

• [SLOW TEST:8.177 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:26:11.607: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2420
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2420
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2420
Jul  5 09:26:11.652: INFO: Found 0 stateful pods, waiting for 1
Jul  5 09:26:21.656: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul  5 09:26:21.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2420 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  5 09:26:21.930: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  5 09:26:21.930: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  5 09:26:21.930: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  5 09:26:21.934: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul  5 09:26:31.938: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 09:26:31.938: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 09:26:31.951: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999282s
Jul  5 09:26:32.956: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997006686s
Jul  5 09:26:33.959: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992018013s
Jul  5 09:26:34.962: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988790876s
Jul  5 09:26:35.965: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985456926s
Jul  5 09:26:36.969: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.982225988s
Jul  5 09:26:37.972: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.978730457s
Jul  5 09:26:38.976: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.975229847s
Jul  5 09:26:39.981: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.971481816s
Jul  5 09:26:40.988: INFO: Verifying statefulset ss doesn't scale past 1 for another 966.661163ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2420
Jul  5 09:26:41.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2420 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:26:42.241: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  5 09:26:42.241: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  5 09:26:42.241: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  5 09:26:42.244: INFO: Found 1 stateful pods, waiting for 3
Jul  5 09:26:52.248: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 09:26:52.248: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 09:26:52.248: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul  5 09:26:52.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2420 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  5 09:26:52.513: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  5 09:26:52.513: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  5 09:26:52.513: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  5 09:26:52.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2420 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  5 09:26:52.761: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  5 09:26:52.761: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  5 09:26:52.761: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  5 09:26:52.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2420 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  5 09:26:53.009: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  5 09:26:53.009: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  5 09:26:53.009: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  5 09:26:53.009: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 09:26:53.012: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul  5 09:27:03.019: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 09:27:03.019: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 09:27:03.019: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul  5 09:27:03.029: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999965s
Jul  5 09:27:04.033: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997190287s
Jul  5 09:27:05.036: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993477573s
Jul  5 09:27:06.040: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989815612s
Jul  5 09:27:07.043: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986541864s
Jul  5 09:27:08.046: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983287193s
Jul  5 09:27:09.050: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.979734938s
Jul  5 09:27:10.053: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.976480353s
Jul  5 09:27:11.057: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.97305858s
Jul  5 09:27:12.060: INFO: Verifying statefulset ss doesn't scale past 3 for another 969.548873ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2420
Jul  5 09:27:13.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2420 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:27:13.328: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  5 09:27:13.328: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  5 09:27:13.328: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  5 09:27:13.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2420 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:27:13.579: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  5 09:27:13.579: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  5 09:27:13.579: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  5 09:27:13.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-2420 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:27:13.812: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  5 09:27:13.812: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  5 09:27:13.812: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  5 09:27:13.812: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul  5 09:27:23.845: INFO: Deleting all statefulset in ns statefulset-2420
Jul  5 09:27:23.847: INFO: Scaling statefulset ss to 0
Jul  5 09:27:23.854: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 09:27:23.856: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:27:23.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2420" for this suite.
Jul  5 09:27:29.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:27:30.021: INFO: namespace statefulset-2420 deletion completed in 6.149369135s

• [SLOW TEST:78.415 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:27:30.022: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0705 09:27:31.100627      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul  5 09:27:31.100: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:27:31.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5511" for this suite.
Jul  5 09:27:37.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:27:37.201: INFO: namespace gc-5511 deletion completed in 6.097292107s

• [SLOW TEST:7.179 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:27:37.201: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-269ad5e7-99da-46ae-9741-74d3c8421c83
STEP: Creating a pod to test consume secrets
Jul  5 09:27:37.238: INFO: Waiting up to 5m0s for pod "pod-secrets-7fedc81d-f4e9-4003-a2c0-0692b67d8a3a" in namespace "secrets-6050" to be "success or failure"
Jul  5 09:27:37.241: INFO: Pod "pod-secrets-7fedc81d-f4e9-4003-a2c0-0692b67d8a3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.876777ms
Jul  5 09:27:39.244: INFO: Pod "pod-secrets-7fedc81d-f4e9-4003-a2c0-0692b67d8a3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006154211s
STEP: Saw pod success
Jul  5 09:27:39.244: INFO: Pod "pod-secrets-7fedc81d-f4e9-4003-a2c0-0692b67d8a3a" satisfied condition "success or failure"
Jul  5 09:27:39.246: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-secrets-7fedc81d-f4e9-4003-a2c0-0692b67d8a3a container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 09:27:39.272: INFO: Waiting for pod pod-secrets-7fedc81d-f4e9-4003-a2c0-0692b67d8a3a to disappear
Jul  5 09:27:39.274: INFO: Pod pod-secrets-7fedc81d-f4e9-4003-a2c0-0692b67d8a3a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:27:39.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6050" for this suite.
Jul  5 09:27:45.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:27:45.382: INFO: namespace secrets-6050 deletion completed in 6.103986544s

• [SLOW TEST:8.181 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:27:45.382: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul  5 09:27:45.418: INFO: Waiting up to 5m0s for pod "pod-4bfdff9d-a83d-4f4d-b39a-2bfa59eba57c" in namespace "emptydir-8166" to be "success or failure"
Jul  5 09:27:45.421: INFO: Pod "pod-4bfdff9d-a83d-4f4d-b39a-2bfa59eba57c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.853542ms
Jul  5 09:27:47.424: INFO: Pod "pod-4bfdff9d-a83d-4f4d-b39a-2bfa59eba57c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005769105s
STEP: Saw pod success
Jul  5 09:27:47.424: INFO: Pod "pod-4bfdff9d-a83d-4f4d-b39a-2bfa59eba57c" satisfied condition "success or failure"
Jul  5 09:27:47.427: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-4bfdff9d-a83d-4f4d-b39a-2bfa59eba57c container test-container: <nil>
STEP: delete the pod
Jul  5 09:27:47.447: INFO: Waiting for pod pod-4bfdff9d-a83d-4f4d-b39a-2bfa59eba57c to disappear
Jul  5 09:27:47.452: INFO: Pod pod-4bfdff9d-a83d-4f4d-b39a-2bfa59eba57c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:27:47.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8166" for this suite.
Jul  5 09:27:53.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:27:53.564: INFO: namespace emptydir-8166 deletion completed in 6.107914881s

• [SLOW TEST:8.182 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:27:53.564: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Jul  5 09:27:53.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 cluster-info'
Jul  5 09:27:53.662: INFO: stderr: ""
Jul  5 09:27:53.662: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:27:53.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9085" for this suite.
Jul  5 09:27:59.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:27:59.796: INFO: namespace kubectl-9085 deletion completed in 6.129854402s

• [SLOW TEST:6.231 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:27:59.796: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul  5 09:27:59.837: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  5 09:27:59.849: INFO: Waiting for terminating namespaces to be deleted...
Jul  5 09:27:59.852: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-1-212.eu-west-3.compute.internal before test
Jul  5 09:27:59.860: INFO: kube-proxy-zpgcv from kube-system started at 2019-07-05 08:50:50 +0000 UTC (1 container statuses recorded)
Jul  5 09:27:59.860: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 09:27:59.860: INFO: canal-5kgvk from kube-system started at 2019-07-05 08:50:50 +0000 UTC (2 container statuses recorded)
Jul  5 09:27:59.860: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 09:27:59.860: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  5 09:27:59.860: INFO: sonobuoy-systemd-logs-daemon-set-e9d729729fbf4cf5-rfc95 from heptio-sonobuoy started at 2019-07-05 08:51:49 +0000 UTC (2 container statuses recorded)
Jul  5 09:27:59.860: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  5 09:27:59.860: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  5 09:27:59.860: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-05 08:51:44 +0000 UTC (1 container statuses recorded)
Jul  5 09:27:59.860: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  5 09:27:59.860: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-11-115.eu-west-3.compute.internal before test
Jul  5 09:27:59.865: INFO: kube-proxy-h7nqv from kube-system started at 2019-07-05 08:50:49 +0000 UTC (1 container statuses recorded)
Jul  5 09:27:59.865: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 09:27:59.865: INFO: canal-m82pj from kube-system started at 2019-07-05 08:50:49 +0000 UTC (2 container statuses recorded)
Jul  5 09:27:59.865: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 09:27:59.865: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  5 09:27:59.865: INFO: sonobuoy-systemd-logs-daemon-set-e9d729729fbf4cf5-rm8ks from heptio-sonobuoy started at 2019-07-05 08:51:49 +0000 UTC (2 container statuses recorded)
Jul  5 09:27:59.865: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  5 09:27:59.865: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  5 09:27:59.865: INFO: metrics-server-6964ccc5b-88zkm from kube-system started at 2019-07-05 08:51:07 +0000 UTC (1 container statuses recorded)
Jul  5 09:27:59.865: INFO: 	Container metrics-server ready: true, restart count 0
Jul  5 09:27:59.865: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-6-226.eu-west-3.compute.internal before test
Jul  5 09:27:59.878: INFO: sonobuoy-systemd-logs-daemon-set-e9d729729fbf4cf5-b7svv from heptio-sonobuoy started at 2019-07-05 08:51:49 +0000 UTC (2 container statuses recorded)
Jul  5 09:27:59.878: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  5 09:27:59.878: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  5 09:27:59.878: INFO: canal-xzlkm from kube-system started at 2019-07-05 08:50:51 +0000 UTC (2 container statuses recorded)
Jul  5 09:27:59.878: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 09:27:59.878: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  5 09:27:59.878: INFO: kube-proxy-zc7nm from kube-system started at 2019-07-05 08:50:51 +0000 UTC (1 container statuses recorded)
Jul  5 09:27:59.878: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 09:27:59.878: INFO: sonobuoy-e2e-job-dddb5268764f464f from heptio-sonobuoy started at 2019-07-05 08:51:49 +0000 UTC (2 container statuses recorded)
Jul  5 09:27:59.878: INFO: 	Container e2e ready: true, restart count 0
Jul  5 09:27:59.878: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node ip-172-31-1-212.eu-west-3.compute.internal
STEP: verifying the node has the label node ip-172-31-11-115.eu-west-3.compute.internal
STEP: verifying the node has the label node ip-172-31-6-226.eu-west-3.compute.internal
Jul  5 09:27:59.940: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-1-212.eu-west-3.compute.internal
Jul  5 09:27:59.940: INFO: Pod sonobuoy-e2e-job-dddb5268764f464f requesting resource cpu=0m on Node ip-172-31-6-226.eu-west-3.compute.internal
Jul  5 09:27:59.940: INFO: Pod sonobuoy-systemd-logs-daemon-set-e9d729729fbf4cf5-b7svv requesting resource cpu=0m on Node ip-172-31-6-226.eu-west-3.compute.internal
Jul  5 09:27:59.940: INFO: Pod sonobuoy-systemd-logs-daemon-set-e9d729729fbf4cf5-rfc95 requesting resource cpu=0m on Node ip-172-31-1-212.eu-west-3.compute.internal
Jul  5 09:27:59.940: INFO: Pod sonobuoy-systemd-logs-daemon-set-e9d729729fbf4cf5-rm8ks requesting resource cpu=0m on Node ip-172-31-11-115.eu-west-3.compute.internal
Jul  5 09:27:59.940: INFO: Pod canal-5kgvk requesting resource cpu=250m on Node ip-172-31-1-212.eu-west-3.compute.internal
Jul  5 09:27:59.940: INFO: Pod canal-m82pj requesting resource cpu=250m on Node ip-172-31-11-115.eu-west-3.compute.internal
Jul  5 09:27:59.940: INFO: Pod canal-xzlkm requesting resource cpu=250m on Node ip-172-31-6-226.eu-west-3.compute.internal
Jul  5 09:27:59.940: INFO: Pod kube-proxy-h7nqv requesting resource cpu=0m on Node ip-172-31-11-115.eu-west-3.compute.internal
Jul  5 09:27:59.940: INFO: Pod kube-proxy-zc7nm requesting resource cpu=0m on Node ip-172-31-6-226.eu-west-3.compute.internal
Jul  5 09:27:59.940: INFO: Pod kube-proxy-zpgcv requesting resource cpu=0m on Node ip-172-31-1-212.eu-west-3.compute.internal
Jul  5 09:27:59.940: INFO: Pod metrics-server-6964ccc5b-88zkm requesting resource cpu=0m on Node ip-172-31-11-115.eu-west-3.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53c0642c-3ce5-4854-b1b1-f46036596210.15ae78be6e65c764], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9052/filler-pod-53c0642c-3ce5-4854-b1b1-f46036596210 to ip-172-31-6-226.eu-west-3.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53c0642c-3ce5-4854-b1b1-f46036596210.15ae78beafd0c3e2], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53c0642c-3ce5-4854-b1b1-f46036596210.15ae78beb4368c76], Reason = [Created], Message = [Created container filler-pod-53c0642c-3ce5-4854-b1b1-f46036596210]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53c0642c-3ce5-4854-b1b1-f46036596210.15ae78bec0573ec6], Reason = [Started], Message = [Started container filler-pod-53c0642c-3ce5-4854-b1b1-f46036596210]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b2a78ddf-2586-4828-8b7b-706419ac2d93.15ae78be6f45b4a5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9052/filler-pod-b2a78ddf-2586-4828-8b7b-706419ac2d93 to ip-172-31-1-212.eu-west-3.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b2a78ddf-2586-4828-8b7b-706419ac2d93.15ae78beaa27a54b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b2a78ddf-2586-4828-8b7b-706419ac2d93.15ae78beae2345ab], Reason = [Created], Message = [Created container filler-pod-b2a78ddf-2586-4828-8b7b-706419ac2d93]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b2a78ddf-2586-4828-8b7b-706419ac2d93.15ae78bebd271bce], Reason = [Started], Message = [Started container filler-pod-b2a78ddf-2586-4828-8b7b-706419ac2d93]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c238dfe3-a0f0-4281-be8f-f6b43956ea46.15ae78be6dd7e623], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9052/filler-pod-c238dfe3-a0f0-4281-be8f-f6b43956ea46 to ip-172-31-11-115.eu-west-3.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c238dfe3-a0f0-4281-be8f-f6b43956ea46.15ae78beaa1f64db], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c238dfe3-a0f0-4281-be8f-f6b43956ea46.15ae78beae000991], Reason = [Created], Message = [Created container filler-pod-c238dfe3-a0f0-4281-be8f-f6b43956ea46]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c238dfe3-a0f0-4281-be8f-f6b43956ea46.15ae78bebc269fd3], Reason = [Started], Message = [Started container filler-pod-c238dfe3-a0f0-4281-be8f-f6b43956ea46]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15ae78bf5f0288b4], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node ip-172-31-1-212.eu-west-3.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-11-115.eu-west-3.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-6-226.eu-west-3.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:28:05.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9052" for this suite.
Jul  5 09:28:11.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:28:11.167: INFO: namespace sched-pred-9052 deletion completed in 6.101325997s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:11.372 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:28:11.168: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-066de3cf-0352-4523-a8df-3dc22064980c
STEP: Creating configMap with name cm-test-opt-upd-c8097758-c29d-4c4f-946d-e22166cc7ded
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-066de3cf-0352-4523-a8df-3dc22064980c
STEP: Updating configmap cm-test-opt-upd-c8097758-c29d-4c4f-946d-e22166cc7ded
STEP: Creating configMap with name cm-test-opt-create-a223f32e-79af-47bc-8364-9aada546fd3f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:29:45.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-192" for this suite.
Jul  5 09:30:07.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:30:07.772: INFO: namespace projected-192 deletion completed in 22.095037663s

• [SLOW TEST:116.604 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:30:07.772: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5748
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5748
STEP: Creating statefulset with conflicting port in namespace statefulset-5748
STEP: Waiting until pod test-pod will start running in namespace statefulset-5748
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5748
Jul  5 09:30:09.851: INFO: Observed stateful pod in namespace: statefulset-5748, name: ss-0, uid: eaaea202-9a5b-4c0f-8c79-098dfe59fbce, status phase: Pending. Waiting for statefulset controller to delete.
Jul  5 09:30:09.998: INFO: Observed stateful pod in namespace: statefulset-5748, name: ss-0, uid: eaaea202-9a5b-4c0f-8c79-098dfe59fbce, status phase: Failed. Waiting for statefulset controller to delete.
Jul  5 09:30:10.006: INFO: Observed stateful pod in namespace: statefulset-5748, name: ss-0, uid: eaaea202-9a5b-4c0f-8c79-098dfe59fbce, status phase: Failed. Waiting for statefulset controller to delete.
Jul  5 09:30:10.012: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5748
STEP: Removing pod with conflicting port in namespace statefulset-5748
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5748 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul  5 09:30:12.044: INFO: Deleting all statefulset in ns statefulset-5748
Jul  5 09:30:12.047: INFO: Scaling statefulset ss to 0
Jul  5 09:30:22.060: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 09:30:22.063: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:30:22.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5748" for this suite.
Jul  5 09:30:28.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:30:28.188: INFO: namespace statefulset-5748 deletion completed in 6.107813131s

• [SLOW TEST:20.415 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:30:28.188: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jul  5 09:30:30.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec pod-sharedvolume-5b6294f2-3b7d-4d26-b3d8-9a2f270859e7 -c busybox-main-container --namespace=emptydir-7758 -- cat /usr/share/volumeshare/shareddata.txt'
Jul  5 09:30:30.494: INFO: stderr: ""
Jul  5 09:30:30.494: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:30:30.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7758" for this suite.
Jul  5 09:30:36.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:30:36.618: INFO: namespace emptydir-7758 deletion completed in 6.119852883s

• [SLOW TEST:8.430 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:30:36.618: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul  5 09:30:39.183: INFO: Successfully updated pod "labelsupdate276b4087-4f1b-4d2d-a378-266cbc934414"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:30:41.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7565" for this suite.
Jul  5 09:31:03.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:31:03.302: INFO: namespace projected-7565 deletion completed in 22.098970134s

• [SLOW TEST:26.683 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:31:03.302: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-a83952f9-af09-47d6-9648-d8e00d71bb3e in namespace container-probe-2266
Jul  5 09:31:07.343: INFO: Started pod liveness-a83952f9-af09-47d6-9648-d8e00d71bb3e in namespace container-probe-2266
STEP: checking the pod's current state and verifying that restartCount is present
Jul  5 09:31:07.346: INFO: Initial restart count of pod liveness-a83952f9-af09-47d6-9648-d8e00d71bb3e is 0
Jul  5 09:31:29.391: INFO: Restart count of pod container-probe-2266/liveness-a83952f9-af09-47d6-9648-d8e00d71bb3e is now 1 (22.044900998s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:31:29.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2266" for this suite.
Jul  5 09:31:35.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:31:35.539: INFO: namespace container-probe-2266 deletion completed in 6.132423477s

• [SLOW TEST:32.237 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:31:35.540: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 09:31:35.568: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:31:36.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-222" for this suite.
Jul  5 09:31:42.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:31:42.709: INFO: namespace custom-resource-definition-222 deletion completed in 6.098214368s

• [SLOW TEST:7.169 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:31:42.709: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul  5 09:31:47.269: INFO: Successfully updated pod "annotationupdate86d9cbd3-aef8-4c77-9125-1ec5c65bc3b8"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:31:49.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5287" for this suite.
Jul  5 09:32:11.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:32:11.386: INFO: namespace downward-api-5287 deletion completed in 22.098093324s

• [SLOW TEST:28.677 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:32:11.386: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Jul  5 09:32:11.469: INFO: Waiting up to 5m0s for pod "client-containers-0dd022e6-c8cd-4909-9ce9-bfd01cbe493d" in namespace "containers-246" to be "success or failure"
Jul  5 09:32:11.472: INFO: Pod "client-containers-0dd022e6-c8cd-4909-9ce9-bfd01cbe493d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.963191ms
Jul  5 09:32:13.476: INFO: Pod "client-containers-0dd022e6-c8cd-4909-9ce9-bfd01cbe493d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006117281s
Jul  5 09:32:15.479: INFO: Pod "client-containers-0dd022e6-c8cd-4909-9ce9-bfd01cbe493d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00927865s
STEP: Saw pod success
Jul  5 09:32:15.479: INFO: Pod "client-containers-0dd022e6-c8cd-4909-9ce9-bfd01cbe493d" satisfied condition "success or failure"
Jul  5 09:32:15.481: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod client-containers-0dd022e6-c8cd-4909-9ce9-bfd01cbe493d container test-container: <nil>
STEP: delete the pod
Jul  5 09:32:15.503: INFO: Waiting for pod client-containers-0dd022e6-c8cd-4909-9ce9-bfd01cbe493d to disappear
Jul  5 09:32:15.505: INFO: Pod client-containers-0dd022e6-c8cd-4909-9ce9-bfd01cbe493d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:32:15.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-246" for this suite.
Jul  5 09:32:21.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:32:21.624: INFO: namespace containers-246 deletion completed in 6.107750406s

• [SLOW TEST:10.238 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:32:21.625: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul  5 09:32:21.660: INFO: Waiting up to 5m0s for pod "pod-d4701bfa-8516-4c2b-8663-cf93b5adf41b" in namespace "emptydir-2469" to be "success or failure"
Jul  5 09:32:21.665: INFO: Pod "pod-d4701bfa-8516-4c2b-8663-cf93b5adf41b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.124785ms
Jul  5 09:32:23.668: INFO: Pod "pod-d4701bfa-8516-4c2b-8663-cf93b5adf41b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008270004s
STEP: Saw pod success
Jul  5 09:32:23.668: INFO: Pod "pod-d4701bfa-8516-4c2b-8663-cf93b5adf41b" satisfied condition "success or failure"
Jul  5 09:32:23.671: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-d4701bfa-8516-4c2b-8663-cf93b5adf41b container test-container: <nil>
STEP: delete the pod
Jul  5 09:32:23.688: INFO: Waiting for pod pod-d4701bfa-8516-4c2b-8663-cf93b5adf41b to disappear
Jul  5 09:32:23.691: INFO: Pod pod-d4701bfa-8516-4c2b-8663-cf93b5adf41b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:32:23.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2469" for this suite.
Jul  5 09:32:29.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:32:29.842: INFO: namespace emptydir-2469 deletion completed in 6.147948972s

• [SLOW TEST:8.217 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:32:29.842: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-59e3e8e1-1e28-4b00-b69e-161fccf27fc8
STEP: Creating a pod to test consume secrets
Jul  5 09:32:29.900: INFO: Waiting up to 5m0s for pod "pod-secrets-932ebc7b-a048-4a2f-bf04-f4206b2e371a" in namespace "secrets-9132" to be "success or failure"
Jul  5 09:32:29.912: INFO: Pod "pod-secrets-932ebc7b-a048-4a2f-bf04-f4206b2e371a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.447119ms
Jul  5 09:32:31.916: INFO: Pod "pod-secrets-932ebc7b-a048-4a2f-bf04-f4206b2e371a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015579199s
Jul  5 09:32:33.919: INFO: Pod "pod-secrets-932ebc7b-a048-4a2f-bf04-f4206b2e371a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018718071s
STEP: Saw pod success
Jul  5 09:32:33.919: INFO: Pod "pod-secrets-932ebc7b-a048-4a2f-bf04-f4206b2e371a" satisfied condition "success or failure"
Jul  5 09:32:33.921: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-secrets-932ebc7b-a048-4a2f-bf04-f4206b2e371a container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 09:32:33.940: INFO: Waiting for pod pod-secrets-932ebc7b-a048-4a2f-bf04-f4206b2e371a to disappear
Jul  5 09:32:33.943: INFO: Pod pod-secrets-932ebc7b-a048-4a2f-bf04-f4206b2e371a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:32:33.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9132" for this suite.
Jul  5 09:32:39.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:32:40.070: INFO: namespace secrets-9132 deletion completed in 6.122806952s

• [SLOW TEST:10.228 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:32:40.070: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul  5 09:32:40.111: INFO: Waiting up to 5m0s for pod "downward-api-7daefc10-4248-45bb-a021-0f2ad1379700" in namespace "downward-api-2761" to be "success or failure"
Jul  5 09:32:40.122: INFO: Pod "downward-api-7daefc10-4248-45bb-a021-0f2ad1379700": Phase="Pending", Reason="", readiness=false. Elapsed: 10.197444ms
Jul  5 09:32:42.125: INFO: Pod "downward-api-7daefc10-4248-45bb-a021-0f2ad1379700": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013371449s
Jul  5 09:32:44.128: INFO: Pod "downward-api-7daefc10-4248-45bb-a021-0f2ad1379700": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016521118s
STEP: Saw pod success
Jul  5 09:32:44.128: INFO: Pod "downward-api-7daefc10-4248-45bb-a021-0f2ad1379700" satisfied condition "success or failure"
Jul  5 09:32:44.132: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod downward-api-7daefc10-4248-45bb-a021-0f2ad1379700 container dapi-container: <nil>
STEP: delete the pod
Jul  5 09:32:44.159: INFO: Waiting for pod downward-api-7daefc10-4248-45bb-a021-0f2ad1379700 to disappear
Jul  5 09:32:44.162: INFO: Pod downward-api-7daefc10-4248-45bb-a021-0f2ad1379700 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:32:44.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2761" for this suite.
Jul  5 09:32:50.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:32:50.278: INFO: namespace downward-api-2761 deletion completed in 6.112664606s

• [SLOW TEST:10.208 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:32:50.279: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 09:32:50.364: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb146334-08a0-453d-b519-b63ff2d15a48" in namespace "projected-7561" to be "success or failure"
Jul  5 09:32:50.369: INFO: Pod "downwardapi-volume-fb146334-08a0-453d-b519-b63ff2d15a48": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764468ms
Jul  5 09:32:52.373: INFO: Pod "downwardapi-volume-fb146334-08a0-453d-b519-b63ff2d15a48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009448194s
Jul  5 09:32:54.377: INFO: Pod "downwardapi-volume-fb146334-08a0-453d-b519-b63ff2d15a48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012941162s
STEP: Saw pod success
Jul  5 09:32:54.377: INFO: Pod "downwardapi-volume-fb146334-08a0-453d-b519-b63ff2d15a48" satisfied condition "success or failure"
Jul  5 09:32:54.379: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod downwardapi-volume-fb146334-08a0-453d-b519-b63ff2d15a48 container client-container: <nil>
STEP: delete the pod
Jul  5 09:32:54.397: INFO: Waiting for pod downwardapi-volume-fb146334-08a0-453d-b519-b63ff2d15a48 to disappear
Jul  5 09:32:54.399: INFO: Pod downwardapi-volume-fb146334-08a0-453d-b519-b63ff2d15a48 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:32:54.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7561" for this suite.
Jul  5 09:33:00.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:33:00.503: INFO: namespace projected-7561 deletion completed in 6.100112215s

• [SLOW TEST:10.224 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:33:00.503: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-6858e40a-e3ec-4fa7-90cc-84fe1aa8e38f
STEP: Creating a pod to test consume configMaps
Jul  5 09:33:00.550: INFO: Waiting up to 5m0s for pod "pod-configmaps-b1c8c731-d640-408e-8172-e04f5a89afc3" in namespace "configmap-4824" to be "success or failure"
Jul  5 09:33:00.555: INFO: Pod "pod-configmaps-b1c8c731-d640-408e-8172-e04f5a89afc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.434104ms
Jul  5 09:33:02.561: INFO: Pod "pod-configmaps-b1c8c731-d640-408e-8172-e04f5a89afc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010600036s
Jul  5 09:33:04.564: INFO: Pod "pod-configmaps-b1c8c731-d640-408e-8172-e04f5a89afc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013735359s
STEP: Saw pod success
Jul  5 09:33:04.564: INFO: Pod "pod-configmaps-b1c8c731-d640-408e-8172-e04f5a89afc3" satisfied condition "success or failure"
Jul  5 09:33:04.566: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-configmaps-b1c8c731-d640-408e-8172-e04f5a89afc3 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 09:33:04.593: INFO: Waiting for pod pod-configmaps-b1c8c731-d640-408e-8172-e04f5a89afc3 to disappear
Jul  5 09:33:04.598: INFO: Pod pod-configmaps-b1c8c731-d640-408e-8172-e04f5a89afc3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:33:04.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4824" for this suite.
Jul  5 09:33:10.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:33:10.703: INFO: namespace configmap-4824 deletion completed in 6.101471089s

• [SLOW TEST:10.200 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:33:10.703: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:33:12.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1030" for this suite.
Jul  5 09:33:50.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:33:50.855: INFO: namespace kubelet-test-1030 deletion completed in 38.098088728s

• [SLOW TEST:40.152 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:33:50.855: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul  5 09:33:50.885: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:33:55.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9018" for this suite.
Jul  5 09:34:01.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:34:01.396: INFO: namespace init-container-9018 deletion completed in 6.099318714s

• [SLOW TEST:10.541 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:34:01.396: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  5 09:34:04.451: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:34:04.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5622" for this suite.
Jul  5 09:34:10.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:34:10.585: INFO: namespace container-runtime-5622 deletion completed in 6.116936948s

• [SLOW TEST:9.189 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:34:10.586: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Jul  5 09:34:10.613: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-272745911 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:34:10.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5230" for this suite.
Jul  5 09:34:16.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:34:16.774: INFO: namespace kubectl-5230 deletion completed in 6.100531173s

• [SLOW TEST:6.188 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:34:16.774: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul  5 09:34:16.808: INFO: Waiting up to 5m0s for pod "downward-api-34c52af0-9505-45a5-859a-e026a6e85650" in namespace "downward-api-8425" to be "success or failure"
Jul  5 09:34:16.814: INFO: Pod "downward-api-34c52af0-9505-45a5-859a-e026a6e85650": Phase="Pending", Reason="", readiness=false. Elapsed: 5.690847ms
Jul  5 09:34:18.817: INFO: Pod "downward-api-34c52af0-9505-45a5-859a-e026a6e85650": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009125643s
Jul  5 09:34:20.821: INFO: Pod "downward-api-34c52af0-9505-45a5-859a-e026a6e85650": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012377758s
STEP: Saw pod success
Jul  5 09:34:20.821: INFO: Pod "downward-api-34c52af0-9505-45a5-859a-e026a6e85650" satisfied condition "success or failure"
Jul  5 09:34:20.823: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod downward-api-34c52af0-9505-45a5-859a-e026a6e85650 container dapi-container: <nil>
STEP: delete the pod
Jul  5 09:34:20.843: INFO: Waiting for pod downward-api-34c52af0-9505-45a5-859a-e026a6e85650 to disappear
Jul  5 09:34:20.845: INFO: Pod downward-api-34c52af0-9505-45a5-859a-e026a6e85650 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:34:20.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8425" for this suite.
Jul  5 09:34:26.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:34:26.950: INFO: namespace downward-api-8425 deletion completed in 6.101274303s

• [SLOW TEST:10.176 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:34:26.951: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Jul  5 09:34:26.988: INFO: Waiting up to 5m0s for pod "pod-2e58243a-7222-4b70-ba8e-7d50c06b9107" in namespace "emptydir-8626" to be "success or failure"
Jul  5 09:34:26.992: INFO: Pod "pod-2e58243a-7222-4b70-ba8e-7d50c06b9107": Phase="Pending", Reason="", readiness=false. Elapsed: 3.219839ms
Jul  5 09:34:28.994: INFO: Pod "pod-2e58243a-7222-4b70-ba8e-7d50c06b9107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00603562s
STEP: Saw pod success
Jul  5 09:34:28.994: INFO: Pod "pod-2e58243a-7222-4b70-ba8e-7d50c06b9107" satisfied condition "success or failure"
Jul  5 09:34:28.997: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-2e58243a-7222-4b70-ba8e-7d50c06b9107 container test-container: <nil>
STEP: delete the pod
Jul  5 09:34:29.015: INFO: Waiting for pod pod-2e58243a-7222-4b70-ba8e-7d50c06b9107 to disappear
Jul  5 09:34:29.018: INFO: Pod pod-2e58243a-7222-4b70-ba8e-7d50c06b9107 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:34:29.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8626" for this suite.
Jul  5 09:34:35.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:34:35.147: INFO: namespace emptydir-8626 deletion completed in 6.126609496s

• [SLOW TEST:8.196 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:34:35.148: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Jul  5 09:34:35.177: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jul  5 09:34:35.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 create -f - --namespace=kubectl-4523'
Jul  5 09:34:35.440: INFO: stderr: ""
Jul  5 09:34:35.440: INFO: stdout: "service/redis-slave created\n"
Jul  5 09:34:35.440: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jul  5 09:34:35.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 create -f - --namespace=kubectl-4523'
Jul  5 09:34:35.695: INFO: stderr: ""
Jul  5 09:34:35.695: INFO: stdout: "service/redis-master created\n"
Jul  5 09:34:35.696: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul  5 09:34:35.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 create -f - --namespace=kubectl-4523'
Jul  5 09:34:35.904: INFO: stderr: ""
Jul  5 09:34:35.904: INFO: stdout: "service/frontend created\n"
Jul  5 09:34:35.905: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jul  5 09:34:35.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 create -f - --namespace=kubectl-4523'
Jul  5 09:34:36.128: INFO: stderr: ""
Jul  5 09:34:36.129: INFO: stdout: "deployment.apps/frontend created\n"
Jul  5 09:34:36.129: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul  5 09:34:36.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 create -f - --namespace=kubectl-4523'
Jul  5 09:34:36.347: INFO: stderr: ""
Jul  5 09:34:36.347: INFO: stdout: "deployment.apps/redis-master created\n"
Jul  5 09:34:36.347: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jul  5 09:34:36.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 create -f - --namespace=kubectl-4523'
Jul  5 09:34:36.604: INFO: stderr: ""
Jul  5 09:34:36.604: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jul  5 09:34:36.604: INFO: Waiting for all frontend pods to be Running.
Jul  5 09:34:56.655: INFO: Waiting for frontend to serve content.
Jul  5 09:34:56.667: INFO: Trying to add a new entry to the guestbook.
Jul  5 09:34:56.679: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul  5 09:34:56.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete --grace-period=0 --force -f - --namespace=kubectl-4523'
Jul  5 09:34:56.789: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 09:34:56.789: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul  5 09:34:56.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete --grace-period=0 --force -f - --namespace=kubectl-4523'
Jul  5 09:34:56.928: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 09:34:56.928: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul  5 09:34:56.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete --grace-period=0 --force -f - --namespace=kubectl-4523'
Jul  5 09:34:57.059: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 09:34:57.059: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul  5 09:34:57.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete --grace-period=0 --force -f - --namespace=kubectl-4523'
Jul  5 09:34:57.177: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 09:34:57.177: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul  5 09:34:57.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete --grace-period=0 --force -f - --namespace=kubectl-4523'
Jul  5 09:34:57.260: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 09:34:57.260: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul  5 09:34:57.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete --grace-period=0 --force -f - --namespace=kubectl-4523'
Jul  5 09:34:57.334: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 09:34:57.334: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:34:57.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4523" for this suite.
Jul  5 09:35:43.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:35:43.439: INFO: namespace kubectl-4523 deletion completed in 46.10160643s

• [SLOW TEST:68.291 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:35:43.441: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:36:09.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7247" for this suite.
Jul  5 09:36:15.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:36:15.760: INFO: namespace container-runtime-7247 deletion completed in 6.097257169s

• [SLOW TEST:32.319 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:36:15.760: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 09:36:15.807: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul  5 09:36:15.815: INFO: Number of nodes with available pods: 0
Jul  5 09:36:15.815: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul  5 09:36:15.833: INFO: Number of nodes with available pods: 0
Jul  5 09:36:15.833: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:16.836: INFO: Number of nodes with available pods: 0
Jul  5 09:36:16.836: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:17.837: INFO: Number of nodes with available pods: 0
Jul  5 09:36:17.837: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:18.836: INFO: Number of nodes with available pods: 1
Jul  5 09:36:18.836: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul  5 09:36:18.852: INFO: Number of nodes with available pods: 1
Jul  5 09:36:18.852: INFO: Number of running nodes: 0, number of available pods: 1
Jul  5 09:36:19.857: INFO: Number of nodes with available pods: 0
Jul  5 09:36:19.857: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul  5 09:36:19.874: INFO: Number of nodes with available pods: 0
Jul  5 09:36:19.874: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:20.877: INFO: Number of nodes with available pods: 0
Jul  5 09:36:20.877: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:21.877: INFO: Number of nodes with available pods: 0
Jul  5 09:36:21.877: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:22.877: INFO: Number of nodes with available pods: 0
Jul  5 09:36:22.877: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:23.877: INFO: Number of nodes with available pods: 0
Jul  5 09:36:23.877: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:24.878: INFO: Number of nodes with available pods: 0
Jul  5 09:36:24.878: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:25.878: INFO: Number of nodes with available pods: 0
Jul  5 09:36:25.878: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:26.877: INFO: Number of nodes with available pods: 0
Jul  5 09:36:26.877: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:27.877: INFO: Number of nodes with available pods: 0
Jul  5 09:36:27.877: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:28.877: INFO: Number of nodes with available pods: 0
Jul  5 09:36:28.877: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:29.879: INFO: Number of nodes with available pods: 0
Jul  5 09:36:29.880: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:30.878: INFO: Number of nodes with available pods: 0
Jul  5 09:36:30.878: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:31.877: INFO: Number of nodes with available pods: 0
Jul  5 09:36:31.877: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:36:32.877: INFO: Number of nodes with available pods: 1
Jul  5 09:36:32.877: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7964, will wait for the garbage collector to delete the pods
Jul  5 09:36:32.942: INFO: Deleting DaemonSet.extensions daemon-set took: 7.94814ms
Jul  5 09:36:33.343: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.281273ms
Jul  5 09:36:36.045: INFO: Number of nodes with available pods: 0
Jul  5 09:36:36.045: INFO: Number of running nodes: 0, number of available pods: 0
Jul  5 09:36:36.048: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7964/daemonsets","resourceVersion":"12162"},"items":null}

Jul  5 09:36:36.050: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7964/pods","resourceVersion":"12162"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:36:36.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7964" for this suite.
Jul  5 09:36:42.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:36:42.183: INFO: namespace daemonsets-7964 deletion completed in 6.112156593s

• [SLOW TEST:26.423 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:36:42.184: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  5 09:36:42.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-65'
Jul  5 09:36:42.473: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  5 09:36:42.473: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Jul  5 09:36:44.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete deployment e2e-test-nginx-deployment --namespace=kubectl-65'
Jul  5 09:36:44.566: INFO: stderr: ""
Jul  5 09:36:44.566: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:36:44.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-65" for this suite.
Jul  5 09:37:06.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:37:06.673: INFO: namespace kubectl-65 deletion completed in 22.10380873s

• [SLOW TEST:24.490 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:37:06.674: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-4b9cfc3e-cccb-4fc6-9ba5-c5197804d061
STEP: Creating a pod to test consume configMaps
Jul  5 09:37:06.711: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bcf1242e-9611-44f7-9fbf-897541261e57" in namespace "projected-3711" to be "success or failure"
Jul  5 09:37:06.715: INFO: Pod "pod-projected-configmaps-bcf1242e-9611-44f7-9fbf-897541261e57": Phase="Pending", Reason="", readiness=false. Elapsed: 3.101758ms
Jul  5 09:37:08.718: INFO: Pod "pod-projected-configmaps-bcf1242e-9611-44f7-9fbf-897541261e57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006265555s
STEP: Saw pod success
Jul  5 09:37:08.718: INFO: Pod "pod-projected-configmaps-bcf1242e-9611-44f7-9fbf-897541261e57" satisfied condition "success or failure"
Jul  5 09:37:08.720: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-projected-configmaps-bcf1242e-9611-44f7-9fbf-897541261e57 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 09:37:08.739: INFO: Waiting for pod pod-projected-configmaps-bcf1242e-9611-44f7-9fbf-897541261e57 to disappear
Jul  5 09:37:08.742: INFO: Pod pod-projected-configmaps-bcf1242e-9611-44f7-9fbf-897541261e57 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:37:08.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3711" for this suite.
Jul  5 09:37:14.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:37:14.846: INFO: namespace projected-3711 deletion completed in 6.100769614s

• [SLOW TEST:8.173 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:37:14.847: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul  5 09:37:14.881: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5550,SelfLink:/api/v1/namespaces/watch-5550/configmaps/e2e-watch-test-configmap-a,UID:708b31ba-5e6e-4400-bca2-12188d09b549,ResourceVersion:12348,Generation:0,CreationTimestamp:2019-07-05 09:37:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  5 09:37:14.881: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5550,SelfLink:/api/v1/namespaces/watch-5550/configmaps/e2e-watch-test-configmap-a,UID:708b31ba-5e6e-4400-bca2-12188d09b549,ResourceVersion:12348,Generation:0,CreationTimestamp:2019-07-05 09:37:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul  5 09:37:24.888: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5550,SelfLink:/api/v1/namespaces/watch-5550/configmaps/e2e-watch-test-configmap-a,UID:708b31ba-5e6e-4400-bca2-12188d09b549,ResourceVersion:12376,Generation:0,CreationTimestamp:2019-07-05 09:37:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul  5 09:37:24.888: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5550,SelfLink:/api/v1/namespaces/watch-5550/configmaps/e2e-watch-test-configmap-a,UID:708b31ba-5e6e-4400-bca2-12188d09b549,ResourceVersion:12376,Generation:0,CreationTimestamp:2019-07-05 09:37:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul  5 09:37:34.896: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5550,SelfLink:/api/v1/namespaces/watch-5550/configmaps/e2e-watch-test-configmap-a,UID:708b31ba-5e6e-4400-bca2-12188d09b549,ResourceVersion:12401,Generation:0,CreationTimestamp:2019-07-05 09:37:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  5 09:37:34.896: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5550,SelfLink:/api/v1/namespaces/watch-5550/configmaps/e2e-watch-test-configmap-a,UID:708b31ba-5e6e-4400-bca2-12188d09b549,ResourceVersion:12401,Generation:0,CreationTimestamp:2019-07-05 09:37:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul  5 09:37:44.904: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5550,SelfLink:/api/v1/namespaces/watch-5550/configmaps/e2e-watch-test-configmap-a,UID:708b31ba-5e6e-4400-bca2-12188d09b549,ResourceVersion:12426,Generation:0,CreationTimestamp:2019-07-05 09:37:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  5 09:37:44.904: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5550,SelfLink:/api/v1/namespaces/watch-5550/configmaps/e2e-watch-test-configmap-a,UID:708b31ba-5e6e-4400-bca2-12188d09b549,ResourceVersion:12426,Generation:0,CreationTimestamp:2019-07-05 09:37:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul  5 09:37:54.911: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5550,SelfLink:/api/v1/namespaces/watch-5550/configmaps/e2e-watch-test-configmap-b,UID:f75c773d-b1fb-4130-a546-84f34912e451,ResourceVersion:12454,Generation:0,CreationTimestamp:2019-07-05 09:37:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  5 09:37:54.912: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5550,SelfLink:/api/v1/namespaces/watch-5550/configmaps/e2e-watch-test-configmap-b,UID:f75c773d-b1fb-4130-a546-84f34912e451,ResourceVersion:12454,Generation:0,CreationTimestamp:2019-07-05 09:37:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul  5 09:38:04.920: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5550,SelfLink:/api/v1/namespaces/watch-5550/configmaps/e2e-watch-test-configmap-b,UID:f75c773d-b1fb-4130-a546-84f34912e451,ResourceVersion:12479,Generation:0,CreationTimestamp:2019-07-05 09:37:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  5 09:38:04.920: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5550,SelfLink:/api/v1/namespaces/watch-5550/configmaps/e2e-watch-test-configmap-b,UID:f75c773d-b1fb-4130-a546-84f34912e451,ResourceVersion:12479,Generation:0,CreationTimestamp:2019-07-05 09:37:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:38:14.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5550" for this suite.
Jul  5 09:38:20.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:38:21.098: INFO: namespace watch-5550 deletion completed in 6.173269335s

• [SLOW TEST:66.251 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:38:21.099: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul  5 09:38:21.140: INFO: Waiting up to 5m0s for pod "downward-api-7f603783-c699-466f-bcd9-343bdb0ed1a2" in namespace "downward-api-1848" to be "success or failure"
Jul  5 09:38:21.146: INFO: Pod "downward-api-7f603783-c699-466f-bcd9-343bdb0ed1a2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.512298ms
Jul  5 09:38:23.149: INFO: Pod "downward-api-7f603783-c699-466f-bcd9-343bdb0ed1a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009730734s
STEP: Saw pod success
Jul  5 09:38:23.149: INFO: Pod "downward-api-7f603783-c699-466f-bcd9-343bdb0ed1a2" satisfied condition "success or failure"
Jul  5 09:38:23.152: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod downward-api-7f603783-c699-466f-bcd9-343bdb0ed1a2 container dapi-container: <nil>
STEP: delete the pod
Jul  5 09:38:23.175: INFO: Waiting for pod downward-api-7f603783-c699-466f-bcd9-343bdb0ed1a2 to disappear
Jul  5 09:38:23.178: INFO: Pod downward-api-7f603783-c699-466f-bcd9-343bdb0ed1a2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:38:23.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1848" for this suite.
Jul  5 09:38:29.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:38:29.281: INFO: namespace downward-api-1848 deletion completed in 6.099363244s

• [SLOW TEST:8.182 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:38:29.281: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-54fd07f6-23d5-4654-a7b7-7d803dc720f7
STEP: Creating a pod to test consume configMaps
Jul  5 09:38:29.326: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2f42daeb-6a73-47fb-90cc-e90fedc2e869" in namespace "projected-59" to be "success or failure"
Jul  5 09:38:29.331: INFO: Pod "pod-projected-configmaps-2f42daeb-6a73-47fb-90cc-e90fedc2e869": Phase="Pending", Reason="", readiness=false. Elapsed: 4.25345ms
Jul  5 09:38:31.333: INFO: Pod "pod-projected-configmaps-2f42daeb-6a73-47fb-90cc-e90fedc2e869": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007192652s
STEP: Saw pod success
Jul  5 09:38:31.333: INFO: Pod "pod-projected-configmaps-2f42daeb-6a73-47fb-90cc-e90fedc2e869" satisfied condition "success or failure"
Jul  5 09:38:31.336: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-projected-configmaps-2f42daeb-6a73-47fb-90cc-e90fedc2e869 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 09:38:31.354: INFO: Waiting for pod pod-projected-configmaps-2f42daeb-6a73-47fb-90cc-e90fedc2e869 to disappear
Jul  5 09:38:31.357: INFO: Pod pod-projected-configmaps-2f42daeb-6a73-47fb-90cc-e90fedc2e869 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:38:31.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-59" for this suite.
Jul  5 09:38:37.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:38:37.458: INFO: namespace projected-59 deletion completed in 6.097791162s

• [SLOW TEST:8.177 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:38:37.459: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-7962/configmap-test-17eb11c9-ad35-4a9c-bb9a-438796cf4807
STEP: Creating a pod to test consume configMaps
Jul  5 09:38:37.497: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b09aa62-54a4-40c1-918e-6e93bdf7f868" in namespace "configmap-7962" to be "success or failure"
Jul  5 09:38:37.501: INFO: Pod "pod-configmaps-9b09aa62-54a4-40c1-918e-6e93bdf7f868": Phase="Pending", Reason="", readiness=false. Elapsed: 4.607324ms
Jul  5 09:38:39.504: INFO: Pod "pod-configmaps-9b09aa62-54a4-40c1-918e-6e93bdf7f868": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007586549s
STEP: Saw pod success
Jul  5 09:38:39.504: INFO: Pod "pod-configmaps-9b09aa62-54a4-40c1-918e-6e93bdf7f868" satisfied condition "success or failure"
Jul  5 09:38:39.508: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-configmaps-9b09aa62-54a4-40c1-918e-6e93bdf7f868 container env-test: <nil>
STEP: delete the pod
Jul  5 09:38:39.527: INFO: Waiting for pod pod-configmaps-9b09aa62-54a4-40c1-918e-6e93bdf7f868 to disappear
Jul  5 09:38:39.530: INFO: Pod pod-configmaps-9b09aa62-54a4-40c1-918e-6e93bdf7f868 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:38:39.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7962" for this suite.
Jul  5 09:38:45.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:38:45.636: INFO: namespace configmap-7962 deletion completed in 6.09926444s

• [SLOW TEST:8.177 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:38:45.636: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul  5 09:38:47.685: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:38:47.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8355" for this suite.
Jul  5 09:38:53.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:38:53.812: INFO: namespace container-runtime-8355 deletion completed in 6.107112488s

• [SLOW TEST:8.175 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:38:53.812: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Jul  5 09:38:53.845: INFO: Waiting up to 5m0s for pod "var-expansion-409aaa10-725c-4753-97e6-90d803f07077" in namespace "var-expansion-7294" to be "success or failure"
Jul  5 09:38:53.849: INFO: Pod "var-expansion-409aaa10-725c-4753-97e6-90d803f07077": Phase="Pending", Reason="", readiness=false. Elapsed: 3.980936ms
Jul  5 09:38:55.853: INFO: Pod "var-expansion-409aaa10-725c-4753-97e6-90d803f07077": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007112968s
STEP: Saw pod success
Jul  5 09:38:55.853: INFO: Pod "var-expansion-409aaa10-725c-4753-97e6-90d803f07077" satisfied condition "success or failure"
Jul  5 09:38:55.855: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod var-expansion-409aaa10-725c-4753-97e6-90d803f07077 container dapi-container: <nil>
STEP: delete the pod
Jul  5 09:38:55.874: INFO: Waiting for pod var-expansion-409aaa10-725c-4753-97e6-90d803f07077 to disappear
Jul  5 09:38:55.877: INFO: Pod var-expansion-409aaa10-725c-4753-97e6-90d803f07077 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:38:55.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7294" for this suite.
Jul  5 09:39:01.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:39:01.983: INFO: namespace var-expansion-7294 deletion completed in 6.101813215s

• [SLOW TEST:8.172 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:39:01.984: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0705 09:39:32.552557      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul  5 09:39:32.552: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:39:32.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1508" for this suite.
Jul  5 09:39:38.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:39:38.659: INFO: namespace gc-1508 deletion completed in 6.102468667s

• [SLOW TEST:36.675 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:39:38.659: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  5 09:39:38.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-528'
Jul  5 09:39:38.785: INFO: stderr: ""
Jul  5 09:39:38.785: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Jul  5 09:39:38.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete pods e2e-test-nginx-pod --namespace=kubectl-528'
Jul  5 09:39:51.770: INFO: stderr: ""
Jul  5 09:39:51.770: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:39:51.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-528" for this suite.
Jul  5 09:39:57.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:39:57.888: INFO: namespace kubectl-528 deletion completed in 6.113923403s

• [SLOW TEST:19.229 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:39:57.889: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 09:39:57.926: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47034b81-9d65-4dfb-a0fe-2ed5fe3bed77" in namespace "downward-api-6651" to be "success or failure"
Jul  5 09:39:57.930: INFO: Pod "downwardapi-volume-47034b81-9d65-4dfb-a0fe-2ed5fe3bed77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.499993ms
Jul  5 09:39:59.934: INFO: Pod "downwardapi-volume-47034b81-9d65-4dfb-a0fe-2ed5fe3bed77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008422811s
STEP: Saw pod success
Jul  5 09:39:59.934: INFO: Pod "downwardapi-volume-47034b81-9d65-4dfb-a0fe-2ed5fe3bed77" satisfied condition "success or failure"
Jul  5 09:39:59.937: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod downwardapi-volume-47034b81-9d65-4dfb-a0fe-2ed5fe3bed77 container client-container: <nil>
STEP: delete the pod
Jul  5 09:39:59.959: INFO: Waiting for pod downwardapi-volume-47034b81-9d65-4dfb-a0fe-2ed5fe3bed77 to disappear
Jul  5 09:39:59.961: INFO: Pod downwardapi-volume-47034b81-9d65-4dfb-a0fe-2ed5fe3bed77 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:39:59.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6651" for this suite.
Jul  5 09:40:05.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:40:06.133: INFO: namespace downward-api-6651 deletion completed in 6.167947248s

• [SLOW TEST:8.244 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:40:06.133: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 09:40:10.218: INFO: Waiting up to 5m0s for pod "client-envvars-12aeeaca-b644-4d3c-81b3-63c58043c703" in namespace "pods-9142" to be "success or failure"
Jul  5 09:40:10.223: INFO: Pod "client-envvars-12aeeaca-b644-4d3c-81b3-63c58043c703": Phase="Pending", Reason="", readiness=false. Elapsed: 4.993763ms
Jul  5 09:40:12.228: INFO: Pod "client-envvars-12aeeaca-b644-4d3c-81b3-63c58043c703": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009660136s
STEP: Saw pod success
Jul  5 09:40:12.228: INFO: Pod "client-envvars-12aeeaca-b644-4d3c-81b3-63c58043c703" satisfied condition "success or failure"
Jul  5 09:40:12.230: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod client-envvars-12aeeaca-b644-4d3c-81b3-63c58043c703 container env3cont: <nil>
STEP: delete the pod
Jul  5 09:40:12.252: INFO: Waiting for pod client-envvars-12aeeaca-b644-4d3c-81b3-63c58043c703 to disappear
Jul  5 09:40:12.255: INFO: Pod client-envvars-12aeeaca-b644-4d3c-81b3-63c58043c703 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:40:12.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9142" for this suite.
Jul  5 09:41:04.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:41:04.399: INFO: namespace pods-9142 deletion completed in 52.140723424s

• [SLOW TEST:58.266 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:41:04.400: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Jul  5 09:41:04.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 api-versions'
Jul  5 09:41:04.509: INFO: stderr: ""
Jul  5 09:41:04.509: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:41:04.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6280" for this suite.
Jul  5 09:41:10.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:41:10.612: INFO: namespace kubectl-6280 deletion completed in 6.099214738s

• [SLOW TEST:6.212 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:41:10.612: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:41:12.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2992" for this suite.
Jul  5 09:41:18.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:41:18.959: INFO: namespace emptydir-wrapper-2992 deletion completed in 6.151195766s

• [SLOW TEST:8.347 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:41:18.959: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 09:41:37.022: INFO: Container started at 2019-07-05 09:41:20 +0000 UTC, pod became ready at 2019-07-05 09:41:36 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:41:37.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2710" for this suite.
Jul  5 09:41:59.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:41:59.134: INFO: namespace container-probe-2710 deletion completed in 22.108245239s

• [SLOW TEST:40.175 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:41:59.134: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-hxpw
STEP: Creating a pod to test atomic-volume-subpath
Jul  5 09:41:59.172: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hxpw" in namespace "subpath-8270" to be "success or failure"
Jul  5 09:41:59.178: INFO: Pod "pod-subpath-test-configmap-hxpw": Phase="Pending", Reason="", readiness=false. Elapsed: 5.339598ms
Jul  5 09:42:01.182: INFO: Pod "pod-subpath-test-configmap-hxpw": Phase="Running", Reason="", readiness=true. Elapsed: 2.009863794s
Jul  5 09:42:03.186: INFO: Pod "pod-subpath-test-configmap-hxpw": Phase="Running", Reason="", readiness=true. Elapsed: 4.013343333s
Jul  5 09:42:05.189: INFO: Pod "pod-subpath-test-configmap-hxpw": Phase="Running", Reason="", readiness=true. Elapsed: 6.016592986s
Jul  5 09:42:07.192: INFO: Pod "pod-subpath-test-configmap-hxpw": Phase="Running", Reason="", readiness=true. Elapsed: 8.019832348s
Jul  5 09:42:09.195: INFO: Pod "pod-subpath-test-configmap-hxpw": Phase="Running", Reason="", readiness=true. Elapsed: 10.022891306s
Jul  5 09:42:11.199: INFO: Pod "pod-subpath-test-configmap-hxpw": Phase="Running", Reason="", readiness=true. Elapsed: 12.026625962s
Jul  5 09:42:13.202: INFO: Pod "pod-subpath-test-configmap-hxpw": Phase="Running", Reason="", readiness=true. Elapsed: 14.029988842s
Jul  5 09:42:15.206: INFO: Pod "pod-subpath-test-configmap-hxpw": Phase="Running", Reason="", readiness=true. Elapsed: 16.033356007s
Jul  5 09:42:17.209: INFO: Pod "pod-subpath-test-configmap-hxpw": Phase="Running", Reason="", readiness=true. Elapsed: 18.03632814s
Jul  5 09:42:19.212: INFO: Pod "pod-subpath-test-configmap-hxpw": Phase="Running", Reason="", readiness=true. Elapsed: 20.039295336s
Jul  5 09:42:21.214: INFO: Pod "pod-subpath-test-configmap-hxpw": Phase="Running", Reason="", readiness=true. Elapsed: 22.042000462s
Jul  5 09:42:23.217: INFO: Pod "pod-subpath-test-configmap-hxpw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.045040596s
STEP: Saw pod success
Jul  5 09:42:23.218: INFO: Pod "pod-subpath-test-configmap-hxpw" satisfied condition "success or failure"
Jul  5 09:42:23.220: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-subpath-test-configmap-hxpw container test-container-subpath-configmap-hxpw: <nil>
STEP: delete the pod
Jul  5 09:42:23.238: INFO: Waiting for pod pod-subpath-test-configmap-hxpw to disappear
Jul  5 09:42:23.241: INFO: Pod pod-subpath-test-configmap-hxpw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hxpw
Jul  5 09:42:23.241: INFO: Deleting pod "pod-subpath-test-configmap-hxpw" in namespace "subpath-8270"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:42:23.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8270" for this suite.
Jul  5 09:42:29.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:42:29.360: INFO: namespace subpath-8270 deletion completed in 6.112662579s

• [SLOW TEST:30.225 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:42:29.360: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul  5 09:42:29.418: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:29.418: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:29.418: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:29.420: INFO: Number of nodes with available pods: 0
Jul  5 09:42:29.421: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:42:30.425: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:30.425: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:30.425: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:30.428: INFO: Number of nodes with available pods: 0
Jul  5 09:42:30.428: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:42:31.434: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:31.436: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:31.438: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:31.441: INFO: Number of nodes with available pods: 2
Jul  5 09:42:31.441: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:42:32.428: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:32.428: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:32.428: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:32.432: INFO: Number of nodes with available pods: 2
Jul  5 09:42:32.433: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:42:33.426: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:33.426: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:33.426: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:33.429: INFO: Number of nodes with available pods: 3
Jul  5 09:42:33.429: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul  5 09:42:33.462: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:33.464: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:33.464: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:33.476: INFO: Number of nodes with available pods: 2
Jul  5 09:42:33.476: INFO: Node ip-172-31-11-115.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:42:34.481: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:34.482: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:34.482: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:34.485: INFO: Number of nodes with available pods: 2
Jul  5 09:42:34.485: INFO: Node ip-172-31-11-115.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 09:42:35.480: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:35.480: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:35.480: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 09:42:35.483: INFO: Number of nodes with available pods: 3
Jul  5 09:42:35.483: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2682, will wait for the garbage collector to delete the pods
Jul  5 09:42:35.551: INFO: Deleting DaemonSet.extensions daemon-set took: 8.997696ms
Jul  5 09:42:35.951: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.232338ms
Jul  5 09:42:39.554: INFO: Number of nodes with available pods: 0
Jul  5 09:42:39.554: INFO: Number of running nodes: 0, number of available pods: 0
Jul  5 09:42:39.557: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2682/daemonsets","resourceVersion":"13614"},"items":null}

Jul  5 09:42:39.559: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2682/pods","resourceVersion":"13614"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:42:39.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2682" for this suite.
Jul  5 09:42:45.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:42:45.676: INFO: namespace daemonsets-2682 deletion completed in 6.101705456s

• [SLOW TEST:16.316 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:42:45.677: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 09:42:45.725: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3de3ba2-dcfd-4428-a29f-f391c347a4ac" in namespace "downward-api-3690" to be "success or failure"
Jul  5 09:42:45.730: INFO: Pod "downwardapi-volume-c3de3ba2-dcfd-4428-a29f-f391c347a4ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.221103ms
Jul  5 09:42:47.733: INFO: Pod "downwardapi-volume-c3de3ba2-dcfd-4428-a29f-f391c347a4ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007253852s
STEP: Saw pod success
Jul  5 09:42:47.733: INFO: Pod "downwardapi-volume-c3de3ba2-dcfd-4428-a29f-f391c347a4ac" satisfied condition "success or failure"
Jul  5 09:42:47.735: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod downwardapi-volume-c3de3ba2-dcfd-4428-a29f-f391c347a4ac container client-container: <nil>
STEP: delete the pod
Jul  5 09:42:47.762: INFO: Waiting for pod downwardapi-volume-c3de3ba2-dcfd-4428-a29f-f391c347a4ac to disappear
Jul  5 09:42:47.765: INFO: Pod downwardapi-volume-c3de3ba2-dcfd-4428-a29f-f391c347a4ac no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:42:47.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3690" for this suite.
Jul  5 09:42:53.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:42:53.922: INFO: namespace downward-api-3690 deletion completed in 6.154114197s

• [SLOW TEST:8.246 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:42:53.923: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6023
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul  5 09:42:53.980: INFO: Found 0 stateful pods, waiting for 3
Jul  5 09:43:03.984: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 09:43:03.984: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 09:43:03.984: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 09:43:03.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-6023 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  5 09:43:04.461: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  5 09:43:04.462: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  5 09:43:04.462: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul  5 09:43:14.493: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul  5 09:43:24.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-6023 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:43:24.745: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  5 09:43:24.745: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  5 09:43:24.745: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  5 09:43:54.762: INFO: Waiting for StatefulSet statefulset-6023/ss2 to complete update
STEP: Rolling back to a previous revision
Jul  5 09:44:04.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-6023 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  5 09:44:05.169: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  5 09:44:05.169: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  5 09:44:05.169: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  5 09:44:15.200: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul  5 09:44:25.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 exec --namespace=statefulset-6023 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  5 09:44:25.468: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  5 09:44:25.468: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  5 09:44:25.468: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  5 09:44:45.485: INFO: Waiting for StatefulSet statefulset-6023/ss2 to complete update
Jul  5 09:44:45.485: INFO: Waiting for Pod statefulset-6023/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul  5 09:44:55.491: INFO: Deleting all statefulset in ns statefulset-6023
Jul  5 09:44:55.493: INFO: Scaling statefulset ss2 to 0
Jul  5 09:45:05.507: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 09:45:05.509: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:45:05.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6023" for this suite.
Jul  5 09:45:11.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:45:11.643: INFO: namespace statefulset-6023 deletion completed in 6.114283728s

• [SLOW TEST:137.720 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:45:11.644: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 09:45:11.681: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4e4da16-7210-4c95-9ed9-ff418df011d5" in namespace "downward-api-9322" to be "success or failure"
Jul  5 09:45:11.684: INFO: Pod "downwardapi-volume-e4e4da16-7210-4c95-9ed9-ff418df011d5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.446454ms
Jul  5 09:45:13.688: INFO: Pod "downwardapi-volume-e4e4da16-7210-4c95-9ed9-ff418df011d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006817372s
Jul  5 09:45:15.691: INFO: Pod "downwardapi-volume-e4e4da16-7210-4c95-9ed9-ff418df011d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010529827s
STEP: Saw pod success
Jul  5 09:45:15.691: INFO: Pod "downwardapi-volume-e4e4da16-7210-4c95-9ed9-ff418df011d5" satisfied condition "success or failure"
Jul  5 09:45:15.694: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod downwardapi-volume-e4e4da16-7210-4c95-9ed9-ff418df011d5 container client-container: <nil>
STEP: delete the pod
Jul  5 09:45:15.716: INFO: Waiting for pod downwardapi-volume-e4e4da16-7210-4c95-9ed9-ff418df011d5 to disappear
Jul  5 09:45:15.718: INFO: Pod downwardapi-volume-e4e4da16-7210-4c95-9ed9-ff418df011d5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:45:15.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9322" for this suite.
Jul  5 09:45:21.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:45:21.822: INFO: namespace downward-api-9322 deletion completed in 6.100654026s

• [SLOW TEST:10.179 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:45:21.823: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5775, will wait for the garbage collector to delete the pods
Jul  5 09:45:25.927: INFO: Deleting Job.batch foo took: 7.245707ms
Jul  5 09:45:26.327: INFO: Terminating Job.batch foo pods took: 400.189516ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:46:09.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5775" for this suite.
Jul  5 09:46:15.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:46:15.443: INFO: namespace job-5775 deletion completed in 6.107961881s

• [SLOW TEST:53.620 seconds]
[sig-apps] Job
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:46:15.443: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-d7777775-2f04-481c-ace6-8c8ccb494ec5
STEP: Creating a pod to test consume configMaps
Jul  5 09:46:15.484: INFO: Waiting up to 5m0s for pod "pod-configmaps-2341f5e3-4916-4cae-940d-63225124e994" in namespace "configmap-4033" to be "success or failure"
Jul  5 09:46:15.491: INFO: Pod "pod-configmaps-2341f5e3-4916-4cae-940d-63225124e994": Phase="Pending", Reason="", readiness=false. Elapsed: 6.208948ms
Jul  5 09:46:17.494: INFO: Pod "pod-configmaps-2341f5e3-4916-4cae-940d-63225124e994": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009574751s
Jul  5 09:46:19.497: INFO: Pod "pod-configmaps-2341f5e3-4916-4cae-940d-63225124e994": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012862798s
STEP: Saw pod success
Jul  5 09:46:19.498: INFO: Pod "pod-configmaps-2341f5e3-4916-4cae-940d-63225124e994" satisfied condition "success or failure"
Jul  5 09:46:19.500: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-configmaps-2341f5e3-4916-4cae-940d-63225124e994 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 09:46:19.517: INFO: Waiting for pod pod-configmaps-2341f5e3-4916-4cae-940d-63225124e994 to disappear
Jul  5 09:46:19.521: INFO: Pod pod-configmaps-2341f5e3-4916-4cae-940d-63225124e994 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:46:19.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4033" for this suite.
Jul  5 09:46:25.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:46:25.623: INFO: namespace configmap-4033 deletion completed in 6.098165448s

• [SLOW TEST:10.180 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:46:25.623: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  5 09:46:25.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9588'
Jul  5 09:46:25.734: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  5 09:46:25.734: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jul  5 09:46:25.746: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-tgq9m]
Jul  5 09:46:25.747: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-tgq9m" in namespace "kubectl-9588" to be "running and ready"
Jul  5 09:46:25.749: INFO: Pod "e2e-test-nginx-rc-tgq9m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.547781ms
Jul  5 09:46:27.752: INFO: Pod "e2e-test-nginx-rc-tgq9m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005510047s
Jul  5 09:46:29.756: INFO: Pod "e2e-test-nginx-rc-tgq9m": Phase="Running", Reason="", readiness=true. Elapsed: 4.009201978s
Jul  5 09:46:29.756: INFO: Pod "e2e-test-nginx-rc-tgq9m" satisfied condition "running and ready"
Jul  5 09:46:29.756: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-tgq9m]
Jul  5 09:46:29.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 logs rc/e2e-test-nginx-rc --namespace=kubectl-9588'
Jul  5 09:46:29.867: INFO: stderr: ""
Jul  5 09:46:29.867: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Jul  5 09:46:29.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete rc e2e-test-nginx-rc --namespace=kubectl-9588'
Jul  5 09:46:29.961: INFO: stderr: ""
Jul  5 09:46:29.961: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:46:29.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9588" for this suite.
Jul  5 09:46:51.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:46:52.062: INFO: namespace kubectl-9588 deletion completed in 22.097885723s

• [SLOW TEST:26.440 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:46:52.063: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4146
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  5 09:46:52.091: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  5 09:47:18.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.80:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4146 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 09:47:18.168: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 09:47:18.353: INFO: Found all expected endpoints: [netserver-0]
Jul  5 09:47:18.357: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.5.25:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4146 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 09:47:18.357: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 09:47:18.538: INFO: Found all expected endpoints: [netserver-1]
Jul  5 09:47:18.540: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.4.74:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4146 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 09:47:18.540: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 09:47:18.720: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:47:18.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4146" for this suite.
Jul  5 09:47:40.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:47:40.830: INFO: namespace pod-network-test-4146 deletion completed in 22.106111628s

• [SLOW TEST:48.767 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:47:40.831: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a7ac739c-af12-43db-8ddb-d09103976337
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-a7ac739c-af12-43db-8ddb-d09103976337
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:47:44.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3173" for this suite.
Jul  5 09:48:06.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:48:07.050: INFO: namespace projected-3173 deletion completed in 22.139417554s

• [SLOW TEST:26.219 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:48:07.051: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3291.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3291.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  5 09:48:11.133: INFO: DNS probes using dns-3291/dns-test-73c7c046-65f0-4808-96df-da255067abaa succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:48:11.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3291" for this suite.
Jul  5 09:48:17.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:48:17.245: INFO: namespace dns-3291 deletion completed in 6.096034847s

• [SLOW TEST:10.194 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:48:17.245: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:48:19.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2986" for this suite.
Jul  5 09:48:57.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:48:57.393: INFO: namespace kubelet-test-2986 deletion completed in 38.093654756s

• [SLOW TEST:40.148 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:48:57.394: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-8febd877-3c39-40fb-a16d-2c04ef6e5fcd
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:48:57.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9333" for this suite.
Jul  5 09:49:03.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:49:03.524: INFO: namespace configmap-9333 deletion completed in 6.098781659s

• [SLOW TEST:6.130 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:49:03.524: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:49:03.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7033" for this suite.
Jul  5 09:49:09.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:49:09.675: INFO: namespace kubelet-test-7033 deletion completed in 6.091990317s

• [SLOW TEST:6.151 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:49:09.676: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:49:09.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7529" for this suite.
Jul  5 09:49:15.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:49:15.813: INFO: namespace services-7529 deletion completed in 6.103546989s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.137 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:49:15.814: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul  5 09:49:17.866: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-5e5bab6d-d905-4075-bb07-6e8f36cea552,GenerateName:,Namespace:events-3840,SelfLink:/api/v1/namespaces/events-3840/pods/send-events-5e5bab6d-d905-4075-bb07-6e8f36cea552,UID:a19031c7-4965-48f7-99e1-51df1fbb2148,ResourceVersion:15390,Generation:0,CreationTimestamp:2019-07-05 09:49:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 842279333,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.77/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6tmj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6tmj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-c6tmj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-212.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a51510} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a51530}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:49:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:49:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:49:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 09:49:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.1.212,PodIP:10.244.4.77,StartTime:2019-07-05 09:49:15 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-07-05 09:49:17 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://613258c6ac77b75f71438d21df33fa2cc341329913d97cc96d2aeb049d24997b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jul  5 09:49:19.870: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul  5 09:49:21.873: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:49:21.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3840" for this suite.
Jul  5 09:49:59.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:50:00.019: INFO: namespace events-3840 deletion completed in 38.135988526s

• [SLOW TEST:44.205 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:50:00.022: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-8207/configmap-test-500da90f-cb9d-4fff-bc2e-32ae930de3ee
STEP: Creating a pod to test consume configMaps
Jul  5 09:50:00.079: INFO: Waiting up to 5m0s for pod "pod-configmaps-b9cb6d95-879c-4a35-8805-2350f88b9077" in namespace "configmap-8207" to be "success or failure"
Jul  5 09:50:00.085: INFO: Pod "pod-configmaps-b9cb6d95-879c-4a35-8805-2350f88b9077": Phase="Pending", Reason="", readiness=false. Elapsed: 6.406791ms
Jul  5 09:50:02.088: INFO: Pod "pod-configmaps-b9cb6d95-879c-4a35-8805-2350f88b9077": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009701632s
Jul  5 09:50:04.092: INFO: Pod "pod-configmaps-b9cb6d95-879c-4a35-8805-2350f88b9077": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013069953s
STEP: Saw pod success
Jul  5 09:50:04.092: INFO: Pod "pod-configmaps-b9cb6d95-879c-4a35-8805-2350f88b9077" satisfied condition "success or failure"
Jul  5 09:50:04.094: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-configmaps-b9cb6d95-879c-4a35-8805-2350f88b9077 container env-test: <nil>
STEP: delete the pod
Jul  5 09:50:04.112: INFO: Waiting for pod pod-configmaps-b9cb6d95-879c-4a35-8805-2350f88b9077 to disappear
Jul  5 09:50:04.114: INFO: Pod pod-configmaps-b9cb6d95-879c-4a35-8805-2350f88b9077 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:50:04.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8207" for this suite.
Jul  5 09:50:10.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:50:10.213: INFO: namespace configmap-8207 deletion completed in 6.095677914s

• [SLOW TEST:10.192 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:50:10.214: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul  5 09:50:10.247: INFO: Waiting up to 5m0s for pod "pod-28d7814e-9bd9-4723-bd48-4463844334ea" in namespace "emptydir-9626" to be "success or failure"
Jul  5 09:50:10.251: INFO: Pod "pod-28d7814e-9bd9-4723-bd48-4463844334ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.157194ms
Jul  5 09:50:12.254: INFO: Pod "pod-28d7814e-9bd9-4723-bd48-4463844334ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007055507s
STEP: Saw pod success
Jul  5 09:50:12.254: INFO: Pod "pod-28d7814e-9bd9-4723-bd48-4463844334ea" satisfied condition "success or failure"
Jul  5 09:50:12.256: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-28d7814e-9bd9-4723-bd48-4463844334ea container test-container: <nil>
STEP: delete the pod
Jul  5 09:50:12.274: INFO: Waiting for pod pod-28d7814e-9bd9-4723-bd48-4463844334ea to disappear
Jul  5 09:50:12.278: INFO: Pod pod-28d7814e-9bd9-4723-bd48-4463844334ea no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:50:12.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9626" for this suite.
Jul  5 09:50:18.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:50:18.376: INFO: namespace emptydir-9626 deletion completed in 6.093781503s

• [SLOW TEST:8.162 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:50:18.376: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6279
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  5 09:50:18.404: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  5 09:50:40.487: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.5.27:8080/dial?request=hostName&protocol=udp&host=10.244.4.79&port=8081&tries=1'] Namespace:pod-network-test-6279 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 09:50:40.487: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 09:50:40.645: INFO: Waiting for endpoints: map[]
Jul  5 09:50:40.648: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.5.27:8080/dial?request=hostName&protocol=udp&host=10.244.3.84&port=8081&tries=1'] Namespace:pod-network-test-6279 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 09:50:40.648: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 09:50:40.812: INFO: Waiting for endpoints: map[]
Jul  5 09:50:40.814: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.5.27:8080/dial?request=hostName&protocol=udp&host=10.244.5.26&port=8081&tries=1'] Namespace:pod-network-test-6279 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 09:50:40.814: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 09:50:40.999: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:50:40.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6279" for this suite.
Jul  5 09:51:03.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:51:03.121: INFO: namespace pod-network-test-6279 deletion completed in 22.118759616s

• [SLOW TEST:44.745 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:51:03.125: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul  5 09:51:07.200: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  5 09:51:07.203: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  5 09:51:09.203: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  5 09:51:09.206: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  5 09:51:11.203: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  5 09:51:11.206: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  5 09:51:13.203: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  5 09:51:13.206: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  5 09:51:15.203: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  5 09:51:15.206: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  5 09:51:17.203: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  5 09:51:17.206: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  5 09:51:19.203: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  5 09:51:19.206: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  5 09:51:21.203: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  5 09:51:21.206: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  5 09:51:23.203: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  5 09:51:23.206: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  5 09:51:25.203: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  5 09:51:25.206: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  5 09:51:27.203: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  5 09:51:27.206: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:51:27.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4169" for this suite.
Jul  5 09:51:49.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:51:49.362: INFO: namespace container-lifecycle-hook-4169 deletion completed in 22.143652609s

• [SLOW TEST:46.237 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:51:49.363: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul  5 09:51:55.422: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:51:55.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0705 09:51:55.422164      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-7227" for this suite.
Jul  5 09:52:01.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:52:01.519: INFO: namespace gc-7227 deletion completed in 6.094143651s

• [SLOW TEST:12.156 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:52:01.519: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Jul  5 09:52:01.555: INFO: Waiting up to 5m0s for pod "client-containers-7bd8a32b-d8a4-4cd8-b340-7eb93671152d" in namespace "containers-7705" to be "success or failure"
Jul  5 09:52:01.559: INFO: Pod "client-containers-7bd8a32b-d8a4-4cd8-b340-7eb93671152d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.110455ms
Jul  5 09:52:03.562: INFO: Pod "client-containers-7bd8a32b-d8a4-4cd8-b340-7eb93671152d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006216924s
Jul  5 09:52:05.565: INFO: Pod "client-containers-7bd8a32b-d8a4-4cd8-b340-7eb93671152d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009506024s
STEP: Saw pod success
Jul  5 09:52:05.565: INFO: Pod "client-containers-7bd8a32b-d8a4-4cd8-b340-7eb93671152d" satisfied condition "success or failure"
Jul  5 09:52:05.568: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod client-containers-7bd8a32b-d8a4-4cd8-b340-7eb93671152d container test-container: <nil>
STEP: delete the pod
Jul  5 09:52:05.585: INFO: Waiting for pod client-containers-7bd8a32b-d8a4-4cd8-b340-7eb93671152d to disappear
Jul  5 09:52:05.588: INFO: Pod client-containers-7bd8a32b-d8a4-4cd8-b340-7eb93671152d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:52:05.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7705" for this suite.
Jul  5 09:52:11.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:52:11.696: INFO: namespace containers-7705 deletion completed in 6.104297645s

• [SLOW TEST:10.176 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:52:11.696: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 09:52:11.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d32948d9-31b4-483d-8118-5a746547b9b7" in namespace "downward-api-3801" to be "success or failure"
Jul  5 09:52:11.733: INFO: Pod "downwardapi-volume-d32948d9-31b4-483d-8118-5a746547b9b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.05491ms
Jul  5 09:52:13.736: INFO: Pod "downwardapi-volume-d32948d9-31b4-483d-8118-5a746547b9b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005813812s
STEP: Saw pod success
Jul  5 09:52:13.736: INFO: Pod "downwardapi-volume-d32948d9-31b4-483d-8118-5a746547b9b7" satisfied condition "success or failure"
Jul  5 09:52:13.738: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod downwardapi-volume-d32948d9-31b4-483d-8118-5a746547b9b7 container client-container: <nil>
STEP: delete the pod
Jul  5 09:52:13.765: INFO: Waiting for pod downwardapi-volume-d32948d9-31b4-483d-8118-5a746547b9b7 to disappear
Jul  5 09:52:13.768: INFO: Pod downwardapi-volume-d32948d9-31b4-483d-8118-5a746547b9b7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:52:13.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3801" for this suite.
Jul  5 09:52:19.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:52:19.896: INFO: namespace downward-api-3801 deletion completed in 6.124878273s

• [SLOW TEST:8.200 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:52:19.896: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul  5 09:52:19.933: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  5 09:52:19.942: INFO: Waiting for terminating namespaces to be deleted...
Jul  5 09:52:19.945: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-1-212.eu-west-3.compute.internal before test
Jul  5 09:52:19.950: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-05 08:51:44 +0000 UTC (1 container statuses recorded)
Jul  5 09:52:19.950: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  5 09:52:19.950: INFO: kube-proxy-zpgcv from kube-system started at 2019-07-05 08:50:50 +0000 UTC (1 container statuses recorded)
Jul  5 09:52:19.950: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 09:52:19.950: INFO: canal-5kgvk from kube-system started at 2019-07-05 08:50:50 +0000 UTC (2 container statuses recorded)
Jul  5 09:52:19.950: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 09:52:19.950: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  5 09:52:19.950: INFO: sonobuoy-systemd-logs-daemon-set-e9d729729fbf4cf5-rfc95 from heptio-sonobuoy started at 2019-07-05 08:51:49 +0000 UTC (2 container statuses recorded)
Jul  5 09:52:19.950: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul  5 09:52:19.950: INFO: 	Container systemd-logs ready: true, restart count 1
Jul  5 09:52:19.950: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-11-115.eu-west-3.compute.internal before test
Jul  5 09:52:19.955: INFO: kube-proxy-h7nqv from kube-system started at 2019-07-05 08:50:49 +0000 UTC (1 container statuses recorded)
Jul  5 09:52:19.955: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 09:52:19.955: INFO: canal-m82pj from kube-system started at 2019-07-05 08:50:49 +0000 UTC (2 container statuses recorded)
Jul  5 09:52:19.955: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 09:52:19.955: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  5 09:52:19.955: INFO: sonobuoy-systemd-logs-daemon-set-e9d729729fbf4cf5-rm8ks from heptio-sonobuoy started at 2019-07-05 08:51:49 +0000 UTC (2 container statuses recorded)
Jul  5 09:52:19.955: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul  5 09:52:19.955: INFO: 	Container systemd-logs ready: true, restart count 1
Jul  5 09:52:19.955: INFO: metrics-server-6964ccc5b-88zkm from kube-system started at 2019-07-05 08:51:07 +0000 UTC (1 container statuses recorded)
Jul  5 09:52:19.955: INFO: 	Container metrics-server ready: true, restart count 0
Jul  5 09:52:19.955: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-6-226.eu-west-3.compute.internal before test
Jul  5 09:52:19.962: INFO: kube-proxy-zc7nm from kube-system started at 2019-07-05 08:50:51 +0000 UTC (1 container statuses recorded)
Jul  5 09:52:19.962: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 09:52:19.962: INFO: sonobuoy-e2e-job-dddb5268764f464f from heptio-sonobuoy started at 2019-07-05 08:51:49 +0000 UTC (2 container statuses recorded)
Jul  5 09:52:19.962: INFO: 	Container e2e ready: true, restart count 0
Jul  5 09:52:19.962: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  5 09:52:19.962: INFO: sonobuoy-systemd-logs-daemon-set-e9d729729fbf4cf5-b7svv from heptio-sonobuoy started at 2019-07-05 08:51:49 +0000 UTC (2 container statuses recorded)
Jul  5 09:52:19.962: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul  5 09:52:19.962: INFO: 	Container systemd-logs ready: true, restart count 1
Jul  5 09:52:19.962: INFO: canal-xzlkm from kube-system started at 2019-07-05 08:50:51 +0000 UTC (2 container statuses recorded)
Jul  5 09:52:19.962: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 09:52:19.962: INFO: 	Container kube-flannel ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-50612641-1f4a-4b18-b05f-f7f411429349 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-50612641-1f4a-4b18-b05f-f7f411429349 off the node ip-172-31-11-115.eu-west-3.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-50612641-1f4a-4b18-b05f-f7f411429349
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:52:24.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1678" for this suite.
Jul  5 09:52:42.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:52:42.163: INFO: namespace sched-pred-1678 deletion completed in 18.12218144s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:22.267 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:52:42.163: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:52:44.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8695" for this suite.
Jul  5 09:53:22.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:53:22.318: INFO: namespace kubelet-test-8695 deletion completed in 38.103209435s

• [SLOW TEST:40.155 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:53:22.319: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 09:53:22.347: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:53:26.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5204" for this suite.
Jul  5 09:54:04.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:54:04.488: INFO: namespace pods-5204 deletion completed in 38.102155697s

• [SLOW TEST:42.169 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:54:04.488: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul  5 09:54:07.057: INFO: Successfully updated pod "annotationupdatecc234712-78f9-45c5-b566-0ef567b9e883"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:54:09.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5209" for this suite.
Jul  5 09:54:31.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:54:31.173: INFO: namespace projected-5209 deletion completed in 22.09778411s

• [SLOW TEST:26.684 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:54:31.173: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 09:54:31.204: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul  5 09:54:33.235: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:54:33.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5797" for this suite.
Jul  5 09:54:39.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:54:39.338: INFO: namespace replication-controller-5797 deletion completed in 6.096890384s

• [SLOW TEST:8.165 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:54:39.338: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul  5 09:54:39.374: INFO: Waiting up to 5m0s for pod "pod-442bdac3-68e1-4a29-a604-07df4f86b3dc" in namespace "emptydir-5826" to be "success or failure"
Jul  5 09:54:39.378: INFO: Pod "pod-442bdac3-68e1-4a29-a604-07df4f86b3dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066549ms
Jul  5 09:54:41.383: INFO: Pod "pod-442bdac3-68e1-4a29-a604-07df4f86b3dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009108364s
Jul  5 09:54:43.393: INFO: Pod "pod-442bdac3-68e1-4a29-a604-07df4f86b3dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018896344s
STEP: Saw pod success
Jul  5 09:54:43.393: INFO: Pod "pod-442bdac3-68e1-4a29-a604-07df4f86b3dc" satisfied condition "success or failure"
Jul  5 09:54:43.395: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-442bdac3-68e1-4a29-a604-07df4f86b3dc container test-container: <nil>
STEP: delete the pod
Jul  5 09:54:43.413: INFO: Waiting for pod pod-442bdac3-68e1-4a29-a604-07df4f86b3dc to disappear
Jul  5 09:54:43.416: INFO: Pod pod-442bdac3-68e1-4a29-a604-07df4f86b3dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:54:43.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5826" for this suite.
Jul  5 09:54:49.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:54:49.522: INFO: namespace emptydir-5826 deletion completed in 6.10245417s

• [SLOW TEST:10.184 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:54:49.522: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul  5 09:54:49.560: INFO: Waiting up to 5m0s for pod "downward-api-060cd529-0b9b-48b0-82b2-8736677e166a" in namespace "downward-api-9074" to be "success or failure"
Jul  5 09:54:49.563: INFO: Pod "downward-api-060cd529-0b9b-48b0-82b2-8736677e166a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.945802ms
Jul  5 09:54:51.566: INFO: Pod "downward-api-060cd529-0b9b-48b0-82b2-8736677e166a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006211262s
STEP: Saw pod success
Jul  5 09:54:51.566: INFO: Pod "downward-api-060cd529-0b9b-48b0-82b2-8736677e166a" satisfied condition "success or failure"
Jul  5 09:54:51.569: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod downward-api-060cd529-0b9b-48b0-82b2-8736677e166a container dapi-container: <nil>
STEP: delete the pod
Jul  5 09:54:51.587: INFO: Waiting for pod downward-api-060cd529-0b9b-48b0-82b2-8736677e166a to disappear
Jul  5 09:54:51.589: INFO: Pod downward-api-060cd529-0b9b-48b0-82b2-8736677e166a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:54:51.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9074" for this suite.
Jul  5 09:54:57.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:54:57.687: INFO: namespace downward-api-9074 deletion completed in 6.094527106s

• [SLOW TEST:8.164 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:54:57.687: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-882a4cc3-7c92-4621-ad03-db0b9df8cbad
STEP: Creating a pod to test consume configMaps
Jul  5 09:54:57.723: INFO: Waiting up to 5m0s for pod "pod-configmaps-f41c55d9-2c61-409a-8dce-015f33f73755" in namespace "configmap-8351" to be "success or failure"
Jul  5 09:54:57.727: INFO: Pod "pod-configmaps-f41c55d9-2c61-409a-8dce-015f33f73755": Phase="Pending", Reason="", readiness=false. Elapsed: 3.901523ms
Jul  5 09:54:59.730: INFO: Pod "pod-configmaps-f41c55d9-2c61-409a-8dce-015f33f73755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006835814s
STEP: Saw pod success
Jul  5 09:54:59.730: INFO: Pod "pod-configmaps-f41c55d9-2c61-409a-8dce-015f33f73755" satisfied condition "success or failure"
Jul  5 09:54:59.733: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-configmaps-f41c55d9-2c61-409a-8dce-015f33f73755 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 09:54:59.756: INFO: Waiting for pod pod-configmaps-f41c55d9-2c61-409a-8dce-015f33f73755 to disappear
Jul  5 09:54:59.762: INFO: Pod pod-configmaps-f41c55d9-2c61-409a-8dce-015f33f73755 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:54:59.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8351" for this suite.
Jul  5 09:55:05.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:55:05.864: INFO: namespace configmap-8351 deletion completed in 6.098477455s

• [SLOW TEST:8.177 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:55:05.865: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 09:55:05.899: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e689868-93c0-4fdd-a4fc-eeb7386f6750" in namespace "downward-api-6194" to be "success or failure"
Jul  5 09:55:05.903: INFO: Pod "downwardapi-volume-5e689868-93c0-4fdd-a4fc-eeb7386f6750": Phase="Pending", Reason="", readiness=false. Elapsed: 3.767071ms
Jul  5 09:55:07.906: INFO: Pod "downwardapi-volume-5e689868-93c0-4fdd-a4fc-eeb7386f6750": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006833697s
STEP: Saw pod success
Jul  5 09:55:07.906: INFO: Pod "downwardapi-volume-5e689868-93c0-4fdd-a4fc-eeb7386f6750" satisfied condition "success or failure"
Jul  5 09:55:07.908: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod downwardapi-volume-5e689868-93c0-4fdd-a4fc-eeb7386f6750 container client-container: <nil>
STEP: delete the pod
Jul  5 09:55:07.925: INFO: Waiting for pod downwardapi-volume-5e689868-93c0-4fdd-a4fc-eeb7386f6750 to disappear
Jul  5 09:55:07.928: INFO: Pod downwardapi-volume-5e689868-93c0-4fdd-a4fc-eeb7386f6750 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:55:07.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6194" for this suite.
Jul  5 09:55:13.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:55:14.029: INFO: namespace downward-api-6194 deletion completed in 6.097680712s

• [SLOW TEST:8.164 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:55:14.029: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul  5 09:55:14.057: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:55:18.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1826" for this suite.
Jul  5 09:55:40.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:55:40.371: INFO: namespace init-container-1826 deletion completed in 22.112921537s

• [SLOW TEST:26.342 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:55:40.371: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-5467fda9-ac44-454b-b68c-bd8df4de97c9
STEP: Creating a pod to test consume configMaps
Jul  5 09:55:40.411: INFO: Waiting up to 5m0s for pod "pod-configmaps-29044b90-23be-4f0b-a9d0-f929f93a5c8b" in namespace "configmap-3699" to be "success or failure"
Jul  5 09:55:40.414: INFO: Pod "pod-configmaps-29044b90-23be-4f0b-a9d0-f929f93a5c8b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.523925ms
Jul  5 09:55:42.417: INFO: Pod "pod-configmaps-29044b90-23be-4f0b-a9d0-f929f93a5c8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006451851s
Jul  5 09:55:44.420: INFO: Pod "pod-configmaps-29044b90-23be-4f0b-a9d0-f929f93a5c8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009284069s
STEP: Saw pod success
Jul  5 09:55:44.420: INFO: Pod "pod-configmaps-29044b90-23be-4f0b-a9d0-f929f93a5c8b" satisfied condition "success or failure"
Jul  5 09:55:44.422: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-configmaps-29044b90-23be-4f0b-a9d0-f929f93a5c8b container configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 09:55:44.442: INFO: Waiting for pod pod-configmaps-29044b90-23be-4f0b-a9d0-f929f93a5c8b to disappear
Jul  5 09:55:44.445: INFO: Pod pod-configmaps-29044b90-23be-4f0b-a9d0-f929f93a5c8b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:55:44.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3699" for this suite.
Jul  5 09:55:50.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:55:50.548: INFO: namespace configmap-3699 deletion completed in 6.099320633s

• [SLOW TEST:10.177 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:55:50.548: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  5 09:55:50.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4080'
Jul  5 09:55:50.941: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  5 09:55:50.941: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Jul  5 09:55:52.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4080'
Jul  5 09:55:53.035: INFO: stderr: ""
Jul  5 09:55:53.035: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:55:53.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4080" for this suite.
Jul  5 09:56:15.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:56:15.156: INFO: namespace kubectl-4080 deletion completed in 22.115287841s

• [SLOW TEST:24.608 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:56:15.157: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul  5 09:56:17.711: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9ab1d1b9-e53e-4526-8402-0a5f391c877e"
Jul  5 09:56:17.711: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9ab1d1b9-e53e-4526-8402-0a5f391c877e" in namespace "pods-5255" to be "terminated due to deadline exceeded"
Jul  5 09:56:17.713: INFO: Pod "pod-update-activedeadlineseconds-9ab1d1b9-e53e-4526-8402-0a5f391c877e": Phase="Running", Reason="", readiness=true. Elapsed: 2.370797ms
Jul  5 09:56:19.717: INFO: Pod "pod-update-activedeadlineseconds-9ab1d1b9-e53e-4526-8402-0a5f391c877e": Phase="Running", Reason="", readiness=true. Elapsed: 2.00665723s
Jul  5 09:56:21.721: INFO: Pod "pod-update-activedeadlineseconds-9ab1d1b9-e53e-4526-8402-0a5f391c877e": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.009982876s
Jul  5 09:56:21.721: INFO: Pod "pod-update-activedeadlineseconds-9ab1d1b9-e53e-4526-8402-0a5f391c877e" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:56:21.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5255" for this suite.
Jul  5 09:56:27.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:56:27.817: INFO: namespace pods-5255 deletion completed in 6.092439883s

• [SLOW TEST:12.660 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:56:27.817: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-6607
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6607 to expose endpoints map[]
Jul  5 09:56:27.865: INFO: successfully validated that service endpoint-test2 in namespace services-6607 exposes endpoints map[] (6.140623ms elapsed)
STEP: Creating pod pod1 in namespace services-6607
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6607 to expose endpoints map[pod1:[80]]
Jul  5 09:56:30.902: INFO: successfully validated that service endpoint-test2 in namespace services-6607 exposes endpoints map[pod1:[80]] (3.028103783s elapsed)
STEP: Creating pod pod2 in namespace services-6607
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6607 to expose endpoints map[pod1:[80] pod2:[80]]
Jul  5 09:56:32.934: INFO: successfully validated that service endpoint-test2 in namespace services-6607 exposes endpoints map[pod1:[80] pod2:[80]] (2.025969756s elapsed)
STEP: Deleting pod pod1 in namespace services-6607
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6607 to expose endpoints map[pod2:[80]]
Jul  5 09:56:33.951: INFO: successfully validated that service endpoint-test2 in namespace services-6607 exposes endpoints map[pod2:[80]] (1.013097133s elapsed)
STEP: Deleting pod pod2 in namespace services-6607
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6607 to expose endpoints map[]
Jul  5 09:56:33.961: INFO: successfully validated that service endpoint-test2 in namespace services-6607 exposes endpoints map[] (2.080868ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:56:33.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6607" for this suite.
Jul  5 09:56:40.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:56:40.091: INFO: namespace services-6607 deletion completed in 6.095108695s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:12.274 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:56:40.092: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul  5 09:56:40.368: INFO: Pod name wrapped-volume-race-38605d81-2a07-4257-a3bd-d623d2ce3e1e: Found 3 pods out of 5
Jul  5 09:56:45.372: INFO: Pod name wrapped-volume-race-38605d81-2a07-4257-a3bd-d623d2ce3e1e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-38605d81-2a07-4257-a3bd-d623d2ce3e1e in namespace emptydir-wrapper-9932, will wait for the garbage collector to delete the pods
Jul  5 09:56:57.454: INFO: Deleting ReplicationController wrapped-volume-race-38605d81-2a07-4257-a3bd-d623d2ce3e1e took: 12.871142ms
Jul  5 09:56:57.854: INFO: Terminating ReplicationController wrapped-volume-race-38605d81-2a07-4257-a3bd-d623d2ce3e1e pods took: 400.215747ms
STEP: Creating RC which spawns configmap-volume pods
Jul  5 09:57:40.069: INFO: Pod name wrapped-volume-race-e12860f0-ac1b-4a4b-abfb-52a01da99063: Found 0 pods out of 5
Jul  5 09:57:45.074: INFO: Pod name wrapped-volume-race-e12860f0-ac1b-4a4b-abfb-52a01da99063: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e12860f0-ac1b-4a4b-abfb-52a01da99063 in namespace emptydir-wrapper-9932, will wait for the garbage collector to delete the pods
Jul  5 09:57:57.152: INFO: Deleting ReplicationController wrapped-volume-race-e12860f0-ac1b-4a4b-abfb-52a01da99063 took: 8.235168ms
Jul  5 09:57:57.552: INFO: Terminating ReplicationController wrapped-volume-race-e12860f0-ac1b-4a4b-abfb-52a01da99063 pods took: 400.18685ms
STEP: Creating RC which spawns configmap-volume pods
Jul  5 09:58:40.184: INFO: Pod name wrapped-volume-race-ce410d2b-796a-44fc-ade3-0bd001702b46: Found 0 pods out of 5
Jul  5 09:58:45.189: INFO: Pod name wrapped-volume-race-ce410d2b-796a-44fc-ade3-0bd001702b46: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ce410d2b-796a-44fc-ade3-0bd001702b46 in namespace emptydir-wrapper-9932, will wait for the garbage collector to delete the pods
Jul  5 09:58:57.267: INFO: Deleting ReplicationController wrapped-volume-race-ce410d2b-796a-44fc-ade3-0bd001702b46 took: 7.786268ms
Jul  5 09:58:57.667: INFO: Terminating ReplicationController wrapped-volume-race-ce410d2b-796a-44fc-ade3-0bd001702b46 pods took: 400.196413ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 09:59:41.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9932" for this suite.
Jul  5 09:59:49.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 09:59:49.475: INFO: namespace emptydir-wrapper-9932 deletion completed in 8.096242737s

• [SLOW TEST:189.384 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 09:59:49.476: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5394
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  5 09:59:49.502: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  5 10:00:13.591: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.101:8080/dial?request=hostName&protocol=http&host=10.244.4.107&port=8080&tries=1'] Namespace:pod-network-test-5394 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:00:13.591: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:00:13.740: INFO: Waiting for endpoints: map[]
Jul  5 10:00:13.743: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.101:8080/dial?request=hostName&protocol=http&host=10.244.3.100&port=8080&tries=1'] Namespace:pod-network-test-5394 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:00:13.743: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:00:13.914: INFO: Waiting for endpoints: map[]
Jul  5 10:00:13.917: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.101:8080/dial?request=hostName&protocol=http&host=10.244.5.32&port=8080&tries=1'] Namespace:pod-network-test-5394 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:00:13.917: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:00:14.105: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:00:14.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5394" for this suite.
Jul  5 10:00:36.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:00:36.207: INFO: namespace pod-network-test-5394 deletion completed in 22.097762065s

• [SLOW TEST:46.732 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:00:36.208: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 10:00:36.245: INFO: (0) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.098974ms)
Jul  5 10:00:36.248: INFO: (1) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.98239ms)
Jul  5 10:00:36.251: INFO: (2) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.669511ms)
Jul  5 10:00:36.254: INFO: (3) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.892569ms)
Jul  5 10:00:36.257: INFO: (4) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 3.050854ms)
Jul  5 10:00:36.259: INFO: (5) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.699921ms)
Jul  5 10:00:36.262: INFO: (6) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.834436ms)
Jul  5 10:00:36.265: INFO: (7) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.696724ms)
Jul  5 10:00:36.268: INFO: (8) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.863769ms)
Jul  5 10:00:36.271: INFO: (9) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.829939ms)
Jul  5 10:00:36.273: INFO: (10) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.70022ms)
Jul  5 10:00:36.276: INFO: (11) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.655911ms)
Jul  5 10:00:36.279: INFO: (12) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.919566ms)
Jul  5 10:00:36.282: INFO: (13) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.776869ms)
Jul  5 10:00:36.285: INFO: (14) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.767001ms)
Jul  5 10:00:36.288: INFO: (15) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 3.041188ms)
Jul  5 10:00:36.290: INFO: (16) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.609278ms)
Jul  5 10:00:36.293: INFO: (17) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.971939ms)
Jul  5 10:00:36.296: INFO: (18) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.64392ms)
Jul  5 10:00:36.299: INFO: (19) /api/v1/nodes/ip-172-31-1-212.eu-west-3.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.656984ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:00:36.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2853" for this suite.
Jul  5 10:00:42.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:00:42.403: INFO: namespace proxy-2853 deletion completed in 6.100795215s

• [SLOW TEST:6.195 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:00:42.403: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-8228
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8228 to expose endpoints map[]
Jul  5 10:00:42.449: INFO: successfully validated that service multi-endpoint-test in namespace services-8228 exposes endpoints map[] (5.752924ms elapsed)
STEP: Creating pod pod1 in namespace services-8228
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8228 to expose endpoints map[pod1:[100]]
Jul  5 10:00:44.485: INFO: successfully validated that service multi-endpoint-test in namespace services-8228 exposes endpoints map[pod1:[100]] (2.026681408s elapsed)
STEP: Creating pod pod2 in namespace services-8228
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8228 to expose endpoints map[pod1:[100] pod2:[101]]
Jul  5 10:00:46.525: INFO: successfully validated that service multi-endpoint-test in namespace services-8228 exposes endpoints map[pod1:[100] pod2:[101]] (2.033955204s elapsed)
STEP: Deleting pod pod1 in namespace services-8228
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8228 to expose endpoints map[pod2:[101]]
Jul  5 10:00:46.555: INFO: successfully validated that service multi-endpoint-test in namespace services-8228 exposes endpoints map[pod2:[101]] (12.537828ms elapsed)
STEP: Deleting pod pod2 in namespace services-8228
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8228 to expose endpoints map[]
Jul  5 10:00:47.574: INFO: successfully validated that service multi-endpoint-test in namespace services-8228 exposes endpoints map[] (1.008083777s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:00:47.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8228" for this suite.
Jul  5 10:01:09.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:01:09.716: INFO: namespace services-8228 deletion completed in 22.108988376s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:27.313 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:01:09.717: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 10:01:09.803: INFO: Creating deployment "test-recreate-deployment"
Jul  5 10:01:09.812: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul  5 10:01:09.821: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul  5 10:01:11.829: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul  5 10:01:11.832: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul  5 10:01:11.840: INFO: Updating deployment test-recreate-deployment
Jul  5 10:01:11.840: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul  5 10:01:11.920: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-6522,SelfLink:/apis/apps/v1/namespaces/deployment-6522/deployments/test-recreate-deployment,UID:37eb51ee-d075-4d81-b476-b2a7b32d4c1f,ResourceVersion:19266,Generation:2,CreationTimestamp:2019-07-05 10:01:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-07-05 10:01:11 +0000 UTC 2019-07-05 10:01:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-05 10:01:11 +0000 UTC 2019-07-05 10:01:09 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jul  5 10:01:11.924: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-6522,SelfLink:/apis/apps/v1/namespaces/deployment-6522/replicasets/test-recreate-deployment-5c8c9cc69d,UID:cc65270a-68ed-40ad-9cad-4b62d3a48025,ResourceVersion:19263,Generation:1,CreationTimestamp:2019-07-05 10:01:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 37eb51ee-d075-4d81-b476-b2a7b32d4c1f 0xc002185ae7 0xc002185ae8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  5 10:01:11.924: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul  5 10:01:11.924: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-6522,SelfLink:/apis/apps/v1/namespaces/deployment-6522/replicasets/test-recreate-deployment-6df85df6b9,UID:8a8ab706-456e-4554-acfd-04441f0e1e6c,ResourceVersion:19252,Generation:2,CreationTimestamp:2019-07-05 10:01:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 37eb51ee-d075-4d81-b476-b2a7b32d4c1f 0xc002185bb7 0xc002185bb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  5 10:01:11.927: INFO: Pod "test-recreate-deployment-5c8c9cc69d-62qgs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-62qgs,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-6522,SelfLink:/api/v1/namespaces/deployment-6522/pods/test-recreate-deployment-5c8c9cc69d-62qgs,UID:fc7639d9-ea97-4221-85e0-197a532d674a,ResourceVersion:19265,Generation:0,CreationTimestamp:2019-07-05 10:01:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d cc65270a-68ed-40ad-9cad-4b62d3a48025 0xc002b564c7 0xc002b564c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hs4s7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hs4s7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hs4s7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-212.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b56530} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b56550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:01:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:01:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:01:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:01:11 +0000 UTC  }],Message:,Reason:,HostIP:172.31.1.212,PodIP:,StartTime:2019-07-05 10:01:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:01:11.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6522" for this suite.
Jul  5 10:01:17.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:01:18.038: INFO: namespace deployment-6522 deletion completed in 6.104926052s

• [SLOW TEST:8.322 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:01:18.038: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-e1736b2e-59ba-46b1-9bb7-7c4daeda94f9
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:01:18.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6502" for this suite.
Jul  5 10:01:24.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:01:24.247: INFO: namespace secrets-6502 deletion completed in 6.172906338s

• [SLOW TEST:6.209 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:01:24.248: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-11fd5e19-b02a-471d-ab44-1f03761acad6
STEP: Creating a pod to test consume secrets
Jul  5 10:01:24.298: INFO: Waiting up to 5m0s for pod "pod-secrets-ecfccf69-083b-4d76-812d-5dfb5273e825" in namespace "secrets-7477" to be "success or failure"
Jul  5 10:01:24.302: INFO: Pod "pod-secrets-ecfccf69-083b-4d76-812d-5dfb5273e825": Phase="Pending", Reason="", readiness=false. Elapsed: 3.957216ms
Jul  5 10:01:26.305: INFO: Pod "pod-secrets-ecfccf69-083b-4d76-812d-5dfb5273e825": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007177446s
STEP: Saw pod success
Jul  5 10:01:26.305: INFO: Pod "pod-secrets-ecfccf69-083b-4d76-812d-5dfb5273e825" satisfied condition "success or failure"
Jul  5 10:01:26.308: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-secrets-ecfccf69-083b-4d76-812d-5dfb5273e825 container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 10:01:26.330: INFO: Waiting for pod pod-secrets-ecfccf69-083b-4d76-812d-5dfb5273e825 to disappear
Jul  5 10:01:26.333: INFO: Pod pod-secrets-ecfccf69-083b-4d76-812d-5dfb5273e825 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:01:26.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7477" for this suite.
Jul  5 10:01:32.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:01:32.445: INFO: namespace secrets-7477 deletion completed in 6.108088524s

• [SLOW TEST:8.197 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:01:32.446: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jul  5 10:02:12.501: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0705 10:02:12.501670      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:02:12.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7684" for this suite.
Jul  5 10:02:18.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:02:18.620: INFO: namespace gc-7684 deletion completed in 6.115486016s

• [SLOW TEST:46.175 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:02:18.620: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Jul  5 10:02:18.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 create -f - --namespace=kubectl-8822'
Jul  5 10:02:19.063: INFO: stderr: ""
Jul  5 10:02:19.063: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Jul  5 10:02:20.066: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 10:02:20.066: INFO: Found 0 / 1
Jul  5 10:02:21.067: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 10:02:21.067: INFO: Found 0 / 1
Jul  5 10:02:22.067: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 10:02:22.067: INFO: Found 1 / 1
Jul  5 10:02:22.067: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  5 10:02:22.069: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 10:02:22.069: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jul  5 10:02:22.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 logs redis-master-kt95t redis-master --namespace=kubectl-8822'
Jul  5 10:02:22.161: INFO: stderr: ""
Jul  5 10:02:22.161: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Jul 10:02:20.458 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Jul 10:02:20.458 # Server started, Redis version 3.2.12\n1:M 05 Jul 10:02:20.458 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Jul 10:02:20.458 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jul  5 10:02:22.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 log redis-master-kt95t redis-master --namespace=kubectl-8822 --tail=1'
Jul  5 10:02:22.253: INFO: stderr: ""
Jul  5 10:02:22.253: INFO: stdout: "1:M 05 Jul 10:02:20.458 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jul  5 10:02:22.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 log redis-master-kt95t redis-master --namespace=kubectl-8822 --limit-bytes=1'
Jul  5 10:02:22.339: INFO: stderr: ""
Jul  5 10:02:22.339: INFO: stdout: " "
STEP: exposing timestamps
Jul  5 10:02:22.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 log redis-master-kt95t redis-master --namespace=kubectl-8822 --tail=1 --timestamps'
Jul  5 10:02:22.422: INFO: stderr: ""
Jul  5 10:02:22.422: INFO: stdout: "2019-07-05T10:02:20.45882006Z 1:M 05 Jul 10:02:20.458 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jul  5 10:02:24.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 log redis-master-kt95t redis-master --namespace=kubectl-8822 --since=1s'
Jul  5 10:02:24.999: INFO: stderr: ""
Jul  5 10:02:24.999: INFO: stdout: ""
Jul  5 10:02:24.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 log redis-master-kt95t redis-master --namespace=kubectl-8822 --since=24h'
Jul  5 10:02:25.095: INFO: stderr: ""
Jul  5 10:02:25.095: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Jul 10:02:20.458 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Jul 10:02:20.458 # Server started, Redis version 3.2.12\n1:M 05 Jul 10:02:20.458 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Jul 10:02:20.458 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Jul  5 10:02:25.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete --grace-period=0 --force -f - --namespace=kubectl-8822'
Jul  5 10:02:25.176: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 10:02:25.176: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jul  5 10:02:25.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get rc,svc -l name=nginx --no-headers --namespace=kubectl-8822'
Jul  5 10:02:25.250: INFO: stderr: "No resources found.\n"
Jul  5 10:02:25.250: INFO: stdout: ""
Jul  5 10:02:25.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -l name=nginx --namespace=kubectl-8822 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  5 10:02:25.317: INFO: stderr: ""
Jul  5 10:02:25.317: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:02:25.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8822" for this suite.
Jul  5 10:02:31.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:02:31.422: INFO: namespace kubectl-8822 deletion completed in 6.101565513s

• [SLOW TEST:12.801 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:02:31.422: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 10:02:31.454: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d793bd3-85a5-4092-b6bd-eb2dcb0cdec7" in namespace "downward-api-5826" to be "success or failure"
Jul  5 10:02:31.457: INFO: Pod "downwardapi-volume-4d793bd3-85a5-4092-b6bd-eb2dcb0cdec7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.992639ms
Jul  5 10:02:33.460: INFO: Pod "downwardapi-volume-4d793bd3-85a5-4092-b6bd-eb2dcb0cdec7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005863287s
STEP: Saw pod success
Jul  5 10:02:33.460: INFO: Pod "downwardapi-volume-4d793bd3-85a5-4092-b6bd-eb2dcb0cdec7" satisfied condition "success or failure"
Jul  5 10:02:33.462: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod downwardapi-volume-4d793bd3-85a5-4092-b6bd-eb2dcb0cdec7 container client-container: <nil>
STEP: delete the pod
Jul  5 10:02:33.482: INFO: Waiting for pod downwardapi-volume-4d793bd3-85a5-4092-b6bd-eb2dcb0cdec7 to disappear
Jul  5 10:02:33.484: INFO: Pod downwardapi-volume-4d793bd3-85a5-4092-b6bd-eb2dcb0cdec7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:02:33.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5826" for this suite.
Jul  5 10:02:39.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:02:39.581: INFO: namespace downward-api-5826 deletion completed in 6.094193316s

• [SLOW TEST:8.160 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:02:39.582: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-3fef52c1-5c39-4fb6-a267-be0f0b352a2b
STEP: Creating secret with name s-test-opt-upd-12b6eb48-2d1c-483a-a1e2-3ef8259084f6
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3fef52c1-5c39-4fb6-a267-be0f0b352a2b
STEP: Updating secret s-test-opt-upd-12b6eb48-2d1c-483a-a1e2-3ef8259084f6
STEP: Creating secret with name s-test-opt-create-b39b04c5-a90b-4cf5-a914-03a4e247bc36
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:02:43.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1548" for this suite.
Jul  5 10:03:05.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:03:05.800: INFO: namespace projected-1548 deletion completed in 22.104241601s

• [SLOW TEST:26.218 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:03:05.800: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Jul  5 10:03:05.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 --namespace=kubectl-8917 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul  5 10:03:08.494: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul  5 10:03:08.494: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:03:10.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8917" for this suite.
Jul  5 10:03:20.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:03:20.618: INFO: namespace kubectl-8917 deletion completed in 10.11216964s

• [SLOW TEST:14.818 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:03:20.618: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul  5 10:03:20.650: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:03:30.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9255" for this suite.
Jul  5 10:03:36.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:03:36.125: INFO: namespace pods-9255 deletion completed in 6.092227067s

• [SLOW TEST:15.506 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:03:36.126: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Jul  5 10:03:40.171: INFO: Pod pod-hostip-c8c0ed2b-682d-47c2-ad6b-793b75a39e67 has hostIP: 172.31.11.115
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:03:40.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7075" for this suite.
Jul  5 10:04:02.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:04:02.277: INFO: namespace pods-7075 deletion completed in 22.103054966s

• [SLOW TEST:26.152 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:04:02.278: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul  5 10:04:02.305: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:04:05.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1038" for this suite.
Jul  5 10:04:11.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:04:11.659: INFO: namespace init-container-1038 deletion completed in 6.096561319s

• [SLOW TEST:9.381 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:04:11.660: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-rhr9
STEP: Creating a pod to test atomic-volume-subpath
Jul  5 10:04:11.699: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rhr9" in namespace "subpath-4160" to be "success or failure"
Jul  5 10:04:11.703: INFO: Pod "pod-subpath-test-projected-rhr9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.84651ms
Jul  5 10:04:13.706: INFO: Pod "pod-subpath-test-projected-rhr9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006761721s
Jul  5 10:04:15.709: INFO: Pod "pod-subpath-test-projected-rhr9": Phase="Running", Reason="", readiness=true. Elapsed: 4.009966707s
Jul  5 10:04:17.712: INFO: Pod "pod-subpath-test-projected-rhr9": Phase="Running", Reason="", readiness=true. Elapsed: 6.013127368s
Jul  5 10:04:19.716: INFO: Pod "pod-subpath-test-projected-rhr9": Phase="Running", Reason="", readiness=true. Elapsed: 8.016500339s
Jul  5 10:04:21.719: INFO: Pod "pod-subpath-test-projected-rhr9": Phase="Running", Reason="", readiness=true. Elapsed: 10.020047007s
Jul  5 10:04:23.722: INFO: Pod "pod-subpath-test-projected-rhr9": Phase="Running", Reason="", readiness=true. Elapsed: 12.022880245s
Jul  5 10:04:25.726: INFO: Pod "pod-subpath-test-projected-rhr9": Phase="Running", Reason="", readiness=true. Elapsed: 14.026291567s
Jul  5 10:04:27.729: INFO: Pod "pod-subpath-test-projected-rhr9": Phase="Running", Reason="", readiness=true. Elapsed: 16.02990253s
Jul  5 10:04:29.733: INFO: Pod "pod-subpath-test-projected-rhr9": Phase="Running", Reason="", readiness=true. Elapsed: 18.033305656s
Jul  5 10:04:31.737: INFO: Pod "pod-subpath-test-projected-rhr9": Phase="Running", Reason="", readiness=true. Elapsed: 20.037451524s
Jul  5 10:04:33.740: INFO: Pod "pod-subpath-test-projected-rhr9": Phase="Running", Reason="", readiness=true. Elapsed: 22.040645566s
Jul  5 10:04:35.743: INFO: Pod "pod-subpath-test-projected-rhr9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.044262314s
STEP: Saw pod success
Jul  5 10:04:35.744: INFO: Pod "pod-subpath-test-projected-rhr9" satisfied condition "success or failure"
Jul  5 10:04:35.746: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-subpath-test-projected-rhr9 container test-container-subpath-projected-rhr9: <nil>
STEP: delete the pod
Jul  5 10:04:35.767: INFO: Waiting for pod pod-subpath-test-projected-rhr9 to disappear
Jul  5 10:04:35.769: INFO: Pod pod-subpath-test-projected-rhr9 no longer exists
STEP: Deleting pod pod-subpath-test-projected-rhr9
Jul  5 10:04:35.769: INFO: Deleting pod "pod-subpath-test-projected-rhr9" in namespace "subpath-4160"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:04:35.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4160" for this suite.
Jul  5 10:04:41.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:04:41.874: INFO: namespace subpath-4160 deletion completed in 6.098287523s

• [SLOW TEST:30.214 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:04:41.875: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 10:04:41.907: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c5c3391-01a2-406d-bfe7-fe1ccb79ecc1" in namespace "projected-2791" to be "success or failure"
Jul  5 10:04:41.913: INFO: Pod "downwardapi-volume-1c5c3391-01a2-406d-bfe7-fe1ccb79ecc1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.395753ms
Jul  5 10:04:43.916: INFO: Pod "downwardapi-volume-1c5c3391-01a2-406d-bfe7-fe1ccb79ecc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009203668s
STEP: Saw pod success
Jul  5 10:04:43.916: INFO: Pod "downwardapi-volume-1c5c3391-01a2-406d-bfe7-fe1ccb79ecc1" satisfied condition "success or failure"
Jul  5 10:04:43.918: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod downwardapi-volume-1c5c3391-01a2-406d-bfe7-fe1ccb79ecc1 container client-container: <nil>
STEP: delete the pod
Jul  5 10:04:43.935: INFO: Waiting for pod downwardapi-volume-1c5c3391-01a2-406d-bfe7-fe1ccb79ecc1 to disappear
Jul  5 10:04:43.938: INFO: Pod downwardapi-volume-1c5c3391-01a2-406d-bfe7-fe1ccb79ecc1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:04:43.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2791" for this suite.
Jul  5 10:04:49.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:04:50.043: INFO: namespace projected-2791 deletion completed in 6.101655959s

• [SLOW TEST:8.168 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:04:50.043: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-081a84ab-0949-4c7b-ba8d-60c6dedbbdf1
STEP: Creating a pod to test consume secrets
Jul  5 10:04:50.079: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-15b64637-efe1-47a7-8f76-6da205f9332e" in namespace "projected-5840" to be "success or failure"
Jul  5 10:04:50.083: INFO: Pod "pod-projected-secrets-15b64637-efe1-47a7-8f76-6da205f9332e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.89466ms
Jul  5 10:04:52.086: INFO: Pod "pod-projected-secrets-15b64637-efe1-47a7-8f76-6da205f9332e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007217251s
Jul  5 10:04:54.090: INFO: Pod "pod-projected-secrets-15b64637-efe1-47a7-8f76-6da205f9332e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010847703s
STEP: Saw pod success
Jul  5 10:04:54.090: INFO: Pod "pod-projected-secrets-15b64637-efe1-47a7-8f76-6da205f9332e" satisfied condition "success or failure"
Jul  5 10:04:54.093: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-projected-secrets-15b64637-efe1-47a7-8f76-6da205f9332e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  5 10:04:54.113: INFO: Waiting for pod pod-projected-secrets-15b64637-efe1-47a7-8f76-6da205f9332e to disappear
Jul  5 10:04:54.117: INFO: Pod pod-projected-secrets-15b64637-efe1-47a7-8f76-6da205f9332e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:04:54.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5840" for this suite.
Jul  5 10:05:00.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:05:00.230: INFO: namespace projected-5840 deletion completed in 6.10882453s

• [SLOW TEST:10.187 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:05:00.231: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-e249400f-3183-4bd2-b5d2-1f68bc890390
STEP: Creating a pod to test consume secrets
Jul  5 10:05:00.268: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-02bd9b08-fc85-445c-90e2-c1c8e94f7758" in namespace "projected-5564" to be "success or failure"
Jul  5 10:05:00.271: INFO: Pod "pod-projected-secrets-02bd9b08-fc85-445c-90e2-c1c8e94f7758": Phase="Pending", Reason="", readiness=false. Elapsed: 2.723168ms
Jul  5 10:05:02.275: INFO: Pod "pod-projected-secrets-02bd9b08-fc85-445c-90e2-c1c8e94f7758": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006708035s
STEP: Saw pod success
Jul  5 10:05:02.275: INFO: Pod "pod-projected-secrets-02bd9b08-fc85-445c-90e2-c1c8e94f7758" satisfied condition "success or failure"
Jul  5 10:05:02.277: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-projected-secrets-02bd9b08-fc85-445c-90e2-c1c8e94f7758 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  5 10:05:02.312: INFO: Waiting for pod pod-projected-secrets-02bd9b08-fc85-445c-90e2-c1c8e94f7758 to disappear
Jul  5 10:05:02.315: INFO: Pod pod-projected-secrets-02bd9b08-fc85-445c-90e2-c1c8e94f7758 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:05:02.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5564" for this suite.
Jul  5 10:05:08.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:05:08.439: INFO: namespace projected-5564 deletion completed in 6.119243602s

• [SLOW TEST:8.209 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:05:08.439: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-9e303bdb-2756-4925-9ff6-9fc293c177de
STEP: Creating a pod to test consume secrets
Jul  5 10:05:08.500: INFO: Waiting up to 5m0s for pod "pod-secrets-9831460b-af92-4e6f-9406-dd450c33d609" in namespace "secrets-4230" to be "success or failure"
Jul  5 10:05:08.506: INFO: Pod "pod-secrets-9831460b-af92-4e6f-9406-dd450c33d609": Phase="Pending", Reason="", readiness=false. Elapsed: 5.670642ms
Jul  5 10:05:10.510: INFO: Pod "pod-secrets-9831460b-af92-4e6f-9406-dd450c33d609": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009273244s
STEP: Saw pod success
Jul  5 10:05:10.510: INFO: Pod "pod-secrets-9831460b-af92-4e6f-9406-dd450c33d609" satisfied condition "success or failure"
Jul  5 10:05:10.512: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-secrets-9831460b-af92-4e6f-9406-dd450c33d609 container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 10:05:10.537: INFO: Waiting for pod pod-secrets-9831460b-af92-4e6f-9406-dd450c33d609 to disappear
Jul  5 10:05:10.541: INFO: Pod pod-secrets-9831460b-af92-4e6f-9406-dd450c33d609 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:05:10.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4230" for this suite.
Jul  5 10:05:16.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:05:16.651: INFO: namespace secrets-4230 deletion completed in 6.107048589s
STEP: Destroying namespace "secret-namespace-7589" for this suite.
Jul  5 10:05:22.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:05:22.751: INFO: namespace secret-namespace-7589 deletion completed in 6.099798975s

• [SLOW TEST:14.311 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:05:22.751: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-25d6b90d-8306-44a9-97f5-6a07d92802ca
STEP: Creating a pod to test consume configMaps
Jul  5 10:05:22.787: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b4692cca-42e7-4366-a0cc-15b01d0ce4ca" in namespace "projected-5639" to be "success or failure"
Jul  5 10:05:22.790: INFO: Pod "pod-projected-configmaps-b4692cca-42e7-4366-a0cc-15b01d0ce4ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.969598ms
Jul  5 10:05:24.793: INFO: Pod "pod-projected-configmaps-b4692cca-42e7-4366-a0cc-15b01d0ce4ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005999797s
Jul  5 10:05:26.796: INFO: Pod "pod-projected-configmaps-b4692cca-42e7-4366-a0cc-15b01d0ce4ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008893492s
STEP: Saw pod success
Jul  5 10:05:26.796: INFO: Pod "pod-projected-configmaps-b4692cca-42e7-4366-a0cc-15b01d0ce4ca" satisfied condition "success or failure"
Jul  5 10:05:26.799: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-projected-configmaps-b4692cca-42e7-4366-a0cc-15b01d0ce4ca container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 10:05:26.818: INFO: Waiting for pod pod-projected-configmaps-b4692cca-42e7-4366-a0cc-15b01d0ce4ca to disappear
Jul  5 10:05:26.821: INFO: Pod pod-projected-configmaps-b4692cca-42e7-4366-a0cc-15b01d0ce4ca no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:05:26.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5639" for this suite.
Jul  5 10:05:32.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:05:32.916: INFO: namespace projected-5639 deletion completed in 6.092255771s

• [SLOW TEST:10.165 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:05:32.917: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jul  5 10:05:42.967: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0705 10:05:42.967491      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:05:42.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8772" for this suite.
Jul  5 10:05:48.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:05:49.066: INFO: namespace gc-8772 deletion completed in 6.095850771s

• [SLOW TEST:16.149 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:05:49.067: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-f31cac75-7f17-4c84-9363-df7daca8d969
STEP: Creating secret with name secret-projected-all-test-volume-d1c90270-f809-458c-add4-e3fcdd3dde8e
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul  5 10:05:49.115: INFO: Waiting up to 5m0s for pod "projected-volume-d90ac8a4-e613-4dd4-9bb2-f9019d20c4d0" in namespace "projected-265" to be "success or failure"
Jul  5 10:05:49.119: INFO: Pod "projected-volume-d90ac8a4-e613-4dd4-9bb2-f9019d20c4d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.134318ms
Jul  5 10:05:51.122: INFO: Pod "projected-volume-d90ac8a4-e613-4dd4-9bb2-f9019d20c4d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006978637s
STEP: Saw pod success
Jul  5 10:05:51.122: INFO: Pod "projected-volume-d90ac8a4-e613-4dd4-9bb2-f9019d20c4d0" satisfied condition "success or failure"
Jul  5 10:05:51.126: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod projected-volume-d90ac8a4-e613-4dd4-9bb2-f9019d20c4d0 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul  5 10:05:51.145: INFO: Waiting for pod projected-volume-d90ac8a4-e613-4dd4-9bb2-f9019d20c4d0 to disappear
Jul  5 10:05:51.148: INFO: Pod projected-volume-d90ac8a4-e613-4dd4-9bb2-f9019d20c4d0 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:05:51.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-265" for this suite.
Jul  5 10:05:57.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:05:57.247: INFO: namespace projected-265 deletion completed in 6.095282983s

• [SLOW TEST:8.180 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:05:57.247: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul  5 10:05:57.290: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6889,SelfLink:/api/v1/namespaces/watch-6889/configmaps/e2e-watch-test-label-changed,UID:8e980efe-855f-4ee2-bae4-59d886b6d02c,ResourceVersion:20718,Generation:0,CreationTimestamp:2019-07-05 10:05:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  5 10:05:57.290: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6889,SelfLink:/api/v1/namespaces/watch-6889/configmaps/e2e-watch-test-label-changed,UID:8e980efe-855f-4ee2-bae4-59d886b6d02c,ResourceVersion:20719,Generation:0,CreationTimestamp:2019-07-05 10:05:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul  5 10:05:57.290: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6889,SelfLink:/api/v1/namespaces/watch-6889/configmaps/e2e-watch-test-label-changed,UID:8e980efe-855f-4ee2-bae4-59d886b6d02c,ResourceVersion:20720,Generation:0,CreationTimestamp:2019-07-05 10:05:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul  5 10:06:07.314: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6889,SelfLink:/api/v1/namespaces/watch-6889/configmaps/e2e-watch-test-label-changed,UID:8e980efe-855f-4ee2-bae4-59d886b6d02c,ResourceVersion:20746,Generation:0,CreationTimestamp:2019-07-05 10:05:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  5 10:06:07.314: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6889,SelfLink:/api/v1/namespaces/watch-6889/configmaps/e2e-watch-test-label-changed,UID:8e980efe-855f-4ee2-bae4-59d886b6d02c,ResourceVersion:20747,Generation:0,CreationTimestamp:2019-07-05 10:05:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul  5 10:06:07.314: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6889,SelfLink:/api/v1/namespaces/watch-6889/configmaps/e2e-watch-test-label-changed,UID:8e980efe-855f-4ee2-bae4-59d886b6d02c,ResourceVersion:20748,Generation:0,CreationTimestamp:2019-07-05 10:05:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:06:07.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6889" for this suite.
Jul  5 10:06:13.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:06:13.412: INFO: namespace watch-6889 deletion completed in 6.095395911s

• [SLOW TEST:16.165 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:06:13.413: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul  5 10:06:13.455: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8998,SelfLink:/api/v1/namespaces/watch-8998/configmaps/e2e-watch-test-watch-closed,UID:fa46a8a0-0224-47e6-a1ae-592e9ca04ba9,ResourceVersion:20768,Generation:0,CreationTimestamp:2019-07-05 10:06:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  5 10:06:13.455: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8998,SelfLink:/api/v1/namespaces/watch-8998/configmaps/e2e-watch-test-watch-closed,UID:fa46a8a0-0224-47e6-a1ae-592e9ca04ba9,ResourceVersion:20769,Generation:0,CreationTimestamp:2019-07-05 10:06:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul  5 10:06:13.468: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8998,SelfLink:/api/v1/namespaces/watch-8998/configmaps/e2e-watch-test-watch-closed,UID:fa46a8a0-0224-47e6-a1ae-592e9ca04ba9,ResourceVersion:20770,Generation:0,CreationTimestamp:2019-07-05 10:06:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  5 10:06:13.468: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8998,SelfLink:/api/v1/namespaces/watch-8998/configmaps/e2e-watch-test-watch-closed,UID:fa46a8a0-0224-47e6-a1ae-592e9ca04ba9,ResourceVersion:20771,Generation:0,CreationTimestamp:2019-07-05 10:06:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:06:13.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8998" for this suite.
Jul  5 10:06:19.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:06:19.566: INFO: namespace watch-8998 deletion completed in 6.095451943s

• [SLOW TEST:6.154 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:06:19.567: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul  5 10:06:19.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 create -f - --namespace=kubectl-2916'
Jul  5 10:06:19.960: INFO: stderr: ""
Jul  5 10:06:19.960: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  5 10:06:19.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2916'
Jul  5 10:06:20.041: INFO: stderr: ""
Jul  5 10:06:20.041: INFO: stdout: "update-demo-nautilus-rtfz6 update-demo-nautilus-wxwd5 "
Jul  5 10:06:20.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-rtfz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2916'
Jul  5 10:06:20.113: INFO: stderr: ""
Jul  5 10:06:20.113: INFO: stdout: ""
Jul  5 10:06:20.113: INFO: update-demo-nautilus-rtfz6 is created but not running
Jul  5 10:06:25.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2916'
Jul  5 10:06:25.204: INFO: stderr: ""
Jul  5 10:06:25.204: INFO: stdout: "update-demo-nautilus-rtfz6 update-demo-nautilus-wxwd5 "
Jul  5 10:06:25.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-rtfz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2916'
Jul  5 10:06:25.271: INFO: stderr: ""
Jul  5 10:06:25.271: INFO: stdout: "true"
Jul  5 10:06:25.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-rtfz6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2916'
Jul  5 10:06:25.342: INFO: stderr: ""
Jul  5 10:06:25.342: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  5 10:06:25.342: INFO: validating pod update-demo-nautilus-rtfz6
Jul  5 10:06:25.346: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 10:06:25.346: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 10:06:25.346: INFO: update-demo-nautilus-rtfz6 is verified up and running
Jul  5 10:06:25.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-wxwd5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2916'
Jul  5 10:06:25.424: INFO: stderr: ""
Jul  5 10:06:25.424: INFO: stdout: "true"
Jul  5 10:06:25.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-wxwd5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2916'
Jul  5 10:06:25.492: INFO: stderr: ""
Jul  5 10:06:25.492: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  5 10:06:25.492: INFO: validating pod update-demo-nautilus-wxwd5
Jul  5 10:06:25.496: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 10:06:25.496: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 10:06:25.496: INFO: update-demo-nautilus-wxwd5 is verified up and running
STEP: using delete to clean up resources
Jul  5 10:06:25.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete --grace-period=0 --force -f - --namespace=kubectl-2916'
Jul  5 10:06:25.571: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  5 10:06:25.571: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul  5 10:06:25.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2916'
Jul  5 10:06:25.642: INFO: stderr: "No resources found.\n"
Jul  5 10:06:25.642: INFO: stdout: ""
Jul  5 10:06:25.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -l name=update-demo --namespace=kubectl-2916 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  5 10:06:25.709: INFO: stderr: ""
Jul  5 10:06:25.709: INFO: stdout: "update-demo-nautilus-rtfz6\nupdate-demo-nautilus-wxwd5\n"
Jul  5 10:06:26.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2916'
Jul  5 10:06:26.284: INFO: stderr: "No resources found.\n"
Jul  5 10:06:26.284: INFO: stdout: ""
Jul  5 10:06:26.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -l name=update-demo --namespace=kubectl-2916 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  5 10:06:26.355: INFO: stderr: ""
Jul  5 10:06:26.355: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:06:26.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2916" for this suite.
Jul  5 10:06:48.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:06:48.451: INFO: namespace kubectl-2916 deletion completed in 22.091248971s

• [SLOW TEST:28.884 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:06:48.451: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul  5 10:06:56.610: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1541 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:06:56.610: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:06:56.756: INFO: Exec stderr: ""
Jul  5 10:06:56.756: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1541 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:06:56.756: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:06:56.934: INFO: Exec stderr: ""
Jul  5 10:06:56.934: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1541 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:06:56.934: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:06:57.099: INFO: Exec stderr: ""
Jul  5 10:06:57.100: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1541 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:06:57.100: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:06:57.277: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul  5 10:06:57.277: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1541 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:06:57.277: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:06:57.436: INFO: Exec stderr: ""
Jul  5 10:06:57.436: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1541 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:06:57.436: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:06:57.609: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul  5 10:06:57.609: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1541 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:06:57.609: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:06:57.759: INFO: Exec stderr: ""
Jul  5 10:06:57.759: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1541 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:06:57.759: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:06:57.935: INFO: Exec stderr: ""
Jul  5 10:06:57.935: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1541 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:06:57.935: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:06:58.099: INFO: Exec stderr: ""
Jul  5 10:06:58.099: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1541 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:06:58.099: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:06:58.272: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:06:58.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1541" for this suite.
Jul  5 10:07:40.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:07:40.378: INFO: namespace e2e-kubelet-etc-hosts-1541 deletion completed in 42.101458912s

• [SLOW TEST:51.927 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:07:40.378: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-rqgk
STEP: Creating a pod to test atomic-volume-subpath
Jul  5 10:07:40.419: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-rqgk" in namespace "subpath-3060" to be "success or failure"
Jul  5 10:07:40.421: INFO: Pod "pod-subpath-test-secret-rqgk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597256ms
Jul  5 10:07:42.424: INFO: Pod "pod-subpath-test-secret-rqgk": Phase="Running", Reason="", readiness=true. Elapsed: 2.005685583s
Jul  5 10:07:44.427: INFO: Pod "pod-subpath-test-secret-rqgk": Phase="Running", Reason="", readiness=true. Elapsed: 4.008576595s
Jul  5 10:07:46.431: INFO: Pod "pod-subpath-test-secret-rqgk": Phase="Running", Reason="", readiness=true. Elapsed: 6.011921538s
Jul  5 10:07:48.434: INFO: Pod "pod-subpath-test-secret-rqgk": Phase="Running", Reason="", readiness=true. Elapsed: 8.015194614s
Jul  5 10:07:50.437: INFO: Pod "pod-subpath-test-secret-rqgk": Phase="Running", Reason="", readiness=true. Elapsed: 10.018331567s
Jul  5 10:07:52.440: INFO: Pod "pod-subpath-test-secret-rqgk": Phase="Running", Reason="", readiness=true. Elapsed: 12.02131089s
Jul  5 10:07:54.443: INFO: Pod "pod-subpath-test-secret-rqgk": Phase="Running", Reason="", readiness=true. Elapsed: 14.024372261s
Jul  5 10:07:56.447: INFO: Pod "pod-subpath-test-secret-rqgk": Phase="Running", Reason="", readiness=true. Elapsed: 16.028219201s
Jul  5 10:07:58.450: INFO: Pod "pod-subpath-test-secret-rqgk": Phase="Running", Reason="", readiness=true. Elapsed: 18.031358085s
Jul  5 10:08:00.453: INFO: Pod "pod-subpath-test-secret-rqgk": Phase="Running", Reason="", readiness=true. Elapsed: 20.034391568s
Jul  5 10:08:02.456: INFO: Pod "pod-subpath-test-secret-rqgk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.037504264s
STEP: Saw pod success
Jul  5 10:08:02.456: INFO: Pod "pod-subpath-test-secret-rqgk" satisfied condition "success or failure"
Jul  5 10:08:02.459: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-subpath-test-secret-rqgk container test-container-subpath-secret-rqgk: <nil>
STEP: delete the pod
Jul  5 10:08:02.479: INFO: Waiting for pod pod-subpath-test-secret-rqgk to disappear
Jul  5 10:08:02.481: INFO: Pod pod-subpath-test-secret-rqgk no longer exists
STEP: Deleting pod pod-subpath-test-secret-rqgk
Jul  5 10:08:02.482: INFO: Deleting pod "pod-subpath-test-secret-rqgk" in namespace "subpath-3060"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:08:02.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3060" for this suite.
Jul  5 10:08:08.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:08:08.603: INFO: namespace subpath-3060 deletion completed in 6.111998113s

• [SLOW TEST:28.225 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:08:08.604: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-da9d1c75-e058-47ea-b04e-a662bd151955
STEP: Creating a pod to test consume configMaps
Jul  5 10:08:08.643: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac0c6ac0-f973-4f2c-ac19-24b1470cb354" in namespace "projected-4171" to be "success or failure"
Jul  5 10:08:08.651: INFO: Pod "pod-projected-configmaps-ac0c6ac0-f973-4f2c-ac19-24b1470cb354": Phase="Pending", Reason="", readiness=false. Elapsed: 7.943813ms
Jul  5 10:08:10.654: INFO: Pod "pod-projected-configmaps-ac0c6ac0-f973-4f2c-ac19-24b1470cb354": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011395898s
STEP: Saw pod success
Jul  5 10:08:10.654: INFO: Pod "pod-projected-configmaps-ac0c6ac0-f973-4f2c-ac19-24b1470cb354" satisfied condition "success or failure"
Jul  5 10:08:10.656: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-projected-configmaps-ac0c6ac0-f973-4f2c-ac19-24b1470cb354 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 10:08:10.677: INFO: Waiting for pod pod-projected-configmaps-ac0c6ac0-f973-4f2c-ac19-24b1470cb354 to disappear
Jul  5 10:08:10.680: INFO: Pod pod-projected-configmaps-ac0c6ac0-f973-4f2c-ac19-24b1470cb354 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:08:10.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4171" for this suite.
Jul  5 10:08:16.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:08:16.784: INFO: namespace projected-4171 deletion completed in 6.100787578s

• [SLOW TEST:8.179 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:08:16.785: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul  5 10:08:16.838: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:16.838: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:16.838: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:16.841: INFO: Number of nodes with available pods: 0
Jul  5 10:08:16.841: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:17.845: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:17.845: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:17.845: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:17.849: INFO: Number of nodes with available pods: 0
Jul  5 10:08:17.849: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:18.846: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:18.846: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:18.846: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:18.848: INFO: Number of nodes with available pods: 3
Jul  5 10:08:18.848: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul  5 10:08:18.866: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:18.866: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:18.867: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:18.869: INFO: Number of nodes with available pods: 2
Jul  5 10:08:18.869: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:19.878: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:19.878: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:19.878: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:19.881: INFO: Number of nodes with available pods: 2
Jul  5 10:08:19.881: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:20.873: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:20.873: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:20.873: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:20.876: INFO: Number of nodes with available pods: 2
Jul  5 10:08:20.876: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:21.873: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:21.873: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:21.873: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:21.876: INFO: Number of nodes with available pods: 2
Jul  5 10:08:21.876: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:22.874: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:22.874: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:22.874: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:22.876: INFO: Number of nodes with available pods: 2
Jul  5 10:08:22.876: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:23.873: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:23.873: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:23.873: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:23.875: INFO: Number of nodes with available pods: 2
Jul  5 10:08:23.875: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:24.873: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:24.873: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:24.873: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:24.876: INFO: Number of nodes with available pods: 2
Jul  5 10:08:24.876: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:25.873: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:25.873: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:25.873: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:25.876: INFO: Number of nodes with available pods: 2
Jul  5 10:08:25.876: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:26.874: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:26.874: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:26.874: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:26.876: INFO: Number of nodes with available pods: 2
Jul  5 10:08:26.877: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:27.873: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:27.873: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:27.874: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:27.876: INFO: Number of nodes with available pods: 2
Jul  5 10:08:27.876: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:28.873: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:28.873: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:28.873: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:28.875: INFO: Number of nodes with available pods: 2
Jul  5 10:08:28.875: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:29.874: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:29.874: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:29.874: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:29.877: INFO: Number of nodes with available pods: 2
Jul  5 10:08:29.877: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:30.873: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:30.873: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:30.873: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:30.876: INFO: Number of nodes with available pods: 2
Jul  5 10:08:30.876: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:31.873: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:31.873: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:31.873: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:31.876: INFO: Number of nodes with available pods: 2
Jul  5 10:08:31.876: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:32.873: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:32.873: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:32.874: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:32.876: INFO: Number of nodes with available pods: 2
Jul  5 10:08:32.876: INFO: Node ip-172-31-6-226.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:08:33.873: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:33.873: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:33.873: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:08:33.876: INFO: Number of nodes with available pods: 3
Jul  5 10:08:33.876: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7152, will wait for the garbage collector to delete the pods
Jul  5 10:08:33.939: INFO: Deleting DaemonSet.extensions daemon-set took: 8.088663ms
Jul  5 10:08:34.339: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.194157ms
Jul  5 10:08:40.042: INFO: Number of nodes with available pods: 0
Jul  5 10:08:40.042: INFO: Number of running nodes: 0, number of available pods: 0
Jul  5 10:08:40.045: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7152/daemonsets","resourceVersion":"21395"},"items":null}

Jul  5 10:08:40.047: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7152/pods","resourceVersion":"21395"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:08:40.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7152" for this suite.
Jul  5 10:08:46.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:08:46.166: INFO: namespace daemonsets-7152 deletion completed in 6.105871412s

• [SLOW TEST:29.382 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:08:46.167: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 10:08:46.198: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31589a9d-bfe0-4838-b4ef-73650dae1e6e" in namespace "projected-6151" to be "success or failure"
Jul  5 10:08:46.204: INFO: Pod "downwardapi-volume-31589a9d-bfe0-4838-b4ef-73650dae1e6e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.692628ms
Jul  5 10:08:48.207: INFO: Pod "downwardapi-volume-31589a9d-bfe0-4838-b4ef-73650dae1e6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008386964s
STEP: Saw pod success
Jul  5 10:08:48.207: INFO: Pod "downwardapi-volume-31589a9d-bfe0-4838-b4ef-73650dae1e6e" satisfied condition "success or failure"
Jul  5 10:08:48.209: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod downwardapi-volume-31589a9d-bfe0-4838-b4ef-73650dae1e6e container client-container: <nil>
STEP: delete the pod
Jul  5 10:08:48.236: INFO: Waiting for pod downwardapi-volume-31589a9d-bfe0-4838-b4ef-73650dae1e6e to disappear
Jul  5 10:08:48.238: INFO: Pod downwardapi-volume-31589a9d-bfe0-4838-b4ef-73650dae1e6e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:08:48.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6151" for this suite.
Jul  5 10:08:54.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:08:54.342: INFO: namespace projected-6151 deletion completed in 6.099893442s

• [SLOW TEST:8.175 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:08:54.342: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 10:08:54.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 version'
Jul  5 10:08:54.435: INFO: stderr: ""
Jul  5 10:08:54.435: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:40:16Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:32:14Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:08:54.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-936" for this suite.
Jul  5 10:09:00.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:09:00.543: INFO: namespace kubectl-936 deletion completed in 6.104289044s

• [SLOW TEST:6.201 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:09:00.544: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-4b0ac289-c912-414e-b88e-fcd3d7ed5645
STEP: Creating a pod to test consume secrets
Jul  5 10:09:00.582: INFO: Waiting up to 5m0s for pod "pod-secrets-775c48cf-9257-4028-a1e0-379a3b9c70ad" in namespace "secrets-5448" to be "success or failure"
Jul  5 10:09:00.586: INFO: Pod "pod-secrets-775c48cf-9257-4028-a1e0-379a3b9c70ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.69102ms
Jul  5 10:09:02.595: INFO: Pod "pod-secrets-775c48cf-9257-4028-a1e0-379a3b9c70ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012751368s
STEP: Saw pod success
Jul  5 10:09:02.595: INFO: Pod "pod-secrets-775c48cf-9257-4028-a1e0-379a3b9c70ad" satisfied condition "success or failure"
Jul  5 10:09:02.598: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-secrets-775c48cf-9257-4028-a1e0-379a3b9c70ad container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 10:09:02.616: INFO: Waiting for pod pod-secrets-775c48cf-9257-4028-a1e0-379a3b9c70ad to disappear
Jul  5 10:09:02.619: INFO: Pod pod-secrets-775c48cf-9257-4028-a1e0-379a3b9c70ad no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:09:02.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5448" for this suite.
Jul  5 10:09:08.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:09:08.722: INFO: namespace secrets-5448 deletion completed in 6.100008979s

• [SLOW TEST:8.178 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:09:08.722: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-197
I0705 10:09:08.755445      15 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-197, replica count: 1
I0705 10:09:09.805873      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0705 10:09:10.806082      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  5 10:09:10.918: INFO: Created: latency-svc-j289w
Jul  5 10:09:10.923: INFO: Got endpoints: latency-svc-j289w [17.538915ms]
Jul  5 10:09:10.970: INFO: Created: latency-svc-nppz4
Jul  5 10:09:10.976: INFO: Got endpoints: latency-svc-nppz4 [52.040444ms]
Jul  5 10:09:10.990: INFO: Created: latency-svc-xp5n6
Jul  5 10:09:10.995: INFO: Got endpoints: latency-svc-xp5n6 [71.47245ms]
Jul  5 10:09:11.004: INFO: Created: latency-svc-5ptgx
Jul  5 10:09:11.009: INFO: Got endpoints: latency-svc-5ptgx [85.616718ms]
Jul  5 10:09:11.019: INFO: Created: latency-svc-mcdws
Jul  5 10:09:11.023: INFO: Got endpoints: latency-svc-mcdws [99.31112ms]
Jul  5 10:09:11.038: INFO: Created: latency-svc-wmjvn
Jul  5 10:09:11.046: INFO: Got endpoints: latency-svc-wmjvn [122.123043ms]
Jul  5 10:09:11.056: INFO: Created: latency-svc-sxx6n
Jul  5 10:09:11.066: INFO: Got endpoints: latency-svc-sxx6n [141.215504ms]
Jul  5 10:09:11.075: INFO: Created: latency-svc-vfw52
Jul  5 10:09:11.082: INFO: Got endpoints: latency-svc-vfw52 [157.682289ms]
Jul  5 10:09:11.092: INFO: Created: latency-svc-z9xq5
Jul  5 10:09:11.098: INFO: Got endpoints: latency-svc-z9xq5 [173.578952ms]
Jul  5 10:09:11.117: INFO: Created: latency-svc-q4gr5
Jul  5 10:09:11.123: INFO: Got endpoints: latency-svc-q4gr5 [198.45574ms]
Jul  5 10:09:11.130: INFO: Created: latency-svc-rtlrd
Jul  5 10:09:11.133: INFO: Got endpoints: latency-svc-rtlrd [208.243562ms]
Jul  5 10:09:11.153: INFO: Created: latency-svc-9jsmc
Jul  5 10:09:11.159: INFO: Got endpoints: latency-svc-9jsmc [234.556073ms]
Jul  5 10:09:11.166: INFO: Created: latency-svc-hvxhn
Jul  5 10:09:11.171: INFO: Got endpoints: latency-svc-hvxhn [246.051544ms]
Jul  5 10:09:11.182: INFO: Created: latency-svc-g79f9
Jul  5 10:09:11.193: INFO: Got endpoints: latency-svc-g79f9 [267.710435ms]
Jul  5 10:09:11.206: INFO: Created: latency-svc-nhqlr
Jul  5 10:09:11.214: INFO: Got endpoints: latency-svc-nhqlr [289.026853ms]
Jul  5 10:09:11.217: INFO: Created: latency-svc-s55wk
Jul  5 10:09:11.229: INFO: Got endpoints: latency-svc-s55wk [303.86193ms]
Jul  5 10:09:11.238: INFO: Created: latency-svc-nqmrr
Jul  5 10:09:11.241: INFO: Got endpoints: latency-svc-nqmrr [264.853807ms]
Jul  5 10:09:11.260: INFO: Created: latency-svc-t6fzg
Jul  5 10:09:11.261: INFO: Got endpoints: latency-svc-t6fzg [265.492398ms]
Jul  5 10:09:11.272: INFO: Created: latency-svc-8ffpb
Jul  5 10:09:11.292: INFO: Created: latency-svc-rzmhh
Jul  5 10:09:11.295: INFO: Got endpoints: latency-svc-8ffpb [285.390045ms]
Jul  5 10:09:11.305: INFO: Got endpoints: latency-svc-rzmhh [281.880969ms]
Jul  5 10:09:11.315: INFO: Created: latency-svc-v2bpt
Jul  5 10:09:11.323: INFO: Got endpoints: latency-svc-v2bpt [276.044608ms]
Jul  5 10:09:11.360: INFO: Created: latency-svc-shsx7
Jul  5 10:09:11.361: INFO: Got endpoints: latency-svc-shsx7 [294.692095ms]
Jul  5 10:09:11.383: INFO: Created: latency-svc-7bllq
Jul  5 10:09:11.386: INFO: Got endpoints: latency-svc-7bllq [304.024605ms]
Jul  5 10:09:11.399: INFO: Created: latency-svc-9gvg6
Jul  5 10:09:11.412: INFO: Got endpoints: latency-svc-9gvg6 [314.266785ms]
Jul  5 10:09:11.419: INFO: Created: latency-svc-q55x5
Jul  5 10:09:11.423: INFO: Got endpoints: latency-svc-q55x5 [299.825287ms]
Jul  5 10:09:11.440: INFO: Created: latency-svc-n5hpv
Jul  5 10:09:11.445: INFO: Got endpoints: latency-svc-n5hpv [312.425331ms]
Jul  5 10:09:11.451: INFO: Created: latency-svc-rlxzz
Jul  5 10:09:11.460: INFO: Got endpoints: latency-svc-rlxzz [301.342272ms]
Jul  5 10:09:11.472: INFO: Created: latency-svc-j8pzf
Jul  5 10:09:11.475: INFO: Got endpoints: latency-svc-j8pzf [303.853667ms]
Jul  5 10:09:11.487: INFO: Created: latency-svc-kn27r
Jul  5 10:09:11.493: INFO: Got endpoints: latency-svc-kn27r [299.933745ms]
Jul  5 10:09:11.505: INFO: Created: latency-svc-6zlgk
Jul  5 10:09:11.512: INFO: Got endpoints: latency-svc-6zlgk [297.44158ms]
Jul  5 10:09:11.531: INFO: Created: latency-svc-7cvv9
Jul  5 10:09:11.533: INFO: Got endpoints: latency-svc-7cvv9 [303.88753ms]
Jul  5 10:09:11.542: INFO: Created: latency-svc-wtl9n
Jul  5 10:09:11.544: INFO: Got endpoints: latency-svc-wtl9n [303.884897ms]
Jul  5 10:09:11.560: INFO: Created: latency-svc-9h8jm
Jul  5 10:09:11.565: INFO: Got endpoints: latency-svc-9h8jm [304.175931ms]
Jul  5 10:09:11.574: INFO: Created: latency-svc-76ntp
Jul  5 10:09:11.577: INFO: Got endpoints: latency-svc-76ntp [281.503294ms]
Jul  5 10:09:11.594: INFO: Created: latency-svc-v4cmt
Jul  5 10:09:11.599: INFO: Got endpoints: latency-svc-v4cmt [293.51377ms]
Jul  5 10:09:11.604: INFO: Created: latency-svc-bq5sf
Jul  5 10:09:11.609: INFO: Got endpoints: latency-svc-bq5sf [285.637905ms]
Jul  5 10:09:11.632: INFO: Created: latency-svc-vdhmq
Jul  5 10:09:11.635: INFO: Got endpoints: latency-svc-vdhmq [274.362414ms]
Jul  5 10:09:11.654: INFO: Created: latency-svc-hdngm
Jul  5 10:09:11.663: INFO: Got endpoints: latency-svc-hdngm [277.465793ms]
Jul  5 10:09:11.667: INFO: Created: latency-svc-xh5l8
Jul  5 10:09:11.670: INFO: Got endpoints: latency-svc-xh5l8 [257.722879ms]
Jul  5 10:09:11.684: INFO: Created: latency-svc-lt2rz
Jul  5 10:09:11.690: INFO: Got endpoints: latency-svc-lt2rz [266.567441ms]
Jul  5 10:09:11.699: INFO: Created: latency-svc-nm55g
Jul  5 10:09:11.704: INFO: Got endpoints: latency-svc-nm55g [259.247074ms]
Jul  5 10:09:11.721: INFO: Created: latency-svc-xr942
Jul  5 10:09:11.725: INFO: Got endpoints: latency-svc-xr942 [264.231666ms]
Jul  5 10:09:11.762: INFO: Created: latency-svc-5kcfg
Jul  5 10:09:11.778: INFO: Got endpoints: latency-svc-5kcfg [303.032116ms]
Jul  5 10:09:11.821: INFO: Created: latency-svc-sgmh9
Jul  5 10:09:11.829: INFO: Got endpoints: latency-svc-sgmh9 [335.624645ms]
Jul  5 10:09:11.834: INFO: Created: latency-svc-kwn25
Jul  5 10:09:11.846: INFO: Got endpoints: latency-svc-kwn25 [334.161508ms]
Jul  5 10:09:11.865: INFO: Created: latency-svc-bkrnd
Jul  5 10:09:11.869: INFO: Got endpoints: latency-svc-bkrnd [335.63965ms]
Jul  5 10:09:11.880: INFO: Created: latency-svc-bwwn6
Jul  5 10:09:11.888: INFO: Got endpoints: latency-svc-bwwn6 [343.812619ms]
Jul  5 10:09:11.900: INFO: Created: latency-svc-sn5wl
Jul  5 10:09:11.907: INFO: Got endpoints: latency-svc-sn5wl [341.372014ms]
Jul  5 10:09:11.919: INFO: Created: latency-svc-bkncx
Jul  5 10:09:11.922: INFO: Got endpoints: latency-svc-bkncx [345.175818ms]
Jul  5 10:09:11.940: INFO: Created: latency-svc-292jv
Jul  5 10:09:11.944: INFO: Got endpoints: latency-svc-292jv [344.65254ms]
Jul  5 10:09:11.962: INFO: Created: latency-svc-snwdz
Jul  5 10:09:11.977: INFO: Created: latency-svc-lh9b4
Jul  5 10:09:11.977: INFO: Got endpoints: latency-svc-snwdz [368.602323ms]
Jul  5 10:09:11.993: INFO: Created: latency-svc-spd8z
Jul  5 10:09:12.012: INFO: Created: latency-svc-bvh84
Jul  5 10:09:12.026: INFO: Got endpoints: latency-svc-lh9b4 [390.529586ms]
Jul  5 10:09:12.027: INFO: Created: latency-svc-2ntlb
Jul  5 10:09:12.043: INFO: Created: latency-svc-ppmlb
Jul  5 10:09:12.065: INFO: Created: latency-svc-kft4n
Jul  5 10:09:12.080: INFO: Got endpoints: latency-svc-spd8z [416.216342ms]
Jul  5 10:09:12.083: INFO: Created: latency-svc-f9q2p
Jul  5 10:09:12.096: INFO: Created: latency-svc-ch8c4
Jul  5 10:09:12.113: INFO: Created: latency-svc-x8xk4
Jul  5 10:09:12.125: INFO: Got endpoints: latency-svc-bvh84 [454.157137ms]
Jul  5 10:09:12.129: INFO: Created: latency-svc-nld4b
Jul  5 10:09:12.144: INFO: Created: latency-svc-fzg6h
Jul  5 10:09:12.161: INFO: Created: latency-svc-lt7dp
Jul  5 10:09:12.175: INFO: Got endpoints: latency-svc-2ntlb [484.642505ms]
Jul  5 10:09:12.182: INFO: Created: latency-svc-2hnb6
Jul  5 10:09:12.199: INFO: Created: latency-svc-hwwbw
Jul  5 10:09:12.214: INFO: Created: latency-svc-6dfjt
Jul  5 10:09:12.230: INFO: Got endpoints: latency-svc-ppmlb [525.348342ms]
Jul  5 10:09:12.272: INFO: Created: latency-svc-tp99f
Jul  5 10:09:12.347: INFO: Got endpoints: latency-svc-kft4n [622.143604ms]
Jul  5 10:09:12.349: INFO: Got endpoints: latency-svc-f9q2p [570.752461ms]
Jul  5 10:09:12.422: INFO: Got endpoints: latency-svc-ch8c4 [593.657341ms]
Jul  5 10:09:12.427: INFO: Created: latency-svc-ztqbx
Jul  5 10:09:12.443: INFO: Got endpoints: latency-svc-x8xk4 [597.670627ms]
Jul  5 10:09:12.448: INFO: Created: latency-svc-w48pm
Jul  5 10:09:12.464: INFO: Created: latency-svc-n6rh2
Jul  5 10:09:12.476: INFO: Got endpoints: latency-svc-nld4b [607.019864ms]
Jul  5 10:09:12.480: INFO: Created: latency-svc-ct962
Jul  5 10:09:12.498: INFO: Created: latency-svc-rv2rc
Jul  5 10:09:12.514: INFO: Created: latency-svc-pwtq6
Jul  5 10:09:12.540: INFO: Got endpoints: latency-svc-fzg6h [651.953566ms]
Jul  5 10:09:12.544: INFO: Created: latency-svc-99nsk
Jul  5 10:09:12.566: INFO: Created: latency-svc-6bztp
Jul  5 10:09:12.574: INFO: Created: latency-svc-qrzlz
Jul  5 10:09:12.579: INFO: Got endpoints: latency-svc-lt7dp [671.63159ms]
Jul  5 10:09:12.586: INFO: Created: latency-svc-b6g7f
Jul  5 10:09:12.602: INFO: Created: latency-svc-tth76
Jul  5 10:09:12.623: INFO: Got endpoints: latency-svc-2hnb6 [701.293701ms]
Jul  5 10:09:12.643: INFO: Created: latency-svc-wz9m7
Jul  5 10:09:12.678: INFO: Got endpoints: latency-svc-hwwbw [733.624216ms]
Jul  5 10:09:12.696: INFO: Created: latency-svc-4gqrw
Jul  5 10:09:12.726: INFO: Got endpoints: latency-svc-6dfjt [748.617919ms]
Jul  5 10:09:12.745: INFO: Created: latency-svc-kn7v6
Jul  5 10:09:12.774: INFO: Got endpoints: latency-svc-tp99f [747.814652ms]
Jul  5 10:09:12.797: INFO: Created: latency-svc-lvvjt
Jul  5 10:09:12.824: INFO: Got endpoints: latency-svc-ztqbx [744.28402ms]
Jul  5 10:09:12.845: INFO: Created: latency-svc-24w6s
Jul  5 10:09:12.877: INFO: Got endpoints: latency-svc-w48pm [751.99185ms]
Jul  5 10:09:12.900: INFO: Created: latency-svc-mx69m
Jul  5 10:09:12.925: INFO: Got endpoints: latency-svc-n6rh2 [750.679144ms]
Jul  5 10:09:12.944: INFO: Created: latency-svc-6dmlt
Jul  5 10:09:12.974: INFO: Got endpoints: latency-svc-ct962 [744.121657ms]
Jul  5 10:09:12.996: INFO: Created: latency-svc-bj4tb
Jul  5 10:09:13.026: INFO: Got endpoints: latency-svc-rv2rc [678.702959ms]
Jul  5 10:09:13.042: INFO: Created: latency-svc-qfdvn
Jul  5 10:09:13.074: INFO: Got endpoints: latency-svc-pwtq6 [724.878151ms]
Jul  5 10:09:13.093: INFO: Created: latency-svc-j455w
Jul  5 10:09:13.123: INFO: Got endpoints: latency-svc-99nsk [701.010937ms]
Jul  5 10:09:13.140: INFO: Created: latency-svc-mm74v
Jul  5 10:09:13.174: INFO: Got endpoints: latency-svc-6bztp [729.807657ms]
Jul  5 10:09:13.195: INFO: Created: latency-svc-bc5cp
Jul  5 10:09:13.224: INFO: Got endpoints: latency-svc-qrzlz [747.635364ms]
Jul  5 10:09:13.242: INFO: Created: latency-svc-x2qtg
Jul  5 10:09:13.274: INFO: Got endpoints: latency-svc-b6g7f [733.022196ms]
Jul  5 10:09:13.291: INFO: Created: latency-svc-z57mg
Jul  5 10:09:13.324: INFO: Got endpoints: latency-svc-tth76 [745.002862ms]
Jul  5 10:09:13.345: INFO: Created: latency-svc-8cc5t
Jul  5 10:09:13.375: INFO: Got endpoints: latency-svc-wz9m7 [751.254026ms]
Jul  5 10:09:13.389: INFO: Created: latency-svc-lqlbg
Jul  5 10:09:13.423: INFO: Got endpoints: latency-svc-4gqrw [745.340937ms]
Jul  5 10:09:13.446: INFO: Created: latency-svc-skclq
Jul  5 10:09:13.475: INFO: Got endpoints: latency-svc-kn7v6 [748.824126ms]
Jul  5 10:09:13.492: INFO: Created: latency-svc-sj4cr
Jul  5 10:09:13.524: INFO: Got endpoints: latency-svc-lvvjt [750.279325ms]
Jul  5 10:09:13.541: INFO: Created: latency-svc-4fgxt
Jul  5 10:09:13.574: INFO: Got endpoints: latency-svc-24w6s [749.391604ms]
Jul  5 10:09:13.590: INFO: Created: latency-svc-vjcl7
Jul  5 10:09:13.624: INFO: Got endpoints: latency-svc-mx69m [746.621132ms]
Jul  5 10:09:13.642: INFO: Created: latency-svc-b6cjx
Jul  5 10:09:13.675: INFO: Got endpoints: latency-svc-6dmlt [749.643311ms]
Jul  5 10:09:13.693: INFO: Created: latency-svc-h57w4
Jul  5 10:09:13.725: INFO: Got endpoints: latency-svc-bj4tb [751.285448ms]
Jul  5 10:09:13.756: INFO: Created: latency-svc-pk2bs
Jul  5 10:09:13.774: INFO: Got endpoints: latency-svc-qfdvn [748.128238ms]
Jul  5 10:09:13.788: INFO: Created: latency-svc-z58vf
Jul  5 10:09:13.823: INFO: Got endpoints: latency-svc-j455w [749.532774ms]
Jul  5 10:09:13.838: INFO: Created: latency-svc-m66ss
Jul  5 10:09:13.875: INFO: Got endpoints: latency-svc-mm74v [751.855031ms]
Jul  5 10:09:13.892: INFO: Created: latency-svc-m6h4w
Jul  5 10:09:13.930: INFO: Got endpoints: latency-svc-bc5cp [756.132188ms]
Jul  5 10:09:13.948: INFO: Created: latency-svc-66wbv
Jul  5 10:09:13.975: INFO: Got endpoints: latency-svc-x2qtg [750.988536ms]
Jul  5 10:09:13.992: INFO: Created: latency-svc-mvdvc
Jul  5 10:09:14.024: INFO: Got endpoints: latency-svc-z57mg [749.629086ms]
Jul  5 10:09:14.048: INFO: Created: latency-svc-sxjkn
Jul  5 10:09:14.074: INFO: Got endpoints: latency-svc-8cc5t [750.128742ms]
Jul  5 10:09:14.097: INFO: Created: latency-svc-5fgfk
Jul  5 10:09:14.124: INFO: Got endpoints: latency-svc-lqlbg [749.069998ms]
Jul  5 10:09:14.142: INFO: Created: latency-svc-99wdj
Jul  5 10:09:14.175: INFO: Got endpoints: latency-svc-skclq [751.414012ms]
Jul  5 10:09:14.192: INFO: Created: latency-svc-6gdph
Jul  5 10:09:14.224: INFO: Got endpoints: latency-svc-sj4cr [748.518525ms]
Jul  5 10:09:14.245: INFO: Created: latency-svc-x2vwf
Jul  5 10:09:14.276: INFO: Got endpoints: latency-svc-4fgxt [751.064451ms]
Jul  5 10:09:14.296: INFO: Created: latency-svc-x4m82
Jul  5 10:09:14.326: INFO: Got endpoints: latency-svc-vjcl7 [751.981389ms]
Jul  5 10:09:14.343: INFO: Created: latency-svc-2tfn5
Jul  5 10:09:14.373: INFO: Got endpoints: latency-svc-b6cjx [749.055274ms]
Jul  5 10:09:14.394: INFO: Created: latency-svc-85v75
Jul  5 10:09:14.424: INFO: Got endpoints: latency-svc-h57w4 [748.879907ms]
Jul  5 10:09:14.442: INFO: Created: latency-svc-w558z
Jul  5 10:09:14.474: INFO: Got endpoints: latency-svc-pk2bs [748.222116ms]
Jul  5 10:09:14.493: INFO: Created: latency-svc-99x7k
Jul  5 10:09:14.525: INFO: Got endpoints: latency-svc-z58vf [750.722867ms]
Jul  5 10:09:14.556: INFO: Created: latency-svc-q5gc9
Jul  5 10:09:14.575: INFO: Got endpoints: latency-svc-m66ss [751.426453ms]
Jul  5 10:09:14.592: INFO: Created: latency-svc-k5s5w
Jul  5 10:09:14.624: INFO: Got endpoints: latency-svc-m6h4w [749.110549ms]
Jul  5 10:09:14.646: INFO: Created: latency-svc-v5hmx
Jul  5 10:09:14.677: INFO: Got endpoints: latency-svc-66wbv [747.648374ms]
Jul  5 10:09:14.696: INFO: Created: latency-svc-2trf8
Jul  5 10:09:14.725: INFO: Got endpoints: latency-svc-mvdvc [749.147094ms]
Jul  5 10:09:14.771: INFO: Created: latency-svc-gjqwl
Jul  5 10:09:14.784: INFO: Got endpoints: latency-svc-sxjkn [759.937269ms]
Jul  5 10:09:14.812: INFO: Created: latency-svc-4h5gg
Jul  5 10:09:14.836: INFO: Got endpoints: latency-svc-5fgfk [761.974649ms]
Jul  5 10:09:14.862: INFO: Created: latency-svc-wfbvt
Jul  5 10:09:14.879: INFO: Got endpoints: latency-svc-99wdj [755.114049ms]
Jul  5 10:09:14.904: INFO: Created: latency-svc-27lck
Jul  5 10:09:14.925: INFO: Got endpoints: latency-svc-6gdph [749.458561ms]
Jul  5 10:09:14.940: INFO: Created: latency-svc-9h88j
Jul  5 10:09:14.977: INFO: Got endpoints: latency-svc-x2vwf [753.040658ms]
Jul  5 10:09:15.003: INFO: Created: latency-svc-fpxxd
Jul  5 10:09:15.025: INFO: Got endpoints: latency-svc-x4m82 [748.932822ms]
Jul  5 10:09:15.043: INFO: Created: latency-svc-5smj7
Jul  5 10:09:15.074: INFO: Got endpoints: latency-svc-2tfn5 [748.296142ms]
Jul  5 10:09:15.094: INFO: Created: latency-svc-4kb2n
Jul  5 10:09:15.124: INFO: Got endpoints: latency-svc-85v75 [750.937118ms]
Jul  5 10:09:15.144: INFO: Created: latency-svc-78gt4
Jul  5 10:09:15.174: INFO: Got endpoints: latency-svc-w558z [750.600923ms]
Jul  5 10:09:15.192: INFO: Created: latency-svc-vd244
Jul  5 10:09:15.224: INFO: Got endpoints: latency-svc-99x7k [749.457968ms]
Jul  5 10:09:15.242: INFO: Created: latency-svc-47rdq
Jul  5 10:09:15.274: INFO: Got endpoints: latency-svc-q5gc9 [749.336869ms]
Jul  5 10:09:15.299: INFO: Created: latency-svc-wtw9j
Jul  5 10:09:15.324: INFO: Got endpoints: latency-svc-k5s5w [748.785495ms]
Jul  5 10:09:15.341: INFO: Created: latency-svc-2gmfq
Jul  5 10:09:15.374: INFO: Got endpoints: latency-svc-v5hmx [749.281057ms]
Jul  5 10:09:15.392: INFO: Created: latency-svc-x4vz4
Jul  5 10:09:15.426: INFO: Got endpoints: latency-svc-2trf8 [748.853415ms]
Jul  5 10:09:15.446: INFO: Created: latency-svc-84dzq
Jul  5 10:09:15.474: INFO: Got endpoints: latency-svc-gjqwl [749.157987ms]
Jul  5 10:09:15.494: INFO: Created: latency-svc-trz7f
Jul  5 10:09:15.525: INFO: Got endpoints: latency-svc-4h5gg [740.841751ms]
Jul  5 10:09:15.544: INFO: Created: latency-svc-p8vhm
Jul  5 10:09:15.574: INFO: Got endpoints: latency-svc-wfbvt [737.786943ms]
Jul  5 10:09:15.592: INFO: Created: latency-svc-sht5l
Jul  5 10:09:15.625: INFO: Got endpoints: latency-svc-27lck [745.268855ms]
Jul  5 10:09:15.642: INFO: Created: latency-svc-s7lcr
Jul  5 10:09:15.675: INFO: Got endpoints: latency-svc-9h88j [749.79809ms]
Jul  5 10:09:15.694: INFO: Created: latency-svc-tx72d
Jul  5 10:09:15.725: INFO: Got endpoints: latency-svc-fpxxd [748.109461ms]
Jul  5 10:09:15.756: INFO: Created: latency-svc-ppkxk
Jul  5 10:09:15.777: INFO: Got endpoints: latency-svc-5smj7 [751.555063ms]
Jul  5 10:09:15.793: INFO: Created: latency-svc-zwqgr
Jul  5 10:09:15.825: INFO: Got endpoints: latency-svc-4kb2n [751.006454ms]
Jul  5 10:09:15.848: INFO: Created: latency-svc-wnck5
Jul  5 10:09:15.874: INFO: Got endpoints: latency-svc-78gt4 [749.848284ms]
Jul  5 10:09:15.897: INFO: Created: latency-svc-w8vh2
Jul  5 10:09:15.923: INFO: Got endpoints: latency-svc-vd244 [748.623854ms]
Jul  5 10:09:15.942: INFO: Created: latency-svc-ps7km
Jul  5 10:09:15.974: INFO: Got endpoints: latency-svc-47rdq [750.552819ms]
Jul  5 10:09:15.993: INFO: Created: latency-svc-fv5jp
Jul  5 10:09:16.024: INFO: Got endpoints: latency-svc-wtw9j [750.114662ms]
Jul  5 10:09:16.044: INFO: Created: latency-svc-f5crw
Jul  5 10:09:16.075: INFO: Got endpoints: latency-svc-2gmfq [750.77169ms]
Jul  5 10:09:16.092: INFO: Created: latency-svc-cs8kk
Jul  5 10:09:16.126: INFO: Got endpoints: latency-svc-x4vz4 [751.845038ms]
Jul  5 10:09:16.149: INFO: Created: latency-svc-ggrq9
Jul  5 10:09:16.174: INFO: Got endpoints: latency-svc-84dzq [747.342654ms]
Jul  5 10:09:16.215: INFO: Created: latency-svc-wkwc5
Jul  5 10:09:16.241: INFO: Got endpoints: latency-svc-trz7f [766.265091ms]
Jul  5 10:09:16.383: INFO: Got endpoints: latency-svc-p8vhm [858.28151ms]
Jul  5 10:09:16.390: INFO: Got endpoints: latency-svc-s7lcr [764.955059ms]
Jul  5 10:09:16.390: INFO: Got endpoints: latency-svc-sht5l [816.166826ms]
Jul  5 10:09:16.409: INFO: Created: latency-svc-62grg
Jul  5 10:09:16.448: INFO: Created: latency-svc-jj6j2
Jul  5 10:09:16.448: INFO: Got endpoints: latency-svc-tx72d [772.69335ms]
Jul  5 10:09:16.481: INFO: Got endpoints: latency-svc-ppkxk [755.299047ms]
Jul  5 10:09:16.485: INFO: Created: latency-svc-4fskr
Jul  5 10:09:16.501: INFO: Created: latency-svc-ctgxz
Jul  5 10:09:16.517: INFO: Created: latency-svc-gl9vd
Jul  5 10:09:16.531: INFO: Got endpoints: latency-svc-zwqgr [754.471375ms]
Jul  5 10:09:16.542: INFO: Created: latency-svc-pxccq
Jul  5 10:09:16.557: INFO: Created: latency-svc-r2wsp
Jul  5 10:09:16.575: INFO: Got endpoints: latency-svc-wnck5 [748.851073ms]
Jul  5 10:09:16.592: INFO: Created: latency-svc-fx2d4
Jul  5 10:09:16.626: INFO: Got endpoints: latency-svc-w8vh2 [751.605037ms]
Jul  5 10:09:16.642: INFO: Created: latency-svc-gvmcv
Jul  5 10:09:16.674: INFO: Got endpoints: latency-svc-ps7km [750.543935ms]
Jul  5 10:09:16.700: INFO: Created: latency-svc-d4ng8
Jul  5 10:09:16.726: INFO: Got endpoints: latency-svc-fv5jp [751.517441ms]
Jul  5 10:09:16.749: INFO: Created: latency-svc-b47k7
Jul  5 10:09:16.775: INFO: Got endpoints: latency-svc-f5crw [750.143628ms]
Jul  5 10:09:16.803: INFO: Created: latency-svc-n6876
Jul  5 10:09:16.823: INFO: Got endpoints: latency-svc-cs8kk [748.040393ms]
Jul  5 10:09:16.839: INFO: Created: latency-svc-qvzh2
Jul  5 10:09:16.874: INFO: Got endpoints: latency-svc-ggrq9 [747.936972ms]
Jul  5 10:09:16.893: INFO: Created: latency-svc-swxvt
Jul  5 10:09:16.924: INFO: Got endpoints: latency-svc-wkwc5 [749.487255ms]
Jul  5 10:09:16.943: INFO: Created: latency-svc-lvm8k
Jul  5 10:09:16.974: INFO: Got endpoints: latency-svc-62grg [733.313489ms]
Jul  5 10:09:16.991: INFO: Created: latency-svc-jkdr7
Jul  5 10:09:17.024: INFO: Got endpoints: latency-svc-jj6j2 [640.937233ms]
Jul  5 10:09:17.043: INFO: Created: latency-svc-jdfvb
Jul  5 10:09:17.074: INFO: Got endpoints: latency-svc-4fskr [684.384507ms]
Jul  5 10:09:17.094: INFO: Created: latency-svc-xvtd2
Jul  5 10:09:17.129: INFO: Got endpoints: latency-svc-ctgxz [739.180473ms]
Jul  5 10:09:17.146: INFO: Created: latency-svc-pnshv
Jul  5 10:09:17.173: INFO: Got endpoints: latency-svc-gl9vd [725.66061ms]
Jul  5 10:09:17.194: INFO: Created: latency-svc-9vr5g
Jul  5 10:09:17.224: INFO: Got endpoints: latency-svc-pxccq [743.260949ms]
Jul  5 10:09:17.247: INFO: Created: latency-svc-b7742
Jul  5 10:09:17.274: INFO: Got endpoints: latency-svc-r2wsp [742.583585ms]
Jul  5 10:09:17.292: INFO: Created: latency-svc-tjk84
Jul  5 10:09:17.326: INFO: Got endpoints: latency-svc-fx2d4 [750.948247ms]
Jul  5 10:09:17.348: INFO: Created: latency-svc-chb5s
Jul  5 10:09:17.375: INFO: Got endpoints: latency-svc-gvmcv [749.472137ms]
Jul  5 10:09:17.396: INFO: Created: latency-svc-l52dw
Jul  5 10:09:17.424: INFO: Got endpoints: latency-svc-d4ng8 [749.999971ms]
Jul  5 10:09:17.443: INFO: Created: latency-svc-sbxqw
Jul  5 10:09:17.477: INFO: Got endpoints: latency-svc-b47k7 [750.970632ms]
Jul  5 10:09:17.498: INFO: Created: latency-svc-vx9fk
Jul  5 10:09:17.524: INFO: Got endpoints: latency-svc-n6876 [749.305167ms]
Jul  5 10:09:17.542: INFO: Created: latency-svc-9hlgf
Jul  5 10:09:17.577: INFO: Got endpoints: latency-svc-qvzh2 [753.094373ms]
Jul  5 10:09:17.594: INFO: Created: latency-svc-xpp96
Jul  5 10:09:17.625: INFO: Got endpoints: latency-svc-swxvt [750.529782ms]
Jul  5 10:09:17.645: INFO: Created: latency-svc-rwsqg
Jul  5 10:09:17.674: INFO: Got endpoints: latency-svc-lvm8k [750.123892ms]
Jul  5 10:09:17.700: INFO: Created: latency-svc-svkl7
Jul  5 10:09:17.726: INFO: Got endpoints: latency-svc-jkdr7 [751.858301ms]
Jul  5 10:09:17.748: INFO: Created: latency-svc-8ddn8
Jul  5 10:09:17.775: INFO: Got endpoints: latency-svc-jdfvb [750.560535ms]
Jul  5 10:09:17.790: INFO: Created: latency-svc-cg6fx
Jul  5 10:09:17.825: INFO: Got endpoints: latency-svc-xvtd2 [750.379012ms]
Jul  5 10:09:17.841: INFO: Created: latency-svc-t499q
Jul  5 10:09:17.874: INFO: Got endpoints: latency-svc-pnshv [744.387509ms]
Jul  5 10:09:17.892: INFO: Created: latency-svc-ppfl7
Jul  5 10:09:17.924: INFO: Got endpoints: latency-svc-9vr5g [749.883004ms]
Jul  5 10:09:17.941: INFO: Created: latency-svc-g7qwp
Jul  5 10:09:17.978: INFO: Got endpoints: latency-svc-b7742 [753.562338ms]
Jul  5 10:09:17.999: INFO: Created: latency-svc-6dk9c
Jul  5 10:09:18.023: INFO: Got endpoints: latency-svc-tjk84 [749.536295ms]
Jul  5 10:09:18.039: INFO: Created: latency-svc-5sp9k
Jul  5 10:09:18.075: INFO: Got endpoints: latency-svc-chb5s [749.62977ms]
Jul  5 10:09:18.100: INFO: Created: latency-svc-zkx9w
Jul  5 10:09:18.124: INFO: Got endpoints: latency-svc-l52dw [747.929451ms]
Jul  5 10:09:18.139: INFO: Created: latency-svc-48n5q
Jul  5 10:09:18.174: INFO: Got endpoints: latency-svc-sbxqw [749.628395ms]
Jul  5 10:09:18.193: INFO: Created: latency-svc-rdd4s
Jul  5 10:09:18.224: INFO: Got endpoints: latency-svc-vx9fk [746.797679ms]
Jul  5 10:09:18.239: INFO: Created: latency-svc-b5v4z
Jul  5 10:09:18.275: INFO: Got endpoints: latency-svc-9hlgf [750.586581ms]
Jul  5 10:09:18.295: INFO: Created: latency-svc-t5wc2
Jul  5 10:09:18.324: INFO: Got endpoints: latency-svc-xpp96 [746.927209ms]
Jul  5 10:09:18.341: INFO: Created: latency-svc-mcmwp
Jul  5 10:09:18.374: INFO: Got endpoints: latency-svc-rwsqg [749.181909ms]
Jul  5 10:09:18.392: INFO: Created: latency-svc-4kkts
Jul  5 10:09:18.426: INFO: Got endpoints: latency-svc-svkl7 [751.780763ms]
Jul  5 10:09:18.442: INFO: Created: latency-svc-s7xgw
Jul  5 10:09:18.473: INFO: Got endpoints: latency-svc-8ddn8 [746.702752ms]
Jul  5 10:09:18.489: INFO: Created: latency-svc-8jz6m
Jul  5 10:09:18.535: INFO: Got endpoints: latency-svc-cg6fx [759.365712ms]
Jul  5 10:09:18.558: INFO: Created: latency-svc-dnz7c
Jul  5 10:09:18.574: INFO: Got endpoints: latency-svc-t499q [748.818815ms]
Jul  5 10:09:18.591: INFO: Created: latency-svc-qh8gr
Jul  5 10:09:18.624: INFO: Got endpoints: latency-svc-ppfl7 [750.415578ms]
Jul  5 10:09:18.644: INFO: Created: latency-svc-pz6fx
Jul  5 10:09:18.674: INFO: Got endpoints: latency-svc-g7qwp [750.631691ms]
Jul  5 10:09:18.692: INFO: Created: latency-svc-7nf9r
Jul  5 10:09:18.724: INFO: Got endpoints: latency-svc-6dk9c [744.318977ms]
Jul  5 10:09:18.741: INFO: Created: latency-svc-c7l5r
Jul  5 10:09:18.783: INFO: Got endpoints: latency-svc-5sp9k [759.79636ms]
Jul  5 10:09:18.825: INFO: Got endpoints: latency-svc-zkx9w [749.423762ms]
Jul  5 10:09:18.873: INFO: Got endpoints: latency-svc-48n5q [749.855748ms]
Jul  5 10:09:18.924: INFO: Got endpoints: latency-svc-rdd4s [750.132651ms]
Jul  5 10:09:18.975: INFO: Got endpoints: latency-svc-b5v4z [750.970333ms]
Jul  5 10:09:19.025: INFO: Got endpoints: latency-svc-t5wc2 [749.792292ms]
Jul  5 10:09:19.074: INFO: Got endpoints: latency-svc-mcmwp [750.258849ms]
Jul  5 10:09:19.124: INFO: Got endpoints: latency-svc-4kkts [749.817798ms]
Jul  5 10:09:19.174: INFO: Got endpoints: latency-svc-s7xgw [748.16754ms]
Jul  5 10:09:19.224: INFO: Got endpoints: latency-svc-8jz6m [751.075648ms]
Jul  5 10:09:19.274: INFO: Got endpoints: latency-svc-dnz7c [738.876663ms]
Jul  5 10:09:19.325: INFO: Got endpoints: latency-svc-qh8gr [751.165066ms]
Jul  5 10:09:19.374: INFO: Got endpoints: latency-svc-pz6fx [749.311791ms]
Jul  5 10:09:19.424: INFO: Got endpoints: latency-svc-7nf9r [749.751789ms]
Jul  5 10:09:19.474: INFO: Got endpoints: latency-svc-c7l5r [749.584086ms]
Jul  5 10:09:19.474: INFO: Latencies: [52.040444ms 71.47245ms 85.616718ms 99.31112ms 122.123043ms 141.215504ms 157.682289ms 173.578952ms 198.45574ms 208.243562ms 234.556073ms 246.051544ms 257.722879ms 259.247074ms 264.231666ms 264.853807ms 265.492398ms 266.567441ms 267.710435ms 274.362414ms 276.044608ms 277.465793ms 281.503294ms 281.880969ms 285.390045ms 285.637905ms 289.026853ms 293.51377ms 294.692095ms 297.44158ms 299.825287ms 299.933745ms 301.342272ms 303.032116ms 303.853667ms 303.86193ms 303.884897ms 303.88753ms 304.024605ms 304.175931ms 312.425331ms 314.266785ms 334.161508ms 335.624645ms 335.63965ms 341.372014ms 343.812619ms 344.65254ms 345.175818ms 368.602323ms 390.529586ms 416.216342ms 454.157137ms 484.642505ms 525.348342ms 570.752461ms 593.657341ms 597.670627ms 607.019864ms 622.143604ms 640.937233ms 651.953566ms 671.63159ms 678.702959ms 684.384507ms 701.010937ms 701.293701ms 724.878151ms 725.66061ms 729.807657ms 733.022196ms 733.313489ms 733.624216ms 737.786943ms 738.876663ms 739.180473ms 740.841751ms 742.583585ms 743.260949ms 744.121657ms 744.28402ms 744.318977ms 744.387509ms 745.002862ms 745.268855ms 745.340937ms 746.621132ms 746.702752ms 746.797679ms 746.927209ms 747.342654ms 747.635364ms 747.648374ms 747.814652ms 747.929451ms 747.936972ms 748.040393ms 748.109461ms 748.128238ms 748.16754ms 748.222116ms 748.296142ms 748.518525ms 748.617919ms 748.623854ms 748.785495ms 748.818815ms 748.824126ms 748.851073ms 748.853415ms 748.879907ms 748.932822ms 749.055274ms 749.069998ms 749.110549ms 749.147094ms 749.157987ms 749.181909ms 749.281057ms 749.305167ms 749.311791ms 749.336869ms 749.391604ms 749.423762ms 749.457968ms 749.458561ms 749.472137ms 749.487255ms 749.532774ms 749.536295ms 749.584086ms 749.628395ms 749.629086ms 749.62977ms 749.643311ms 749.751789ms 749.792292ms 749.79809ms 749.817798ms 749.848284ms 749.855748ms 749.883004ms 749.999971ms 750.114662ms 750.123892ms 750.128742ms 750.132651ms 750.143628ms 750.258849ms 750.279325ms 750.379012ms 750.415578ms 750.529782ms 750.543935ms 750.552819ms 750.560535ms 750.586581ms 750.600923ms 750.631691ms 750.679144ms 750.722867ms 750.77169ms 750.937118ms 750.948247ms 750.970333ms 750.970632ms 750.988536ms 751.006454ms 751.064451ms 751.075648ms 751.165066ms 751.254026ms 751.285448ms 751.414012ms 751.426453ms 751.517441ms 751.555063ms 751.605037ms 751.780763ms 751.845038ms 751.855031ms 751.858301ms 751.981389ms 751.99185ms 753.040658ms 753.094373ms 753.562338ms 754.471375ms 755.114049ms 755.299047ms 756.132188ms 759.365712ms 759.79636ms 759.937269ms 761.974649ms 764.955059ms 766.265091ms 772.69335ms 816.166826ms 858.28151ms]
Jul  5 10:09:19.474: INFO: 50 %ile: 748.222116ms
Jul  5 10:09:19.474: INFO: 90 %ile: 751.855031ms
Jul  5 10:09:19.475: INFO: 99 %ile: 816.166826ms
Jul  5 10:09:19.475: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:09:19.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-197" for this suite.
Jul  5 10:09:33.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:09:33.579: INFO: namespace svc-latency-197 deletion completed in 14.097795749s

• [SLOW TEST:24.857 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:09:33.580: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Jul  5 10:09:33.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 create -f - --namespace=kubectl-3283'
Jul  5 10:09:33.786: INFO: stderr: ""
Jul  5 10:09:33.786: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  5 10:09:33.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3283'
Jul  5 10:09:33.860: INFO: stderr: ""
Jul  5 10:09:33.860: INFO: stdout: "update-demo-nautilus-hkzgz update-demo-nautilus-pv4mc "
Jul  5 10:09:33.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-hkzgz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3283'
Jul  5 10:09:33.932: INFO: stderr: ""
Jul  5 10:09:33.932: INFO: stdout: ""
Jul  5 10:09:33.932: INFO: update-demo-nautilus-hkzgz is created but not running
Jul  5 10:09:38.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3283'
Jul  5 10:09:39.062: INFO: stderr: ""
Jul  5 10:09:39.062: INFO: stdout: "update-demo-nautilus-hkzgz update-demo-nautilus-pv4mc "
Jul  5 10:09:39.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-hkzgz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3283'
Jul  5 10:09:39.196: INFO: stderr: ""
Jul  5 10:09:39.196: INFO: stdout: "true"
Jul  5 10:09:39.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-hkzgz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3283'
Jul  5 10:09:39.320: INFO: stderr: ""
Jul  5 10:09:39.320: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  5 10:09:39.320: INFO: validating pod update-demo-nautilus-hkzgz
Jul  5 10:09:39.324: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 10:09:39.324: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 10:09:39.325: INFO: update-demo-nautilus-hkzgz is verified up and running
Jul  5 10:09:39.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-pv4mc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3283'
Jul  5 10:09:39.440: INFO: stderr: ""
Jul  5 10:09:39.440: INFO: stdout: "true"
Jul  5 10:09:39.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-nautilus-pv4mc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3283'
Jul  5 10:09:39.560: INFO: stderr: ""
Jul  5 10:09:39.560: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  5 10:09:39.560: INFO: validating pod update-demo-nautilus-pv4mc
Jul  5 10:09:39.565: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  5 10:09:39.565: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  5 10:09:39.565: INFO: update-demo-nautilus-pv4mc is verified up and running
STEP: rolling-update to new replication controller
Jul  5 10:09:39.566: INFO: scanned /root for discovery docs: <nil>
Jul  5 10:09:39.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-3283'
Jul  5 10:10:02.019: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul  5 10:10:02.019: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  5 10:10:02.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3283'
Jul  5 10:10:02.105: INFO: stderr: ""
Jul  5 10:10:02.105: INFO: stdout: "update-demo-kitten-9kcwb update-demo-kitten-m6pgz "
Jul  5 10:10:02.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-kitten-9kcwb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3283'
Jul  5 10:10:02.207: INFO: stderr: ""
Jul  5 10:10:02.207: INFO: stdout: "true"
Jul  5 10:10:02.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-kitten-9kcwb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3283'
Jul  5 10:10:02.301: INFO: stderr: ""
Jul  5 10:10:02.301: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul  5 10:10:02.301: INFO: validating pod update-demo-kitten-9kcwb
Jul  5 10:10:02.305: INFO: got data: {
  "image": "kitten.jpg"
}

Jul  5 10:10:02.305: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul  5 10:10:02.305: INFO: update-demo-kitten-9kcwb is verified up and running
Jul  5 10:10:02.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-kitten-m6pgz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3283'
Jul  5 10:10:02.398: INFO: stderr: ""
Jul  5 10:10:02.398: INFO: stdout: "true"
Jul  5 10:10:02.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods update-demo-kitten-m6pgz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3283'
Jul  5 10:10:02.480: INFO: stderr: ""
Jul  5 10:10:02.480: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul  5 10:10:02.480: INFO: validating pod update-demo-kitten-m6pgz
Jul  5 10:10:02.484: INFO: got data: {
  "image": "kitten.jpg"
}

Jul  5 10:10:02.484: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul  5 10:10:02.484: INFO: update-demo-kitten-m6pgz is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:10:02.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3283" for this suite.
Jul  5 10:10:24.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:10:24.598: INFO: namespace kubectl-3283 deletion completed in 22.110481329s

• [SLOW TEST:51.018 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:10:24.599: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul  5 10:10:24.632: INFO: Waiting up to 5m0s for pod "pod-9e9213c1-e076-4da4-b25f-173279ab0dde" in namespace "emptydir-8654" to be "success or failure"
Jul  5 10:10:24.637: INFO: Pod "pod-9e9213c1-e076-4da4-b25f-173279ab0dde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.098091ms
Jul  5 10:10:26.640: INFO: Pod "pod-9e9213c1-e076-4da4-b25f-173279ab0dde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008259031s
STEP: Saw pod success
Jul  5 10:10:26.640: INFO: Pod "pod-9e9213c1-e076-4da4-b25f-173279ab0dde" satisfied condition "success or failure"
Jul  5 10:10:26.642: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-9e9213c1-e076-4da4-b25f-173279ab0dde container test-container: <nil>
STEP: delete the pod
Jul  5 10:10:26.661: INFO: Waiting for pod pod-9e9213c1-e076-4da4-b25f-173279ab0dde to disappear
Jul  5 10:10:26.663: INFO: Pod pod-9e9213c1-e076-4da4-b25f-173279ab0dde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:10:26.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8654" for this suite.
Jul  5 10:10:32.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:10:32.763: INFO: namespace emptydir-8654 deletion completed in 6.096238475s

• [SLOW TEST:8.164 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:10:32.764: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 10:10:32.797: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7595ade5-d2ae-469a-b6dc-9513e75a7636" in namespace "projected-6519" to be "success or failure"
Jul  5 10:10:32.801: INFO: Pod "downwardapi-volume-7595ade5-d2ae-469a-b6dc-9513e75a7636": Phase="Pending", Reason="", readiness=false. Elapsed: 3.116579ms
Jul  5 10:10:34.804: INFO: Pod "downwardapi-volume-7595ade5-d2ae-469a-b6dc-9513e75a7636": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006651843s
STEP: Saw pod success
Jul  5 10:10:34.804: INFO: Pod "downwardapi-volume-7595ade5-d2ae-469a-b6dc-9513e75a7636" satisfied condition "success or failure"
Jul  5 10:10:34.807: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod downwardapi-volume-7595ade5-d2ae-469a-b6dc-9513e75a7636 container client-container: <nil>
STEP: delete the pod
Jul  5 10:10:34.831: INFO: Waiting for pod downwardapi-volume-7595ade5-d2ae-469a-b6dc-9513e75a7636 to disappear
Jul  5 10:10:34.833: INFO: Pod downwardapi-volume-7595ade5-d2ae-469a-b6dc-9513e75a7636 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:10:34.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6519" for this suite.
Jul  5 10:10:40.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:10:40.931: INFO: namespace projected-6519 deletion completed in 6.093804015s

• [SLOW TEST:8.167 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:10:40.931: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul  5 10:10:40.965: INFO: Waiting up to 5m0s for pod "pod-395551a1-1481-46eb-a215-17111401ee11" in namespace "emptydir-5728" to be "success or failure"
Jul  5 10:10:40.970: INFO: Pod "pod-395551a1-1481-46eb-a215-17111401ee11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.37109ms
Jul  5 10:10:42.973: INFO: Pod "pod-395551a1-1481-46eb-a215-17111401ee11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007618776s
STEP: Saw pod success
Jul  5 10:10:42.973: INFO: Pod "pod-395551a1-1481-46eb-a215-17111401ee11" satisfied condition "success or failure"
Jul  5 10:10:42.975: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-395551a1-1481-46eb-a215-17111401ee11 container test-container: <nil>
STEP: delete the pod
Jul  5 10:10:42.998: INFO: Waiting for pod pod-395551a1-1481-46eb-a215-17111401ee11 to disappear
Jul  5 10:10:43.000: INFO: Pod pod-395551a1-1481-46eb-a215-17111401ee11 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:10:43.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5728" for this suite.
Jul  5 10:10:49.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:10:49.107: INFO: namespace emptydir-5728 deletion completed in 6.103633975s

• [SLOW TEST:8.176 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:10:49.109: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-99.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-99.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-99.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-99.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  5 10:10:53.164: INFO: DNS probes using dns-test-2f256716-4e49-4a99-a7d5-6fbbf33e70d8 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-99.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-99.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-99.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-99.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  5 10:10:55.209: INFO: File wheezy_udp@dns-test-service-3.dns-99.svc.cluster.local from pod  dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 10:10:55.211: INFO: File jessie_udp@dns-test-service-3.dns-99.svc.cluster.local from pod  dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 10:10:55.211: INFO: Lookups using dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 failed for: [wheezy_udp@dns-test-service-3.dns-99.svc.cluster.local jessie_udp@dns-test-service-3.dns-99.svc.cluster.local]

Jul  5 10:11:00.215: INFO: File wheezy_udp@dns-test-service-3.dns-99.svc.cluster.local from pod  dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 10:11:00.218: INFO: File jessie_udp@dns-test-service-3.dns-99.svc.cluster.local from pod  dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 10:11:00.218: INFO: Lookups using dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 failed for: [wheezy_udp@dns-test-service-3.dns-99.svc.cluster.local jessie_udp@dns-test-service-3.dns-99.svc.cluster.local]

Jul  5 10:11:05.216: INFO: File wheezy_udp@dns-test-service-3.dns-99.svc.cluster.local from pod  dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 10:11:05.219: INFO: File jessie_udp@dns-test-service-3.dns-99.svc.cluster.local from pod  dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 10:11:05.219: INFO: Lookups using dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 failed for: [wheezy_udp@dns-test-service-3.dns-99.svc.cluster.local jessie_udp@dns-test-service-3.dns-99.svc.cluster.local]

Jul  5 10:11:10.215: INFO: File wheezy_udp@dns-test-service-3.dns-99.svc.cluster.local from pod  dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 10:11:10.218: INFO: File jessie_udp@dns-test-service-3.dns-99.svc.cluster.local from pod  dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 10:11:10.218: INFO: Lookups using dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 failed for: [wheezy_udp@dns-test-service-3.dns-99.svc.cluster.local jessie_udp@dns-test-service-3.dns-99.svc.cluster.local]

Jul  5 10:11:15.216: INFO: File wheezy_udp@dns-test-service-3.dns-99.svc.cluster.local from pod  dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 10:11:15.219: INFO: File jessie_udp@dns-test-service-3.dns-99.svc.cluster.local from pod  dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 10:11:15.219: INFO: Lookups using dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 failed for: [wheezy_udp@dns-test-service-3.dns-99.svc.cluster.local jessie_udp@dns-test-service-3.dns-99.svc.cluster.local]

Jul  5 10:11:20.215: INFO: File wheezy_udp@dns-test-service-3.dns-99.svc.cluster.local from pod  dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 10:11:20.218: INFO: File jessie_udp@dns-test-service-3.dns-99.svc.cluster.local from pod  dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul  5 10:11:20.218: INFO: Lookups using dns-99/dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 failed for: [wheezy_udp@dns-test-service-3.dns-99.svc.cluster.local jessie_udp@dns-test-service-3.dns-99.svc.cluster.local]

Jul  5 10:11:25.219: INFO: DNS probes using dns-test-7cfce19c-a3ce-41e2-8ab0-6302dbfb1c68 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-99.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-99.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-99.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-99.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  5 10:11:29.281: INFO: DNS probes using dns-test-a2e11388-f893-4dab-b841-1730fb868937 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:11:29.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-99" for this suite.
Jul  5 10:11:35.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:11:35.422: INFO: namespace dns-99 deletion completed in 6.098636852s

• [SLOW TEST:46.314 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:11:35.423: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-af9577ef-cbe8-462e-9b1d-cc24f4ca6348
STEP: Creating a pod to test consume configMaps
Jul  5 10:11:35.459: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-23968ec9-71a9-4c47-94cb-e7f70e9faacb" in namespace "projected-7496" to be "success or failure"
Jul  5 10:11:35.464: INFO: Pod "pod-projected-configmaps-23968ec9-71a9-4c47-94cb-e7f70e9faacb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.917433ms
Jul  5 10:11:37.467: INFO: Pod "pod-projected-configmaps-23968ec9-71a9-4c47-94cb-e7f70e9faacb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007806651s
STEP: Saw pod success
Jul  5 10:11:37.467: INFO: Pod "pod-projected-configmaps-23968ec9-71a9-4c47-94cb-e7f70e9faacb" satisfied condition "success or failure"
Jul  5 10:11:37.469: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-projected-configmaps-23968ec9-71a9-4c47-94cb-e7f70e9faacb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 10:11:37.494: INFO: Waiting for pod pod-projected-configmaps-23968ec9-71a9-4c47-94cb-e7f70e9faacb to disappear
Jul  5 10:11:37.496: INFO: Pod pod-projected-configmaps-23968ec9-71a9-4c47-94cb-e7f70e9faacb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:11:37.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7496" for this suite.
Jul  5 10:11:43.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:11:43.595: INFO: namespace projected-7496 deletion completed in 6.09527916s

• [SLOW TEST:8.172 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:11:43.595: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 10:11:43.629: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul  5 10:11:48.633: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  5 10:11:48.633: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul  5 10:11:50.636: INFO: Creating deployment "test-rollover-deployment"
Jul  5 10:11:50.647: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul  5 10:11:52.654: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul  5 10:11:52.659: INFO: Ensure that both replica sets have 1 created replica
Jul  5 10:11:52.663: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul  5 10:11:52.671: INFO: Updating deployment test-rollover-deployment
Jul  5 10:11:52.671: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul  5 10:11:54.679: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul  5 10:11:54.684: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul  5 10:11:54.688: INFO: all replica sets need to contain the pod-template-hash label
Jul  5 10:11:54.688: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918310, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918310, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918314, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918310, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 10:11:56.694: INFO: all replica sets need to contain the pod-template-hash label
Jul  5 10:11:56.694: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918310, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918310, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918314, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918310, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 10:11:58.694: INFO: all replica sets need to contain the pod-template-hash label
Jul  5 10:11:58.694: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918310, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918310, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918314, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918310, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 10:12:00.694: INFO: all replica sets need to contain the pod-template-hash label
Jul  5 10:12:00.694: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918310, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918310, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918314, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918310, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 10:12:02.694: INFO: all replica sets need to contain the pod-template-hash label
Jul  5 10:12:02.694: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918310, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918310, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918314, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918310, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 10:12:04.694: INFO: 
Jul  5 10:12:04.694: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul  5 10:12:04.701: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-9938,SelfLink:/apis/apps/v1/namespaces/deployment-9938/deployments/test-rollover-deployment,UID:24c11c9b-8ba6-4a1b-90a3-8ebfa6824ffc,ResourceVersion:23762,Generation:2,CreationTimestamp:2019-07-05 10:11:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-05 10:11:50 +0000 UTC 2019-07-05 10:11:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-05 10:12:04 +0000 UTC 2019-07-05 10:11:50 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul  5 10:12:04.704: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-9938,SelfLink:/apis/apps/v1/namespaces/deployment-9938/replicasets/test-rollover-deployment-854595fc44,UID:d84bb7fe-c504-45a0-8266-82607889cd6b,ResourceVersion:23751,Generation:2,CreationTimestamp:2019-07-05 10:11:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 24c11c9b-8ba6-4a1b-90a3-8ebfa6824ffc 0xc0016cc397 0xc0016cc398}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul  5 10:12:04.705: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul  5 10:12:04.705: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-9938,SelfLink:/apis/apps/v1/namespaces/deployment-9938/replicasets/test-rollover-controller,UID:d6c02318-bab3-4735-97e1-51b53cd08b32,ResourceVersion:23760,Generation:2,CreationTimestamp:2019-07-05 10:11:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 24c11c9b-8ba6-4a1b-90a3-8ebfa6824ffc 0xc0016cc2c7 0xc0016cc2c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  5 10:12:04.705: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-9938,SelfLink:/apis/apps/v1/namespaces/deployment-9938/replicasets/test-rollover-deployment-9b8b997cf,UID:da40fbea-26d8-406f-a347-92ae17f835d3,ResourceVersion:23714,Generation:2,CreationTimestamp:2019-07-05 10:11:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 24c11c9b-8ba6-4a1b-90a3-8ebfa6824ffc 0xc0016cc460 0xc0016cc461}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  5 10:12:04.708: INFO: Pod "test-rollover-deployment-854595fc44-mlxnf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-mlxnf,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-9938,SelfLink:/api/v1/namespaces/deployment-9938/pods/test-rollover-deployment-854595fc44-mlxnf,UID:b2698d4a-c210-49cd-9f04-8416f7918e03,ResourceVersion:23724,Generation:0,CreationTimestamp:2019-07-05 10:11:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.4.132/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 d84bb7fe-c504-45a0-8266-82607889cd6b 0xc0016cd0b7 0xc0016cd0b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4cv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4cv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-mv4cv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-212.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016cd120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016cd140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:11:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:11:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:11:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:11:52 +0000 UTC  }],Message:,Reason:,HostIP:172.31.1.212,PodIP:10.244.4.132,StartTime:2019-07-05 10:11:52 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-05 10:11:54 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://c25b77473467008dc42c9ea9f382c992a124fe5918059ae14b94776668b5da14}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:12:04.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9938" for this suite.
Jul  5 10:12:10.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:12:10.807: INFO: namespace deployment-9938 deletion completed in 6.096157285s

• [SLOW TEST:27.212 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:12:10.808: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul  5 10:12:10.845: INFO: Waiting up to 5m0s for pod "downward-api-0aeee1ca-5f93-44f7-8335-e1e5854f2e8c" in namespace "downward-api-7966" to be "success or failure"
Jul  5 10:12:10.848: INFO: Pod "downward-api-0aeee1ca-5f93-44f7-8335-e1e5854f2e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.931573ms
Jul  5 10:12:12.851: INFO: Pod "downward-api-0aeee1ca-5f93-44f7-8335-e1e5854f2e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005844643s
Jul  5 10:12:14.854: INFO: Pod "downward-api-0aeee1ca-5f93-44f7-8335-e1e5854f2e8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008782235s
STEP: Saw pod success
Jul  5 10:12:14.854: INFO: Pod "downward-api-0aeee1ca-5f93-44f7-8335-e1e5854f2e8c" satisfied condition "success or failure"
Jul  5 10:12:14.856: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod downward-api-0aeee1ca-5f93-44f7-8335-e1e5854f2e8c container dapi-container: <nil>
STEP: delete the pod
Jul  5 10:12:14.874: INFO: Waiting for pod downward-api-0aeee1ca-5f93-44f7-8335-e1e5854f2e8c to disappear
Jul  5 10:12:14.878: INFO: Pod downward-api-0aeee1ca-5f93-44f7-8335-e1e5854f2e8c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:12:14.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7966" for this suite.
Jul  5 10:12:20.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:12:20.983: INFO: namespace downward-api-7966 deletion completed in 6.102043945s

• [SLOW TEST:10.175 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:12:20.984: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul  5 10:12:21.017: INFO: Pod name pod-release: Found 0 pods out of 1
Jul  5 10:12:26.020: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:12:27.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8441" for this suite.
Jul  5 10:12:33.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:12:33.136: INFO: namespace replication-controller-8441 deletion completed in 6.097209882s

• [SLOW TEST:12.153 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:12:33.137: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-9843/secret-test-5396215b-aa88-409a-acd8-28b34090921a
STEP: Creating a pod to test consume secrets
Jul  5 10:12:33.174: INFO: Waiting up to 5m0s for pod "pod-configmaps-338a1663-620e-4f7a-9b15-730d95e89460" in namespace "secrets-9843" to be "success or failure"
Jul  5 10:12:33.180: INFO: Pod "pod-configmaps-338a1663-620e-4f7a-9b15-730d95e89460": Phase="Pending", Reason="", readiness=false. Elapsed: 6.326225ms
Jul  5 10:12:35.184: INFO: Pod "pod-configmaps-338a1663-620e-4f7a-9b15-730d95e89460": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010478927s
Jul  5 10:12:37.187: INFO: Pod "pod-configmaps-338a1663-620e-4f7a-9b15-730d95e89460": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013535489s
STEP: Saw pod success
Jul  5 10:12:37.187: INFO: Pod "pod-configmaps-338a1663-620e-4f7a-9b15-730d95e89460" satisfied condition "success or failure"
Jul  5 10:12:37.190: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-configmaps-338a1663-620e-4f7a-9b15-730d95e89460 container env-test: <nil>
STEP: delete the pod
Jul  5 10:12:37.209: INFO: Waiting for pod pod-configmaps-338a1663-620e-4f7a-9b15-730d95e89460 to disappear
Jul  5 10:12:37.212: INFO: Pod pod-configmaps-338a1663-620e-4f7a-9b15-730d95e89460 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:12:37.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9843" for this suite.
Jul  5 10:12:43.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:12:43.307: INFO: namespace secrets-9843 deletion completed in 6.09187187s

• [SLOW TEST:10.171 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:12:43.308: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-9a4d465e-9914-4b7d-bc04-082d8d85af27
STEP: Creating a pod to test consume configMaps
Jul  5 10:12:43.356: INFO: Waiting up to 5m0s for pod "pod-configmaps-c55f71a7-2f12-4579-9e96-2a5764e12140" in namespace "configmap-9510" to be "success or failure"
Jul  5 10:12:43.361: INFO: Pod "pod-configmaps-c55f71a7-2f12-4579-9e96-2a5764e12140": Phase="Pending", Reason="", readiness=false. Elapsed: 4.722293ms
Jul  5 10:12:45.364: INFO: Pod "pod-configmaps-c55f71a7-2f12-4579-9e96-2a5764e12140": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007858669s
STEP: Saw pod success
Jul  5 10:12:45.364: INFO: Pod "pod-configmaps-c55f71a7-2f12-4579-9e96-2a5764e12140" satisfied condition "success or failure"
Jul  5 10:12:45.366: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-configmaps-c55f71a7-2f12-4579-9e96-2a5764e12140 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 10:12:45.384: INFO: Waiting for pod pod-configmaps-c55f71a7-2f12-4579-9e96-2a5764e12140 to disappear
Jul  5 10:12:45.386: INFO: Pod pod-configmaps-c55f71a7-2f12-4579-9e96-2a5764e12140 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:12:45.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9510" for this suite.
Jul  5 10:12:51.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:12:51.485: INFO: namespace configmap-9510 deletion completed in 6.095539457s

• [SLOW TEST:8.177 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:12:51.485: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 10:12:51.511: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul  5 10:12:51.517: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul  5 10:12:56.521: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  5 10:12:56.521: INFO: Creating deployment "test-rolling-update-deployment"
Jul  5 10:12:56.531: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul  5 10:12:56.541: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul  5 10:12:58.549: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul  5 10:12:58.553: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918376, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918376, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918376, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918376, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 10:13:00.556: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul  5 10:13:00.564: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-4124,SelfLink:/apis/apps/v1/namespaces/deployment-4124/deployments/test-rolling-update-deployment,UID:487ca0d2-dae1-4a66-8f0b-23d673124930,ResourceVersion:24105,Generation:1,CreationTimestamp:2019-07-05 10:12:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-05 10:12:56 +0000 UTC 2019-07-05 10:12:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-05 10:12:58 +0000 UTC 2019-07-05 10:12:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul  5 10:13:00.566: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-4124,SelfLink:/apis/apps/v1/namespaces/deployment-4124/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:02a8a75b-52f0-44e2-bfbe-c8638f6b1aac,ResourceVersion:24094,Generation:1,CreationTimestamp:2019-07-05 10:12:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 487ca0d2-dae1-4a66-8f0b-23d673124930 0xc0026eeba7 0xc0026eeba8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul  5 10:13:00.566: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul  5 10:13:00.567: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-4124,SelfLink:/apis/apps/v1/namespaces/deployment-4124/replicasets/test-rolling-update-controller,UID:c7f9e64c-c489-481f-b922-dd3545fa4fa5,ResourceVersion:24104,Generation:2,CreationTimestamp:2019-07-05 10:12:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 487ca0d2-dae1-4a66-8f0b-23d673124930 0xc0026eead7 0xc0026eead8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  5 10:13:00.569: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-kc4d9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-kc4d9,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-4124,SelfLink:/api/v1/namespaces/deployment-4124/pods/test-rolling-update-deployment-79f6b9d75c-kc4d9,UID:e42413c0-643e-4fdd-b311-dc6ef3134560,ResourceVersion:24093,Generation:0,CreationTimestamp:2019-07-05 10:12:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.131/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 02a8a75b-52f0-44e2-bfbe-c8638f6b1aac 0xc0026ef4c7 0xc0026ef4c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-j6th2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-j6th2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-j6th2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-115.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026ef530} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026ef550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:12:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:12:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:12:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:12:56 +0000 UTC  }],Message:,Reason:,HostIP:172.31.11.115,PodIP:10.244.3.131,StartTime:2019-07-05 10:12:56 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-05 10:12:58 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://f817fb39e1a5328f89f95ae0ec3d3722311bab01c1ff8cc3199de3d523e1b0ed}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:13:00.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4124" for this suite.
Jul  5 10:13:06.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:13:06.671: INFO: namespace deployment-4124 deletion completed in 6.09845991s

• [SLOW TEST:15.186 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:13:06.671: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-1b506611-1297-4636-b1ba-5fd5950b5323
STEP: Creating a pod to test consume secrets
Jul  5 10:13:06.710: INFO: Waiting up to 5m0s for pod "pod-secrets-e2821892-ab63-46a4-b7f4-e60de0c625ae" in namespace "secrets-6753" to be "success or failure"
Jul  5 10:13:06.714: INFO: Pod "pod-secrets-e2821892-ab63-46a4-b7f4-e60de0c625ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07455ms
Jul  5 10:13:08.717: INFO: Pod "pod-secrets-e2821892-ab63-46a4-b7f4-e60de0c625ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007141758s
Jul  5 10:13:10.720: INFO: Pod "pod-secrets-e2821892-ab63-46a4-b7f4-e60de0c625ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01054479s
STEP: Saw pod success
Jul  5 10:13:10.720: INFO: Pod "pod-secrets-e2821892-ab63-46a4-b7f4-e60de0c625ae" satisfied condition "success or failure"
Jul  5 10:13:10.723: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-secrets-e2821892-ab63-46a4-b7f4-e60de0c625ae container secret-env-test: <nil>
STEP: delete the pod
Jul  5 10:13:10.741: INFO: Waiting for pod pod-secrets-e2821892-ab63-46a4-b7f4-e60de0c625ae to disappear
Jul  5 10:13:10.745: INFO: Pod pod-secrets-e2821892-ab63-46a4-b7f4-e60de0c625ae no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:13:10.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6753" for this suite.
Jul  5 10:13:16.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:13:16.846: INFO: namespace secrets-6753 deletion completed in 6.09787717s

• [SLOW TEST:10.175 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:13:16.847: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-80969b71-4d51-4761-aafc-8441f605608d
STEP: Creating a pod to test consume configMaps
Jul  5 10:13:16.883: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-365df368-6ab1-4bbc-83d8-3fb744bea732" in namespace "projected-1286" to be "success or failure"
Jul  5 10:13:16.888: INFO: Pod "pod-projected-configmaps-365df368-6ab1-4bbc-83d8-3fb744bea732": Phase="Pending", Reason="", readiness=false. Elapsed: 5.257655ms
Jul  5 10:13:18.891: INFO: Pod "pod-projected-configmaps-365df368-6ab1-4bbc-83d8-3fb744bea732": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008200252s
Jul  5 10:13:20.894: INFO: Pod "pod-projected-configmaps-365df368-6ab1-4bbc-83d8-3fb744bea732": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01163071s
STEP: Saw pod success
Jul  5 10:13:20.894: INFO: Pod "pod-projected-configmaps-365df368-6ab1-4bbc-83d8-3fb744bea732" satisfied condition "success or failure"
Jul  5 10:13:20.897: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-projected-configmaps-365df368-6ab1-4bbc-83d8-3fb744bea732 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 10:13:20.916: INFO: Waiting for pod pod-projected-configmaps-365df368-6ab1-4bbc-83d8-3fb744bea732 to disappear
Jul  5 10:13:20.918: INFO: Pod pod-projected-configmaps-365df368-6ab1-4bbc-83d8-3fb744bea732 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:13:20.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1286" for this suite.
Jul  5 10:13:26.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:13:27.019: INFO: namespace projected-1286 deletion completed in 6.098202579s

• [SLOW TEST:10.172 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:13:27.021: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-f37a5133-213f-4e9d-9dfc-815a4abb43e3
STEP: Creating a pod to test consume secrets
Jul  5 10:13:27.057: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8d3fcc01-4806-49a0-bb5f-de51373c14ac" in namespace "projected-8601" to be "success or failure"
Jul  5 10:13:27.061: INFO: Pod "pod-projected-secrets-8d3fcc01-4806-49a0-bb5f-de51373c14ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.438078ms
Jul  5 10:13:29.064: INFO: Pod "pod-projected-secrets-8d3fcc01-4806-49a0-bb5f-de51373c14ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007325966s
Jul  5 10:13:31.067: INFO: Pod "pod-projected-secrets-8d3fcc01-4806-49a0-bb5f-de51373c14ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010560086s
STEP: Saw pod success
Jul  5 10:13:31.067: INFO: Pod "pod-projected-secrets-8d3fcc01-4806-49a0-bb5f-de51373c14ac" satisfied condition "success or failure"
Jul  5 10:13:31.070: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-projected-secrets-8d3fcc01-4806-49a0-bb5f-de51373c14ac container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  5 10:13:31.092: INFO: Waiting for pod pod-projected-secrets-8d3fcc01-4806-49a0-bb5f-de51373c14ac to disappear
Jul  5 10:13:31.095: INFO: Pod pod-projected-secrets-8d3fcc01-4806-49a0-bb5f-de51373c14ac no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:13:31.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8601" for this suite.
Jul  5 10:13:37.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:13:37.194: INFO: namespace projected-8601 deletion completed in 6.095951503s

• [SLOW TEST:10.173 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:13:37.194: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-4336b5fa-a991-4ac3-9973-69a0cab8e006
STEP: Creating a pod to test consume secrets
Jul  5 10:13:37.232: INFO: Waiting up to 5m0s for pod "pod-secrets-0ea8cdd0-65dc-4d6e-9978-757b76c4b06c" in namespace "secrets-4056" to be "success or failure"
Jul  5 10:13:37.236: INFO: Pod "pod-secrets-0ea8cdd0-65dc-4d6e-9978-757b76c4b06c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.799938ms
Jul  5 10:13:39.239: INFO: Pod "pod-secrets-0ea8cdd0-65dc-4d6e-9978-757b76c4b06c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00711152s
Jul  5 10:13:41.242: INFO: Pod "pod-secrets-0ea8cdd0-65dc-4d6e-9978-757b76c4b06c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010522358s
STEP: Saw pod success
Jul  5 10:13:41.242: INFO: Pod "pod-secrets-0ea8cdd0-65dc-4d6e-9978-757b76c4b06c" satisfied condition "success or failure"
Jul  5 10:13:41.245: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-secrets-0ea8cdd0-65dc-4d6e-9978-757b76c4b06c container secret-volume-test: <nil>
STEP: delete the pod
Jul  5 10:13:41.262: INFO: Waiting for pod pod-secrets-0ea8cdd0-65dc-4d6e-9978-757b76c4b06c to disappear
Jul  5 10:13:41.264: INFO: Pod pod-secrets-0ea8cdd0-65dc-4d6e-9978-757b76c4b06c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:13:41.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4056" for this suite.
Jul  5 10:13:47.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:13:47.361: INFO: namespace secrets-4056 deletion completed in 6.093220577s

• [SLOW TEST:10.166 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:13:47.361: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul  5 10:13:47.393: INFO: Waiting up to 5m0s for pod "pod-f85d4662-8171-4315-85e8-e9051668fcdc" in namespace "emptydir-3466" to be "success or failure"
Jul  5 10:13:47.400: INFO: Pod "pod-f85d4662-8171-4315-85e8-e9051668fcdc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.857649ms
Jul  5 10:13:49.403: INFO: Pod "pod-f85d4662-8171-4315-85e8-e9051668fcdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00981516s
Jul  5 10:13:51.406: INFO: Pod "pod-f85d4662-8171-4315-85e8-e9051668fcdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013393739s
STEP: Saw pod success
Jul  5 10:13:51.406: INFO: Pod "pod-f85d4662-8171-4315-85e8-e9051668fcdc" satisfied condition "success or failure"
Jul  5 10:13:51.409: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-f85d4662-8171-4315-85e8-e9051668fcdc container test-container: <nil>
STEP: delete the pod
Jul  5 10:13:51.426: INFO: Waiting for pod pod-f85d4662-8171-4315-85e8-e9051668fcdc to disappear
Jul  5 10:13:51.428: INFO: Pod pod-f85d4662-8171-4315-85e8-e9051668fcdc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:13:51.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3466" for this suite.
Jul  5 10:13:57.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:13:57.529: INFO: namespace emptydir-3466 deletion completed in 6.095185328s

• [SLOW TEST:10.168 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:13:57.529: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Jul  5 10:13:57.560: INFO: Waiting up to 5m0s for pod "client-containers-f8bdf88a-c6ac-4ca5-85f6-c8373647215c" in namespace "containers-6539" to be "success or failure"
Jul  5 10:13:57.563: INFO: Pod "client-containers-f8bdf88a-c6ac-4ca5-85f6-c8373647215c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552768ms
Jul  5 10:13:59.566: INFO: Pod "client-containers-f8bdf88a-c6ac-4ca5-85f6-c8373647215c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005217241s
STEP: Saw pod success
Jul  5 10:13:59.566: INFO: Pod "client-containers-f8bdf88a-c6ac-4ca5-85f6-c8373647215c" satisfied condition "success or failure"
Jul  5 10:13:59.568: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod client-containers-f8bdf88a-c6ac-4ca5-85f6-c8373647215c container test-container: <nil>
STEP: delete the pod
Jul  5 10:13:59.585: INFO: Waiting for pod client-containers-f8bdf88a-c6ac-4ca5-85f6-c8373647215c to disappear
Jul  5 10:13:59.587: INFO: Pod client-containers-f8bdf88a-c6ac-4ca5-85f6-c8373647215c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:13:59.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6539" for this suite.
Jul  5 10:14:05.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:14:05.695: INFO: namespace containers-6539 deletion completed in 6.105035355s

• [SLOW TEST:8.166 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:14:05.696: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 10:14:05.730: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul  5 10:14:10.734: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  5 10:14:10.734: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul  5 10:14:14.760: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-4823,SelfLink:/apis/apps/v1/namespaces/deployment-4823/deployments/test-cleanup-deployment,UID:1a7c9871-1445-4a8b-9c4a-5dbea82a22e7,ResourceVersion:24507,Generation:1,CreationTimestamp:2019-07-05 10:14:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-05 10:14:10 +0000 UTC 2019-07-05 10:14:10 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-05 10:14:12 +0000 UTC 2019-07-05 10:14:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul  5 10:14:14.765: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-4823,SelfLink:/apis/apps/v1/namespaces/deployment-4823/replicasets/test-cleanup-deployment-55bbcbc84c,UID:ae04e5c3-e509-405b-9ea9-805a0bd30a9e,ResourceVersion:24496,Generation:1,CreationTimestamp:2019-07-05 10:14:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 1a7c9871-1445-4a8b-9c4a-5dbea82a22e7 0xc00350e107 0xc00350e108}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul  5 10:14:14.772: INFO: Pod "test-cleanup-deployment-55bbcbc84c-v7jnf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-v7jnf,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-4823,SelfLink:/api/v1/namespaces/deployment-4823/pods/test-cleanup-deployment-55bbcbc84c-v7jnf,UID:3799a84e-20e5-4f3c-8594-86bfcc24a1cb,ResourceVersion:24495,Generation:0,CreationTimestamp:2019-07-05 10:14:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.3.136/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c ae04e5c3-e509-405b-9ea9-805a0bd30a9e 0xc00350e727 0xc00350e728}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jm57j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jm57j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-jm57j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-115.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00350e790} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00350e7b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:14:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:14:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:14:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-05 10:14:10 +0000 UTC  }],Message:,Reason:,HostIP:172.31.11.115,PodIP:10.244.3.136,StartTime:2019-07-05 10:14:10 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-05 10:14:12 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://4749e1fd7c32c88835779851e4a7ff5a52d2fbe30cb98da5e4d043fcca20322f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:14:14.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4823" for this suite.
Jul  5 10:14:20.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:14:20.869: INFO: namespace deployment-4823 deletion completed in 6.093000258s

• [SLOW TEST:15.173 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:14:20.869: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-1a38be9d-6a3e-4e69-bdfd-01ba5e01d3d1
STEP: Creating a pod to test consume configMaps
Jul  5 10:14:20.907: INFO: Waiting up to 5m0s for pod "pod-configmaps-a5eccbc1-dbe6-4131-8d62-d1c41900e712" in namespace "configmap-9023" to be "success or failure"
Jul  5 10:14:20.910: INFO: Pod "pod-configmaps-a5eccbc1-dbe6-4131-8d62-d1c41900e712": Phase="Pending", Reason="", readiness=false. Elapsed: 3.13011ms
Jul  5 10:14:22.913: INFO: Pod "pod-configmaps-a5eccbc1-dbe6-4131-8d62-d1c41900e712": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006579325s
Jul  5 10:14:24.916: INFO: Pod "pod-configmaps-a5eccbc1-dbe6-4131-8d62-d1c41900e712": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009744532s
STEP: Saw pod success
Jul  5 10:14:24.916: INFO: Pod "pod-configmaps-a5eccbc1-dbe6-4131-8d62-d1c41900e712" satisfied condition "success or failure"
Jul  5 10:14:24.919: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-configmaps-a5eccbc1-dbe6-4131-8d62-d1c41900e712 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  5 10:14:24.938: INFO: Waiting for pod pod-configmaps-a5eccbc1-dbe6-4131-8d62-d1c41900e712 to disappear
Jul  5 10:14:24.940: INFO: Pod pod-configmaps-a5eccbc1-dbe6-4131-8d62-d1c41900e712 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:14:24.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9023" for this suite.
Jul  5 10:14:30.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:14:31.044: INFO: namespace configmap-9023 deletion completed in 6.100238551s

• [SLOW TEST:10.174 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:14:31.047: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Jul  5 10:14:31.076: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-272745911 proxy --unix-socket=/tmp/kubectl-proxy-unix221390250/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:14:31.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2472" for this suite.
Jul  5 10:14:37.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:14:37.239: INFO: namespace kubectl-2472 deletion completed in 6.100554959s

• [SLOW TEST:6.191 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:14:37.239: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:15:01.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2609" for this suite.
Jul  5 10:15:07.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:15:07.461: INFO: namespace namespaces-2609 deletion completed in 6.104295574s
STEP: Destroying namespace "nsdeletetest-7266" for this suite.
Jul  5 10:15:07.463: INFO: Namespace nsdeletetest-7266 was already deleted
STEP: Destroying namespace "nsdeletetest-5087" for this suite.
Jul  5 10:15:13.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:15:13.557: INFO: namespace nsdeletetest-5087 deletion completed in 6.094591568s

• [SLOW TEST:36.318 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:15:13.557: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 10:15:13.590: INFO: Waiting up to 5m0s for pod "downwardapi-volume-63b05fcf-b417-4cc3-bbd9-96953cbcd38e" in namespace "projected-7647" to be "success or failure"
Jul  5 10:15:13.593: INFO: Pod "downwardapi-volume-63b05fcf-b417-4cc3-bbd9-96953cbcd38e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.643868ms
Jul  5 10:15:15.597: INFO: Pod "downwardapi-volume-63b05fcf-b417-4cc3-bbd9-96953cbcd38e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006252561s
Jul  5 10:15:17.600: INFO: Pod "downwardapi-volume-63b05fcf-b417-4cc3-bbd9-96953cbcd38e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00927906s
STEP: Saw pod success
Jul  5 10:15:17.600: INFO: Pod "downwardapi-volume-63b05fcf-b417-4cc3-bbd9-96953cbcd38e" satisfied condition "success or failure"
Jul  5 10:15:17.602: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod downwardapi-volume-63b05fcf-b417-4cc3-bbd9-96953cbcd38e container client-container: <nil>
STEP: delete the pod
Jul  5 10:15:17.623: INFO: Waiting for pod downwardapi-volume-63b05fcf-b417-4cc3-bbd9-96953cbcd38e to disappear
Jul  5 10:15:17.627: INFO: Pod downwardapi-volume-63b05fcf-b417-4cc3-bbd9-96953cbcd38e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:15:17.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7647" for this suite.
Jul  5 10:15:23.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:15:23.731: INFO: namespace projected-7647 deletion completed in 6.101017089s

• [SLOW TEST:10.173 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:15:23.731: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 10:15:23.769: INFO: Waiting up to 5m0s for pod "downwardapi-volume-250fd6b0-f25e-4f86-bacc-6b1d746e0760" in namespace "projected-198" to be "success or failure"
Jul  5 10:15:23.773: INFO: Pod "downwardapi-volume-250fd6b0-f25e-4f86-bacc-6b1d746e0760": Phase="Pending", Reason="", readiness=false. Elapsed: 4.411801ms
Jul  5 10:15:25.777: INFO: Pod "downwardapi-volume-250fd6b0-f25e-4f86-bacc-6b1d746e0760": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007672338s
STEP: Saw pod success
Jul  5 10:15:25.777: INFO: Pod "downwardapi-volume-250fd6b0-f25e-4f86-bacc-6b1d746e0760" satisfied condition "success or failure"
Jul  5 10:15:25.779: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod downwardapi-volume-250fd6b0-f25e-4f86-bacc-6b1d746e0760 container client-container: <nil>
STEP: delete the pod
Jul  5 10:15:25.797: INFO: Waiting for pod downwardapi-volume-250fd6b0-f25e-4f86-bacc-6b1d746e0760 to disappear
Jul  5 10:15:25.799: INFO: Pod downwardapi-volume-250fd6b0-f25e-4f86-bacc-6b1d746e0760 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:15:25.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-198" for this suite.
Jul  5 10:15:31.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:15:31.896: INFO: namespace projected-198 deletion completed in 6.09360067s

• [SLOW TEST:8.165 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:15:31.896: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul  5 10:15:31.946: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1597,SelfLink:/api/v1/namespaces/watch-1597/configmaps/e2e-watch-test-resource-version,UID:ca2e477e-a7b3-460c-9cd4-879edb2ee2c5,ResourceVersion:24863,Generation:0,CreationTimestamp:2019-07-05 10:15:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  5 10:15:31.947: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1597,SelfLink:/api/v1/namespaces/watch-1597/configmaps/e2e-watch-test-resource-version,UID:ca2e477e-a7b3-460c-9cd4-879edb2ee2c5,ResourceVersion:24864,Generation:0,CreationTimestamp:2019-07-05 10:15:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:15:31.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1597" for this suite.
Jul  5 10:15:37.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:15:38.044: INFO: namespace watch-1597 deletion completed in 6.093876277s

• [SLOW TEST:6.147 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:15:38.045: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jul  5 10:15:42.091: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-272745911 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul  5 10:15:47.156: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:15:47.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6008" for this suite.
Jul  5 10:15:53.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:15:53.256: INFO: namespace pods-6008 deletion completed in 6.094084247s

• [SLOW TEST:15.211 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:15:53.256: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-b2cd569c-26be-458e-b394-945fcc5f2e17
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:15:55.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9364" for this suite.
Jul  5 10:16:17.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:16:17.416: INFO: namespace configmap-9364 deletion completed in 22.095280361s

• [SLOW TEST:24.160 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:16:17.417: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul  5 10:16:17.452: INFO: Waiting up to 5m0s for pod "pod-83c64fa8-772e-4ea7-ae68-1b7fe8868373" in namespace "emptydir-9929" to be "success or failure"
Jul  5 10:16:17.455: INFO: Pod "pod-83c64fa8-772e-4ea7-ae68-1b7fe8868373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.786666ms
Jul  5 10:16:19.458: INFO: Pod "pod-83c64fa8-772e-4ea7-ae68-1b7fe8868373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005839177s
STEP: Saw pod success
Jul  5 10:16:19.458: INFO: Pod "pod-83c64fa8-772e-4ea7-ae68-1b7fe8868373" satisfied condition "success or failure"
Jul  5 10:16:19.460: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod pod-83c64fa8-772e-4ea7-ae68-1b7fe8868373 container test-container: <nil>
STEP: delete the pod
Jul  5 10:16:19.478: INFO: Waiting for pod pod-83c64fa8-772e-4ea7-ae68-1b7fe8868373 to disappear
Jul  5 10:16:19.480: INFO: Pod pod-83c64fa8-772e-4ea7-ae68-1b7fe8868373 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:16:19.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9929" for this suite.
Jul  5 10:16:25.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:16:25.588: INFO: namespace emptydir-9929 deletion completed in 6.10101313s

• [SLOW TEST:8.171 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:16:25.588: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1519
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  5 10:16:25.614: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  5 10:16:51.698: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.139 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1519 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:16:51.698: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:16:52.867: INFO: Found all expected endpoints: [netserver-0]
Jul  5 10:16:52.869: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.5.39 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1519 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:16:52.869: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:16:54.057: INFO: Found all expected endpoints: [netserver-1]
Jul  5 10:16:54.060: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.4.144 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1519 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  5 10:16:54.060: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
Jul  5 10:16:55.241: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:16:55.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1519" for this suite.
Jul  5 10:17:17.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:17:17.415: INFO: namespace pod-network-test-1519 deletion completed in 22.169063541s

• [SLOW TEST:51.828 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:17:17.416: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:17:22.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7054" for this suite.
Jul  5 10:17:29.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:17:29.159: INFO: namespace watch-7054 deletion completed in 6.188332613s

• [SLOW TEST:11.743 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:17:29.159: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul  5 10:17:29.187: INFO: PodSpec: initContainers in spec.initContainers
Jul  5 10:18:14.976: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b2127655-c05e-4622-871b-f5efc765011a", GenerateName:"", Namespace:"init-container-2560", SelfLink:"/api/v1/namespaces/init-container-2560/pods/pod-init-b2127655-c05e-4622-871b-f5efc765011a", UID:"b878cbf4-8be6-4d94-9c0a-33508fe68431", ResourceVersion:"25590", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63697918649, loc:(*time.Location)(0x80bb5c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"187007201"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.3.141/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-44px8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0007b8200), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-44px8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-44px8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-44px8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0036e2f48), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-11-115.eu-west-3.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003a0b320), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0036e2fc0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0036e2fe0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0036e2fe8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0036e2fec), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918649, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918649, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918649, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697918649, loc:(*time.Location)(0x80bb5c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.11.115", PodIP:"10.244.3.141", StartTime:(*v1.Time)(0xc00385e880), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00294c1c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00294c230)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://130bf3e4c41fd22686129ad222959db224a74f4cbbb63695efa8e274c5424dcb"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00385e8c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00385e8a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:18:14.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2560" for this suite.
Jul  5 10:18:36.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:18:37.079: INFO: namespace init-container-2560 deletion completed in 22.097702596s

• [SLOW TEST:67.920 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:18:37.080: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul  5 10:18:37.106: INFO: namespace kubectl-1642
Jul  5 10:18:37.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 create -f - --namespace=kubectl-1642'
Jul  5 10:18:37.481: INFO: stderr: ""
Jul  5 10:18:37.481: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul  5 10:18:38.484: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 10:18:38.484: INFO: Found 0 / 1
Jul  5 10:18:39.484: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 10:18:39.484: INFO: Found 0 / 1
Jul  5 10:18:40.484: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 10:18:40.484: INFO: Found 1 / 1
Jul  5 10:18:40.484: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  5 10:18:40.487: INFO: Selector matched 1 pods for map[app:redis]
Jul  5 10:18:40.487: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  5 10:18:40.487: INFO: wait on redis-master startup in kubectl-1642 
Jul  5 10:18:40.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 logs redis-master-pd55w redis-master --namespace=kubectl-1642'
Jul  5 10:18:40.617: INFO: stderr: ""
Jul  5 10:18:40.617: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Jul 10:18:39.149 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Jul 10:18:39.149 # Server started, Redis version 3.2.12\n1:M 05 Jul 10:18:39.149 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Jul 10:18:39.149 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jul  5 10:18:40.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1642'
Jul  5 10:18:40.721: INFO: stderr: ""
Jul  5 10:18:40.721: INFO: stdout: "service/rm2 exposed\n"
Jul  5 10:18:40.725: INFO: Service rm2 in namespace kubectl-1642 found.
STEP: exposing service
Jul  5 10:18:42.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1642'
Jul  5 10:18:42.830: INFO: stderr: ""
Jul  5 10:18:42.830: INFO: stdout: "service/rm3 exposed\n"
Jul  5 10:18:42.833: INFO: Service rm3 in namespace kubectl-1642 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:18:44.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1642" for this suite.
Jul  5 10:19:06.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:19:06.939: INFO: namespace kubectl-1642 deletion completed in 22.097198962s

• [SLOW TEST:29.859 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:19:06.940: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul  5 10:19:09.507: INFO: Successfully updated pod "labelsupdate9b6da7b9-0458-4381-b213-7191a6837e0f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:19:13.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4862" for this suite.
Jul  5 10:19:35.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:19:35.637: INFO: namespace downward-api-4862 deletion completed in 22.102831687s

• [SLOW TEST:28.698 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:19:35.637: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-3a8fa7d6-0199-4a2f-810f-2d188b24161e in namespace container-probe-1125
Jul  5 10:19:39.676: INFO: Started pod liveness-3a8fa7d6-0199-4a2f-810f-2d188b24161e in namespace container-probe-1125
STEP: checking the pod's current state and verifying that restartCount is present
Jul  5 10:19:39.679: INFO: Initial restart count of pod liveness-3a8fa7d6-0199-4a2f-810f-2d188b24161e is 0
Jul  5 10:19:57.710: INFO: Restart count of pod container-probe-1125/liveness-3a8fa7d6-0199-4a2f-810f-2d188b24161e is now 1 (18.031314796s elapsed)
Jul  5 10:20:17.744: INFO: Restart count of pod container-probe-1125/liveness-3a8fa7d6-0199-4a2f-810f-2d188b24161e is now 2 (38.065436319s elapsed)
Jul  5 10:20:37.780: INFO: Restart count of pod container-probe-1125/liveness-3a8fa7d6-0199-4a2f-810f-2d188b24161e is now 3 (58.101050159s elapsed)
Jul  5 10:20:57.813: INFO: Restart count of pod container-probe-1125/liveness-3a8fa7d6-0199-4a2f-810f-2d188b24161e is now 4 (1m18.134133214s elapsed)
Jul  5 10:22:07.938: INFO: Restart count of pod container-probe-1125/liveness-3a8fa7d6-0199-4a2f-810f-2d188b24161e is now 5 (2m28.259515278s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:22:07.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1125" for this suite.
Jul  5 10:22:13.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:22:14.054: INFO: namespace container-probe-1125 deletion completed in 6.100713029s

• [SLOW TEST:158.417 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:22:14.055: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul  5 10:22:14.082: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  5 10:22:14.089: INFO: Waiting for terminating namespaces to be deleted...
Jul  5 10:22:14.091: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-1-212.eu-west-3.compute.internal before test
Jul  5 10:22:14.099: INFO: kube-proxy-zpgcv from kube-system started at 2019-07-05 08:50:50 +0000 UTC (1 container statuses recorded)
Jul  5 10:22:14.099: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 10:22:14.099: INFO: canal-5kgvk from kube-system started at 2019-07-05 08:50:50 +0000 UTC (2 container statuses recorded)
Jul  5 10:22:14.099: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 10:22:14.099: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  5 10:22:14.099: INFO: sonobuoy-systemd-logs-daemon-set-e9d729729fbf4cf5-rfc95 from heptio-sonobuoy started at 2019-07-05 08:51:49 +0000 UTC (2 container statuses recorded)
Jul  5 10:22:14.099: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul  5 10:22:14.099: INFO: 	Container systemd-logs ready: true, restart count 1
Jul  5 10:22:14.099: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-05 08:51:44 +0000 UTC (1 container statuses recorded)
Jul  5 10:22:14.099: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  5 10:22:14.099: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-11-115.eu-west-3.compute.internal before test
Jul  5 10:22:14.103: INFO: kube-proxy-h7nqv from kube-system started at 2019-07-05 08:50:49 +0000 UTC (1 container statuses recorded)
Jul  5 10:22:14.103: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 10:22:14.103: INFO: canal-m82pj from kube-system started at 2019-07-05 08:50:49 +0000 UTC (2 container statuses recorded)
Jul  5 10:22:14.103: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 10:22:14.103: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  5 10:22:14.103: INFO: sonobuoy-systemd-logs-daemon-set-e9d729729fbf4cf5-rm8ks from heptio-sonobuoy started at 2019-07-05 08:51:49 +0000 UTC (2 container statuses recorded)
Jul  5 10:22:14.103: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul  5 10:22:14.103: INFO: 	Container systemd-logs ready: true, restart count 1
Jul  5 10:22:14.103: INFO: metrics-server-6964ccc5b-88zkm from kube-system started at 2019-07-05 08:51:07 +0000 UTC (1 container statuses recorded)
Jul  5 10:22:14.103: INFO: 	Container metrics-server ready: true, restart count 0
Jul  5 10:22:14.103: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-6-226.eu-west-3.compute.internal before test
Jul  5 10:22:14.108: INFO: kube-proxy-zc7nm from kube-system started at 2019-07-05 08:50:51 +0000 UTC (1 container statuses recorded)
Jul  5 10:22:14.108: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  5 10:22:14.108: INFO: sonobuoy-e2e-job-dddb5268764f464f from heptio-sonobuoy started at 2019-07-05 08:51:49 +0000 UTC (2 container statuses recorded)
Jul  5 10:22:14.108: INFO: 	Container e2e ready: true, restart count 0
Jul  5 10:22:14.108: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  5 10:22:14.108: INFO: sonobuoy-systemd-logs-daemon-set-e9d729729fbf4cf5-b7svv from heptio-sonobuoy started at 2019-07-05 08:51:49 +0000 UTC (2 container statuses recorded)
Jul  5 10:22:14.108: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul  5 10:22:14.108: INFO: 	Container systemd-logs ready: true, restart count 1
Jul  5 10:22:14.108: INFO: canal-xzlkm from kube-system started at 2019-07-05 08:50:51 +0000 UTC (2 container statuses recorded)
Jul  5 10:22:14.108: INFO: 	Container calico-node ready: true, restart count 0
Jul  5 10:22:14.108: INFO: 	Container kube-flannel ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15ae7bb416ada8bc], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:22:15.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1129" for this suite.
Jul  5 10:22:21.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:22:21.285: INFO: namespace sched-pred-1129 deletion completed in 6.156080459s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.231 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:22:21.286: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0705 10:22:31.373882      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul  5 10:22:31.373: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:22:31.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5856" for this suite.
Jul  5 10:22:37.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:22:37.469: INFO: namespace gc-5856 deletion completed in 6.092621145s

• [SLOW TEST:16.184 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:22:37.470: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul  5 10:22:40.018: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2300 pod-service-account-a2db69be-7390-4552-8296-065137c89566 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul  5 10:22:40.233: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2300 pod-service-account-a2db69be-7390-4552-8296-065137c89566 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul  5 10:22:40.462: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2300 pod-service-account-a2db69be-7390-4552-8296-065137c89566 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:22:40.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2300" for this suite.
Jul  5 10:22:46.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:22:46.789: INFO: namespace svcaccounts-2300 deletion completed in 6.093483596s

• [SLOW TEST:9.319 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:22:46.789: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  5 10:22:46.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7145'
Jul  5 10:22:46.899: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  5 10:22:46.899: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jul  5 10:22:46.906: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jul  5 10:22:46.911: INFO: scanned /root for discovery docs: <nil>
Jul  5 10:22:46.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-7145'
Jul  5 10:23:02.688: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul  5 10:23:02.688: INFO: stdout: "Created e2e-test-nginx-rc-6432d0d26bdbc18976f75c96c681509e\nScaling up e2e-test-nginx-rc-6432d0d26bdbc18976f75c96c681509e from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-6432d0d26bdbc18976f75c96c681509e up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-6432d0d26bdbc18976f75c96c681509e to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jul  5 10:23:02.688: INFO: stdout: "Created e2e-test-nginx-rc-6432d0d26bdbc18976f75c96c681509e\nScaling up e2e-test-nginx-rc-6432d0d26bdbc18976f75c96c681509e from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-6432d0d26bdbc18976f75c96c681509e up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-6432d0d26bdbc18976f75c96c681509e to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jul  5 10:23:02.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-7145'
Jul  5 10:23:02.804: INFO: stderr: ""
Jul  5 10:23:02.804: INFO: stdout: "e2e-test-nginx-rc-6432d0d26bdbc18976f75c96c681509e-jsgfz "
Jul  5 10:23:02.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods e2e-test-nginx-rc-6432d0d26bdbc18976f75c96c681509e-jsgfz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7145'
Jul  5 10:23:02.924: INFO: stderr: ""
Jul  5 10:23:02.924: INFO: stdout: "true"
Jul  5 10:23:02.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 get pods e2e-test-nginx-rc-6432d0d26bdbc18976f75c96c681509e-jsgfz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7145'
Jul  5 10:23:02.995: INFO: stderr: ""
Jul  5 10:23:02.995: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jul  5 10:23:02.995: INFO: e2e-test-nginx-rc-6432d0d26bdbc18976f75c96c681509e-jsgfz is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Jul  5 10:23:02.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-272745911 delete rc e2e-test-nginx-rc --namespace=kubectl-7145'
Jul  5 10:23:03.078: INFO: stderr: ""
Jul  5 10:23:03.078: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:23:03.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7145" for this suite.
Jul  5 10:23:25.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:23:25.198: INFO: namespace kubectl-7145 deletion completed in 22.113801208s

• [SLOW TEST:38.409 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:23:25.198: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Jul  5 10:23:25.747: INFO: created pod pod-service-account-defaultsa
Jul  5 10:23:25.747: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul  5 10:23:25.757: INFO: created pod pod-service-account-mountsa
Jul  5 10:23:25.757: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul  5 10:23:25.763: INFO: created pod pod-service-account-nomountsa
Jul  5 10:23:25.763: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul  5 10:23:25.776: INFO: created pod pod-service-account-defaultsa-mountspec
Jul  5 10:23:25.776: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul  5 10:23:25.782: INFO: created pod pod-service-account-mountsa-mountspec
Jul  5 10:23:25.782: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul  5 10:23:25.790: INFO: created pod pod-service-account-nomountsa-mountspec
Jul  5 10:23:25.790: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul  5 10:23:25.809: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul  5 10:23:25.809: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul  5 10:23:25.816: INFO: created pod pod-service-account-mountsa-nomountspec
Jul  5 10:23:25.816: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul  5 10:23:25.824: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul  5 10:23:25.824: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:23:25.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2296" for this suite.
Jul  5 10:23:31.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:23:31.934: INFO: namespace svcaccounts-2296 deletion completed in 6.100585846s

• [SLOW TEST:6.736 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:23:31.934: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:23:36.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7684" for this suite.
Jul  5 10:23:58.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:23:59.081: INFO: namespace replication-controller-7684 deletion completed in 22.092700897s

• [SLOW TEST:27.147 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:23:59.081: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:24:59.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3579" for this suite.
Jul  5 10:25:21.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:25:21.298: INFO: namespace container-probe-3579 deletion completed in 22.171446829s

• [SLOW TEST:82.217 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:25:21.299: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-8118
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul  5 10:25:21.354: INFO: Found 0 stateful pods, waiting for 3
Jul  5 10:25:31.359: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 10:25:31.359: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 10:25:31.359: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul  5 10:25:31.382: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul  5 10:25:41.415: INFO: Updating stateful set ss2
Jul  5 10:25:41.421: INFO: Waiting for Pod statefulset-8118/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Jul  5 10:25:51.506: INFO: Found 2 stateful pods, waiting for 3
Jul  5 10:26:01.510: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 10:26:01.510: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  5 10:26:01.510: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul  5 10:26:01.532: INFO: Updating stateful set ss2
Jul  5 10:26:01.539: INFO: Waiting for Pod statefulset-8118/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul  5 10:26:11.563: INFO: Updating stateful set ss2
Jul  5 10:26:11.574: INFO: Waiting for StatefulSet statefulset-8118/ss2 to complete update
Jul  5 10:26:11.574: INFO: Waiting for Pod statefulset-8118/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul  5 10:26:21.580: INFO: Deleting all statefulset in ns statefulset-8118
Jul  5 10:26:21.583: INFO: Scaling statefulset ss2 to 0
Jul  5 10:26:41.598: INFO: Waiting for statefulset status.replicas updated to 0
Jul  5 10:26:41.600: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:26:41.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8118" for this suite.
Jul  5 10:26:47.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:26:47.723: INFO: namespace statefulset-8118 deletion completed in 6.102650701s

• [SLOW TEST:86.424 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:26:47.723: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul  5 10:26:47.772: INFO: Create a RollingUpdate DaemonSet
Jul  5 10:26:47.777: INFO: Check that daemon pods launch on every node of the cluster
Jul  5 10:26:47.780: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:47.780: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:47.780: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:47.783: INFO: Number of nodes with available pods: 0
Jul  5 10:26:47.783: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:26:48.790: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:48.790: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:48.790: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:48.793: INFO: Number of nodes with available pods: 0
Jul  5 10:26:48.793: INFO: Node ip-172-31-1-212.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:26:49.790: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:49.790: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:49.790: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:49.795: INFO: Number of nodes with available pods: 1
Jul  5 10:26:49.795: INFO: Node ip-172-31-11-115.eu-west-3.compute.internal is running more than one daemon pod
Jul  5 10:26:50.787: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:50.787: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:50.787: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:50.791: INFO: Number of nodes with available pods: 3
Jul  5 10:26:50.791: INFO: Number of running nodes: 3, number of available pods: 3
Jul  5 10:26:50.791: INFO: Update the DaemonSet to trigger a rollout
Jul  5 10:26:50.799: INFO: Updating DaemonSet daemon-set
Jul  5 10:26:54.815: INFO: Roll back the DaemonSet before rollout is complete
Jul  5 10:26:54.829: INFO: Updating DaemonSet daemon-set
Jul  5 10:26:54.829: INFO: Make sure DaemonSet rollback is complete
Jul  5 10:26:54.832: INFO: Wrong image for pod: daemon-set-66bb2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul  5 10:26:54.832: INFO: Pod daemon-set-66bb2 is not available
Jul  5 10:26:54.838: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:54.838: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:54.838: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:55.842: INFO: Wrong image for pod: daemon-set-66bb2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul  5 10:26:55.842: INFO: Pod daemon-set-66bb2 is not available
Jul  5 10:26:55.845: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:55.845: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:55.845: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:56.842: INFO: Pod daemon-set-hrbqq is not available
Jul  5 10:26:56.845: INFO: DaemonSet pods can't tolerate node ip-172-31-24-174.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:56.845: INFO: DaemonSet pods can't tolerate node ip-172-31-39-12.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul  5 10:26:56.845: INFO: DaemonSet pods can't tolerate node ip-172-31-5-222.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5848, will wait for the garbage collector to delete the pods
Jul  5 10:26:56.910: INFO: Deleting DaemonSet.extensions daemon-set took: 8.373853ms
Jul  5 10:26:57.311: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.205425ms
Jul  5 10:28:19.316: INFO: Number of nodes with available pods: 0
Jul  5 10:28:19.316: INFO: Number of running nodes: 0, number of available pods: 0
Jul  5 10:28:19.319: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5848/daemonsets","resourceVersion":"28129"},"items":null}

Jul  5 10:28:19.321: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5848/pods","resourceVersion":"28129"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:28:19.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5848" for this suite.
Jul  5 10:28:25.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:28:25.441: INFO: namespace daemonsets-5848 deletion completed in 6.106855474s

• [SLOW TEST:97.718 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:28:25.441: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jul  5 10:28:25.469: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Jul  5 10:28:25.759: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul  5 10:28:27.799: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 10:28:29.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 10:28:31.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 10:28:33.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697919305, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  5 10:28:36.626: INFO: Waited 818.828072ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:28:37.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4641" for this suite.
Jul  5 10:28:43.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:28:43.564: INFO: namespace aggregator-4641 deletion completed in 6.176196889s

• [SLOW TEST:18.123 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:28:43.564: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 10:28:43.601: INFO: Waiting up to 5m0s for pod "downwardapi-volume-03e2a149-8333-44f6-a4ad-771aaf0dbeab" in namespace "projected-7250" to be "success or failure"
Jul  5 10:28:43.605: INFO: Pod "downwardapi-volume-03e2a149-8333-44f6-a4ad-771aaf0dbeab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.547326ms
Jul  5 10:28:45.608: INFO: Pod "downwardapi-volume-03e2a149-8333-44f6-a4ad-771aaf0dbeab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007641488s
STEP: Saw pod success
Jul  5 10:28:45.608: INFO: Pod "downwardapi-volume-03e2a149-8333-44f6-a4ad-771aaf0dbeab" satisfied condition "success or failure"
Jul  5 10:28:45.611: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod downwardapi-volume-03e2a149-8333-44f6-a4ad-771aaf0dbeab container client-container: <nil>
STEP: delete the pod
Jul  5 10:28:45.654: INFO: Waiting for pod downwardapi-volume-03e2a149-8333-44f6-a4ad-771aaf0dbeab to disappear
Jul  5 10:28:45.656: INFO: Pod downwardapi-volume-03e2a149-8333-44f6-a4ad-771aaf0dbeab no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:28:45.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7250" for this suite.
Jul  5 10:28:51.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:28:51.757: INFO: namespace projected-7250 deletion completed in 6.098071985s

• [SLOW TEST:8.193 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:28:51.758: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul  5 10:28:51.800: INFO: Waiting up to 5m0s for pod "pod-5001badb-51fb-4bf1-9df1-d3befcea19b0" in namespace "emptydir-758" to be "success or failure"
Jul  5 10:28:51.802: INFO: Pod "pod-5001badb-51fb-4bf1-9df1-d3befcea19b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.73887ms
Jul  5 10:28:53.805: INFO: Pod "pod-5001badb-51fb-4bf1-9df1-d3befcea19b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005644464s
STEP: Saw pod success
Jul  5 10:28:53.805: INFO: Pod "pod-5001badb-51fb-4bf1-9df1-d3befcea19b0" satisfied condition "success or failure"
Jul  5 10:28:53.808: INFO: Trying to get logs from node ip-172-31-11-115.eu-west-3.compute.internal pod pod-5001badb-51fb-4bf1-9df1-d3befcea19b0 container test-container: <nil>
STEP: delete the pod
Jul  5 10:28:53.834: INFO: Waiting for pod pod-5001badb-51fb-4bf1-9df1-d3befcea19b0 to disappear
Jul  5 10:28:53.837: INFO: Pod pod-5001badb-51fb-4bf1-9df1-d3befcea19b0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:28:53.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-758" for this suite.
Jul  5 10:28:59.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:28:59.989: INFO: namespace emptydir-758 deletion completed in 6.149222804s

• [SLOW TEST:8.232 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:28:59.990: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul  5 10:29:00.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bff66c3e-3f36-486b-b937-8cad9cd98358" in namespace "downward-api-4278" to be "success or failure"
Jul  5 10:29:00.034: INFO: Pod "downwardapi-volume-bff66c3e-3f36-486b-b937-8cad9cd98358": Phase="Pending", Reason="", readiness=false. Elapsed: 3.304426ms
Jul  5 10:29:02.037: INFO: Pod "downwardapi-volume-bff66c3e-3f36-486b-b937-8cad9cd98358": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006321578s
STEP: Saw pod success
Jul  5 10:29:02.037: INFO: Pod "downwardapi-volume-bff66c3e-3f36-486b-b937-8cad9cd98358" satisfied condition "success or failure"
Jul  5 10:29:02.040: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod downwardapi-volume-bff66c3e-3f36-486b-b937-8cad9cd98358 container client-container: <nil>
STEP: delete the pod
Jul  5 10:29:02.061: INFO: Waiting for pod downwardapi-volume-bff66c3e-3f36-486b-b937-8cad9cd98358 to disappear
Jul  5 10:29:02.063: INFO: Pod downwardapi-volume-bff66c3e-3f36-486b-b937-8cad9cd98358 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:29:02.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4278" for this suite.
Jul  5 10:29:08.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:29:08.161: INFO: namespace downward-api-4278 deletion completed in 6.094571334s

• [SLOW TEST:8.171 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:29:08.162: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-4ce50350-32f3-49ec-bf67-b46746682585
Jul  5 10:29:08.198: INFO: Pod name my-hostname-basic-4ce50350-32f3-49ec-bf67-b46746682585: Found 0 pods out of 1
Jul  5 10:29:13.202: INFO: Pod name my-hostname-basic-4ce50350-32f3-49ec-bf67-b46746682585: Found 1 pods out of 1
Jul  5 10:29:13.202: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-4ce50350-32f3-49ec-bf67-b46746682585" are running
Jul  5 10:29:13.205: INFO: Pod "my-hostname-basic-4ce50350-32f3-49ec-bf67-b46746682585-tlrcn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-05 10:29:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-05 10:29:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-05 10:29:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-05 10:29:08 +0000 UTC Reason: Message:}])
Jul  5 10:29:13.205: INFO: Trying to dial the pod
Jul  5 10:29:18.214: INFO: Controller my-hostname-basic-4ce50350-32f3-49ec-bf67-b46746682585: Got expected result from replica 1 [my-hostname-basic-4ce50350-32f3-49ec-bf67-b46746682585-tlrcn]: "my-hostname-basic-4ce50350-32f3-49ec-bf67-b46746682585-tlrcn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:29:18.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3579" for this suite.
Jul  5 10:29:24.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:29:24.316: INFO: namespace replication-controller-3579 deletion completed in 6.098846411s

• [SLOW TEST:16.154 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul  5 10:29:24.316: INFO: >>> kubeConfig: /tmp/kubeconfig-272745911
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Jul  5 10:29:24.352: INFO: Waiting up to 5m0s for pod "client-containers-809ef518-10dd-4d0d-83b3-2aa11b3bdca0" in namespace "containers-2473" to be "success or failure"
Jul  5 10:29:24.356: INFO: Pod "client-containers-809ef518-10dd-4d0d-83b3-2aa11b3bdca0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.610153ms
Jul  5 10:29:26.359: INFO: Pod "client-containers-809ef518-10dd-4d0d-83b3-2aa11b3bdca0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006607406s
Jul  5 10:29:28.362: INFO: Pod "client-containers-809ef518-10dd-4d0d-83b3-2aa11b3bdca0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009280866s
STEP: Saw pod success
Jul  5 10:29:28.362: INFO: Pod "client-containers-809ef518-10dd-4d0d-83b3-2aa11b3bdca0" satisfied condition "success or failure"
Jul  5 10:29:28.364: INFO: Trying to get logs from node ip-172-31-1-212.eu-west-3.compute.internal pod client-containers-809ef518-10dd-4d0d-83b3-2aa11b3bdca0 container test-container: <nil>
STEP: delete the pod
Jul  5 10:29:28.381: INFO: Waiting for pod client-containers-809ef518-10dd-4d0d-83b3-2aa11b3bdca0 to disappear
Jul  5 10:29:28.384: INFO: Pod client-containers-809ef518-10dd-4d0d-83b3-2aa11b3bdca0 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul  5 10:29:28.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2473" for this suite.
Jul  5 10:29:34.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  5 10:29:34.484: INFO: namespace containers-2473 deletion completed in 6.096421958s

• [SLOW TEST:10.168 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSJul  5 10:29:34.484: INFO: Running AfterSuite actions on all nodes
Jul  5 10:29:34.484: INFO: Running AfterSuite actions on node 1
Jul  5 10:29:34.484: INFO: Skipping dumping logs from cluster

Ran 215 of 4411 Specs in 5842.896 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4196 Skipped
PASS

Ginkgo ran 1 suite in 1h37m24.708179091s
Test Suite Passed
