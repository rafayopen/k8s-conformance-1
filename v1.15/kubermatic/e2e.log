I0919 13:04:43.849246      15 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-889697622
I0919 13:04:43.849413      15 e2e.go:241] Starting e2e run "96e9650b-c6d4-4375-b956-0d40d4726d0b" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1568898282 - Will randomize all specs
Will run 215 of 4413 specs

Sep 19 13:04:44.172: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:04:44.174: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 19 13:04:44.216: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 19 13:04:44.277: INFO: 24 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 19 13:04:44.277: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Sep 19 13:04:44.277: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 19 13:04:44.292: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Sep 19 13:04:44.292: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep 19 13:04:44.292: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Sep 19 13:04:44.292: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Sep 19 13:04:44.292: INFO: e2e test version: v1.15.3
Sep 19 13:04:44.296: INFO: kube-apiserver version: v1.15.3
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:04:44.296: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename gc
Sep 19 13:04:44.357: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Sep 19 13:04:44.381: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1149
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 13:04:44.602: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"cdd8e2de-bdfa-4594-a442-bbe66ddf1453", Controller:(*bool)(0xc003ab3a46), BlockOwnerDeletion:(*bool)(0xc003ab3a47)}}
Sep 19 13:04:44.623: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4af6d444-f789-4620-8894-889d77ef19d8", Controller:(*bool)(0xc003fd37a6), BlockOwnerDeletion:(*bool)(0xc003fd37a7)}}
Sep 19 13:04:44.643: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"09aef911-ff6f-4b2b-87b5-8b599089e57e", Controller:(*bool)(0xc003ab3c06), BlockOwnerDeletion:(*bool)(0xc003ab3c07)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:04:49.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1149" for this suite.
Sep 19 13:04:55.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:04:55.991: INFO: namespace gc-1149 deletion completed in 6.319400527s

• [SLOW TEST:11.695 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:04:55.991: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3665
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-be690117-9fe1-454b-a33c-040c9a95914c
STEP: Creating a pod to test consume configMaps
Sep 19 13:04:56.188: INFO: Waiting up to 5m0s for pod "pod-configmaps-257db244-8ea6-4f9b-be28-2fd8dbbe0f56" in namespace "configmap-3665" to be "success or failure"
Sep 19 13:04:56.195: INFO: Pod "pod-configmaps-257db244-8ea6-4f9b-be28-2fd8dbbe0f56": Phase="Pending", Reason="", readiness=false. Elapsed: 7.658205ms
Sep 19 13:04:58.202: INFO: Pod "pod-configmaps-257db244-8ea6-4f9b-be28-2fd8dbbe0f56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013946058s
Sep 19 13:05:00.209: INFO: Pod "pod-configmaps-257db244-8ea6-4f9b-be28-2fd8dbbe0f56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020871436s
STEP: Saw pod success
Sep 19 13:05:00.209: INFO: Pod "pod-configmaps-257db244-8ea6-4f9b-be28-2fd8dbbe0f56" satisfied condition "success or failure"
Sep 19 13:05:00.216: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod pod-configmaps-257db244-8ea6-4f9b-be28-2fd8dbbe0f56 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 19 13:05:00.290: INFO: Waiting for pod pod-configmaps-257db244-8ea6-4f9b-be28-2fd8dbbe0f56 to disappear
Sep 19 13:05:00.296: INFO: Pod pod-configmaps-257db244-8ea6-4f9b-be28-2fd8dbbe0f56 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:05:00.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3665" for this suite.
Sep 19 13:05:06.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:05:06.614: INFO: namespace configmap-3665 deletion completed in 6.310072137s

• [SLOW TEST:10.623 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:05:06.615: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7943
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-f70a83c7-b272-46ae-a263-81e2983acd1e in namespace container-probe-7943
Sep 19 13:05:10.840: INFO: Started pod liveness-f70a83c7-b272-46ae-a263-81e2983acd1e in namespace container-probe-7943
STEP: checking the pod's current state and verifying that restartCount is present
Sep 19 13:05:10.848: INFO: Initial restart count of pod liveness-f70a83c7-b272-46ae-a263-81e2983acd1e is 0
Sep 19 13:05:34.946: INFO: Restart count of pod container-probe-7943/liveness-f70a83c7-b272-46ae-a263-81e2983acd1e is now 1 (24.098027428s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:05:34.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7943" for this suite.
Sep 19 13:05:40.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:05:41.256: INFO: namespace container-probe-7943 deletion completed in 6.289049739s

• [SLOW TEST:34.641 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:05:41.257: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2219
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 19 13:05:41.451: INFO: Waiting up to 5m0s for pod "pod-28ab03ea-1fdc-4fd9-b8d7-b45bace03c75" in namespace "emptydir-2219" to be "success or failure"
Sep 19 13:05:41.457: INFO: Pod "pod-28ab03ea-1fdc-4fd9-b8d7-b45bace03c75": Phase="Pending", Reason="", readiness=false. Elapsed: 5.855791ms
Sep 19 13:05:43.464: INFO: Pod "pod-28ab03ea-1fdc-4fd9-b8d7-b45bace03c75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013139696s
Sep 19 13:05:45.473: INFO: Pod "pod-28ab03ea-1fdc-4fd9-b8d7-b45bace03c75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0228247s
STEP: Saw pod success
Sep 19 13:05:45.474: INFO: Pod "pod-28ab03ea-1fdc-4fd9-b8d7-b45bace03c75" satisfied condition "success or failure"
Sep 19 13:05:45.483: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod pod-28ab03ea-1fdc-4fd9-b8d7-b45bace03c75 container test-container: <nil>
STEP: delete the pod
Sep 19 13:05:45.620: INFO: Waiting for pod pod-28ab03ea-1fdc-4fd9-b8d7-b45bace03c75 to disappear
Sep 19 13:05:45.628: INFO: Pod pod-28ab03ea-1fdc-4fd9-b8d7-b45bace03c75 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:05:45.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2219" for this suite.
Sep 19 13:05:51.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:05:52.012: INFO: namespace emptydir-2219 deletion completed in 6.371341872s

• [SLOW TEST:10.756 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:05:52.012: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 19 13:05:57.243: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:05:57.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6937" for this suite.
Sep 19 13:06:03.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:06:03.592: INFO: namespace container-runtime-6937 deletion completed in 6.314412226s

• [SLOW TEST:11.580 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:06:03.593: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6076
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-d0f070a2-63a3-4a7a-8464-5ad0beebb7d8
STEP: Creating a pod to test consume secrets
Sep 19 13:06:03.794: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e8f3bd51-93c0-4cdd-9a16-2737dca0f069" in namespace "projected-6076" to be "success or failure"
Sep 19 13:06:03.802: INFO: Pod "pod-projected-secrets-e8f3bd51-93c0-4cdd-9a16-2737dca0f069": Phase="Pending", Reason="", readiness=false. Elapsed: 7.959655ms
Sep 19 13:06:05.811: INFO: Pod "pod-projected-secrets-e8f3bd51-93c0-4cdd-9a16-2737dca0f069": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016618294s
Sep 19 13:06:07.819: INFO: Pod "pod-projected-secrets-e8f3bd51-93c0-4cdd-9a16-2737dca0f069": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024535265s
STEP: Saw pod success
Sep 19 13:06:07.819: INFO: Pod "pod-projected-secrets-e8f3bd51-93c0-4cdd-9a16-2737dca0f069" satisfied condition "success or failure"
Sep 19 13:06:07.825: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod pod-projected-secrets-e8f3bd51-93c0-4cdd-9a16-2737dca0f069 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 19 13:06:07.866: INFO: Waiting for pod pod-projected-secrets-e8f3bd51-93c0-4cdd-9a16-2737dca0f069 to disappear
Sep 19 13:06:07.872: INFO: Pod pod-projected-secrets-e8f3bd51-93c0-4cdd-9a16-2737dca0f069 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:06:07.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6076" for this suite.
Sep 19 13:06:13.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:06:14.233: INFO: namespace projected-6076 deletion completed in 6.351975174s

• [SLOW TEST:10.640 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:06:14.234: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6080
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-0d4b45b1-72be-4dff-8e03-ff1cc0243def
STEP: Creating a pod to test consume secrets
Sep 19 13:06:14.437: INFO: Waiting up to 5m0s for pod "pod-secrets-8e35e12c-8678-4508-b2a7-d0fabc4f4794" in namespace "secrets-6080" to be "success or failure"
Sep 19 13:06:14.444: INFO: Pod "pod-secrets-8e35e12c-8678-4508-b2a7-d0fabc4f4794": Phase="Pending", Reason="", readiness=false. Elapsed: 7.107552ms
Sep 19 13:06:16.456: INFO: Pod "pod-secrets-8e35e12c-8678-4508-b2a7-d0fabc4f4794": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019060237s
Sep 19 13:06:18.464: INFO: Pod "pod-secrets-8e35e12c-8678-4508-b2a7-d0fabc4f4794": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026590602s
STEP: Saw pod success
Sep 19 13:06:18.464: INFO: Pod "pod-secrets-8e35e12c-8678-4508-b2a7-d0fabc4f4794" satisfied condition "success or failure"
Sep 19 13:06:18.471: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod pod-secrets-8e35e12c-8678-4508-b2a7-d0fabc4f4794 container secret-volume-test: <nil>
STEP: delete the pod
Sep 19 13:06:18.583: INFO: Waiting for pod pod-secrets-8e35e12c-8678-4508-b2a7-d0fabc4f4794 to disappear
Sep 19 13:06:18.591: INFO: Pod pod-secrets-8e35e12c-8678-4508-b2a7-d0fabc4f4794 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:06:18.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6080" for this suite.
Sep 19 13:06:24.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:06:24.896: INFO: namespace secrets-6080 deletion completed in 6.290010922s

• [SLOW TEST:10.663 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:06:24.897: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4511
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 19 13:06:25.125: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-4511,SelfLink:/api/v1/namespaces/watch-4511/configmaps/e2e-watch-test-resource-version,UID:8cd24dcc-1e15-4d99-b249-f2802df92260,ResourceVersion:15388,Generation:0,CreationTimestamp:2019-09-19 13:06:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 19 13:06:25.125: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-4511,SelfLink:/api/v1/namespaces/watch-4511/configmaps/e2e-watch-test-resource-version,UID:8cd24dcc-1e15-4d99-b249-f2802df92260,ResourceVersion:15389,Generation:0,CreationTimestamp:2019-09-19 13:06:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:06:25.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4511" for this suite.
Sep 19 13:06:31.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:06:31.439: INFO: namespace watch-4511 deletion completed in 6.306524923s

• [SLOW TEST:6.542 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:06:31.439: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9913
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep 19 13:06:31.623: INFO: Waiting up to 5m0s for pod "downward-api-982c1b8e-b65b-4e95-a300-5111c4385c1d" in namespace "downward-api-9913" to be "success or failure"
Sep 19 13:06:31.631: INFO: Pod "downward-api-982c1b8e-b65b-4e95-a300-5111c4385c1d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.041609ms
Sep 19 13:06:33.638: INFO: Pod "downward-api-982c1b8e-b65b-4e95-a300-5111c4385c1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014909776s
Sep 19 13:06:35.645: INFO: Pod "downward-api-982c1b8e-b65b-4e95-a300-5111c4385c1d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021828688s
Sep 19 13:06:37.652: INFO: Pod "downward-api-982c1b8e-b65b-4e95-a300-5111c4385c1d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029378879s
Sep 19 13:06:39.668: INFO: Pod "downward-api-982c1b8e-b65b-4e95-a300-5111c4385c1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.044791095s
STEP: Saw pod success
Sep 19 13:06:39.668: INFO: Pod "downward-api-982c1b8e-b65b-4e95-a300-5111c4385c1d" satisfied condition "success or failure"
Sep 19 13:06:39.678: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod downward-api-982c1b8e-b65b-4e95-a300-5111c4385c1d container dapi-container: <nil>
STEP: delete the pod
Sep 19 13:06:39.755: INFO: Waiting for pod downward-api-982c1b8e-b65b-4e95-a300-5111c4385c1d to disappear
Sep 19 13:06:39.762: INFO: Pod downward-api-982c1b8e-b65b-4e95-a300-5111c4385c1d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:06:39.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9913" for this suite.
Sep 19 13:06:45.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:06:46.130: INFO: namespace downward-api-9913 deletion completed in 6.359397563s

• [SLOW TEST:14.690 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:06:46.130: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1686
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1686
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-1686
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1686
Sep 19 13:06:46.353: INFO: Found 0 stateful pods, waiting for 1
Sep 19 13:06:56.365: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 19 13:06:56.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-1686 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 19 13:06:57.254: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 19 13:06:57.254: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 19 13:06:57.254: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 19 13:06:57.261: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 19 13:07:07.270: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 19 13:07:07.270: INFO: Waiting for statefulset status.replicas updated to 0
Sep 19 13:07:07.304: INFO: POD   NODE                                           PHASE    GRACE  CONDITIONS
Sep 19 13:07:07.304: INFO: ss-0  ip-172-31-33-83.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:06:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:06:46 +0000 UTC  }]
Sep 19 13:07:07.304: INFO: ss-1                                                 Pending         []
Sep 19 13:07:07.304: INFO: 
Sep 19 13:07:07.304: INFO: StatefulSet ss has not reached scale 3, at 2
Sep 19 13:07:08.311: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991266833s
Sep 19 13:07:09.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984113452s
Sep 19 13:07:10.326: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977318295s
Sep 19 13:07:11.333: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.968836966s
Sep 19 13:07:12.340: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962069854s
Sep 19 13:07:13.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.954519387s
Sep 19 13:07:14.358: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.947278018s
Sep 19 13:07:15.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.936586636s
Sep 19 13:07:16.376: INFO: Verifying statefulset ss doesn't scale past 3 for another 927.395248ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1686
Sep 19 13:07:17.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-1686 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 19 13:07:17.967: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 19 13:07:17.967: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 19 13:07:17.967: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 19 13:07:17.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-1686 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 19 13:07:18.635: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 19 13:07:18.635: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 19 13:07:18.635: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 19 13:07:18.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-1686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 19 13:07:19.295: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 19 13:07:19.295: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 19 13:07:19.295: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 19 13:07:19.301: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 19 13:07:19.301: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 19 13:07:19.301: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 19 13:07:19.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-1686 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 19 13:07:19.948: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 19 13:07:19.948: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 19 13:07:19.948: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 19 13:07:19.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-1686 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 19 13:07:20.773: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 19 13:07:20.773: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 19 13:07:20.773: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 19 13:07:20.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-1686 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 19 13:07:21.421: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 19 13:07:21.421: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 19 13:07:21.421: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 19 13:07:21.421: INFO: Waiting for statefulset status.replicas updated to 0
Sep 19 13:07:21.428: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Sep 19 13:07:31.451: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 19 13:07:31.451: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 19 13:07:31.451: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 19 13:07:31.479: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Sep 19 13:07:31.479: INFO: ss-0  ip-172-31-33-83.eu-central-1.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:06:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:06:46 +0000 UTC  }]
Sep 19 13:07:31.479: INFO: ss-1  ip-172-31-46-204.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  }]
Sep 19 13:07:31.479: INFO: ss-2  ip-172-31-36-147.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  }]
Sep 19 13:07:31.479: INFO: 
Sep 19 13:07:31.479: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 19 13:07:32.488: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Sep 19 13:07:32.488: INFO: ss-0  ip-172-31-33-83.eu-central-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:06:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:06:46 +0000 UTC  }]
Sep 19 13:07:32.488: INFO: ss-1  ip-172-31-46-204.eu-central-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  }]
Sep 19 13:07:32.488: INFO: ss-2  ip-172-31-36-147.eu-central-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  }]
Sep 19 13:07:32.488: INFO: 
Sep 19 13:07:32.488: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 19 13:07:33.496: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Sep 19 13:07:33.496: INFO: ss-0  ip-172-31-33-83.eu-central-1.compute.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:06:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:06:46 +0000 UTC  }]
Sep 19 13:07:33.496: INFO: ss-1  ip-172-31-46-204.eu-central-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  }]
Sep 19 13:07:33.496: INFO: ss-2  ip-172-31-36-147.eu-central-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  }]
Sep 19 13:07:33.496: INFO: 
Sep 19 13:07:33.496: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 19 13:07:34.503: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Sep 19 13:07:34.503: INFO: ss-1  ip-172-31-46-204.eu-central-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  }]
Sep 19 13:07:34.503: INFO: ss-2  ip-172-31-36-147.eu-central-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  }]
Sep 19 13:07:34.503: INFO: 
Sep 19 13:07:34.503: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 19 13:07:35.510: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Sep 19 13:07:35.510: INFO: ss-1  ip-172-31-46-204.eu-central-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  }]
Sep 19 13:07:35.510: INFO: 
Sep 19 13:07:35.510: INFO: StatefulSet ss has not reached scale 0, at 1
Sep 19 13:07:36.518: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Sep 19 13:07:36.518: INFO: ss-1  ip-172-31-46-204.eu-central-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  }]
Sep 19 13:07:36.518: INFO: 
Sep 19 13:07:36.518: INFO: StatefulSet ss has not reached scale 0, at 1
Sep 19 13:07:37.525: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Sep 19 13:07:37.525: INFO: ss-1  ip-172-31-46-204.eu-central-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  }]
Sep 19 13:07:37.526: INFO: 
Sep 19 13:07:37.526: INFO: StatefulSet ss has not reached scale 0, at 1
Sep 19 13:07:38.537: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Sep 19 13:07:38.537: INFO: ss-1  ip-172-31-46-204.eu-central-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:07:07 +0000 UTC  }]
Sep 19 13:07:38.537: INFO: 
Sep 19 13:07:38.537: INFO: StatefulSet ss has not reached scale 0, at 1
Sep 19 13:07:39.543: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.933879652s
Sep 19 13:07:40.550: INFO: Verifying statefulset ss doesn't scale past 0 for another 927.498566ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1686
Sep 19 13:07:41.561: INFO: Scaling statefulset ss to 0
Sep 19 13:07:41.585: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep 19 13:07:41.592: INFO: Deleting all statefulset in ns statefulset-1686
Sep 19 13:07:41.598: INFO: Scaling statefulset ss to 0
Sep 19 13:07:41.619: INFO: Waiting for statefulset status.replicas updated to 0
Sep 19 13:07:41.627: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:07:41.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1686" for this suite.
Sep 19 13:07:47.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:07:47.960: INFO: namespace statefulset-1686 deletion completed in 6.2899812s

• [SLOW TEST:61.831 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:07:47.960: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8469
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:08:12.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8469" for this suite.
Sep 19 13:08:18.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:08:18.870: INFO: namespace container-runtime-8469 deletion completed in 6.337221893s

• [SLOW TEST:30.910 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:08:18.871: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7507
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-875g
STEP: Creating a pod to test atomic-volume-subpath
Sep 19 13:08:19.081: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-875g" in namespace "subpath-7507" to be "success or failure"
Sep 19 13:08:19.089: INFO: Pod "pod-subpath-test-configmap-875g": Phase="Pending", Reason="", readiness=false. Elapsed: 8.180077ms
Sep 19 13:08:21.097: INFO: Pod "pod-subpath-test-configmap-875g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015768418s
Sep 19 13:08:23.107: INFO: Pod "pod-subpath-test-configmap-875g": Phase="Running", Reason="", readiness=true. Elapsed: 4.026149053s
Sep 19 13:08:25.114: INFO: Pod "pod-subpath-test-configmap-875g": Phase="Running", Reason="", readiness=true. Elapsed: 6.03272966s
Sep 19 13:08:27.121: INFO: Pod "pod-subpath-test-configmap-875g": Phase="Running", Reason="", readiness=true. Elapsed: 8.039249978s
Sep 19 13:08:29.127: INFO: Pod "pod-subpath-test-configmap-875g": Phase="Running", Reason="", readiness=true. Elapsed: 10.046029482s
Sep 19 13:08:31.134: INFO: Pod "pod-subpath-test-configmap-875g": Phase="Running", Reason="", readiness=true. Elapsed: 12.052830631s
Sep 19 13:08:33.141: INFO: Pod "pod-subpath-test-configmap-875g": Phase="Running", Reason="", readiness=true. Elapsed: 14.060132618s
Sep 19 13:08:35.149: INFO: Pod "pod-subpath-test-configmap-875g": Phase="Running", Reason="", readiness=true. Elapsed: 16.067510394s
Sep 19 13:08:37.160: INFO: Pod "pod-subpath-test-configmap-875g": Phase="Running", Reason="", readiness=true. Elapsed: 18.078982556s
Sep 19 13:08:39.167: INFO: Pod "pod-subpath-test-configmap-875g": Phase="Running", Reason="", readiness=true. Elapsed: 20.085542624s
Sep 19 13:08:41.174: INFO: Pod "pod-subpath-test-configmap-875g": Phase="Running", Reason="", readiness=true. Elapsed: 22.092513186s
Sep 19 13:08:43.189: INFO: Pod "pod-subpath-test-configmap-875g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.107677238s
STEP: Saw pod success
Sep 19 13:08:43.189: INFO: Pod "pod-subpath-test-configmap-875g" satisfied condition "success or failure"
Sep 19 13:08:43.197: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod pod-subpath-test-configmap-875g container test-container-subpath-configmap-875g: <nil>
STEP: delete the pod
Sep 19 13:08:43.239: INFO: Waiting for pod pod-subpath-test-configmap-875g to disappear
Sep 19 13:08:43.250: INFO: Pod pod-subpath-test-configmap-875g no longer exists
STEP: Deleting pod pod-subpath-test-configmap-875g
Sep 19 13:08:43.250: INFO: Deleting pod "pod-subpath-test-configmap-875g" in namespace "subpath-7507"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:08:43.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7507" for this suite.
Sep 19 13:08:49.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:08:49.558: INFO: namespace subpath-7507 deletion completed in 6.287696592s

• [SLOW TEST:30.687 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:08:49.558: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1742
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1742, will wait for the garbage collector to delete the pods
Sep 19 13:08:53.814: INFO: Deleting Job.batch foo took: 13.356824ms
Sep 19 13:08:54.215: INFO: Terminating Job.batch foo pods took: 400.186055ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:09:35.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1742" for this suite.
Sep 19 13:09:41.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:09:41.439: INFO: namespace job-1742 deletion completed in 6.305838147s

• [SLOW TEST:51.881 seconds]
[sig-apps] Job
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:09:41.439: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7227
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 13:09:41.637: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff19b813-ee2b-4f94-91f3-8bbd7f025bf4" in namespace "downward-api-7227" to be "success or failure"
Sep 19 13:09:41.644: INFO: Pod "downwardapi-volume-ff19b813-ee2b-4f94-91f3-8bbd7f025bf4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.628656ms
Sep 19 13:09:43.652: INFO: Pod "downwardapi-volume-ff19b813-ee2b-4f94-91f3-8bbd7f025bf4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014525353s
Sep 19 13:09:45.659: INFO: Pod "downwardapi-volume-ff19b813-ee2b-4f94-91f3-8bbd7f025bf4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021753393s
STEP: Saw pod success
Sep 19 13:09:45.659: INFO: Pod "downwardapi-volume-ff19b813-ee2b-4f94-91f3-8bbd7f025bf4" satisfied condition "success or failure"
Sep 19 13:09:45.666: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod downwardapi-volume-ff19b813-ee2b-4f94-91f3-8bbd7f025bf4 container client-container: <nil>
STEP: delete the pod
Sep 19 13:09:45.696: INFO: Waiting for pod downwardapi-volume-ff19b813-ee2b-4f94-91f3-8bbd7f025bf4 to disappear
Sep 19 13:09:45.702: INFO: Pod downwardapi-volume-ff19b813-ee2b-4f94-91f3-8bbd7f025bf4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:09:45.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7227" for this suite.
Sep 19 13:09:51.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:09:52.039: INFO: namespace downward-api-7227 deletion completed in 6.328874752s

• [SLOW TEST:10.600 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:09:52.039: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-539
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:09:52.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-539" for this suite.
Sep 19 13:10:14.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:10:14.549: INFO: namespace kubelet-test-539 deletion completed in 22.301306163s

• [SLOW TEST:22.510 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:10:14.550: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 13:10:36.768: INFO: Container started at 2019-09-19 13:10:17 +0000 UTC, pod became ready at 2019-09-19 13:10:36 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:10:36.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-344" for this suite.
Sep 19 13:10:58.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:10:59.066: INFO: namespace container-probe-344 deletion completed in 22.290481396s

• [SLOW TEST:44.516 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:10:59.066: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-5388
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-5388
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5388
STEP: Deleting pre-stop pod
Sep 19 13:11:12.410: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:11:12.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5388" for this suite.
Sep 19 13:11:50.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:11:50.744: INFO: namespace prestop-5388 deletion completed in 38.312044979s

• [SLOW TEST:51.678 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:11:50.745: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9716
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 19 13:11:54.029: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:11:54.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9716" for this suite.
Sep 19 13:12:00.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:12:00.370: INFO: namespace container-runtime-9716 deletion completed in 6.302208244s

• [SLOW TEST:9.624 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:12:00.370: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8280
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 19 13:12:08.646: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 19 13:12:08.655: INFO: Pod pod-with-poststart-http-hook still exists
Sep 19 13:12:10.655: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 19 13:12:10.664: INFO: Pod pod-with-poststart-http-hook still exists
Sep 19 13:12:12.655: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 19 13:12:12.662: INFO: Pod pod-with-poststart-http-hook still exists
Sep 19 13:12:14.655: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 19 13:12:14.666: INFO: Pod pod-with-poststart-http-hook still exists
Sep 19 13:12:16.655: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 19 13:12:16.662: INFO: Pod pod-with-poststart-http-hook still exists
Sep 19 13:12:18.655: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 19 13:12:18.665: INFO: Pod pod-with-poststart-http-hook still exists
Sep 19 13:12:20.655: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 19 13:12:20.669: INFO: Pod pod-with-poststart-http-hook still exists
Sep 19 13:12:22.655: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 19 13:12:22.666: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:12:22.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8280" for this suite.
Sep 19 13:12:44.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:12:44.974: INFO: namespace container-lifecycle-hook-8280 deletion completed in 22.299971686s

• [SLOW TEST:44.604 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:12:44.974: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2488
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Sep 19 13:12:45.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 create -f - --namespace=kubectl-2488'
Sep 19 13:12:45.416: INFO: stderr: ""
Sep 19 13:12:45.416: INFO: stdout: "pod/pause created\n"
Sep 19 13:12:45.416: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 19 13:12:45.416: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2488" to be "running and ready"
Sep 19 13:12:45.422: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.794686ms
Sep 19 13:12:47.433: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.016617286s
Sep 19 13:12:47.433: INFO: Pod "pause" satisfied condition "running and ready"
Sep 19 13:12:47.433: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 19 13:12:47.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 label pods pause testing-label=testing-label-value --namespace=kubectl-2488'
Sep 19 13:12:47.588: INFO: stderr: ""
Sep 19 13:12:47.588: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 19 13:12:47.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pod pause -L testing-label --namespace=kubectl-2488'
Sep 19 13:12:47.714: INFO: stderr: ""
Sep 19 13:12:47.714: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 19 13:12:47.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 label pods pause testing-label- --namespace=kubectl-2488'
Sep 19 13:12:47.795: INFO: stderr: ""
Sep 19 13:12:47.795: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 19 13:12:47.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pod pause -L testing-label --namespace=kubectl-2488'
Sep 19 13:12:47.874: INFO: stderr: ""
Sep 19 13:12:47.874: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Sep 19 13:12:47.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete --grace-period=0 --force -f - --namespace=kubectl-2488'
Sep 19 13:12:47.966: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 19 13:12:47.966: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 19 13:12:47.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get rc,svc -l name=pause --no-headers --namespace=kubectl-2488'
Sep 19 13:12:48.054: INFO: stderr: "No resources found.\n"
Sep 19 13:12:48.054: INFO: stdout: ""
Sep 19 13:12:48.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -l name=pause --namespace=kubectl-2488 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 19 13:12:48.134: INFO: stderr: ""
Sep 19 13:12:48.134: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:12:48.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2488" for this suite.
Sep 19 13:12:54.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:12:54.456: INFO: namespace kubectl-2488 deletion completed in 6.303213027s

• [SLOW TEST:9.482 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:12:54.456: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2477.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2477.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 19 13:13:09.385: INFO: DNS probes using dns-2477/dns-test-d26895db-da61-4fca-915d-18a6c3d21ae1 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:13:09.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2477" for this suite.
Sep 19 13:13:15.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:13:15.715: INFO: namespace dns-2477 deletion completed in 6.303730941s

• [SLOW TEST:21.259 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:13:15.716: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5132
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-e9f37ec2-a71e-4f1e-b32d-81d096dd186e
STEP: Creating a pod to test consume configMaps
Sep 19 13:13:15.911: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3177a461-8da3-43b8-bebe-3b153cbd4a26" in namespace "projected-5132" to be "success or failure"
Sep 19 13:13:15.921: INFO: Pod "pod-projected-configmaps-3177a461-8da3-43b8-bebe-3b153cbd4a26": Phase="Pending", Reason="", readiness=false. Elapsed: 9.947917ms
Sep 19 13:13:17.927: INFO: Pod "pod-projected-configmaps-3177a461-8da3-43b8-bebe-3b153cbd4a26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016449662s
Sep 19 13:13:19.934: INFO: Pod "pod-projected-configmaps-3177a461-8da3-43b8-bebe-3b153cbd4a26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02336622s
STEP: Saw pod success
Sep 19 13:13:19.934: INFO: Pod "pod-projected-configmaps-3177a461-8da3-43b8-bebe-3b153cbd4a26" satisfied condition "success or failure"
Sep 19 13:13:19.941: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod pod-projected-configmaps-3177a461-8da3-43b8-bebe-3b153cbd4a26 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 19 13:13:19.988: INFO: Waiting for pod pod-projected-configmaps-3177a461-8da3-43b8-bebe-3b153cbd4a26 to disappear
Sep 19 13:13:19.994: INFO: Pod pod-projected-configmaps-3177a461-8da3-43b8-bebe-3b153cbd4a26 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:13:19.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5132" for this suite.
Sep 19 13:13:26.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:13:26.323: INFO: namespace projected-5132 deletion completed in 6.322385294s

• [SLOW TEST:10.608 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:13:26.324: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-653
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-653
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 19 13:13:26.497: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 19 13:13:54.709: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.8.8 8081 | grep -v '^\s*$'] Namespace:pod-network-test-653 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:13:54.709: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:13:56.213: INFO: Found all expected endpoints: [netserver-0]
Sep 19 13:13:56.220: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.12.11 8081 | grep -v '^\s*$'] Namespace:pod-network-test-653 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:13:56.220: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:13:57.737: INFO: Found all expected endpoints: [netserver-1]
Sep 19 13:13:57.744: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.11.9 8081 | grep -v '^\s*$'] Namespace:pod-network-test-653 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:13:57.744: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:13:59.262: INFO: Found all expected endpoints: [netserver-2]
Sep 19 13:13:59.269: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.10.11 8081 | grep -v '^\s*$'] Namespace:pod-network-test-653 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:13:59.269: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:14:00.788: INFO: Found all expected endpoints: [netserver-3]
Sep 19 13:14:00.795: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.9.3 8081 | grep -v '^\s*$'] Namespace:pod-network-test-653 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:14:00.795: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:14:02.373: INFO: Found all expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:14:02.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-653" for this suite.
Sep 19 13:14:26.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:14:26.710: INFO: namespace pod-network-test-653 deletion completed in 24.329953807s

• [SLOW TEST:60.387 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:14:26.711: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4347
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Sep 19 13:14:26.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 create -f - --namespace=kubectl-4347'
Sep 19 13:14:27.069: INFO: stderr: ""
Sep 19 13:14:27.069: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 19 13:14:27.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4347'
Sep 19 13:14:27.154: INFO: stderr: ""
Sep 19 13:14:27.154: INFO: stdout: "update-demo-nautilus-9z276 update-demo-nautilus-gkpx8 "
Sep 19 13:14:27.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-9z276 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4347'
Sep 19 13:14:27.230: INFO: stderr: ""
Sep 19 13:14:27.230: INFO: stdout: ""
Sep 19 13:14:27.230: INFO: update-demo-nautilus-9z276 is created but not running
Sep 19 13:14:32.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4347'
Sep 19 13:14:32.312: INFO: stderr: ""
Sep 19 13:14:32.312: INFO: stdout: "update-demo-nautilus-9z276 update-demo-nautilus-gkpx8 "
Sep 19 13:14:32.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-9z276 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4347'
Sep 19 13:14:32.396: INFO: stderr: ""
Sep 19 13:14:32.396: INFO: stdout: "true"
Sep 19 13:14:32.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-9z276 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4347'
Sep 19 13:14:32.479: INFO: stderr: ""
Sep 19 13:14:32.479: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 19 13:14:32.479: INFO: validating pod update-demo-nautilus-9z276
Sep 19 13:14:32.578: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 19 13:14:32.578: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 19 13:14:32.578: INFO: update-demo-nautilus-9z276 is verified up and running
Sep 19 13:14:32.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-gkpx8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4347'
Sep 19 13:14:32.653: INFO: stderr: ""
Sep 19 13:14:32.653: INFO: stdout: "true"
Sep 19 13:14:32.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-gkpx8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4347'
Sep 19 13:14:32.738: INFO: stderr: ""
Sep 19 13:14:32.738: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 19 13:14:32.738: INFO: validating pod update-demo-nautilus-gkpx8
Sep 19 13:14:32.835: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 19 13:14:32.835: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 19 13:14:32.835: INFO: update-demo-nautilus-gkpx8 is verified up and running
STEP: scaling down the replication controller
Sep 19 13:14:32.836: INFO: scanned /root for discovery docs: <nil>
Sep 19 13:14:32.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-4347'
Sep 19 13:14:33.960: INFO: stderr: ""
Sep 19 13:14:33.960: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 19 13:14:33.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4347'
Sep 19 13:14:34.042: INFO: stderr: ""
Sep 19 13:14:34.042: INFO: stdout: "update-demo-nautilus-9z276 update-demo-nautilus-gkpx8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 19 13:14:39.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4347'
Sep 19 13:14:39.122: INFO: stderr: ""
Sep 19 13:14:39.122: INFO: stdout: "update-demo-nautilus-gkpx8 "
Sep 19 13:14:39.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-gkpx8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4347'
Sep 19 13:14:39.203: INFO: stderr: ""
Sep 19 13:14:39.203: INFO: stdout: "true"
Sep 19 13:14:39.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-gkpx8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4347'
Sep 19 13:14:39.284: INFO: stderr: ""
Sep 19 13:14:39.284: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 19 13:14:39.284: INFO: validating pod update-demo-nautilus-gkpx8
Sep 19 13:14:39.336: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 19 13:14:39.336: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 19 13:14:39.336: INFO: update-demo-nautilus-gkpx8 is verified up and running
STEP: scaling up the replication controller
Sep 19 13:14:39.338: INFO: scanned /root for discovery docs: <nil>
Sep 19 13:14:39.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-4347'
Sep 19 13:14:40.446: INFO: stderr: ""
Sep 19 13:14:40.446: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 19 13:14:40.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4347'
Sep 19 13:14:40.528: INFO: stderr: ""
Sep 19 13:14:40.528: INFO: stdout: "update-demo-nautilus-f2krg update-demo-nautilus-gkpx8 "
Sep 19 13:14:40.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-f2krg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4347'
Sep 19 13:14:40.613: INFO: stderr: ""
Sep 19 13:14:40.613: INFO: stdout: ""
Sep 19 13:14:40.613: INFO: update-demo-nautilus-f2krg is created but not running
Sep 19 13:14:45.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4347'
Sep 19 13:14:45.700: INFO: stderr: ""
Sep 19 13:14:45.700: INFO: stdout: "update-demo-nautilus-f2krg update-demo-nautilus-gkpx8 "
Sep 19 13:14:45.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-f2krg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4347'
Sep 19 13:14:45.788: INFO: stderr: ""
Sep 19 13:14:45.788: INFO: stdout: "true"
Sep 19 13:14:45.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-f2krg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4347'
Sep 19 13:14:45.869: INFO: stderr: ""
Sep 19 13:14:45.869: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 19 13:14:45.869: INFO: validating pod update-demo-nautilus-f2krg
Sep 19 13:14:45.969: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 19 13:14:45.969: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 19 13:14:45.969: INFO: update-demo-nautilus-f2krg is verified up and running
Sep 19 13:14:45.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-gkpx8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4347'
Sep 19 13:14:46.046: INFO: stderr: ""
Sep 19 13:14:46.046: INFO: stdout: "true"
Sep 19 13:14:46.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-gkpx8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4347'
Sep 19 13:14:46.121: INFO: stderr: ""
Sep 19 13:14:46.121: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 19 13:14:46.121: INFO: validating pod update-demo-nautilus-gkpx8
Sep 19 13:14:46.142: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 19 13:14:46.142: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 19 13:14:46.142: INFO: update-demo-nautilus-gkpx8 is verified up and running
STEP: using delete to clean up resources
Sep 19 13:14:46.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete --grace-period=0 --force -f - --namespace=kubectl-4347'
Sep 19 13:14:46.234: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 19 13:14:46.234: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 19 13:14:46.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4347'
Sep 19 13:14:46.311: INFO: stderr: "No resources found.\n"
Sep 19 13:14:46.311: INFO: stdout: ""
Sep 19 13:14:46.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -l name=update-demo --namespace=kubectl-4347 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 19 13:14:46.405: INFO: stderr: ""
Sep 19 13:14:46.405: INFO: stdout: "update-demo-nautilus-f2krg\nupdate-demo-nautilus-gkpx8\n"
Sep 19 13:14:46.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4347'
Sep 19 13:14:46.987: INFO: stderr: "No resources found.\n"
Sep 19 13:14:46.988: INFO: stdout: ""
Sep 19 13:14:46.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -l name=update-demo --namespace=kubectl-4347 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 19 13:14:47.071: INFO: stderr: ""
Sep 19 13:14:47.071: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:14:47.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4347" for this suite.
Sep 19 13:15:09.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:15:09.378: INFO: namespace kubectl-4347 deletion completed in 22.296752152s

• [SLOW TEST:42.666 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:15:09.378: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6977
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-6977/configmap-test-bb0eca02-1cc6-4647-8ecf-b73ce2f270d6
STEP: Creating a pod to test consume configMaps
Sep 19 13:15:09.573: INFO: Waiting up to 5m0s for pod "pod-configmaps-b2255e8b-026a-482a-be4d-e697f8fc92e7" in namespace "configmap-6977" to be "success or failure"
Sep 19 13:15:09.583: INFO: Pod "pod-configmaps-b2255e8b-026a-482a-be4d-e697f8fc92e7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.143164ms
Sep 19 13:15:11.591: INFO: Pod "pod-configmaps-b2255e8b-026a-482a-be4d-e697f8fc92e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017479029s
STEP: Saw pod success
Sep 19 13:15:11.591: INFO: Pod "pod-configmaps-b2255e8b-026a-482a-be4d-e697f8fc92e7" satisfied condition "success or failure"
Sep 19 13:15:11.597: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod pod-configmaps-b2255e8b-026a-482a-be4d-e697f8fc92e7 container env-test: <nil>
STEP: delete the pod
Sep 19 13:15:11.633: INFO: Waiting for pod pod-configmaps-b2255e8b-026a-482a-be4d-e697f8fc92e7 to disappear
Sep 19 13:15:11.643: INFO: Pod pod-configmaps-b2255e8b-026a-482a-be4d-e697f8fc92e7 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:15:11.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6977" for this suite.
Sep 19 13:15:17.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:15:17.941: INFO: namespace configmap-6977 deletion completed in 6.285296829s

• [SLOW TEST:8.563 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:15:17.941: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8680
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-x5lsh in namespace proxy-8680
I0919 13:15:18.143452      15 runners.go:180] Created replication controller with name: proxy-service-x5lsh, namespace: proxy-8680, replica count: 1
I0919 13:15:19.193784      15 runners.go:180] proxy-service-x5lsh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0919 13:15:20.193969      15 runners.go:180] proxy-service-x5lsh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0919 13:15:21.194179      15 runners.go:180] proxy-service-x5lsh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0919 13:15:22.194363      15 runners.go:180] proxy-service-x5lsh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0919 13:15:23.194862      15 runners.go:180] proxy-service-x5lsh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0919 13:15:24.197089      15 runners.go:180] proxy-service-x5lsh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0919 13:15:25.197237      15 runners.go:180] proxy-service-x5lsh Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 19 13:15:25.204: INFO: setup took 7.087885022s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 19 13:15:25.237: INFO: (0) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 32.2384ms)
Sep 19 13:15:25.237: INFO: (0) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 32.359105ms)
Sep 19 13:15:25.237: INFO: (0) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 32.559062ms)
Sep 19 13:15:25.237: INFO: (0) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 32.413964ms)
Sep 19 13:15:25.237: INFO: (0) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 32.465275ms)
Sep 19 13:15:25.237: INFO: (0) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 32.103566ms)
Sep 19 13:15:25.237: INFO: (0) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 32.255994ms)
Sep 19 13:15:25.237: INFO: (0) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 32.555285ms)
Sep 19 13:15:25.237: INFO: (0) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 32.175659ms)
Sep 19 13:15:25.275: INFO: (0) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 70.150976ms)
Sep 19 13:15:25.277: INFO: (0) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 71.997409ms)
Sep 19 13:15:25.283: INFO: (0) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 78.747108ms)
Sep 19 13:15:25.285: INFO: (0) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 80.233338ms)
Sep 19 13:15:25.285: INFO: (0) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 80.256619ms)
Sep 19 13:15:25.287: INFO: (0) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 82.462265ms)
Sep 19 13:15:25.330: INFO: (0) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 125.850055ms)
Sep 19 13:15:25.344: INFO: (1) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 13.096782ms)
Sep 19 13:15:25.344: INFO: (1) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 13.415114ms)
Sep 19 13:15:25.345: INFO: (1) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 13.610362ms)
Sep 19 13:15:25.345: INFO: (1) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 13.641559ms)
Sep 19 13:15:25.345: INFO: (1) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 13.814401ms)
Sep 19 13:15:25.345: INFO: (1) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 13.957305ms)
Sep 19 13:15:25.345: INFO: (1) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 14.229594ms)
Sep 19 13:15:25.345: INFO: (1) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 14.229583ms)
Sep 19 13:15:25.346: INFO: (1) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 15.532506ms)
Sep 19 13:15:25.346: INFO: (1) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 15.29002ms)
Sep 19 13:15:25.351: INFO: (1) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 20.235449ms)
Sep 19 13:15:25.351: INFO: (1) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 20.552853ms)
Sep 19 13:15:25.351: INFO: (1) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 20.248052ms)
Sep 19 13:15:25.351: INFO: (1) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 20.492066ms)
Sep 19 13:15:25.353: INFO: (1) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 21.762067ms)
Sep 19 13:15:25.353: INFO: (1) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 21.784258ms)
Sep 19 13:15:25.368: INFO: (2) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 15.1422ms)
Sep 19 13:15:25.369: INFO: (2) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 15.866519ms)
Sep 19 13:15:25.369: INFO: (2) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 15.83058ms)
Sep 19 13:15:25.369: INFO: (2) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 16.030716ms)
Sep 19 13:15:25.369: INFO: (2) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 15.881224ms)
Sep 19 13:15:25.369: INFO: (2) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 15.622043ms)
Sep 19 13:15:25.369: INFO: (2) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 16.065877ms)
Sep 19 13:15:25.369: INFO: (2) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 15.921684ms)
Sep 19 13:15:25.373: INFO: (2) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 19.914154ms)
Sep 19 13:15:25.373: INFO: (2) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 20.139219ms)
Sep 19 13:15:25.373: INFO: (2) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 20.17707ms)
Sep 19 13:15:25.373: INFO: (2) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 20.394913ms)
Sep 19 13:15:25.374: INFO: (2) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 20.460962ms)
Sep 19 13:15:25.374: INFO: (2) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 20.654963ms)
Sep 19 13:15:25.376: INFO: (2) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 22.576702ms)
Sep 19 13:15:25.418: INFO: (2) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 64.454173ms)
Sep 19 13:15:25.439: INFO: (3) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 20.433251ms)
Sep 19 13:15:25.439: INFO: (3) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 20.239466ms)
Sep 19 13:15:25.439: INFO: (3) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 20.15587ms)
Sep 19 13:15:25.439: INFO: (3) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 20.805573ms)
Sep 19 13:15:25.439: INFO: (3) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 21.183882ms)
Sep 19 13:15:25.439: INFO: (3) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 21.246286ms)
Sep 19 13:15:25.440: INFO: (3) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 21.34328ms)
Sep 19 13:15:25.440: INFO: (3) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 21.577571ms)
Sep 19 13:15:25.440: INFO: (3) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 21.395859ms)
Sep 19 13:15:25.440: INFO: (3) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 21.583627ms)
Sep 19 13:15:25.440: INFO: (3) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 21.406325ms)
Sep 19 13:15:25.481: INFO: (3) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 62.1479ms)
Sep 19 13:15:25.481: INFO: (3) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 62.87539ms)
Sep 19 13:15:25.481: INFO: (3) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 62.960892ms)
Sep 19 13:15:25.484: INFO: (3) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 65.189514ms)
Sep 19 13:15:25.489: INFO: (3) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 70.585994ms)
Sep 19 13:15:25.504: INFO: (4) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 14.790277ms)
Sep 19 13:15:25.504: INFO: (4) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 15.029864ms)
Sep 19 13:15:25.504: INFO: (4) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 14.834733ms)
Sep 19 13:15:25.504: INFO: (4) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 15.160309ms)
Sep 19 13:15:25.504: INFO: (4) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 15.548375ms)
Sep 19 13:15:25.504: INFO: (4) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 15.247228ms)
Sep 19 13:15:25.504: INFO: (4) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 15.395733ms)
Sep 19 13:15:25.504: INFO: (4) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 15.376159ms)
Sep 19 13:15:25.504: INFO: (4) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 15.419067ms)
Sep 19 13:15:25.504: INFO: (4) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 15.781013ms)
Sep 19 13:15:25.507: INFO: (4) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 17.97644ms)
Sep 19 13:15:25.507: INFO: (4) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 17.785897ms)
Sep 19 13:15:25.508: INFO: (4) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 19.504848ms)
Sep 19 13:15:25.509: INFO: (4) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 19.964595ms)
Sep 19 13:15:25.510: INFO: (4) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 21.462038ms)
Sep 19 13:15:25.511: INFO: (4) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 21.890535ms)
Sep 19 13:15:25.525: INFO: (5) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 14.118053ms)
Sep 19 13:15:25.525: INFO: (5) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 13.969386ms)
Sep 19 13:15:25.525: INFO: (5) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 13.886927ms)
Sep 19 13:15:25.525: INFO: (5) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 14.046579ms)
Sep 19 13:15:25.525: INFO: (5) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 13.842631ms)
Sep 19 13:15:25.525: INFO: (5) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 14.314354ms)
Sep 19 13:15:25.525: INFO: (5) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 14.157758ms)
Sep 19 13:15:25.525: INFO: (5) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 14.313301ms)
Sep 19 13:15:25.549: INFO: (5) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 38.330435ms)
Sep 19 13:15:25.550: INFO: (5) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 38.362384ms)
Sep 19 13:15:25.550: INFO: (5) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 38.792588ms)
Sep 19 13:15:25.550: INFO: (5) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 38.441ms)
Sep 19 13:15:25.552: INFO: (5) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 40.533096ms)
Sep 19 13:15:25.552: INFO: (5) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 40.716284ms)
Sep 19 13:15:25.552: INFO: (5) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 40.937988ms)
Sep 19 13:15:25.566: INFO: (5) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 54.950928ms)
Sep 19 13:15:25.582: INFO: (6) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 15.80543ms)
Sep 19 13:15:25.582: INFO: (6) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 15.659133ms)
Sep 19 13:15:25.582: INFO: (6) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 15.529983ms)
Sep 19 13:15:25.582: INFO: (6) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 15.491612ms)
Sep 19 13:15:25.582: INFO: (6) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 15.463512ms)
Sep 19 13:15:25.582: INFO: (6) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 16.138815ms)
Sep 19 13:15:25.582: INFO: (6) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 16.097748ms)
Sep 19 13:15:25.625: INFO: (6) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 58.618841ms)
Sep 19 13:15:25.625: INFO: (6) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 58.804437ms)
Sep 19 13:15:25.625: INFO: (6) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 58.6422ms)
Sep 19 13:15:25.625: INFO: (6) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 58.848352ms)
Sep 19 13:15:25.626: INFO: (6) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 59.206666ms)
Sep 19 13:15:25.632: INFO: (6) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 65.183474ms)
Sep 19 13:15:25.632: INFO: (6) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 65.317322ms)
Sep 19 13:15:25.632: INFO: (6) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 66.009724ms)
Sep 19 13:15:25.633: INFO: (6) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 66.309829ms)
Sep 19 13:15:25.653: INFO: (7) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 20.160848ms)
Sep 19 13:15:25.653: INFO: (7) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 20.048126ms)
Sep 19 13:15:25.653: INFO: (7) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 20.111428ms)
Sep 19 13:15:25.653: INFO: (7) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 20.550946ms)
Sep 19 13:15:25.654: INFO: (7) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 20.501118ms)
Sep 19 13:15:25.654: INFO: (7) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 20.696078ms)
Sep 19 13:15:25.654: INFO: (7) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 20.742264ms)
Sep 19 13:15:25.654: INFO: (7) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 20.765351ms)
Sep 19 13:15:25.654: INFO: (7) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 20.735908ms)
Sep 19 13:15:25.656: INFO: (7) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 22.895027ms)
Sep 19 13:15:25.656: INFO: (7) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 22.967273ms)
Sep 19 13:15:25.656: INFO: (7) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 22.500296ms)
Sep 19 13:15:25.694: INFO: (7) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 60.979122ms)
Sep 19 13:15:25.694: INFO: (7) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 61.321325ms)
Sep 19 13:15:25.694: INFO: (7) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 61.294141ms)
Sep 19 13:15:25.694: INFO: (7) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 61.430653ms)
Sep 19 13:15:25.712: INFO: (8) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 17.683366ms)
Sep 19 13:15:25.712: INFO: (8) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 17.596089ms)
Sep 19 13:15:25.714: INFO: (8) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 19.12601ms)
Sep 19 13:15:25.719: INFO: (8) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 24.895311ms)
Sep 19 13:15:25.720: INFO: (8) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 24.628345ms)
Sep 19 13:15:25.720: INFO: (8) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 24.925133ms)
Sep 19 13:15:25.720: INFO: (8) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 24.701173ms)
Sep 19 13:15:25.720: INFO: (8) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 24.828785ms)
Sep 19 13:15:25.720: INFO: (8) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 24.778063ms)
Sep 19 13:15:25.720: INFO: (8) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 25.175623ms)
Sep 19 13:15:25.730: INFO: (8) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 34.818623ms)
Sep 19 13:15:25.770: INFO: (8) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 75.249766ms)
Sep 19 13:15:25.770: INFO: (8) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 75.124236ms)
Sep 19 13:15:25.770: INFO: (8) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 75.416088ms)
Sep 19 13:15:25.770: INFO: (8) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 75.46267ms)
Sep 19 13:15:25.772: INFO: (8) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 77.828123ms)
Sep 19 13:15:25.783: INFO: (9) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 10.645169ms)
Sep 19 13:15:25.789: INFO: (9) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 16.066519ms)
Sep 19 13:15:25.790: INFO: (9) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 16.648114ms)
Sep 19 13:15:25.790: INFO: (9) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 16.628146ms)
Sep 19 13:15:25.790: INFO: (9) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 17.398933ms)
Sep 19 13:15:25.790: INFO: (9) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 16.742139ms)
Sep 19 13:15:25.790: INFO: (9) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 17.397692ms)
Sep 19 13:15:25.790: INFO: (9) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 17.036078ms)
Sep 19 13:15:25.791: INFO: (9) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 18.367228ms)
Sep 19 13:15:25.832: INFO: (9) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 58.907367ms)
Sep 19 13:15:25.832: INFO: (9) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 59.212771ms)
Sep 19 13:15:25.832: INFO: (9) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 59.330764ms)
Sep 19 13:15:25.833: INFO: (9) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 59.642437ms)
Sep 19 13:15:25.836: INFO: (9) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 62.327696ms)
Sep 19 13:15:25.836: INFO: (9) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 62.38114ms)
Sep 19 13:15:25.873: INFO: (9) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 100.582708ms)
Sep 19 13:15:25.890: INFO: (10) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 15.833781ms)
Sep 19 13:15:25.890: INFO: (10) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 15.794288ms)
Sep 19 13:15:25.890: INFO: (10) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 16.006787ms)
Sep 19 13:15:25.890: INFO: (10) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 16.450494ms)
Sep 19 13:15:25.891: INFO: (10) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 16.451904ms)
Sep 19 13:15:25.891: INFO: (10) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 16.709465ms)
Sep 19 13:15:25.891: INFO: (10) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 16.760884ms)
Sep 19 13:15:25.891: INFO: (10) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 16.921592ms)
Sep 19 13:15:25.891: INFO: (10) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 16.534916ms)
Sep 19 13:15:25.891: INFO: (10) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 17.407584ms)
Sep 19 13:15:25.891: INFO: (10) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 16.911113ms)
Sep 19 13:15:25.892: INFO: (10) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 17.692895ms)
Sep 19 13:15:25.895: INFO: (10) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 20.680847ms)
Sep 19 13:15:25.897: INFO: (10) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 22.852782ms)
Sep 19 13:15:25.941: INFO: (10) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 66.938842ms)
Sep 19 13:15:25.941: INFO: (10) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 67.047878ms)
Sep 19 13:15:25.958: INFO: (11) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 16.755968ms)
Sep 19 13:15:25.958: INFO: (11) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 16.630724ms)
Sep 19 13:15:25.958: INFO: (11) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 16.761839ms)
Sep 19 13:15:25.958: INFO: (11) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 17.071494ms)
Sep 19 13:15:25.958: INFO: (11) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 16.889595ms)
Sep 19 13:15:25.958: INFO: (11) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 16.777983ms)
Sep 19 13:15:25.958: INFO: (11) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 17.005431ms)
Sep 19 13:15:25.958: INFO: (11) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 17.060601ms)
Sep 19 13:15:25.999: INFO: (11) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 57.555847ms)
Sep 19 13:15:25.999: INFO: (11) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 57.326162ms)
Sep 19 13:15:25.999: INFO: (11) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 57.867716ms)
Sep 19 13:15:25.999: INFO: (11) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 57.979491ms)
Sep 19 13:15:25.999: INFO: (11) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 57.768437ms)
Sep 19 13:15:26.000: INFO: (11) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 58.634488ms)
Sep 19 13:15:26.006: INFO: (11) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 64.162872ms)
Sep 19 13:15:26.049: INFO: (11) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 107.705656ms)
Sep 19 13:15:26.068: INFO: (12) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 18.667421ms)
Sep 19 13:15:26.068: INFO: (12) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 18.747883ms)
Sep 19 13:15:26.068: INFO: (12) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 18.424286ms)
Sep 19 13:15:26.068: INFO: (12) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 18.33406ms)
Sep 19 13:15:26.068: INFO: (12) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 18.774836ms)
Sep 19 13:15:26.069: INFO: (12) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 19.551477ms)
Sep 19 13:15:26.069: INFO: (12) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 19.642041ms)
Sep 19 13:15:26.069: INFO: (12) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 19.492208ms)
Sep 19 13:15:26.069: INFO: (12) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 19.519815ms)
Sep 19 13:15:26.069: INFO: (12) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 19.699702ms)
Sep 19 13:15:26.070: INFO: (12) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 20.298331ms)
Sep 19 13:15:26.073: INFO: (12) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 22.813999ms)
Sep 19 13:15:26.077: INFO: (12) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 26.792919ms)
Sep 19 13:15:26.077: INFO: (12) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 26.989894ms)
Sep 19 13:15:26.077: INFO: (12) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 27.223329ms)
Sep 19 13:15:26.078: INFO: (12) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 28.633077ms)
Sep 19 13:15:26.095: INFO: (13) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 16.801074ms)
Sep 19 13:15:26.095: INFO: (13) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 16.249985ms)
Sep 19 13:15:26.095: INFO: (13) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 16.550078ms)
Sep 19 13:15:26.095: INFO: (13) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 16.979689ms)
Sep 19 13:15:26.095: INFO: (13) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 16.968216ms)
Sep 19 13:15:26.099: INFO: (13) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 19.905525ms)
Sep 19 13:15:26.099: INFO: (13) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 19.765241ms)
Sep 19 13:15:26.101: INFO: (13) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 22.406697ms)
Sep 19 13:15:26.101: INFO: (13) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 22.462779ms)
Sep 19 13:15:26.102: INFO: (13) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 23.118498ms)
Sep 19 13:15:26.102: INFO: (13) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 23.561973ms)
Sep 19 13:15:26.102: INFO: (13) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 23.483435ms)
Sep 19 13:15:26.102: INFO: (13) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 23.926002ms)
Sep 19 13:15:26.103: INFO: (13) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 24.390059ms)
Sep 19 13:15:26.144: INFO: (13) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 65.812058ms)
Sep 19 13:15:26.146: INFO: (13) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 67.46197ms)
Sep 19 13:15:26.168: INFO: (14) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 21.581419ms)
Sep 19 13:15:26.168: INFO: (14) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 21.679574ms)
Sep 19 13:15:26.168: INFO: (14) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 21.335705ms)
Sep 19 13:15:26.168: INFO: (14) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 21.756518ms)
Sep 19 13:15:26.168: INFO: (14) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 21.340726ms)
Sep 19 13:15:26.169: INFO: (14) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 22.194829ms)
Sep 19 13:15:26.169: INFO: (14) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 22.641176ms)
Sep 19 13:15:26.170: INFO: (14) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 22.758972ms)
Sep 19 13:15:26.170: INFO: (14) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 22.733114ms)
Sep 19 13:15:26.170: INFO: (14) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 22.919531ms)
Sep 19 13:15:26.171: INFO: (14) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 23.719028ms)
Sep 19 13:15:26.174: INFO: (14) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 27.557269ms)
Sep 19 13:15:26.174: INFO: (14) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 28.147167ms)
Sep 19 13:15:26.175: INFO: (14) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 27.866478ms)
Sep 19 13:15:26.175: INFO: (14) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 28.444717ms)
Sep 19 13:15:26.175: INFO: (14) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 27.981122ms)
Sep 19 13:15:26.192: INFO: (15) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 16.703977ms)
Sep 19 13:15:26.192: INFO: (15) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 16.855404ms)
Sep 19 13:15:26.192: INFO: (15) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 16.421022ms)
Sep 19 13:15:26.192: INFO: (15) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 16.997662ms)
Sep 19 13:15:26.192: INFO: (15) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 16.766494ms)
Sep 19 13:15:26.193: INFO: (15) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 17.25104ms)
Sep 19 13:15:26.193: INFO: (15) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 17.206704ms)
Sep 19 13:15:26.193: INFO: (15) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 18.24046ms)
Sep 19 13:15:26.194: INFO: (15) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 17.954216ms)
Sep 19 13:15:26.195: INFO: (15) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 19.5922ms)
Sep 19 13:15:26.196: INFO: (15) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 20.100544ms)
Sep 19 13:15:26.198: INFO: (15) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 22.95265ms)
Sep 19 13:15:26.198: INFO: (15) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 22.687785ms)
Sep 19 13:15:26.198: INFO: (15) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 23.21461ms)
Sep 19 13:15:26.198: INFO: (15) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 23.166228ms)
Sep 19 13:15:26.200: INFO: (15) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 24.762421ms)
Sep 19 13:15:26.223: INFO: (16) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 22.1064ms)
Sep 19 13:15:26.223: INFO: (16) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 22.19513ms)
Sep 19 13:15:26.223: INFO: (16) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 22.405939ms)
Sep 19 13:15:26.223: INFO: (16) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 22.337614ms)
Sep 19 13:15:26.223: INFO: (16) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 22.166782ms)
Sep 19 13:15:26.224: INFO: (16) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 23.260859ms)
Sep 19 13:15:26.224: INFO: (16) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 23.467102ms)
Sep 19 13:15:26.265: INFO: (16) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 64.546101ms)
Sep 19 13:15:26.265: INFO: (16) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 64.541632ms)
Sep 19 13:15:26.265: INFO: (16) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 64.772909ms)
Sep 19 13:15:26.265: INFO: (16) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 64.790909ms)
Sep 19 13:15:26.266: INFO: (16) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 65.265214ms)
Sep 19 13:15:26.266: INFO: (16) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 65.235126ms)
Sep 19 13:15:26.268: INFO: (16) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 67.757396ms)
Sep 19 13:15:26.269: INFO: (16) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 68.024411ms)
Sep 19 13:15:26.273: INFO: (16) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 72.171109ms)
Sep 19 13:15:26.284: INFO: (17) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 11.081605ms)
Sep 19 13:15:26.300: INFO: (17) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 26.78003ms)
Sep 19 13:15:26.302: INFO: (17) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 28.862096ms)
Sep 19 13:15:26.302: INFO: (17) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 29.029803ms)
Sep 19 13:15:26.307: INFO: (17) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 34.70051ms)
Sep 19 13:15:26.308: INFO: (17) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 34.608648ms)
Sep 19 13:15:26.308: INFO: (17) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 34.441582ms)
Sep 19 13:15:26.308: INFO: (17) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 34.686596ms)
Sep 19 13:15:26.308: INFO: (17) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 34.921934ms)
Sep 19 13:15:26.308: INFO: (17) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 34.865211ms)
Sep 19 13:15:26.308: INFO: (17) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 34.996609ms)
Sep 19 13:15:26.308: INFO: (17) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 35.454838ms)
Sep 19 13:15:26.308: INFO: (17) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 35.056971ms)
Sep 19 13:15:26.339: INFO: (17) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 65.468921ms)
Sep 19 13:15:26.339: INFO: (17) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 65.367035ms)
Sep 19 13:15:26.341: INFO: (17) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 67.879198ms)
Sep 19 13:15:26.366: INFO: (18) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 24.929222ms)
Sep 19 13:15:26.366: INFO: (18) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 24.09975ms)
Sep 19 13:15:26.366: INFO: (18) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 25.097635ms)
Sep 19 13:15:26.366: INFO: (18) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 24.706187ms)
Sep 19 13:15:26.366: INFO: (18) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 24.380382ms)
Sep 19 13:15:26.366: INFO: (18) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 24.864261ms)
Sep 19 13:15:26.368: INFO: (18) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 26.689583ms)
Sep 19 13:15:26.368: INFO: (18) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 26.267726ms)
Sep 19 13:15:26.368: INFO: (18) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 26.428194ms)
Sep 19 13:15:26.368: INFO: (18) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 27.103026ms)
Sep 19 13:15:26.376: INFO: (18) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 34.019559ms)
Sep 19 13:15:26.406: INFO: (18) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 64.345188ms)
Sep 19 13:15:26.406: INFO: (18) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 65.054918ms)
Sep 19 13:15:26.406: INFO: (18) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 64.427749ms)
Sep 19 13:15:26.406: INFO: (18) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 64.266313ms)
Sep 19 13:15:26.406: INFO: (18) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 65.018081ms)
Sep 19 13:15:26.428: INFO: (19) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 21.052794ms)
Sep 19 13:15:26.428: INFO: (19) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname1/proxy/: tls baz (200; 21.253176ms)
Sep 19 13:15:26.428: INFO: (19) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q/proxy/rewriteme">test</a> (200; 21.302683ms)
Sep 19 13:15:26.431: INFO: (19) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:460/proxy/: tls baz (200; 24.274639ms)
Sep 19 13:15:26.432: INFO: (19) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">test<... (200; 25.29742ms)
Sep 19 13:15:26.432: INFO: (19) /api/v1/namespaces/proxy-8680/pods/proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 24.939924ms)
Sep 19 13:15:26.432: INFO: (19) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:443/proxy/tlsrewritem... (200; 25.542199ms)
Sep 19 13:15:26.433: INFO: (19) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:162/proxy/: bar (200; 25.846141ms)
Sep 19 13:15:26.433: INFO: (19) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/: <a href="/api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:1080/proxy/rewriteme">... (200; 26.130257ms)
Sep 19 13:15:26.433: INFO: (19) /api/v1/namespaces/proxy-8680/pods/https:proxy-service-x5lsh-bd75q:462/proxy/: tls qux (200; 25.857823ms)
Sep 19 13:15:26.433: INFO: (19) /api/v1/namespaces/proxy-8680/pods/http:proxy-service-x5lsh-bd75q:160/proxy/: foo (200; 26.181206ms)
Sep 19 13:15:26.435: INFO: (19) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname1/proxy/: foo (200; 27.784895ms)
Sep 19 13:15:26.435: INFO: (19) /api/v1/namespaces/proxy-8680/services/https:proxy-service-x5lsh:tlsportname2/proxy/: tls qux (200; 28.235552ms)
Sep 19 13:15:26.437: INFO: (19) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname2/proxy/: bar (200; 30.367477ms)
Sep 19 13:15:26.437: INFO: (19) /api/v1/namespaces/proxy-8680/services/proxy-service-x5lsh:portname2/proxy/: bar (200; 30.41338ms)
Sep 19 13:15:26.439: INFO: (19) /api/v1/namespaces/proxy-8680/services/http:proxy-service-x5lsh:portname1/proxy/: foo (200; 32.323403ms)
STEP: deleting ReplicationController proxy-service-x5lsh in namespace proxy-8680, will wait for the garbage collector to delete the pods
Sep 19 13:15:26.507: INFO: Deleting ReplicationController proxy-service-x5lsh took: 12.304529ms
Sep 19 13:15:26.907: INFO: Terminating ReplicationController proxy-service-x5lsh pods took: 400.21834ms
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:15:39.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8680" for this suite.
Sep 19 13:15:45.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:15:45.725: INFO: namespace proxy-8680 deletion completed in 6.30959918s

• [SLOW TEST:27.784 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:15:45.725: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4404
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:15:49.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4404" for this suite.
Sep 19 13:15:55.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:15:56.240: INFO: namespace kubelet-test-4404 deletion completed in 6.314040373s

• [SLOW TEST:10.514 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:15:56.240: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7219
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 13:15:56.415: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep 19 13:15:57.473: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:15:58.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7219" for this suite.
Sep 19 13:16:04.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:16:04.787: INFO: namespace replication-controller-7219 deletion completed in 6.291997017s

• [SLOW TEST:8.547 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:16:04.788: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7474
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Sep 19 13:16:04.973: INFO: Waiting up to 5m0s for pod "client-containers-f269e1c9-8a36-4831-92b6-0f347795145a" in namespace "containers-7474" to be "success or failure"
Sep 19 13:16:04.980: INFO: Pod "client-containers-f269e1c9-8a36-4831-92b6-0f347795145a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.374123ms
Sep 19 13:16:06.990: INFO: Pod "client-containers-f269e1c9-8a36-4831-92b6-0f347795145a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016834386s
Sep 19 13:16:09.000: INFO: Pod "client-containers-f269e1c9-8a36-4831-92b6-0f347795145a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026784485s
STEP: Saw pod success
Sep 19 13:16:09.000: INFO: Pod "client-containers-f269e1c9-8a36-4831-92b6-0f347795145a" satisfied condition "success or failure"
Sep 19 13:16:09.009: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod client-containers-f269e1c9-8a36-4831-92b6-0f347795145a container test-container: <nil>
STEP: delete the pod
Sep 19 13:16:09.064: INFO: Waiting for pod client-containers-f269e1c9-8a36-4831-92b6-0f347795145a to disappear
Sep 19 13:16:09.072: INFO: Pod client-containers-f269e1c9-8a36-4831-92b6-0f347795145a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:16:09.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7474" for this suite.
Sep 19 13:16:15.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:16:15.417: INFO: namespace containers-7474 deletion completed in 6.332872899s

• [SLOW TEST:10.629 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:16:15.417: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6478
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 19 13:16:15.601: INFO: Waiting up to 5m0s for pod "pod-2f69d465-868f-45d4-83cd-3f944163b4f3" in namespace "emptydir-6478" to be "success or failure"
Sep 19 13:16:15.607: INFO: Pod "pod-2f69d465-868f-45d4-83cd-3f944163b4f3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084731ms
Sep 19 13:16:17.621: INFO: Pod "pod-2f69d465-868f-45d4-83cd-3f944163b4f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019612031s
Sep 19 13:16:19.628: INFO: Pod "pod-2f69d465-868f-45d4-83cd-3f944163b4f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027041312s
Sep 19 13:16:21.635: INFO: Pod "pod-2f69d465-868f-45d4-83cd-3f944163b4f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033836375s
STEP: Saw pod success
Sep 19 13:16:21.635: INFO: Pod "pod-2f69d465-868f-45d4-83cd-3f944163b4f3" satisfied condition "success or failure"
Sep 19 13:16:21.643: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod pod-2f69d465-868f-45d4-83cd-3f944163b4f3 container test-container: <nil>
STEP: delete the pod
Sep 19 13:16:21.721: INFO: Waiting for pod pod-2f69d465-868f-45d4-83cd-3f944163b4f3 to disappear
Sep 19 13:16:21.734: INFO: Pod pod-2f69d465-868f-45d4-83cd-3f944163b4f3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:16:21.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6478" for this suite.
Sep 19 13:16:27.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:16:28.065: INFO: namespace emptydir-6478 deletion completed in 6.322997553s

• [SLOW TEST:12.648 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:16:28.065: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2007
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 19 13:16:32.797: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2007 pod-service-account-b46aa8ca-7456-4f04-9b3a-9e5ad510c237 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 19 13:16:33.395: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2007 pod-service-account-b46aa8ca-7456-4f04-9b3a-9e5ad510c237 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 19 13:16:33.989: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2007 pod-service-account-b46aa8ca-7456-4f04-9b3a-9e5ad510c237 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:16:34.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2007" for this suite.
Sep 19 13:16:40.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:16:40.924: INFO: namespace svcaccounts-2007 deletion completed in 6.291301819s

• [SLOW TEST:12.859 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:16:40.924: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3681
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-e33103ea-0c56-4ea7-90bb-18a3aad195a1
STEP: Creating a pod to test consume secrets
Sep 19 13:16:41.114: INFO: Waiting up to 5m0s for pod "pod-secrets-f9f319fc-1bce-4da8-9c22-71fddc912e33" in namespace "secrets-3681" to be "success or failure"
Sep 19 13:16:41.123: INFO: Pod "pod-secrets-f9f319fc-1bce-4da8-9c22-71fddc912e33": Phase="Pending", Reason="", readiness=false. Elapsed: 9.021044ms
Sep 19 13:16:43.130: INFO: Pod "pod-secrets-f9f319fc-1bce-4da8-9c22-71fddc912e33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01657829s
Sep 19 13:16:45.137: INFO: Pod "pod-secrets-f9f319fc-1bce-4da8-9c22-71fddc912e33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023425658s
STEP: Saw pod success
Sep 19 13:16:45.137: INFO: Pod "pod-secrets-f9f319fc-1bce-4da8-9c22-71fddc912e33" satisfied condition "success or failure"
Sep 19 13:16:45.144: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod pod-secrets-f9f319fc-1bce-4da8-9c22-71fddc912e33 container secret-volume-test: <nil>
STEP: delete the pod
Sep 19 13:16:45.229: INFO: Waiting for pod pod-secrets-f9f319fc-1bce-4da8-9c22-71fddc912e33 to disappear
Sep 19 13:16:45.235: INFO: Pod pod-secrets-f9f319fc-1bce-4da8-9c22-71fddc912e33 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:16:45.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3681" for this suite.
Sep 19 13:16:51.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:16:51.662: INFO: namespace secrets-3681 deletion completed in 6.41941315s

• [SLOW TEST:10.738 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:16:51.662: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9547
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 19 13:16:51.906: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9547,SelfLink:/api/v1/namespaces/watch-9547/configmaps/e2e-watch-test-configmap-a,UID:9f5361de-fb11-4015-b2b2-d8a2d666fb7f,ResourceVersion:18349,Generation:0,CreationTimestamp:2019-09-19 13:16:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 19 13:16:51.906: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9547,SelfLink:/api/v1/namespaces/watch-9547/configmaps/e2e-watch-test-configmap-a,UID:9f5361de-fb11-4015-b2b2-d8a2d666fb7f,ResourceVersion:18349,Generation:0,CreationTimestamp:2019-09-19 13:16:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 19 13:17:01.930: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9547,SelfLink:/api/v1/namespaces/watch-9547/configmaps/e2e-watch-test-configmap-a,UID:9f5361de-fb11-4015-b2b2-d8a2d666fb7f,ResourceVersion:18378,Generation:0,CreationTimestamp:2019-09-19 13:16:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 19 13:17:01.930: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9547,SelfLink:/api/v1/namespaces/watch-9547/configmaps/e2e-watch-test-configmap-a,UID:9f5361de-fb11-4015-b2b2-d8a2d666fb7f,ResourceVersion:18378,Generation:0,CreationTimestamp:2019-09-19 13:16:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 19 13:17:11.950: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9547,SelfLink:/api/v1/namespaces/watch-9547/configmaps/e2e-watch-test-configmap-a,UID:9f5361de-fb11-4015-b2b2-d8a2d666fb7f,ResourceVersion:18406,Generation:0,CreationTimestamp:2019-09-19 13:16:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 19 13:17:11.951: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9547,SelfLink:/api/v1/namespaces/watch-9547/configmaps/e2e-watch-test-configmap-a,UID:9f5361de-fb11-4015-b2b2-d8a2d666fb7f,ResourceVersion:18406,Generation:0,CreationTimestamp:2019-09-19 13:16:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 19 13:17:21.966: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9547,SelfLink:/api/v1/namespaces/watch-9547/configmaps/e2e-watch-test-configmap-a,UID:9f5361de-fb11-4015-b2b2-d8a2d666fb7f,ResourceVersion:18435,Generation:0,CreationTimestamp:2019-09-19 13:16:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 19 13:17:21.966: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9547,SelfLink:/api/v1/namespaces/watch-9547/configmaps/e2e-watch-test-configmap-a,UID:9f5361de-fb11-4015-b2b2-d8a2d666fb7f,ResourceVersion:18435,Generation:0,CreationTimestamp:2019-09-19 13:16:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 19 13:17:31.979: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9547,SelfLink:/api/v1/namespaces/watch-9547/configmaps/e2e-watch-test-configmap-b,UID:0e3ced62-779f-4e99-972c-bc37db0daae2,ResourceVersion:18463,Generation:0,CreationTimestamp:2019-09-19 13:17:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 19 13:17:31.980: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9547,SelfLink:/api/v1/namespaces/watch-9547/configmaps/e2e-watch-test-configmap-b,UID:0e3ced62-779f-4e99-972c-bc37db0daae2,ResourceVersion:18463,Generation:0,CreationTimestamp:2019-09-19 13:17:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 19 13:17:41.994: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9547,SelfLink:/api/v1/namespaces/watch-9547/configmaps/e2e-watch-test-configmap-b,UID:0e3ced62-779f-4e99-972c-bc37db0daae2,ResourceVersion:18492,Generation:0,CreationTimestamp:2019-09-19 13:17:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 19 13:17:41.994: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9547,SelfLink:/api/v1/namespaces/watch-9547/configmaps/e2e-watch-test-configmap-b,UID:0e3ced62-779f-4e99-972c-bc37db0daae2,ResourceVersion:18492,Generation:0,CreationTimestamp:2019-09-19 13:17:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:17:51.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9547" for this suite.
Sep 19 13:17:58.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:17:58.298: INFO: namespace watch-9547 deletion completed in 6.29365595s

• [SLOW TEST:66.635 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:17:58.298: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5678
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 19 13:17:58.491: INFO: Waiting up to 5m0s for pod "pod-6a529c5b-f799-4526-a540-0fa903d7a6f6" in namespace "emptydir-5678" to be "success or failure"
Sep 19 13:17:58.500: INFO: Pod "pod-6a529c5b-f799-4526-a540-0fa903d7a6f6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.74113ms
Sep 19 13:18:00.507: INFO: Pod "pod-6a529c5b-f799-4526-a540-0fa903d7a6f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015494065s
STEP: Saw pod success
Sep 19 13:18:00.507: INFO: Pod "pod-6a529c5b-f799-4526-a540-0fa903d7a6f6" satisfied condition "success or failure"
Sep 19 13:18:00.513: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod pod-6a529c5b-f799-4526-a540-0fa903d7a6f6 container test-container: <nil>
STEP: delete the pod
Sep 19 13:18:00.546: INFO: Waiting for pod pod-6a529c5b-f799-4526-a540-0fa903d7a6f6 to disappear
Sep 19 13:18:00.554: INFO: Pod pod-6a529c5b-f799-4526-a540-0fa903d7a6f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:18:00.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5678" for this suite.
Sep 19 13:18:06.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:18:06.908: INFO: namespace emptydir-5678 deletion completed in 6.346115036s

• [SLOW TEST:8.610 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:18:06.909: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1823
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep 19 13:18:11.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec pod-sharedvolume-b4d89de4-4c7b-420d-9a97-ef5778202cd6 -c busybox-main-container --namespace=emptydir-1823 -- cat /usr/share/volumeshare/shareddata.txt'
Sep 19 13:18:11.931: INFO: stderr: ""
Sep 19 13:18:11.931: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:18:11.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1823" for this suite.
Sep 19 13:18:17.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:18:18.297: INFO: namespace emptydir-1823 deletion completed in 6.358337802s

• [SLOW TEST:11.388 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:18:18.297: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7935
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-44741c17-d709-46ab-a5ed-7e0197c9f690
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:18:18.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7935" for this suite.
Sep 19 13:18:24.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:18:24.888: INFO: namespace secrets-7935 deletion completed in 6.392595583s

• [SLOW TEST:6.591 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:18:24.888: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 19 13:18:25.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-4917'
Sep 19 13:18:25.173: INFO: stderr: ""
Sep 19 13:18:25.173: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Sep 19 13:18:30.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pod e2e-test-nginx-pod --namespace=kubectl-4917 -o json'
Sep 19 13:18:30.303: INFO: stderr: ""
Sep 19 13:18:30.303: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.25.8.15/32\"\n        },\n        \"creationTimestamp\": \"2019-09-19T13:18:25Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-4917\",\n        \"resourceVersion\": \"18702\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-4917/pods/e2e-test-nginx-pod\",\n        \"uid\": \"2431b9a5-9c37-4e0f-8a8b-88a21db7fb5d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-rr2lw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-33-83.eu-central-1.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-rr2lw\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-rr2lw\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-19T13:18:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-19T13:18:27Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-19T13:18:27Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-19T13:18:25Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://97015debd99e988f1562a5f8d97d04b1f056c871e8849a005c90984d4f5b7493\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-09-19T13:18:26Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.33.83\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.8.15\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-09-19T13:18:25Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 19 13:18:30.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 replace -f - --namespace=kubectl-4917'
Sep 19 13:18:30.653: INFO: stderr: ""
Sep 19 13:18:30.653: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Sep 19 13:18:30.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete pods e2e-test-nginx-pod --namespace=kubectl-4917'
Sep 19 13:18:35.024: INFO: stderr: ""
Sep 19 13:18:35.024: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:18:35.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4917" for this suite.
Sep 19 13:18:41.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:18:41.394: INFO: namespace kubectl-4917 deletion completed in 6.342483401s

• [SLOW TEST:16.506 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:18:41.395: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1171
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Sep 19 13:18:41.565: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 19 13:18:41.581: INFO: Waiting for terminating namespaces to be deleted...
Sep 19 13:18:41.598: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-33-83.eu-central-1.compute.internal before test
Sep 19 13:18:41.618: INFO: node-local-dns-twjtr from kube-system started at 2019-09-19 12:50:15 +0000 UTC (1 container statuses recorded)
Sep 19 13:18:41.618: INFO: 	Container node-cache ready: true, restart count 0
Sep 19 13:18:41.618: INFO: kubernetes-dashboard-584d5ffc75-jvnx5 from kube-system started at 2019-09-19 12:50:46 +0000 UTC (1 container statuses recorded)
Sep 19 13:18:41.618: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep 19 13:18:41.618: INFO: sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-zgrfd from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.618: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 19 13:18:41.618: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 19 13:18:41.618: INFO: kube-proxy-wcj22 from kube-system started at 2019-09-19 12:49:55 +0000 UTC (1 container statuses recorded)
Sep 19 13:18:41.618: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 19 13:18:41.618: INFO: canal-4wkpv from kube-system started at 2019-09-19 12:49:55 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.618: INFO: 	Container calico-node ready: true, restart count 0
Sep 19 13:18:41.618: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 19 13:18:41.618: INFO: node-exporter-7txrv from kube-system started at 2019-09-19 12:49:55 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.618: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 19 13:18:41.618: INFO: 	Container node-exporter ready: true, restart count 0
Sep 19 13:18:41.618: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-36-147.eu-central-1.compute.internal before test
Sep 19 13:18:41.707: INFO: node-local-dns-zdxc4 from kube-system started at 2019-09-19 12:50:45 +0000 UTC (1 container statuses recorded)
Sep 19 13:18:41.707: INFO: 	Container node-cache ready: true, restart count 0
Sep 19 13:18:41.707: INFO: coredns-9b6865ff9-mhjqc from kube-system started at 2019-09-19 12:50:46 +0000 UTC (1 container statuses recorded)
Sep 19 13:18:41.707: INFO: 	Container coredns ready: true, restart count 0
Sep 19 13:18:41.707: INFO: sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-ccgl7 from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.707: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 19 13:18:41.707: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 19 13:18:41.707: INFO: node-exporter-hnxl5 from kube-system started at 2019-09-19 12:50:25 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.707: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 19 13:18:41.707: INFO: 	Container node-exporter ready: true, restart count 0
Sep 19 13:18:41.707: INFO: canal-txnzq from kube-system started at 2019-09-19 12:50:25 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.707: INFO: 	Container calico-node ready: true, restart count 0
Sep 19 13:18:41.707: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 19 13:18:41.707: INFO: kube-proxy-hk5tn from kube-system started at 2019-09-19 12:50:25 +0000 UTC (1 container statuses recorded)
Sep 19 13:18:41.707: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 19 13:18:41.707: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-42-91.eu-central-1.compute.internal before test
Sep 19 13:18:41.792: INFO: node-exporter-5vls6 from kube-system started at 2019-09-19 12:50:04 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.792: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 19 13:18:41.792: INFO: 	Container node-exporter ready: true, restart count 0
Sep 19 13:18:41.792: INFO: canal-dnm9w from kube-system started at 2019-09-19 12:50:04 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.792: INFO: 	Container calico-node ready: true, restart count 0
Sep 19 13:18:41.792: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 19 13:18:41.792: INFO: kube-proxy-h4md4 from kube-system started at 2019-09-19 12:50:04 +0000 UTC (1 container statuses recorded)
Sep 19 13:18:41.792: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 19 13:18:41.792: INFO: node-local-dns-7rcvh from kube-system started at 2019-09-19 12:50:14 +0000 UTC (1 container statuses recorded)
Sep 19 13:18:41.792: INFO: 	Container node-cache ready: true, restart count 0
Sep 19 13:18:41.792: INFO: sonobuoy-e2e-job-57f4e2d668874fc2 from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.792: INFO: 	Container e2e ready: true, restart count 0
Sep 19 13:18:41.792: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 19 13:18:41.792: INFO: sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-q2sj9 from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.792: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 19 13:18:41.792: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 19 13:18:41.793: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-44-209.eu-central-1.compute.internal before test
Sep 19 13:18:41.874: INFO: sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-7bv2k from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.874: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 19 13:18:41.874: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 19 13:18:41.874: INFO: kube-proxy-29rs8 from kube-system started at 2019-09-19 12:50:08 +0000 UTC (1 container statuses recorded)
Sep 19 13:18:41.874: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 19 13:18:41.874: INFO: canal-rg8t2 from kube-system started at 2019-09-19 12:50:08 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.874: INFO: 	Container calico-node ready: true, restart count 0
Sep 19 13:18:41.874: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 19 13:18:41.874: INFO: node-exporter-6lvwr from kube-system started at 2019-09-19 12:50:08 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.875: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 19 13:18:41.875: INFO: 	Container node-exporter ready: true, restart count 0
Sep 19 13:18:41.875: INFO: node-local-dns-rd9dn from kube-system started at 2019-09-19 12:50:28 +0000 UTC (1 container statuses recorded)
Sep 19 13:18:41.875: INFO: 	Container node-cache ready: true, restart count 0
Sep 19 13:18:41.875: INFO: sonobuoy from sonobuoy started at 2019-09-19 13:04:24 +0000 UTC (1 container statuses recorded)
Sep 19 13:18:41.875: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 19 13:18:41.875: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-46-204.eu-central-1.compute.internal before test
Sep 19 13:18:41.928: INFO: canal-9jtwv from kube-system started at 2019-09-19 12:50:22 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.928: INFO: 	Container calico-node ready: true, restart count 0
Sep 19 13:18:41.928: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 19 13:18:41.928: INFO: coredns-9b6865ff9-mks62 from kube-system started at 2019-09-19 12:53:47 +0000 UTC (1 container statuses recorded)
Sep 19 13:18:41.928: INFO: 	Container coredns ready: true, restart count 0
Sep 19 13:18:41.928: INFO: sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-k8z66 from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.928: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 19 13:18:41.928: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 19 13:18:41.928: INFO: node-exporter-tsf9h from kube-system started at 2019-09-19 12:50:22 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.928: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 19 13:18:41.928: INFO: 	Container node-exporter ready: true, restart count 0
Sep 19 13:18:41.928: INFO: kube-proxy-p4xds from kube-system started at 2019-09-19 12:50:22 +0000 UTC (1 container statuses recorded)
Sep 19 13:18:41.928: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 19 13:18:41.928: INFO: node-local-dns-t4859 from kube-system started at 2019-09-19 12:50:42 +0000 UTC (1 container statuses recorded)
Sep 19 13:18:41.928: INFO: 	Container node-cache ready: true, restart count 0
Sep 19 13:18:41.928: INFO: openvpn-client-5fbd4fdb44-g28ss from kube-system started at 2019-09-19 12:50:46 +0000 UTC (2 container statuses recorded)
Sep 19 13:18:41.928: INFO: 	Container dnat-controller ready: true, restart count 0
Sep 19 13:18:41.928: INFO: 	Container openvpn-client ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node ip-172-31-33-83.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-172-31-36-147.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-172-31-42-91.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-172-31-44-209.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-172-31-46-204.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod canal-4wkpv requesting resource cpu=250m on Node ip-172-31-33-83.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod canal-9jtwv requesting resource cpu=250m on Node ip-172-31-46-204.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod canal-dnm9w requesting resource cpu=250m on Node ip-172-31-42-91.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod canal-rg8t2 requesting resource cpu=250m on Node ip-172-31-44-209.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod canal-txnzq requesting resource cpu=250m on Node ip-172-31-36-147.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod coredns-9b6865ff9-mhjqc requesting resource cpu=100m on Node ip-172-31-36-147.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod coredns-9b6865ff9-mks62 requesting resource cpu=100m on Node ip-172-31-46-204.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod kube-proxy-29rs8 requesting resource cpu=75m on Node ip-172-31-44-209.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod kube-proxy-h4md4 requesting resource cpu=75m on Node ip-172-31-42-91.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod kube-proxy-hk5tn requesting resource cpu=75m on Node ip-172-31-36-147.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod kube-proxy-p4xds requesting resource cpu=75m on Node ip-172-31-46-204.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod kube-proxy-wcj22 requesting resource cpu=75m on Node ip-172-31-33-83.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod kubernetes-dashboard-584d5ffc75-jvnx5 requesting resource cpu=75m on Node ip-172-31-33-83.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod node-exporter-5vls6 requesting resource cpu=20m on Node ip-172-31-42-91.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod node-exporter-6lvwr requesting resource cpu=20m on Node ip-172-31-44-209.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod node-exporter-7txrv requesting resource cpu=20m on Node ip-172-31-33-83.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod node-exporter-hnxl5 requesting resource cpu=20m on Node ip-172-31-36-147.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod node-exporter-tsf9h requesting resource cpu=20m on Node ip-172-31-46-204.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod node-local-dns-7rcvh requesting resource cpu=25m on Node ip-172-31-42-91.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod node-local-dns-rd9dn requesting resource cpu=25m on Node ip-172-31-44-209.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod node-local-dns-t4859 requesting resource cpu=25m on Node ip-172-31-46-204.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod node-local-dns-twjtr requesting resource cpu=25m on Node ip-172-31-33-83.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod node-local-dns-zdxc4 requesting resource cpu=25m on Node ip-172-31-36-147.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod openvpn-client-5fbd4fdb44-g28ss requesting resource cpu=30m on Node ip-172-31-46-204.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-44-209.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod sonobuoy-e2e-job-57f4e2d668874fc2 requesting resource cpu=0m on Node ip-172-31-42-91.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-7bv2k requesting resource cpu=0m on Node ip-172-31-44-209.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-ccgl7 requesting resource cpu=0m on Node ip-172-31-36-147.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-k8z66 requesting resource cpu=0m on Node ip-172-31-46-204.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-q2sj9 requesting resource cpu=0m on Node ip-172-31-42-91.eu-central-1.compute.internal
Sep 19 13:18:42.067: INFO: Pod sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-zgrfd requesting resource cpu=0m on Node ip-172-31-33-83.eu-central-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60b2c067-f62e-4a31-8d40-ef320cc56405.15c5d97068ca1dc5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1171/filler-pod-60b2c067-f62e-4a31-8d40-ef320cc56405 to ip-172-31-46-204.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60b2c067-f62e-4a31-8d40-ef320cc56405.15c5d970ae40960b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60b2c067-f62e-4a31-8d40-ef320cc56405.15c5d970c698dab7], Reason = [Created], Message = [Created container filler-pod-60b2c067-f62e-4a31-8d40-ef320cc56405]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60b2c067-f62e-4a31-8d40-ef320cc56405.15c5d970d5f9c370], Reason = [Started], Message = [Started container filler-pod-60b2c067-f62e-4a31-8d40-ef320cc56405]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b0770966-0c8a-40f1-87c6-56389639896a.15c5d970696872b2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1171/filler-pod-b0770966-0c8a-40f1-87c6-56389639896a to ip-172-31-33-83.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b0770966-0c8a-40f1-87c6-56389639896a.15c5d970a842762f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b0770966-0c8a-40f1-87c6-56389639896a.15c5d970acf5fc73], Reason = [Created], Message = [Created container filler-pod-b0770966-0c8a-40f1-87c6-56389639896a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b0770966-0c8a-40f1-87c6-56389639896a.15c5d970bb95ce2e], Reason = [Started], Message = [Started container filler-pod-b0770966-0c8a-40f1-87c6-56389639896a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c70866ac-f161-4ea1-a900-79b7b32ff136.15c5d97066438308], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1171/filler-pod-c70866ac-f161-4ea1-a900-79b7b32ff136 to ip-172-31-36-147.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c70866ac-f161-4ea1-a900-79b7b32ff136.15c5d970acae8442], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c70866ac-f161-4ea1-a900-79b7b32ff136.15c5d970b1a762d8], Reason = [Created], Message = [Created container filler-pod-c70866ac-f161-4ea1-a900-79b7b32ff136]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c70866ac-f161-4ea1-a900-79b7b32ff136.15c5d970c1ba68dd], Reason = [Started], Message = [Started container filler-pod-c70866ac-f161-4ea1-a900-79b7b32ff136]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f6f70db8-bbb8-4549-bcfb-62eb09130912.15c5d97066bea0e9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1171/filler-pod-f6f70db8-bbb8-4549-bcfb-62eb09130912 to ip-172-31-42-91.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f6f70db8-bbb8-4549-bcfb-62eb09130912.15c5d970ae676676], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f6f70db8-bbb8-4549-bcfb-62eb09130912.15c5d970b28200ed], Reason = [Created], Message = [Created container filler-pod-f6f70db8-bbb8-4549-bcfb-62eb09130912]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f6f70db8-bbb8-4549-bcfb-62eb09130912.15c5d970c2177090], Reason = [Started], Message = [Started container filler-pod-f6f70db8-bbb8-4549-bcfb-62eb09130912]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcd40901-7122-4fc0-a685-86d91f41a9f7.15c5d9706750a8ab], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1171/filler-pod-fcd40901-7122-4fc0-a685-86d91f41a9f7 to ip-172-31-44-209.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcd40901-7122-4fc0-a685-86d91f41a9f7.15c5d970a9ddf314], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcd40901-7122-4fc0-a685-86d91f41a9f7.15c5d970ae7bc6ab], Reason = [Created], Message = [Created container filler-pod-fcd40901-7122-4fc0-a685-86d91f41a9f7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fcd40901-7122-4fc0-a685-86d91f41a9f7.15c5d970be365e49], Reason = [Started], Message = [Started container filler-pod-fcd40901-7122-4fc0-a685-86d91f41a9f7]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15c5d9715b50c755], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: removing the label node off the node ip-172-31-33-83.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-36-147.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-42-91.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-44-209.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-46-204.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:18:47.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1171" for this suite.
Sep 19 13:18:53.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:18:53.680: INFO: namespace sched-pred-1171 deletion completed in 6.329567786s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:12.285 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:18:53.680: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-366
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-d533aa46-1242-4efc-940f-d107ff2b2ec4
STEP: Creating a pod to test consume secrets
Sep 19 13:18:53.870: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-41f3c22c-43f0-4491-a35b-16a6ee80b19a" in namespace "projected-366" to be "success or failure"
Sep 19 13:18:53.876: INFO: Pod "pod-projected-secrets-41f3c22c-43f0-4491-a35b-16a6ee80b19a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0364ms
Sep 19 13:18:55.884: INFO: Pod "pod-projected-secrets-41f3c22c-43f0-4491-a35b-16a6ee80b19a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013959459s
STEP: Saw pod success
Sep 19 13:18:55.884: INFO: Pod "pod-projected-secrets-41f3c22c-43f0-4491-a35b-16a6ee80b19a" satisfied condition "success or failure"
Sep 19 13:18:55.891: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod pod-projected-secrets-41f3c22c-43f0-4491-a35b-16a6ee80b19a container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 19 13:18:55.927: INFO: Waiting for pod pod-projected-secrets-41f3c22c-43f0-4491-a35b-16a6ee80b19a to disappear
Sep 19 13:18:55.936: INFO: Pod pod-projected-secrets-41f3c22c-43f0-4491-a35b-16a6ee80b19a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:18:55.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-366" for this suite.
Sep 19 13:19:01.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:19:02.278: INFO: namespace projected-366 deletion completed in 6.334330819s

• [SLOW TEST:8.598 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:19:02.278: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4110
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 13:19:02.508: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 19 13:19:02.524: INFO: Number of nodes with available pods: 0
Sep 19 13:19:02.524: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep 19 13:19:02.562: INFO: Number of nodes with available pods: 0
Sep 19 13:19:02.562: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:19:03.568: INFO: Number of nodes with available pods: 0
Sep 19 13:19:03.568: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:19:04.572: INFO: Number of nodes with available pods: 0
Sep 19 13:19:04.572: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:19:05.571: INFO: Number of nodes with available pods: 1
Sep 19 13:19:05.571: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 19 13:19:05.604: INFO: Number of nodes with available pods: 1
Sep 19 13:19:05.604: INFO: Number of running nodes: 0, number of available pods: 1
Sep 19 13:19:06.611: INFO: Number of nodes with available pods: 0
Sep 19 13:19:06.611: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 19 13:19:06.629: INFO: Number of nodes with available pods: 0
Sep 19 13:19:06.629: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:19:07.638: INFO: Number of nodes with available pods: 0
Sep 19 13:19:07.638: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:19:08.635: INFO: Number of nodes with available pods: 0
Sep 19 13:19:08.635: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:19:09.636: INFO: Number of nodes with available pods: 0
Sep 19 13:19:09.636: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:19:10.637: INFO: Number of nodes with available pods: 0
Sep 19 13:19:10.637: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:19:11.637: INFO: Number of nodes with available pods: 0
Sep 19 13:19:11.637: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:19:12.635: INFO: Number of nodes with available pods: 0
Sep 19 13:19:12.635: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:19:13.639: INFO: Number of nodes with available pods: 0
Sep 19 13:19:13.639: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:19:14.637: INFO: Number of nodes with available pods: 0
Sep 19 13:19:14.637: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:19:15.636: INFO: Number of nodes with available pods: 0
Sep 19 13:19:15.636: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:19:16.636: INFO: Number of nodes with available pods: 0
Sep 19 13:19:16.636: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:19:17.636: INFO: Number of nodes with available pods: 1
Sep 19 13:19:17.636: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4110, will wait for the garbage collector to delete the pods
Sep 19 13:19:17.719: INFO: Deleting DaemonSet.extensions daemon-set took: 13.202791ms
Sep 19 13:19:18.119: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.178354ms
Sep 19 13:19:25.027: INFO: Number of nodes with available pods: 0
Sep 19 13:19:25.027: INFO: Number of running nodes: 0, number of available pods: 0
Sep 19 13:19:25.035: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4110/daemonsets","resourceVersion":"19067"},"items":null}

Sep 19 13:19:25.042: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4110/pods","resourceVersion":"19067"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:19:25.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4110" for this suite.
Sep 19 13:19:31.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:19:31.439: INFO: namespace daemonsets-4110 deletion completed in 6.334157215s

• [SLOW TEST:29.161 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:19:31.439: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3018
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-b2f65384-55eb-493e-82d8-7a66b9feae41
STEP: Creating a pod to test consume configMaps
Sep 19 13:19:31.643: INFO: Waiting up to 5m0s for pod "pod-configmaps-23740704-6fa9-4fb6-b987-285a811d6e08" in namespace "configmap-3018" to be "success or failure"
Sep 19 13:19:31.654: INFO: Pod "pod-configmaps-23740704-6fa9-4fb6-b987-285a811d6e08": Phase="Pending", Reason="", readiness=false. Elapsed: 10.411127ms
Sep 19 13:19:33.661: INFO: Pod "pod-configmaps-23740704-6fa9-4fb6-b987-285a811d6e08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01769819s
Sep 19 13:19:35.668: INFO: Pod "pod-configmaps-23740704-6fa9-4fb6-b987-285a811d6e08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024382763s
STEP: Saw pod success
Sep 19 13:19:35.668: INFO: Pod "pod-configmaps-23740704-6fa9-4fb6-b987-285a811d6e08" satisfied condition "success or failure"
Sep 19 13:19:35.675: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod pod-configmaps-23740704-6fa9-4fb6-b987-285a811d6e08 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 19 13:19:35.708: INFO: Waiting for pod pod-configmaps-23740704-6fa9-4fb6-b987-285a811d6e08 to disappear
Sep 19 13:19:35.714: INFO: Pod pod-configmaps-23740704-6fa9-4fb6-b987-285a811d6e08 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:19:35.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3018" for this suite.
Sep 19 13:19:41.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:19:42.008: INFO: namespace configmap-3018 deletion completed in 6.285349929s

• [SLOW TEST:10.569 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:19:42.010: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8294
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-bb96e872-610c-4e00-a3ce-b70afde16c55
STEP: Creating a pod to test consume secrets
Sep 19 13:19:42.209: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b06db076-66a0-4d66-950e-5e2518a14d5b" in namespace "projected-8294" to be "success or failure"
Sep 19 13:19:42.216: INFO: Pod "pod-projected-secrets-b06db076-66a0-4d66-950e-5e2518a14d5b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.160746ms
Sep 19 13:19:44.224: INFO: Pod "pod-projected-secrets-b06db076-66a0-4d66-950e-5e2518a14d5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015176834s
Sep 19 13:19:46.232: INFO: Pod "pod-projected-secrets-b06db076-66a0-4d66-950e-5e2518a14d5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022936288s
STEP: Saw pod success
Sep 19 13:19:46.232: INFO: Pod "pod-projected-secrets-b06db076-66a0-4d66-950e-5e2518a14d5b" satisfied condition "success or failure"
Sep 19 13:19:46.244: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod pod-projected-secrets-b06db076-66a0-4d66-950e-5e2518a14d5b container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 19 13:19:46.280: INFO: Waiting for pod pod-projected-secrets-b06db076-66a0-4d66-950e-5e2518a14d5b to disappear
Sep 19 13:19:46.287: INFO: Pod pod-projected-secrets-b06db076-66a0-4d66-950e-5e2518a14d5b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:19:46.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8294" for this suite.
Sep 19 13:19:52.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:19:52.593: INFO: namespace projected-8294 deletion completed in 6.298016118s

• [SLOW TEST:10.583 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:19:52.593: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-f648bcde-6a60-4731-8ce2-dccb34450768
STEP: Creating a pod to test consume secrets
Sep 19 13:19:52.789: INFO: Waiting up to 5m0s for pod "pod-secrets-d3286d82-73d8-447e-ad6b-ab0cc43095d7" in namespace "secrets-5800" to be "success or failure"
Sep 19 13:19:52.796: INFO: Pod "pod-secrets-d3286d82-73d8-447e-ad6b-ab0cc43095d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.620112ms
Sep 19 13:19:54.802: INFO: Pod "pod-secrets-d3286d82-73d8-447e-ad6b-ab0cc43095d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0130836s
STEP: Saw pod success
Sep 19 13:19:54.802: INFO: Pod "pod-secrets-d3286d82-73d8-447e-ad6b-ab0cc43095d7" satisfied condition "success or failure"
Sep 19 13:19:54.809: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod pod-secrets-d3286d82-73d8-447e-ad6b-ab0cc43095d7 container secret-env-test: <nil>
STEP: delete the pod
Sep 19 13:19:54.861: INFO: Waiting for pod pod-secrets-d3286d82-73d8-447e-ad6b-ab0cc43095d7 to disappear
Sep 19 13:19:54.867: INFO: Pod pod-secrets-d3286d82-73d8-447e-ad6b-ab0cc43095d7 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:19:54.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5800" for this suite.
Sep 19 13:20:00.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:20:01.169: INFO: namespace secrets-5800 deletion completed in 6.294311934s

• [SLOW TEST:8.576 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:20:01.169: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6871
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:20:05.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6871" for this suite.
Sep 19 13:20:11.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:20:11.755: INFO: namespace emptydir-wrapper-6871 deletion completed in 6.302286893s

• [SLOW TEST:10.586 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:20:11.755: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4125
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Sep 19 13:20:15.971: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-889697622 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Sep 19 13:20:31.117: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:20:31.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4125" for this suite.
Sep 19 13:20:37.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:20:37.418: INFO: namespace pods-4125 deletion completed in 6.2865569s

• [SLOW TEST:25.663 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:20:37.419: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2006
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-714294b2-df72-46a0-b269-9f3f10789a56
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-714294b2-df72-46a0-b269-9f3f10789a56
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:22:12.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2006" for this suite.
Sep 19 13:22:34.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:22:35.238: INFO: namespace configmap-2006 deletion completed in 22.299046039s

• [SLOW TEST:117.819 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:22:35.238: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9696
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9696
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 19 13:22:35.427: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 19 13:22:51.727: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.8.22:8080/dial?request=hostName&protocol=udp&host=172.25.12.16&port=8081&tries=1'] Namespace:pod-network-test-9696 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:22:51.727: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:22:52.236: INFO: Waiting for endpoints: map[]
Sep 19 13:22:52.243: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.8.22:8080/dial?request=hostName&protocol=udp&host=172.25.10.18&port=8081&tries=1'] Namespace:pod-network-test-9696 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:22:52.243: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:22:52.873: INFO: Waiting for endpoints: map[]
Sep 19 13:22:52.879: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.8.22:8080/dial?request=hostName&protocol=udp&host=172.25.9.5&port=8081&tries=1'] Namespace:pod-network-test-9696 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:22:52.879: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:22:53.606: INFO: Waiting for endpoints: map[]
Sep 19 13:22:53.613: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.8.22:8080/dial?request=hostName&protocol=udp&host=172.25.8.21&port=8081&tries=1'] Namespace:pod-network-test-9696 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:22:53.613: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:22:54.174: INFO: Waiting for endpoints: map[]
Sep 19 13:22:54.182: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.8.22:8080/dial?request=hostName&protocol=udp&host=172.25.11.17&port=8081&tries=1'] Namespace:pod-network-test-9696 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:22:54.182: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:22:54.709: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:22:54.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9696" for this suite.
Sep 19 13:23:18.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:23:19.047: INFO: namespace pod-network-test-9696 deletion completed in 24.33037573s

• [SLOW TEST:43.810 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:23:19.048: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4124
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 13:23:19.234: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0a41f97-7a2d-4d50-942c-d7ed8d6de9e8" in namespace "projected-4124" to be "success or failure"
Sep 19 13:23:19.241: INFO: Pod "downwardapi-volume-f0a41f97-7a2d-4d50-942c-d7ed8d6de9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.016162ms
Sep 19 13:23:21.249: INFO: Pod "downwardapi-volume-f0a41f97-7a2d-4d50-942c-d7ed8d6de9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014542361s
Sep 19 13:23:23.257: INFO: Pod "downwardapi-volume-f0a41f97-7a2d-4d50-942c-d7ed8d6de9e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022826028s
STEP: Saw pod success
Sep 19 13:23:23.257: INFO: Pod "downwardapi-volume-f0a41f97-7a2d-4d50-942c-d7ed8d6de9e8" satisfied condition "success or failure"
Sep 19 13:23:23.267: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod downwardapi-volume-f0a41f97-7a2d-4d50-942c-d7ed8d6de9e8 container client-container: <nil>
STEP: delete the pod
Sep 19 13:23:23.347: INFO: Waiting for pod downwardapi-volume-f0a41f97-7a2d-4d50-942c-d7ed8d6de9e8 to disappear
Sep 19 13:23:23.356: INFO: Pod downwardapi-volume-f0a41f97-7a2d-4d50-942c-d7ed8d6de9e8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:23:23.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4124" for this suite.
Sep 19 13:23:29.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:23:29.675: INFO: namespace projected-4124 deletion completed in 6.309191043s

• [SLOW TEST:10.628 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:23:29.676: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3426
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-3023
STEP: Creating secret with name secret-test-484a50cf-feda-49aa-8537-33b183bcc1a8
STEP: Creating a pod to test consume secrets
Sep 19 13:23:30.041: INFO: Waiting up to 5m0s for pod "pod-secrets-503a5f9d-6a80-4442-abf2-473eefccaa67" in namespace "secrets-3426" to be "success or failure"
Sep 19 13:23:30.048: INFO: Pod "pod-secrets-503a5f9d-6a80-4442-abf2-473eefccaa67": Phase="Pending", Reason="", readiness=false. Elapsed: 6.400601ms
Sep 19 13:23:32.055: INFO: Pod "pod-secrets-503a5f9d-6a80-4442-abf2-473eefccaa67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013433801s
Sep 19 13:23:34.061: INFO: Pod "pod-secrets-503a5f9d-6a80-4442-abf2-473eefccaa67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020070671s
STEP: Saw pod success
Sep 19 13:23:34.062: INFO: Pod "pod-secrets-503a5f9d-6a80-4442-abf2-473eefccaa67" satisfied condition "success or failure"
Sep 19 13:23:34.068: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod pod-secrets-503a5f9d-6a80-4442-abf2-473eefccaa67 container secret-volume-test: <nil>
STEP: delete the pod
Sep 19 13:23:34.107: INFO: Waiting for pod pod-secrets-503a5f9d-6a80-4442-abf2-473eefccaa67 to disappear
Sep 19 13:23:34.113: INFO: Pod pod-secrets-503a5f9d-6a80-4442-abf2-473eefccaa67 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:23:34.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3426" for this suite.
Sep 19 13:23:40.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:23:40.482: INFO: namespace secrets-3426 deletion completed in 6.357599713s
STEP: Destroying namespace "secret-namespace-3023" for this suite.
Sep 19 13:23:46.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:23:46.800: INFO: namespace secret-namespace-3023 deletion completed in 6.318131017s

• [SLOW TEST:17.124 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:23:46.800: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2111
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 13:23:51.045: INFO: Waiting up to 5m0s for pod "client-envvars-d7ad4f02-f023-45df-90b6-e0a364952004" in namespace "pods-2111" to be "success or failure"
Sep 19 13:23:51.052: INFO: Pod "client-envvars-d7ad4f02-f023-45df-90b6-e0a364952004": Phase="Pending", Reason="", readiness=false. Elapsed: 7.511189ms
Sep 19 13:23:53.059: INFO: Pod "client-envvars-d7ad4f02-f023-45df-90b6-e0a364952004": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013540804s
Sep 19 13:23:55.065: INFO: Pod "client-envvars-d7ad4f02-f023-45df-90b6-e0a364952004": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019753418s
STEP: Saw pod success
Sep 19 13:23:55.065: INFO: Pod "client-envvars-d7ad4f02-f023-45df-90b6-e0a364952004" satisfied condition "success or failure"
Sep 19 13:23:55.070: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod client-envvars-d7ad4f02-f023-45df-90b6-e0a364952004 container env3cont: <nil>
STEP: delete the pod
Sep 19 13:23:55.111: INFO: Waiting for pod client-envvars-d7ad4f02-f023-45df-90b6-e0a364952004 to disappear
Sep 19 13:23:55.117: INFO: Pod client-envvars-d7ad4f02-f023-45df-90b6-e0a364952004 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:23:55.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2111" for this suite.
Sep 19 13:24:35.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:24:35.465: INFO: namespace pods-2111 deletion completed in 40.338390655s

• [SLOW TEST:48.665 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:24:35.465: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8268
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-84f05037-7d4e-4cb8-af4a-f228de0f57f9
STEP: Creating a pod to test consume configMaps
Sep 19 13:24:35.677: INFO: Waiting up to 5m0s for pod "pod-configmaps-b1c1a513-0dcd-4b58-97c1-fc570091f5a7" in namespace "configmap-8268" to be "success or failure"
Sep 19 13:24:35.689: INFO: Pod "pod-configmaps-b1c1a513-0dcd-4b58-97c1-fc570091f5a7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.819519ms
Sep 19 13:24:37.696: INFO: Pod "pod-configmaps-b1c1a513-0dcd-4b58-97c1-fc570091f5a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018857645s
Sep 19 13:24:39.702: INFO: Pod "pod-configmaps-b1c1a513-0dcd-4b58-97c1-fc570091f5a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02539594s
STEP: Saw pod success
Sep 19 13:24:39.702: INFO: Pod "pod-configmaps-b1c1a513-0dcd-4b58-97c1-fc570091f5a7" satisfied condition "success or failure"
Sep 19 13:24:39.709: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod pod-configmaps-b1c1a513-0dcd-4b58-97c1-fc570091f5a7 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 19 13:24:39.788: INFO: Waiting for pod pod-configmaps-b1c1a513-0dcd-4b58-97c1-fc570091f5a7 to disappear
Sep 19 13:24:39.793: INFO: Pod pod-configmaps-b1c1a513-0dcd-4b58-97c1-fc570091f5a7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:24:39.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8268" for this suite.
Sep 19 13:24:45.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:24:46.189: INFO: namespace configmap-8268 deletion completed in 6.387990416s

• [SLOW TEST:10.724 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:24:46.190: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6175
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-501c2371-c997-4b03-a7e4-528ebf9db573
STEP: Creating a pod to test consume configMaps
Sep 19 13:24:46.389: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-72e01b6d-876e-4fd7-8f2c-2717cf76dd78" in namespace "projected-6175" to be "success or failure"
Sep 19 13:24:46.397: INFO: Pod "pod-projected-configmaps-72e01b6d-876e-4fd7-8f2c-2717cf76dd78": Phase="Pending", Reason="", readiness=false. Elapsed: 7.658699ms
Sep 19 13:24:48.410: INFO: Pod "pod-projected-configmaps-72e01b6d-876e-4fd7-8f2c-2717cf76dd78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021584528s
Sep 19 13:24:50.419: INFO: Pod "pod-projected-configmaps-72e01b6d-876e-4fd7-8f2c-2717cf76dd78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03047831s
STEP: Saw pod success
Sep 19 13:24:50.419: INFO: Pod "pod-projected-configmaps-72e01b6d-876e-4fd7-8f2c-2717cf76dd78" satisfied condition "success or failure"
Sep 19 13:24:50.426: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod pod-projected-configmaps-72e01b6d-876e-4fd7-8f2c-2717cf76dd78 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 19 13:24:50.522: INFO: Waiting for pod pod-projected-configmaps-72e01b6d-876e-4fd7-8f2c-2717cf76dd78 to disappear
Sep 19 13:24:50.535: INFO: Pod pod-projected-configmaps-72e01b6d-876e-4fd7-8f2c-2717cf76dd78 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:24:50.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6175" for this suite.
Sep 19 13:24:56.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:24:56.951: INFO: namespace projected-6175 deletion completed in 6.398442117s

• [SLOW TEST:10.762 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:24:56.952: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6570
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Sep 19 13:24:57.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 create -f - --namespace=kubectl-6570'
Sep 19 13:24:57.367: INFO: stderr: ""
Sep 19 13:24:57.367: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 19 13:24:57.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6570'
Sep 19 13:24:57.500: INFO: stderr: ""
Sep 19 13:24:57.500: INFO: stdout: "update-demo-nautilus-5cznm update-demo-nautilus-mqjgk "
Sep 19 13:24:57.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-5cznm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6570'
Sep 19 13:24:57.584: INFO: stderr: ""
Sep 19 13:24:57.584: INFO: stdout: ""
Sep 19 13:24:57.584: INFO: update-demo-nautilus-5cznm is created but not running
Sep 19 13:25:02.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6570'
Sep 19 13:25:02.707: INFO: stderr: ""
Sep 19 13:25:02.707: INFO: stdout: "update-demo-nautilus-5cznm update-demo-nautilus-mqjgk "
Sep 19 13:25:02.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-5cznm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6570'
Sep 19 13:25:02.802: INFO: stderr: ""
Sep 19 13:25:02.802: INFO: stdout: "true"
Sep 19 13:25:02.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-5cznm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6570'
Sep 19 13:25:02.886: INFO: stderr: ""
Sep 19 13:25:02.886: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 19 13:25:02.886: INFO: validating pod update-demo-nautilus-5cznm
Sep 19 13:25:02.987: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 19 13:25:02.987: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 19 13:25:02.987: INFO: update-demo-nautilus-5cznm is verified up and running
Sep 19 13:25:02.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-mqjgk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6570'
Sep 19 13:25:03.066: INFO: stderr: ""
Sep 19 13:25:03.066: INFO: stdout: "true"
Sep 19 13:25:03.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-mqjgk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6570'
Sep 19 13:25:03.151: INFO: stderr: ""
Sep 19 13:25:03.151: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 19 13:25:03.151: INFO: validating pod update-demo-nautilus-mqjgk
Sep 19 13:25:03.252: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 19 13:25:03.252: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 19 13:25:03.252: INFO: update-demo-nautilus-mqjgk is verified up and running
STEP: using delete to clean up resources
Sep 19 13:25:03.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete --grace-period=0 --force -f - --namespace=kubectl-6570'
Sep 19 13:25:03.349: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 19 13:25:03.349: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 19 13:25:03.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6570'
Sep 19 13:25:03.443: INFO: stderr: "No resources found.\n"
Sep 19 13:25:03.443: INFO: stdout: ""
Sep 19 13:25:03.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -l name=update-demo --namespace=kubectl-6570 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 19 13:25:03.527: INFO: stderr: ""
Sep 19 13:25:03.527: INFO: stdout: "update-demo-nautilus-5cznm\nupdate-demo-nautilus-mqjgk\n"
Sep 19 13:25:04.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6570'
Sep 19 13:25:04.124: INFO: stderr: "No resources found.\n"
Sep 19 13:25:04.124: INFO: stdout: ""
Sep 19 13:25:04.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -l name=update-demo --namespace=kubectl-6570 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 19 13:25:04.208: INFO: stderr: ""
Sep 19 13:25:04.208: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:25:04.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6570" for this suite.
Sep 19 13:25:26.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:25:26.643: INFO: namespace kubectl-6570 deletion completed in 22.420843083s

• [SLOW TEST:29.691 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:25:26.643: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 19 13:25:36.925: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 19 13:25:36.931: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 19 13:25:38.932: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 19 13:25:38.940: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 19 13:25:40.932: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 19 13:25:40.938: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 19 13:25:42.932: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 19 13:25:42.939: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 19 13:25:44.932: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 19 13:25:44.939: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 19 13:25:46.932: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 19 13:25:46.946: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 19 13:25:48.932: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 19 13:25:48.939: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 19 13:25:50.932: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 19 13:25:50.939: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 19 13:25:52.932: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 19 13:25:52.939: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 19 13:25:54.932: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 19 13:25:54.939: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 19 13:25:56.932: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 19 13:25:56.939: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 19 13:25:58.932: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 19 13:25:58.938: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 19 13:26:00.932: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 19 13:26:00.939: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 19 13:26:02.933: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 19 13:26:02.940: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:26:02.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6244" for this suite.
Sep 19 13:26:27.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:26:27.332: INFO: namespace container-lifecycle-hook-6244 deletion completed in 24.325931341s

• [SLOW TEST:60.689 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:26:27.332: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5055
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Sep 19 13:26:27.533: INFO: Waiting up to 5m0s for pod "client-containers-994f15a9-f8e3-4c5e-8b1d-71d6bc418b56" in namespace "containers-5055" to be "success or failure"
Sep 19 13:26:27.541: INFO: Pod "client-containers-994f15a9-f8e3-4c5e-8b1d-71d6bc418b56": Phase="Pending", Reason="", readiness=false. Elapsed: 8.04186ms
Sep 19 13:26:29.554: INFO: Pod "client-containers-994f15a9-f8e3-4c5e-8b1d-71d6bc418b56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020338717s
Sep 19 13:26:31.563: INFO: Pod "client-containers-994f15a9-f8e3-4c5e-8b1d-71d6bc418b56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029514022s
STEP: Saw pod success
Sep 19 13:26:31.563: INFO: Pod "client-containers-994f15a9-f8e3-4c5e-8b1d-71d6bc418b56" satisfied condition "success or failure"
Sep 19 13:26:31.572: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod client-containers-994f15a9-f8e3-4c5e-8b1d-71d6bc418b56 container test-container: <nil>
STEP: delete the pod
Sep 19 13:26:31.613: INFO: Waiting for pod client-containers-994f15a9-f8e3-4c5e-8b1d-71d6bc418b56 to disappear
Sep 19 13:26:31.620: INFO: Pod client-containers-994f15a9-f8e3-4c5e-8b1d-71d6bc418b56 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:26:31.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5055" for this suite.
Sep 19 13:26:37.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:26:37.934: INFO: namespace containers-5055 deletion completed in 6.30265328s

• [SLOW TEST:10.602 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:26:37.935: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8164
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Sep 19 13:26:38.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 --namespace=kubectl-8164 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Sep 19 13:26:41.142: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Sep 19 13:26:41.142: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:26:43.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8164" for this suite.
Sep 19 13:26:49.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:26:49.484: INFO: namespace kubectl-8164 deletion completed in 6.317071128s

• [SLOW TEST:11.549 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:26:49.484: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4171
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-vjd5
STEP: Creating a pod to test atomic-volume-subpath
Sep 19 13:26:49.705: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-vjd5" in namespace "subpath-4171" to be "success or failure"
Sep 19 13:26:49.714: INFO: Pod "pod-subpath-test-secret-vjd5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.893095ms
Sep 19 13:26:51.721: INFO: Pod "pod-subpath-test-secret-vjd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014883223s
Sep 19 13:26:53.729: INFO: Pod "pod-subpath-test-secret-vjd5": Phase="Running", Reason="", readiness=true. Elapsed: 4.023461281s
Sep 19 13:26:55.737: INFO: Pod "pod-subpath-test-secret-vjd5": Phase="Running", Reason="", readiness=true. Elapsed: 6.031185913s
Sep 19 13:26:57.746: INFO: Pod "pod-subpath-test-secret-vjd5": Phase="Running", Reason="", readiness=true. Elapsed: 8.040082672s
Sep 19 13:26:59.755: INFO: Pod "pod-subpath-test-secret-vjd5": Phase="Running", Reason="", readiness=true. Elapsed: 10.049066673s
Sep 19 13:27:01.770: INFO: Pod "pod-subpath-test-secret-vjd5": Phase="Running", Reason="", readiness=true. Elapsed: 12.064382138s
Sep 19 13:27:03.782: INFO: Pod "pod-subpath-test-secret-vjd5": Phase="Running", Reason="", readiness=true. Elapsed: 14.075788942s
Sep 19 13:27:05.788: INFO: Pod "pod-subpath-test-secret-vjd5": Phase="Running", Reason="", readiness=true. Elapsed: 16.082408015s
Sep 19 13:27:07.795: INFO: Pod "pod-subpath-test-secret-vjd5": Phase="Running", Reason="", readiness=true. Elapsed: 18.089194435s
Sep 19 13:27:09.807: INFO: Pod "pod-subpath-test-secret-vjd5": Phase="Running", Reason="", readiness=true. Elapsed: 20.100757768s
Sep 19 13:27:11.814: INFO: Pod "pod-subpath-test-secret-vjd5": Phase="Running", Reason="", readiness=true. Elapsed: 22.108438588s
Sep 19 13:27:13.821: INFO: Pod "pod-subpath-test-secret-vjd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.115222726s
STEP: Saw pod success
Sep 19 13:27:13.821: INFO: Pod "pod-subpath-test-secret-vjd5" satisfied condition "success or failure"
Sep 19 13:27:13.828: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod pod-subpath-test-secret-vjd5 container test-container-subpath-secret-vjd5: <nil>
STEP: delete the pod
Sep 19 13:27:13.873: INFO: Waiting for pod pod-subpath-test-secret-vjd5 to disappear
Sep 19 13:27:13.878: INFO: Pod pod-subpath-test-secret-vjd5 no longer exists
STEP: Deleting pod pod-subpath-test-secret-vjd5
Sep 19 13:27:13.879: INFO: Deleting pod "pod-subpath-test-secret-vjd5" in namespace "subpath-4171"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:27:13.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4171" for this suite.
Sep 19 13:27:19.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:27:20.200: INFO: namespace subpath-4171 deletion completed in 6.307378339s

• [SLOW TEST:30.716 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:27:20.200: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5562
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-3554a2c7-6823-4f0e-b805-547608a0e720
Sep 19 13:27:20.384: INFO: Pod name my-hostname-basic-3554a2c7-6823-4f0e-b805-547608a0e720: Found 0 pods out of 1
Sep 19 13:27:25.392: INFO: Pod name my-hostname-basic-3554a2c7-6823-4f0e-b805-547608a0e720: Found 1 pods out of 1
Sep 19 13:27:25.392: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3554a2c7-6823-4f0e-b805-547608a0e720" are running
Sep 19 13:27:25.399: INFO: Pod "my-hostname-basic-3554a2c7-6823-4f0e-b805-547608a0e720-w24q5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-19 13:27:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-19 13:27:23 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-19 13:27:23 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-19 13:27:20 +0000 UTC Reason: Message:}])
Sep 19 13:27:25.399: INFO: Trying to dial the pod
Sep 19 13:27:30.511: INFO: Controller my-hostname-basic-3554a2c7-6823-4f0e-b805-547608a0e720: Got expected result from replica 1 [my-hostname-basic-3554a2c7-6823-4f0e-b805-547608a0e720-w24q5]: "my-hostname-basic-3554a2c7-6823-4f0e-b805-547608a0e720-w24q5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:27:30.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5562" for this suite.
Sep 19 13:27:36.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:27:36.881: INFO: namespace replication-controller-5562 deletion completed in 6.361773795s

• [SLOW TEST:16.682 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:27:36.882: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5445
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep 19 13:27:37.071: INFO: Waiting up to 5m0s for pod "downward-api-6942a6c9-c3b4-4456-baa6-ccb6d826b90d" in namespace "downward-api-5445" to be "success or failure"
Sep 19 13:27:37.080: INFO: Pod "downward-api-6942a6c9-c3b4-4456-baa6-ccb6d826b90d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.174531ms
Sep 19 13:27:39.087: INFO: Pod "downward-api-6942a6c9-c3b4-4456-baa6-ccb6d826b90d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016772839s
Sep 19 13:27:41.095: INFO: Pod "downward-api-6942a6c9-c3b4-4456-baa6-ccb6d826b90d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023988331s
STEP: Saw pod success
Sep 19 13:27:41.095: INFO: Pod "downward-api-6942a6c9-c3b4-4456-baa6-ccb6d826b90d" satisfied condition "success or failure"
Sep 19 13:27:41.101: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod downward-api-6942a6c9-c3b4-4456-baa6-ccb6d826b90d container dapi-container: <nil>
STEP: delete the pod
Sep 19 13:27:41.135: INFO: Waiting for pod downward-api-6942a6c9-c3b4-4456-baa6-ccb6d826b90d to disappear
Sep 19 13:27:41.141: INFO: Pod downward-api-6942a6c9-c3b4-4456-baa6-ccb6d826b90d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:27:41.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5445" for this suite.
Sep 19 13:27:47.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:27:47.486: INFO: namespace downward-api-5445 deletion completed in 6.336375759s

• [SLOW TEST:10.604 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:27:47.486: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7925
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 13:27:47.657: INFO: Creating deployment "nginx-deployment"
Sep 19 13:27:47.668: INFO: Waiting for observed generation 1
Sep 19 13:27:49.686: INFO: Waiting for all required pods to come up
Sep 19 13:27:49.693: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 19 13:27:55.708: INFO: Waiting for deployment "nginx-deployment" to complete
Sep 19 13:27:55.722: INFO: Updating deployment "nginx-deployment" with a non-existent image
Sep 19 13:27:55.740: INFO: Updating deployment nginx-deployment
Sep 19 13:27:55.740: INFO: Waiting for observed generation 2
Sep 19 13:27:57.755: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 19 13:27:57.763: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 19 13:27:57.771: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep 19 13:27:57.795: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 19 13:27:57.795: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 19 13:27:57.805: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep 19 13:27:57.818: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Sep 19 13:27:57.818: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Sep 19 13:27:57.838: INFO: Updating deployment nginx-deployment
Sep 19 13:27:57.838: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Sep 19 13:27:57.859: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 19 13:27:59.880: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep 19 13:27:59.896: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-7925,SelfLink:/apis/apps/v1/namespaces/deployment-7925/deployments/nginx-deployment,UID:5d92f2f1-eda2-46b5-a0c4-cdeb5cbe04de,ResourceVersion:21529,Generation:3,CreationTimestamp:2019-09-19 13:27:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-09-19 13:27:57 +0000 UTC 2019-09-19 13:27:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-09-19 13:27:58 +0000 UTC 2019-09-19 13:27:47 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Sep 19 13:27:59.906: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-7925,SelfLink:/apis/apps/v1/namespaces/deployment-7925/replicasets/nginx-deployment-55fb7cb77f,UID:bb887462-8f3e-482e-bf80-70242ca061ab,ResourceVersion:21518,Generation:3,CreationTimestamp:2019-09-19 13:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 5d92f2f1-eda2-46b5-a0c4-cdeb5cbe04de 0xc001603e27 0xc001603e28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 19 13:27:59.906: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Sep 19 13:27:59.906: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-7925,SelfLink:/apis/apps/v1/namespaces/deployment-7925/replicasets/nginx-deployment-7b8c6f4498,UID:f510cdba-9cee-4d71-8f5a-f6bedd6c77ca,ResourceVersion:21514,Generation:3,CreationTimestamp:2019-09-19 13:27:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 5d92f2f1-eda2-46b5-a0c4-cdeb5cbe04de 0xc001603ef7 0xc001603ef8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Sep 19 13:27:59.918: INFO: Pod "nginx-deployment-55fb7cb77f-2dwlt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2dwlt,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-55fb7cb77f-2dwlt,UID:94d69d56-9d82-4c0e-a949-9cf8f36ed0a1,ResourceVersion:21601,Generation:0,CreationTimestamp:2019-09-19 13:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.12.24/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f bb887462-8f3e-482e-bf80-70242ca061ab 0xc002d690b7 0xc002d690b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-36-147.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d69120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d69140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC  }],Message:,Reason:,HostIP:172.31.36.147,PodIP:172.25.12.24,StartTime:2019-09-19 13:27:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.918: INFO: Pod "nginx-deployment-55fb7cb77f-46hcs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-46hcs,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-55fb7cb77f-46hcs,UID:a9e1ad40-939b-4996-a82a-312da0bc09f7,ResourceVersion:21582,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.12.25/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f bb887462-8f3e-482e-bf80-70242ca061ab 0xc002d69240 0xc002d69241}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-36-147.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d692b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d692d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  }],Message:,Reason:,HostIP:172.31.36.147,PodIP:,StartTime:2019-09-19 13:27:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.918: INFO: Pod "nginx-deployment-55fb7cb77f-6rt6x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6rt6x,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-55fb7cb77f-6rt6x,UID:e733e006-a74a-4226-8434-9f9bea1cf989,ResourceVersion:21589,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.11.25/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f bb887462-8f3e-482e-bf80-70242ca061ab 0xc002d693b0 0xc002d693b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-46-204.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d69420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d69440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  }],Message:,Reason:,HostIP:172.31.46.204,PodIP:,StartTime:2019-09-19 13:27:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.918: INFO: Pod "nginx-deployment-55fb7cb77f-7nlph" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-7nlph,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-55fb7cb77f-7nlph,UID:48701f6c-0fe6-4a5e-8cdd-1e2683b2bfe8,ResourceVersion:21598,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.12.27/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f bb887462-8f3e-482e-bf80-70242ca061ab 0xc002d69520 0xc002d69521}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-36-147.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d69590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d695b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC  }],Message:,Reason:,HostIP:172.31.36.147,PodIP:,StartTime:2019-09-19 13:27:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.918: INFO: Pod "nginx-deployment-55fb7cb77f-kgj9t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-kgj9t,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-55fb7cb77f-kgj9t,UID:8a6edc44-6cc3-4384-88ce-8fc3860c0041,ResourceVersion:21425,Generation:0,CreationTimestamp:2019-09-19 13:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.10.23/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f bb887462-8f3e-482e-bf80-70242ca061ab 0xc002d696a0 0xc002d696a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-44-209.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d69710} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d69730}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC  }],Message:,Reason:,HostIP:172.31.44.209,PodIP:,StartTime:2019-09-19 13:27:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.918: INFO: Pod "nginx-deployment-55fb7cb77f-kvnbx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-kvnbx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-55fb7cb77f-kvnbx,UID:13438c45-c361-454a-a8e2-1e99aed368bd,ResourceVersion:21527,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f bb887462-8f3e-482e-bf80-70242ca061ab 0xc002d69800 0xc002d69801}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-83.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d69870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d69890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC  }],Message:,Reason:,HostIP:172.31.33.83,PodIP:,StartTime:2019-09-19 13:27:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.918: INFO: Pod "nginx-deployment-55fb7cb77f-l57c8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-l57c8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-55fb7cb77f-l57c8,UID:a6fc220a-fd6f-4a6f-a3b9-ba79bc213d25,ResourceVersion:21519,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f bb887462-8f3e-482e-bf80-70242ca061ab 0xc002d69960 0xc002d69961}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-91.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d699d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d699f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  }],Message:,Reason:,HostIP:172.31.42.91,PodIP:,StartTime:2019-09-19 13:27:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.919: INFO: Pod "nginx-deployment-55fb7cb77f-mmlb8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mmlb8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-55fb7cb77f-mmlb8,UID:2a597923-5331-4892-b184-92cdc1a29710,ResourceVersion:21539,Generation:0,CreationTimestamp:2019-09-19 13:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.11.23/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f bb887462-8f3e-482e-bf80-70242ca061ab 0xc002d69ad0 0xc002d69ad1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-46-204.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d69b40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d69b60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC  }],Message:,Reason:,HostIP:172.31.46.204,PodIP:172.25.11.23,StartTime:2019-09-19 13:27:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.919: INFO: Pod "nginx-deployment-55fb7cb77f-n5px5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-n5px5,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-55fb7cb77f-n5px5,UID:f6818ae1-e059-4f21-89f3-a72a5ac280d0,ResourceVersion:21534,Generation:0,CreationTimestamp:2019-09-19 13:27:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f bb887462-8f3e-482e-bf80-70242ca061ab 0xc002d69c50 0xc002d69c51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-44-209.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d69cc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d69ce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC  }],Message:,Reason:,HostIP:172.31.44.209,PodIP:,StartTime:2019-09-19 13:27:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.919: INFO: Pod "nginx-deployment-55fb7cb77f-r6z6b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-r6z6b,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-55fb7cb77f-r6z6b,UID:2f753a99-3a57-4bff-bd86-5462c3baa6f7,ResourceVersion:21491,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f bb887462-8f3e-482e-bf80-70242ca061ab 0xc002d69db0 0xc002d69db1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-44-209.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d69e20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d69e40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  }],Message:,Reason:,HostIP:172.31.44.209,PodIP:,StartTime:2019-09-19 13:27:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.919: INFO: Pod "nginx-deployment-55fb7cb77f-x7bv5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-x7bv5,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-55fb7cb77f-x7bv5,UID:09bb9927-7626-47ae-bb20-68ab37f90776,ResourceVersion:21599,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.11.26/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f bb887462-8f3e-482e-bf80-70242ca061ab 0xc002d69f20 0xc002d69f21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-46-204.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d69f90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d69fb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC  }],Message:,Reason:,HostIP:172.31.46.204,PodIP:,StartTime:2019-09-19 13:27:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.919: INFO: Pod "nginx-deployment-55fb7cb77f-xbwlp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-xbwlp,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-55fb7cb77f-xbwlp,UID:783c3246-f932-4f81-8a3f-dfa069a28674,ResourceVersion:21557,Generation:0,CreationTimestamp:2019-09-19 13:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.9.8/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f bb887462-8f3e-482e-bf80-70242ca061ab 0xc002540090 0xc002540091}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-91.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002540100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002540120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC  }],Message:,Reason:,HostIP:172.31.42.91,PodIP:172.25.9.8,StartTime:2019-09-19 13:27:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.919: INFO: Pod "nginx-deployment-55fb7cb77f-zxhpm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-zxhpm,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-55fb7cb77f-zxhpm,UID:e9799c01-a3f3-45a2-96ba-8e122321358d,ResourceVersion:21537,Generation:0,CreationTimestamp:2019-09-19 13:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.8.30/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f bb887462-8f3e-482e-bf80-70242ca061ab 0xc002540220 0xc002540221}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-83.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002540290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025402b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:55 +0000 UTC  }],Message:,Reason:,HostIP:172.31.33.83,PodIP:172.25.8.30,StartTime:2019-09-19 13:27:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.919: INFO: Pod "nginx-deployment-7b8c6f4498-59qn8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-59qn8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-59qn8,UID:5484ce26-affd-473e-992a-765627d21a0b,ResourceVersion:21322,Generation:0,CreationTimestamp:2019-09-19 13:27:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.8.28/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc0025403b0 0xc0025403b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-83.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002540410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002540430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  }],Message:,Reason:,HostIP:172.31.33.83,PodIP:172.25.8.28,StartTime:2019-09-19 13:27:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-19 13:27:49 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://7f2342f61c906ff51348cc7c17aa7f00961d35ab1c5b9829054b318f96d1e785}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.919: INFO: Pod "nginx-deployment-7b8c6f4498-5z79g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5z79g,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-5z79g,UID:b7b70a8f-4ea4-48d2-8b22-c9ec32cc276a,ResourceVersion:21507,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002540500 0xc002540501}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-36-147.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002540560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002540580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  }],Message:,Reason:,HostIP:172.31.36.147,PodIP:,StartTime:2019-09-19 13:27:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.920: INFO: Pod "nginx-deployment-7b8c6f4498-6zzmk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-6zzmk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-6zzmk,UID:48ea7def-7e3c-4e18-9c51-6cec85e4a90c,ResourceVersion:21319,Generation:0,CreationTimestamp:2019-09-19 13:27:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.8.29/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002540657 0xc002540658}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-83.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025406c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025406e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  }],Message:,Reason:,HostIP:172.31.33.83,PodIP:172.25.8.29,StartTime:2019-09-19 13:27:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-19 13:27:49 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://a9af1867960651ae4a4700cdbcaaa208a9544be930ad37240edb712add05d67a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.920: INFO: Pod "nginx-deployment-7b8c6f4498-8cb27" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8cb27,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-8cb27,UID:70875c96-73d7-4bbe-b7b5-d70a0db6877d,ResourceVersion:21526,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc0025407b0 0xc0025407b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-44-209.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002540820} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002540840}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC  }],Message:,Reason:,HostIP:172.31.44.209,PodIP:,StartTime:2019-09-19 13:27:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.920: INFO: Pod "nginx-deployment-7b8c6f4498-bbrbh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-bbrbh,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-bbrbh,UID:ccb74afe-4ec2-4ac1-aaa7-79c1adec970b,ResourceVersion:21498,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002540907 0xc002540908}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-83.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002540970} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002540990}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  }],Message:,Reason:,HostIP:172.31.33.83,PodIP:,StartTime:2019-09-19 13:27:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.920: INFO: Pod "nginx-deployment-7b8c6f4498-cxzlq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cxzlq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-cxzlq,UID:42706017-65df-4acf-b337-902a81c4e66d,ResourceVersion:21593,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.10.25/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002540a67 0xc002540a68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-44-209.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002540ad0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002540af0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  }],Message:,Reason:,HostIP:172.31.44.209,PodIP:,StartTime:2019-09-19 13:27:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.920: INFO: Pod "nginx-deployment-7b8c6f4498-d2bvm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-d2bvm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-d2bvm,UID:60b8ea09-2752-435c-83fe-a34ba0aa8219,ResourceVersion:21315,Generation:0,CreationTimestamp:2019-09-19 13:27:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.11.21/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002540bc7 0xc002540bc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-46-204.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002540c30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002540c50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  }],Message:,Reason:,HostIP:172.31.46.204,PodIP:172.25.11.21,StartTime:2019-09-19 13:27:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-19 13:27:49 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://76f9a91c76f580f5db631bae7ffe57f8298f14ccb886444ec2681a4b1b720534}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.920: INFO: Pod "nginx-deployment-7b8c6f4498-dpksm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dpksm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-dpksm,UID:8492c532-23e0-42ef-bac7-7f9f9d88b8b6,ResourceVersion:21596,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.12.26/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002540d37 0xc002540d38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-36-147.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002540da0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002540dc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  }],Message:,Reason:,HostIP:172.31.36.147,PodIP:,StartTime:2019-09-19 13:27:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.920: INFO: Pod "nginx-deployment-7b8c6f4498-fkxr5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fkxr5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-fkxr5,UID:e05ca063-32ed-427f-a3ac-75386ff8f2aa,ResourceVersion:21577,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.9.9/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002540e97 0xc002540e98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-91.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002540f00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002540f20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  }],Message:,Reason:,HostIP:172.31.42.91,PodIP:,StartTime:2019-09-19 13:27:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.921: INFO: Pod "nginx-deployment-7b8c6f4498-ftc6k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ftc6k,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-ftc6k,UID:7d28b240-032c-4c51-b77d-3a1d602f911f,ResourceVersion:21573,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.8.31/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002540ff7 0xc002540ff8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-83.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002541060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002541080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  }],Message:,Reason:,HostIP:172.31.33.83,PodIP:,StartTime:2019-09-19 13:27:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.921: INFO: Pod "nginx-deployment-7b8c6f4498-hnfjf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hnfjf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-hnfjf,UID:58a4bf5b-d1ca-4964-9396-21fa8af23daa,ResourceVersion:21574,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.11.24/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002541157 0xc002541158}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-46-204.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025411c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025411e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  }],Message:,Reason:,HostIP:172.31.46.204,PodIP:,StartTime:2019-09-19 13:27:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.922: INFO: Pod "nginx-deployment-7b8c6f4498-hr9ns" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hr9ns,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-hr9ns,UID:7f3248f0-6593-4974-a88d-dd2c763fa6ef,ResourceVersion:21326,Generation:0,CreationTimestamp:2019-09-19 13:27:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.12.22/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc0025412b7 0xc0025412b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-36-147.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002541320} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002541340}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  }],Message:,Reason:,HostIP:172.31.36.147,PodIP:172.25.12.22,StartTime:2019-09-19 13:27:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-19 13:27:49 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://0021d6138a838214f44483ce61e757fa0d8f3ac32751dbca14db04f9a162dfce}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.922: INFO: Pod "nginx-deployment-7b8c6f4498-j472q" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-j472q,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-j472q,UID:44007770-7493-440f-915e-baca69839eb4,ResourceVersion:21329,Generation:0,CreationTimestamp:2019-09-19 13:27:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.12.23/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002541427 0xc002541428}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-36-147.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002541490} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025414b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  }],Message:,Reason:,HostIP:172.31.36.147,PodIP:172.25.12.23,StartTime:2019-09-19 13:27:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-19 13:27:49 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c962b7c750df2803c21ed4af0781b81bc9918c50525264b6525b6466a8f37845}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.922: INFO: Pod "nginx-deployment-7b8c6f4498-kpl5k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kpl5k,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-kpl5k,UID:80a070c2-02c4-46ef-a74d-c124135ab485,ResourceVersion:21342,Generation:0,CreationTimestamp:2019-09-19 13:27:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.10.21/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002541597 0xc002541598}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-44-209.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002541600} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002541620}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  }],Message:,Reason:,HostIP:172.31.44.209,PodIP:172.25.10.21,StartTime:2019-09-19 13:27:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-19 13:27:52 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ae7d5e3f5f234d3da837e6c8f859181b916a21c45373fdab2062e727eab66e4c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.922: INFO: Pod "nginx-deployment-7b8c6f4498-ls4c5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ls4c5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-ls4c5,UID:ace04285-abdb-4435-96a9-09d48effb331,ResourceVersion:21352,Generation:0,CreationTimestamp:2019-09-19 13:27:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.9.6/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002541707 0xc002541708}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-91.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002541770} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002541790}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  }],Message:,Reason:,HostIP:172.31.42.91,PodIP:172.25.9.6,StartTime:2019-09-19 13:27:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-19 13:27:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2ad8f506804c0651f532abb0c96c3703200eaee59a2b43c64fd091cacf347f5f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.922: INFO: Pod "nginx-deployment-7b8c6f4498-mdntf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mdntf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-mdntf,UID:d615c0a2-35c8-4813-953a-282d3fd44d3b,ResourceVersion:21312,Generation:0,CreationTimestamp:2019-09-19 13:27:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.11.22/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002541870 0xc002541871}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-46-204.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025418d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025418f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:47 +0000 UTC  }],Message:,Reason:,HostIP:172.31.46.204,PodIP:172.25.11.22,StartTime:2019-09-19 13:27:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-19 13:27:49 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c6320c2f224d0029c4fbf8f96cfd6691040102fe6c214ccccf45e23174065e2f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.923: INFO: Pod "nginx-deployment-7b8c6f4498-mvr4x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mvr4x,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-mvr4x,UID:9f89e4a6-49ce-4752-a34b-c3872f207092,ResourceVersion:21581,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.10.24/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc0025419d7 0xc0025419d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-44-209.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002541a40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002541a60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  }],Message:,Reason:,HostIP:172.31.44.209,PodIP:,StartTime:2019-09-19 13:27:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.923: INFO: Pod "nginx-deployment-7b8c6f4498-nrfkp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-nrfkp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-nrfkp,UID:773d52f8-efb3-42da-8cc2-84e7b9d59c7d,ResourceVersion:21501,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002541b27 0xc002541b28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-91.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002541b90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002541bb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  }],Message:,Reason:,HostIP:172.31.42.91,PodIP:,StartTime:2019-09-19 13:27:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.923: INFO: Pod "nginx-deployment-7b8c6f4498-smk8v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-smk8v,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-smk8v,UID:0ca690fa-8439-4a94-bcfa-a94506e8851c,ResourceVersion:21484,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002541c77 0xc002541c78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-83.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002541ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002541d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  }],Message:,Reason:,HostIP:172.31.33.83,PodIP:,StartTime:2019-09-19 13:27:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 19 13:27:59.923: INFO: Pod "nginx-deployment-7b8c6f4498-tsjs6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tsjs6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7925,SelfLink:/api/v1/namespaces/deployment-7925/pods/nginx-deployment-7b8c6f4498-tsjs6,UID:1eae852b-64ba-48a1-b9a4-60762599b8b1,ResourceVersion:21590,Generation:0,CreationTimestamp:2019-09-19 13:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.9.10/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f510cdba-9cee-4d71-8f5a-f6bedd6c77ca 0xc002541dd7 0xc002541dd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45jmx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45jmx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-45jmx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-91.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002541e40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002541e60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:27:57 +0000 UTC  }],Message:,Reason:,HostIP:172.31.42.91,PodIP:,StartTime:2019-09-19 13:27:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:27:59.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7925" for this suite.
Sep 19 13:28:07.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:28:08.258: INFO: namespace deployment-7925 deletion completed in 8.320052712s

• [SLOW TEST:20.772 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:28:08.259: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6077
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 13:28:08.448: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d713ed3-0cd7-4371-8e1c-ec2b38307a75" in namespace "projected-6077" to be "success or failure"
Sep 19 13:28:08.459: INFO: Pod "downwardapi-volume-2d713ed3-0cd7-4371-8e1c-ec2b38307a75": Phase="Pending", Reason="", readiness=false. Elapsed: 11.128066ms
Sep 19 13:28:10.465: INFO: Pod "downwardapi-volume-2d713ed3-0cd7-4371-8e1c-ec2b38307a75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017355162s
STEP: Saw pod success
Sep 19 13:28:10.465: INFO: Pod "downwardapi-volume-2d713ed3-0cd7-4371-8e1c-ec2b38307a75" satisfied condition "success or failure"
Sep 19 13:28:10.472: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod downwardapi-volume-2d713ed3-0cd7-4371-8e1c-ec2b38307a75 container client-container: <nil>
STEP: delete the pod
Sep 19 13:28:10.508: INFO: Waiting for pod downwardapi-volume-2d713ed3-0cd7-4371-8e1c-ec2b38307a75 to disappear
Sep 19 13:28:10.513: INFO: Pod downwardapi-volume-2d713ed3-0cd7-4371-8e1c-ec2b38307a75 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:28:10.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6077" for this suite.
Sep 19 13:28:16.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:28:16.886: INFO: namespace projected-6077 deletion completed in 6.365046916s

• [SLOW TEST:8.628 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:28:16.886: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8007
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-f8dffcfa-689e-4faf-97fb-efbf756b73f6
STEP: Creating a pod to test consume secrets
Sep 19 13:28:17.082: INFO: Waiting up to 5m0s for pod "pod-secrets-50e77819-2e6b-4bff-93a2-64457bcfae77" in namespace "secrets-8007" to be "success or failure"
Sep 19 13:28:17.090: INFO: Pod "pod-secrets-50e77819-2e6b-4bff-93a2-64457bcfae77": Phase="Pending", Reason="", readiness=false. Elapsed: 7.904493ms
Sep 19 13:28:19.097: INFO: Pod "pod-secrets-50e77819-2e6b-4bff-93a2-64457bcfae77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015047356s
Sep 19 13:28:21.105: INFO: Pod "pod-secrets-50e77819-2e6b-4bff-93a2-64457bcfae77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022886813s
STEP: Saw pod success
Sep 19 13:28:21.105: INFO: Pod "pod-secrets-50e77819-2e6b-4bff-93a2-64457bcfae77" satisfied condition "success or failure"
Sep 19 13:28:21.111: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod pod-secrets-50e77819-2e6b-4bff-93a2-64457bcfae77 container secret-volume-test: <nil>
STEP: delete the pod
Sep 19 13:28:21.197: INFO: Waiting for pod pod-secrets-50e77819-2e6b-4bff-93a2-64457bcfae77 to disappear
Sep 19 13:28:21.203: INFO: Pod pod-secrets-50e77819-2e6b-4bff-93a2-64457bcfae77 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:28:21.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8007" for this suite.
Sep 19 13:28:27.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:28:27.517: INFO: namespace secrets-8007 deletion completed in 6.306524s

• [SLOW TEST:10.631 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:28:27.517: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4097
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 19 13:28:27.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-4097'
Sep 19 13:28:27.970: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 19 13:28:27.970: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Sep 19 13:28:31.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4097'
Sep 19 13:28:32.090: INFO: stderr: ""
Sep 19 13:28:32.090: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:28:32.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4097" for this suite.
Sep 19 13:28:38.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:28:38.404: INFO: namespace kubectl-4097 deletion completed in 6.305532119s

• [SLOW TEST:10.886 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:28:38.404: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-9ebc2372-2b8a-4786-8cde-10b2e797b00b in namespace container-probe-4844
Sep 19 13:28:42.610: INFO: Started pod busybox-9ebc2372-2b8a-4786-8cde-10b2e797b00b in namespace container-probe-4844
STEP: checking the pod's current state and verifying that restartCount is present
Sep 19 13:28:42.616: INFO: Initial restart count of pod busybox-9ebc2372-2b8a-4786-8cde-10b2e797b00b is 0
Sep 19 13:29:34.823: INFO: Restart count of pod container-probe-4844/busybox-9ebc2372-2b8a-4786-8cde-10b2e797b00b is now 1 (52.206677018s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:29:34.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4844" for this suite.
Sep 19 13:29:40.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:29:41.134: INFO: namespace container-probe-4844 deletion completed in 6.284538829s

• [SLOW TEST:62.730 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:29:41.135: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1857
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1857
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Sep 19 13:29:41.333: INFO: Found 0 stateful pods, waiting for 3
Sep 19 13:29:51.341: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 19 13:29:51.341: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 19 13:29:51.341: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep 19 13:29:51.404: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 19 13:30:01.466: INFO: Updating stateful set ss2
Sep 19 13:30:01.481: INFO: Waiting for Pod statefulset-1857/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Sep 19 13:30:11.556: INFO: Found 2 stateful pods, waiting for 3
Sep 19 13:30:21.566: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 19 13:30:21.566: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 19 13:30:21.566: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 19 13:30:21.613: INFO: Updating stateful set ss2
Sep 19 13:30:21.638: INFO: Waiting for Pod statefulset-1857/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep 19 13:30:31.654: INFO: Waiting for Pod statefulset-1857/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep 19 13:30:41.680: INFO: Updating stateful set ss2
Sep 19 13:30:41.707: INFO: Waiting for StatefulSet statefulset-1857/ss2 to complete update
Sep 19 13:30:41.707: INFO: Waiting for Pod statefulset-1857/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep 19 13:30:51.723: INFO: Waiting for StatefulSet statefulset-1857/ss2 to complete update
Sep 19 13:30:51.723: INFO: Waiting for Pod statefulset-1857/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep 19 13:31:01.729: INFO: Deleting all statefulset in ns statefulset-1857
Sep 19 13:31:01.741: INFO: Scaling statefulset ss2 to 0
Sep 19 13:31:11.777: INFO: Waiting for statefulset status.replicas updated to 0
Sep 19 13:31:11.783: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:31:11.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1857" for this suite.
Sep 19 13:31:17.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:31:18.140: INFO: namespace statefulset-1857 deletion completed in 6.319612114s

• [SLOW TEST:97.005 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:31:18.140: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5844
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-dfac64b9-6f82-4c02-8d24-89054420d17b
STEP: Creating a pod to test consume secrets
Sep 19 13:31:18.333: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-71985b80-a54f-4149-82bb-5ff15ecdfecb" in namespace "projected-5844" to be "success or failure"
Sep 19 13:31:18.343: INFO: Pod "pod-projected-secrets-71985b80-a54f-4149-82bb-5ff15ecdfecb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.351913ms
Sep 19 13:31:20.350: INFO: Pod "pod-projected-secrets-71985b80-a54f-4149-82bb-5ff15ecdfecb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016500615s
STEP: Saw pod success
Sep 19 13:31:20.350: INFO: Pod "pod-projected-secrets-71985b80-a54f-4149-82bb-5ff15ecdfecb" satisfied condition "success or failure"
Sep 19 13:31:20.356: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod pod-projected-secrets-71985b80-a54f-4149-82bb-5ff15ecdfecb container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 19 13:31:20.394: INFO: Waiting for pod pod-projected-secrets-71985b80-a54f-4149-82bb-5ff15ecdfecb to disappear
Sep 19 13:31:20.400: INFO: Pod pod-projected-secrets-71985b80-a54f-4149-82bb-5ff15ecdfecb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:31:20.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5844" for this suite.
Sep 19 13:31:26.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:31:26.722: INFO: namespace projected-5844 deletion completed in 6.304442925s

• [SLOW TEST:8.582 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:31:26.722: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:31:26.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-986" for this suite.
Sep 19 13:31:32.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:31:33.244: INFO: namespace services-986 deletion completed in 6.321411027s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.522 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:31:33.245: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7838
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-7838
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 19 13:31:33.413: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 19 13:31:57.645: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.11.29:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7838 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:31:57.645: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:31:58.175: INFO: Found all expected endpoints: [netserver-0]
Sep 19 13:31:58.183: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.8.39:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7838 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:31:58.183: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:31:58.790: INFO: Found all expected endpoints: [netserver-1]
Sep 19 13:31:58.797: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.10.32:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7838 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:31:58.797: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:31:59.379: INFO: Found all expected endpoints: [netserver-2]
Sep 19 13:31:59.385: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.9.13:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7838 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:31:59.385: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:31:59.947: INFO: Found all expected endpoints: [netserver-3]
Sep 19 13:31:59.954: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.12.33:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7838 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:31:59.954: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:32:00.527: INFO: Found all expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:32:00.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7838" for this suite.
Sep 19 13:32:22.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:32:22.873: INFO: namespace pod-network-test-7838 deletion completed in 22.336281407s

• [SLOW TEST:49.628 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:32:22.876: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Sep 19 13:32:23.049: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-889697622 proxy --unix-socket=/tmp/kubectl-proxy-unix418667709/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:32:23.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6307" for this suite.
Sep 19 13:32:29.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:32:29.416: INFO: namespace kubectl-6307 deletion completed in 6.31004149s

• [SLOW TEST:6.540 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:32:29.416: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 19 13:32:37.660: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 19 13:32:37.668: INFO: Pod pod-with-prestop-http-hook still exists
Sep 19 13:32:39.669: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 19 13:32:39.675: INFO: Pod pod-with-prestop-http-hook still exists
Sep 19 13:32:41.669: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 19 13:32:41.675: INFO: Pod pod-with-prestop-http-hook still exists
Sep 19 13:32:43.669: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 19 13:32:43.675: INFO: Pod pod-with-prestop-http-hook still exists
Sep 19 13:32:45.669: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 19 13:32:45.675: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:32:45.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1798" for this suite.
Sep 19 13:33:07.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:33:08.019: INFO: namespace container-lifecycle-hook-1798 deletion completed in 22.313986209s

• [SLOW TEST:38.604 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:33:08.020: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6571
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4327
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-362
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:33:14.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6571" for this suite.
Sep 19 13:33:20.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:33:20.908: INFO: namespace namespaces-6571 deletion completed in 6.321156865s
STEP: Destroying namespace "nsdeletetest-4327" for this suite.
Sep 19 13:33:20.916: INFO: Namespace nsdeletetest-4327 was already deleted
STEP: Destroying namespace "nsdeletetest-362" for this suite.
Sep 19 13:33:26.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:33:27.248: INFO: namespace nsdeletetest-362 deletion completed in 6.331756314s

• [SLOW TEST:19.228 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:33:27.248: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4591
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Sep 19 13:33:27.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 create -f - --namespace=kubectl-4591'
Sep 19 13:33:27.655: INFO: stderr: ""
Sep 19 13:33:27.655: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 19 13:33:27.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4591'
Sep 19 13:33:27.741: INFO: stderr: ""
Sep 19 13:33:27.741: INFO: stdout: "update-demo-nautilus-cbjnc update-demo-nautilus-slsrm "
Sep 19 13:33:27.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-cbjnc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4591'
Sep 19 13:33:27.821: INFO: stderr: ""
Sep 19 13:33:27.821: INFO: stdout: ""
Sep 19 13:33:27.821: INFO: update-demo-nautilus-cbjnc is created but not running
Sep 19 13:33:32.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4591'
Sep 19 13:33:32.906: INFO: stderr: ""
Sep 19 13:33:32.906: INFO: stdout: "update-demo-nautilus-cbjnc update-demo-nautilus-slsrm "
Sep 19 13:33:32.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-cbjnc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4591'
Sep 19 13:33:32.981: INFO: stderr: ""
Sep 19 13:33:32.981: INFO: stdout: "true"
Sep 19 13:33:32.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-cbjnc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4591'
Sep 19 13:33:33.055: INFO: stderr: ""
Sep 19 13:33:33.055: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 19 13:33:33.055: INFO: validating pod update-demo-nautilus-cbjnc
Sep 19 13:33:33.156: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 19 13:33:33.156: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 19 13:33:33.156: INFO: update-demo-nautilus-cbjnc is verified up and running
Sep 19 13:33:33.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-slsrm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4591'
Sep 19 13:33:33.235: INFO: stderr: ""
Sep 19 13:33:33.235: INFO: stdout: "true"
Sep 19 13:33:33.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-nautilus-slsrm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4591'
Sep 19 13:33:33.323: INFO: stderr: ""
Sep 19 13:33:33.323: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 19 13:33:33.323: INFO: validating pod update-demo-nautilus-slsrm
Sep 19 13:33:33.422: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 19 13:33:33.422: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 19 13:33:33.422: INFO: update-demo-nautilus-slsrm is verified up and running
STEP: rolling-update to new replication controller
Sep 19 13:33:33.424: INFO: scanned /root for discovery docs: <nil>
Sep 19 13:33:33.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-4591'
Sep 19 13:33:55.969: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 19 13:33:55.969: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 19 13:33:55.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4591'
Sep 19 13:33:56.057: INFO: stderr: ""
Sep 19 13:33:56.057: INFO: stdout: "update-demo-kitten-2kzfb update-demo-kitten-zvrnp "
Sep 19 13:33:56.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-kitten-2kzfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4591'
Sep 19 13:33:56.133: INFO: stderr: ""
Sep 19 13:33:56.133: INFO: stdout: "true"
Sep 19 13:33:56.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-kitten-2kzfb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4591'
Sep 19 13:33:56.216: INFO: stderr: ""
Sep 19 13:33:56.216: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 19 13:33:56.216: INFO: validating pod update-demo-kitten-2kzfb
Sep 19 13:33:56.313: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 19 13:33:56.313: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 19 13:33:56.313: INFO: update-demo-kitten-2kzfb is verified up and running
Sep 19 13:33:56.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-kitten-zvrnp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4591'
Sep 19 13:33:56.400: INFO: stderr: ""
Sep 19 13:33:56.400: INFO: stdout: "true"
Sep 19 13:33:56.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods update-demo-kitten-zvrnp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4591'
Sep 19 13:33:56.475: INFO: stderr: ""
Sep 19 13:33:56.475: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 19 13:33:56.475: INFO: validating pod update-demo-kitten-zvrnp
Sep 19 13:33:56.573: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 19 13:33:56.573: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 19 13:33:56.573: INFO: update-demo-kitten-zvrnp is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:33:56.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4591" for this suite.
Sep 19 13:34:18.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:34:18.964: INFO: namespace kubectl-4591 deletion completed in 22.383390523s

• [SLOW TEST:51.716 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:34:18.966: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3571
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 13:34:19.148: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51d3a8ab-65cf-4efb-9001-85e6881ac1ca" in namespace "projected-3571" to be "success or failure"
Sep 19 13:34:19.157: INFO: Pod "downwardapi-volume-51d3a8ab-65cf-4efb-9001-85e6881ac1ca": Phase="Pending", Reason="", readiness=false. Elapsed: 8.703971ms
Sep 19 13:34:21.166: INFO: Pod "downwardapi-volume-51d3a8ab-65cf-4efb-9001-85e6881ac1ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017739773s
Sep 19 13:34:23.173: INFO: Pod "downwardapi-volume-51d3a8ab-65cf-4efb-9001-85e6881ac1ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025180388s
STEP: Saw pod success
Sep 19 13:34:23.173: INFO: Pod "downwardapi-volume-51d3a8ab-65cf-4efb-9001-85e6881ac1ca" satisfied condition "success or failure"
Sep 19 13:34:23.180: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod downwardapi-volume-51d3a8ab-65cf-4efb-9001-85e6881ac1ca container client-container: <nil>
STEP: delete the pod
Sep 19 13:34:23.219: INFO: Waiting for pod downwardapi-volume-51d3a8ab-65cf-4efb-9001-85e6881ac1ca to disappear
Sep 19 13:34:23.225: INFO: Pod downwardapi-volume-51d3a8ab-65cf-4efb-9001-85e6881ac1ca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:34:23.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3571" for this suite.
Sep 19 13:34:29.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:34:29.524: INFO: namespace projected-3571 deletion completed in 6.290019384s

• [SLOW TEST:10.559 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:34:29.525: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-852
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep 19 13:34:34.304: INFO: Successfully updated pod "annotationupdateddc0c947-d8d5-4638-8924-fb71fe4a435f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:34:36.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-852" for this suite.
Sep 19 13:34:58.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:34:58.676: INFO: namespace downward-api-852 deletion completed in 22.321647738s

• [SLOW TEST:29.151 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:34:58.676: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6361
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 19 13:34:58.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6361'
Sep 19 13:34:58.944: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 19 13:34:58.944: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Sep 19 13:35:00.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete deployment e2e-test-nginx-deployment --namespace=kubectl-6361'
Sep 19 13:35:01.122: INFO: stderr: ""
Sep 19 13:35:01.122: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:35:01.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6361" for this suite.
Sep 19 13:35:07.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:35:07.469: INFO: namespace kubectl-6361 deletion completed in 6.338086811s

• [SLOW TEST:8.792 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:35:07.470: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-241
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 13:35:07.636: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:35:08.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-241" for this suite.
Sep 19 13:35:14.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:35:15.129: INFO: namespace custom-resource-definition-241 deletion completed in 6.334097909s

• [SLOW TEST:7.660 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:35:15.130: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3186
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 13:35:15.380: INFO: (0) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 55.344886ms)
Sep 19 13:35:15.427: INFO: (1) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 47.654983ms)
Sep 19 13:35:15.438: INFO: (2) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 11.141708ms)
Sep 19 13:35:15.454: INFO: (3) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 15.627311ms)
Sep 19 13:35:15.467: INFO: (4) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 13.217163ms)
Sep 19 13:35:15.481: INFO: (5) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 13.758135ms)
Sep 19 13:35:15.495: INFO: (6) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 14.101794ms)
Sep 19 13:35:15.507: INFO: (7) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 11.550806ms)
Sep 19 13:35:15.518: INFO: (8) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 11.288589ms)
Sep 19 13:35:15.530: INFO: (9) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 11.370706ms)
Sep 19 13:35:15.541: INFO: (10) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 11.802292ms)
Sep 19 13:35:15.554: INFO: (11) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 12.591398ms)
Sep 19 13:35:15.570: INFO: (12) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 15.5284ms)
Sep 19 13:35:15.588: INFO: (13) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 18.741934ms)
Sep 19 13:35:15.603: INFO: (14) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 14.583943ms)
Sep 19 13:35:15.624: INFO: (15) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 20.532589ms)
Sep 19 13:35:15.635: INFO: (16) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 11.11487ms)
Sep 19 13:35:15.647: INFO: (17) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 11.939173ms)
Sep 19 13:35:15.658: INFO: (18) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 11.005365ms)
Sep 19 13:35:15.673: INFO: (19) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 14.973056ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:35:15.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3186" for this suite.
Sep 19 13:35:21.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:35:21.988: INFO: namespace proxy-3186 deletion completed in 6.307326394s

• [SLOW TEST:6.858 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:35:21.989: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7264
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Sep 19 13:35:22.162: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 19 13:35:22.182: INFO: Waiting for terminating namespaces to be deleted...
Sep 19 13:35:22.188: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-33-83.eu-central-1.compute.internal before test
Sep 19 13:35:22.228: INFO: kubernetes-dashboard-584d5ffc75-jvnx5 from kube-system started at 2019-09-19 12:50:46 +0000 UTC (1 container statuses recorded)
Sep 19 13:35:22.228: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep 19 13:35:22.228: INFO: kube-proxy-wcj22 from kube-system started at 2019-09-19 12:49:55 +0000 UTC (1 container statuses recorded)
Sep 19 13:35:22.228: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 19 13:35:22.228: INFO: canal-4wkpv from kube-system started at 2019-09-19 12:49:55 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.228: INFO: 	Container calico-node ready: true, restart count 0
Sep 19 13:35:22.228: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 19 13:35:22.228: INFO: node-exporter-7txrv from kube-system started at 2019-09-19 12:49:55 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.228: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 19 13:35:22.228: INFO: 	Container node-exporter ready: true, restart count 0
Sep 19 13:35:22.228: INFO: node-local-dns-twjtr from kube-system started at 2019-09-19 12:50:15 +0000 UTC (1 container statuses recorded)
Sep 19 13:35:22.228: INFO: 	Container node-cache ready: true, restart count 0
Sep 19 13:35:22.228: INFO: sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-zgrfd from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.228: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 19 13:35:22.228: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 19 13:35:22.228: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-36-147.eu-central-1.compute.internal before test
Sep 19 13:35:22.326: INFO: node-exporter-hnxl5 from kube-system started at 2019-09-19 12:50:25 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.326: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 19 13:35:22.326: INFO: 	Container node-exporter ready: true, restart count 0
Sep 19 13:35:22.326: INFO: canal-txnzq from kube-system started at 2019-09-19 12:50:25 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.326: INFO: 	Container calico-node ready: true, restart count 0
Sep 19 13:35:22.326: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 19 13:35:22.326: INFO: kube-proxy-hk5tn from kube-system started at 2019-09-19 12:50:25 +0000 UTC (1 container statuses recorded)
Sep 19 13:35:22.326: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 19 13:35:22.326: INFO: node-local-dns-zdxc4 from kube-system started at 2019-09-19 12:50:45 +0000 UTC (1 container statuses recorded)
Sep 19 13:35:22.326: INFO: 	Container node-cache ready: true, restart count 0
Sep 19 13:35:22.326: INFO: coredns-9b6865ff9-mhjqc from kube-system started at 2019-09-19 12:50:46 +0000 UTC (1 container statuses recorded)
Sep 19 13:35:22.326: INFO: 	Container coredns ready: true, restart count 0
Sep 19 13:35:22.326: INFO: sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-ccgl7 from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.326: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 19 13:35:22.326: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 19 13:35:22.326: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-42-91.eu-central-1.compute.internal before test
Sep 19 13:35:22.437: INFO: node-exporter-5vls6 from kube-system started at 2019-09-19 12:50:04 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.437: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 19 13:35:22.437: INFO: 	Container node-exporter ready: true, restart count 0
Sep 19 13:35:22.437: INFO: canal-dnm9w from kube-system started at 2019-09-19 12:50:04 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.437: INFO: 	Container calico-node ready: true, restart count 0
Sep 19 13:35:22.437: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 19 13:35:22.437: INFO: kube-proxy-h4md4 from kube-system started at 2019-09-19 12:50:04 +0000 UTC (1 container statuses recorded)
Sep 19 13:35:22.437: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 19 13:35:22.437: INFO: node-local-dns-7rcvh from kube-system started at 2019-09-19 12:50:14 +0000 UTC (1 container statuses recorded)
Sep 19 13:35:22.437: INFO: 	Container node-cache ready: true, restart count 0
Sep 19 13:35:22.437: INFO: sonobuoy-e2e-job-57f4e2d668874fc2 from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.437: INFO: 	Container e2e ready: true, restart count 0
Sep 19 13:35:22.437: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 19 13:35:22.437: INFO: sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-q2sj9 from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.437: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 19 13:35:22.437: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 19 13:35:22.437: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-44-209.eu-central-1.compute.internal before test
Sep 19 13:35:22.492: INFO: kube-proxy-29rs8 from kube-system started at 2019-09-19 12:50:08 +0000 UTC (1 container statuses recorded)
Sep 19 13:35:22.492: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 19 13:35:22.492: INFO: canal-rg8t2 from kube-system started at 2019-09-19 12:50:08 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.492: INFO: 	Container calico-node ready: true, restart count 0
Sep 19 13:35:22.492: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 19 13:35:22.492: INFO: node-exporter-6lvwr from kube-system started at 2019-09-19 12:50:08 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 19 13:35:22.492: INFO: 	Container node-exporter ready: true, restart count 0
Sep 19 13:35:22.492: INFO: node-local-dns-rd9dn from kube-system started at 2019-09-19 12:50:28 +0000 UTC (1 container statuses recorded)
Sep 19 13:35:22.492: INFO: 	Container node-cache ready: true, restart count 0
Sep 19 13:35:22.492: INFO: sonobuoy from sonobuoy started at 2019-09-19 13:04:24 +0000 UTC (1 container statuses recorded)
Sep 19 13:35:22.492: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 19 13:35:22.492: INFO: sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-7bv2k from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.492: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 19 13:35:22.492: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 19 13:35:22.492: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-46-204.eu-central-1.compute.internal before test
Sep 19 13:35:22.582: INFO: openvpn-client-5fbd4fdb44-g28ss from kube-system started at 2019-09-19 12:50:46 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.582: INFO: 	Container dnat-controller ready: true, restart count 0
Sep 19 13:35:22.582: INFO: 	Container openvpn-client ready: true, restart count 0
Sep 19 13:35:22.582: INFO: coredns-9b6865ff9-mks62 from kube-system started at 2019-09-19 12:53:47 +0000 UTC (1 container statuses recorded)
Sep 19 13:35:22.582: INFO: 	Container coredns ready: true, restart count 0
Sep 19 13:35:22.582: INFO: sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-k8z66 from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.582: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 19 13:35:22.582: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 19 13:35:22.582: INFO: node-exporter-tsf9h from kube-system started at 2019-09-19 12:50:22 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.582: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 19 13:35:22.582: INFO: 	Container node-exporter ready: true, restart count 0
Sep 19 13:35:22.582: INFO: kube-proxy-p4xds from kube-system started at 2019-09-19 12:50:22 +0000 UTC (1 container statuses recorded)
Sep 19 13:35:22.582: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 19 13:35:22.582: INFO: node-local-dns-t4859 from kube-system started at 2019-09-19 12:50:42 +0000 UTC (1 container statuses recorded)
Sep 19 13:35:22.582: INFO: 	Container node-cache ready: true, restart count 0
Sep 19 13:35:22.582: INFO: canal-9jtwv from kube-system started at 2019-09-19 12:50:22 +0000 UTC (2 container statuses recorded)
Sep 19 13:35:22.582: INFO: 	Container calico-node ready: true, restart count 0
Sep 19 13:35:22.582: INFO: 	Container kube-flannel ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15c5da596a35e531], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:35:23.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7264" for this suite.
Sep 19 13:35:29.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:35:29.967: INFO: namespace sched-pred-7264 deletion completed in 6.328633956s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.978 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:35:29.967: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5164
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Sep 19 13:35:30.689: INFO: created pod pod-service-account-defaultsa
Sep 19 13:35:30.689: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 19 13:35:30.711: INFO: created pod pod-service-account-mountsa
Sep 19 13:35:30.711: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 19 13:35:30.719: INFO: created pod pod-service-account-nomountsa
Sep 19 13:35:30.719: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 19 13:35:30.726: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 19 13:35:30.726: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 19 13:35:30.738: INFO: created pod pod-service-account-mountsa-mountspec
Sep 19 13:35:30.738: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 19 13:35:30.749: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 19 13:35:30.749: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 19 13:35:30.758: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 19 13:35:30.758: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 19 13:35:30.767: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 19 13:35:30.767: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 19 13:35:30.777: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 19 13:35:30.777: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:35:30.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5164" for this suite.
Sep 19 13:35:54.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:35:55.098: INFO: namespace svcaccounts-5164 deletion completed in 24.311517968s

• [SLOW TEST:25.132 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:35:55.099: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1523
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Sep 19 13:35:59.311: INFO: Pod pod-hostip-5f96d7bc-31ca-469e-95ee-7c5dee42e971 has hostIP: 172.31.44.209
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:35:59.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1523" for this suite.
Sep 19 13:36:27.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:36:27.652: INFO: namespace pods-1523 deletion completed in 28.334163665s

• [SLOW TEST:32.554 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:36:27.653: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2291
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 13:36:27.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 version'
Sep 19 13:36:27.906: INFO: stderr: ""
Sep 19 13:36:27.906: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:13:54Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:05:50Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:36:27.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2291" for this suite.
Sep 19 13:36:33.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:36:34.209: INFO: namespace kubectl-2291 deletion completed in 6.295244103s

• [SLOW TEST:6.557 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:36:34.210: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8630
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 19 13:36:34.391: INFO: Waiting up to 5m0s for pod "pod-eb1a3ae9-8883-4887-a582-c5a09a52a23e" in namespace "emptydir-8630" to be "success or failure"
Sep 19 13:36:34.399: INFO: Pod "pod-eb1a3ae9-8883-4887-a582-c5a09a52a23e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.721281ms
Sep 19 13:36:36.409: INFO: Pod "pod-eb1a3ae9-8883-4887-a582-c5a09a52a23e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017603646s
Sep 19 13:36:38.415: INFO: Pod "pod-eb1a3ae9-8883-4887-a582-c5a09a52a23e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024116353s
STEP: Saw pod success
Sep 19 13:36:38.415: INFO: Pod "pod-eb1a3ae9-8883-4887-a582-c5a09a52a23e" satisfied condition "success or failure"
Sep 19 13:36:38.421: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod pod-eb1a3ae9-8883-4887-a582-c5a09a52a23e container test-container: <nil>
STEP: delete the pod
Sep 19 13:36:38.522: INFO: Waiting for pod pod-eb1a3ae9-8883-4887-a582-c5a09a52a23e to disappear
Sep 19 13:36:38.528: INFO: Pod pod-eb1a3ae9-8883-4887-a582-c5a09a52a23e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:36:38.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8630" for this suite.
Sep 19 13:36:44.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:36:44.844: INFO: namespace emptydir-8630 deletion completed in 6.309169797s

• [SLOW TEST:10.634 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:36:44.844: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-408
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 13:36:45.022: INFO: Waiting up to 5m0s for pod "downwardapi-volume-64ed5a53-d57b-47a4-a85b-e85024e488de" in namespace "projected-408" to be "success or failure"
Sep 19 13:36:45.029: INFO: Pod "downwardapi-volume-64ed5a53-d57b-47a4-a85b-e85024e488de": Phase="Pending", Reason="", readiness=false. Elapsed: 7.210811ms
Sep 19 13:36:47.036: INFO: Pod "downwardapi-volume-64ed5a53-d57b-47a4-a85b-e85024e488de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014011142s
Sep 19 13:36:49.043: INFO: Pod "downwardapi-volume-64ed5a53-d57b-47a4-a85b-e85024e488de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020820372s
STEP: Saw pod success
Sep 19 13:36:49.043: INFO: Pod "downwardapi-volume-64ed5a53-d57b-47a4-a85b-e85024e488de" satisfied condition "success or failure"
Sep 19 13:36:49.049: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod downwardapi-volume-64ed5a53-d57b-47a4-a85b-e85024e488de container client-container: <nil>
STEP: delete the pod
Sep 19 13:36:49.082: INFO: Waiting for pod downwardapi-volume-64ed5a53-d57b-47a4-a85b-e85024e488de to disappear
Sep 19 13:36:49.087: INFO: Pod downwardapi-volume-64ed5a53-d57b-47a4-a85b-e85024e488de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:36:49.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-408" for this suite.
Sep 19 13:36:55.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:36:55.382: INFO: namespace projected-408 deletion completed in 6.28725585s

• [SLOW TEST:10.537 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:36:55.382: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-9018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Sep 19 13:36:55.569: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-9018" to be "success or failure"
Sep 19 13:36:55.576: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 7.018776ms
Sep 19 13:36:57.583: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013923325s
Sep 19 13:36:59.590: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020658035s
STEP: Saw pod success
Sep 19 13:36:59.590: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Sep 19 13:36:59.596: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep 19 13:36:59.670: INFO: Waiting for pod pod-host-path-test to disappear
Sep 19 13:36:59.676: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:36:59.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-9018" for this suite.
Sep 19 13:37:05.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:37:05.973: INFO: namespace hostpath-9018 deletion completed in 6.28912364s

• [SLOW TEST:10.591 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:37:05.973: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5409
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Sep 19 13:37:06.141: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Sep 19 13:37:06.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 create -f - --namespace=kubectl-5409'
Sep 19 13:37:06.437: INFO: stderr: ""
Sep 19 13:37:06.437: INFO: stdout: "service/redis-slave created\n"
Sep 19 13:37:06.437: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Sep 19 13:37:06.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 create -f - --namespace=kubectl-5409'
Sep 19 13:37:06.813: INFO: stderr: ""
Sep 19 13:37:06.813: INFO: stdout: "service/redis-master created\n"
Sep 19 13:37:06.815: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 19 13:37:06.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 create -f - --namespace=kubectl-5409'
Sep 19 13:37:07.132: INFO: stderr: ""
Sep 19 13:37:07.132: INFO: stdout: "service/frontend created\n"
Sep 19 13:37:07.133: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Sep 19 13:37:07.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 create -f - --namespace=kubectl-5409'
Sep 19 13:37:07.330: INFO: stderr: ""
Sep 19 13:37:07.330: INFO: stdout: "deployment.apps/frontend created\n"
Sep 19 13:37:07.330: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 19 13:37:07.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 create -f - --namespace=kubectl-5409'
Sep 19 13:37:07.539: INFO: stderr: ""
Sep 19 13:37:07.539: INFO: stdout: "deployment.apps/redis-master created\n"
Sep 19 13:37:07.539: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Sep 19 13:37:07.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 create -f - --namespace=kubectl-5409'
Sep 19 13:37:07.741: INFO: stderr: ""
Sep 19 13:37:07.741: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Sep 19 13:37:07.741: INFO: Waiting for all frontend pods to be Running.
Sep 19 13:37:27.792: INFO: Waiting for frontend to serve content.
Sep 19 13:37:27.892: INFO: Trying to add a new entry to the guestbook.
Sep 19 13:37:27.941: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep 19 13:37:28.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete --grace-period=0 --force -f - --namespace=kubectl-5409'
Sep 19 13:37:28.143: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 19 13:37:28.143: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep 19 13:37:28.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete --grace-period=0 --force -f - --namespace=kubectl-5409'
Sep 19 13:37:28.249: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 19 13:37:28.249: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 19 13:37:28.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete --grace-period=0 --force -f - --namespace=kubectl-5409'
Sep 19 13:37:28.379: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 19 13:37:28.379: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 19 13:37:28.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete --grace-period=0 --force -f - --namespace=kubectl-5409'
Sep 19 13:37:28.490: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 19 13:37:28.490: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 19 13:37:28.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete --grace-period=0 --force -f - --namespace=kubectl-5409'
Sep 19 13:37:28.591: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 19 13:37:28.591: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 19 13:37:28.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete --grace-period=0 --force -f - --namespace=kubectl-5409'
Sep 19 13:37:28.702: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 19 13:37:28.702: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:37:28.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5409" for this suite.
Sep 19 13:38:10.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:38:11.015: INFO: namespace kubectl-5409 deletion completed in 42.303533961s

• [SLOW TEST:65.042 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:38:11.016: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3341
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-3341/configmap-test-d923550c-6d80-45a6-8f91-3136637b914e
STEP: Creating a pod to test consume configMaps
Sep 19 13:38:11.218: INFO: Waiting up to 5m0s for pod "pod-configmaps-d9e579e7-1f7b-4312-bb8b-b8a8964e376c" in namespace "configmap-3341" to be "success or failure"
Sep 19 13:38:11.225: INFO: Pod "pod-configmaps-d9e579e7-1f7b-4312-bb8b-b8a8964e376c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.704026ms
Sep 19 13:38:13.231: INFO: Pod "pod-configmaps-d9e579e7-1f7b-4312-bb8b-b8a8964e376c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01294722s
Sep 19 13:38:15.241: INFO: Pod "pod-configmaps-d9e579e7-1f7b-4312-bb8b-b8a8964e376c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022969633s
STEP: Saw pod success
Sep 19 13:38:15.241: INFO: Pod "pod-configmaps-d9e579e7-1f7b-4312-bb8b-b8a8964e376c" satisfied condition "success or failure"
Sep 19 13:38:15.247: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod pod-configmaps-d9e579e7-1f7b-4312-bb8b-b8a8964e376c container env-test: <nil>
STEP: delete the pod
Sep 19 13:38:15.305: INFO: Waiting for pod pod-configmaps-d9e579e7-1f7b-4312-bb8b-b8a8964e376c to disappear
Sep 19 13:38:15.311: INFO: Pod pod-configmaps-d9e579e7-1f7b-4312-bb8b-b8a8964e376c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:38:15.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3341" for this suite.
Sep 19 13:38:21.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:38:21.651: INFO: namespace configmap-3341 deletion completed in 6.332617262s

• [SLOW TEST:10.635 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:38:21.651: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9194
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:38:25.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9194" for this suite.
Sep 19 13:39:17.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:39:18.234: INFO: namespace kubelet-test-9194 deletion completed in 52.338377916s

• [SLOW TEST:56.582 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:39:18.234: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3632
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-39afa7f7-3288-4284-97c6-d5ad650525a7
STEP: Creating configMap with name cm-test-opt-upd-ee0c073b-9e2d-46f8-8506-1aacc4b6fbf7
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-39afa7f7-3288-4284-97c6-d5ad650525a7
STEP: Updating configmap cm-test-opt-upd-ee0c073b-9e2d-46f8-8506-1aacc4b6fbf7
STEP: Creating configMap with name cm-test-opt-create-f86ecc9c-bf51-44dd-8fa0-68414baf2bd6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:39:27.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3632" for this suite.
Sep 19 13:39:49.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:39:49.332: INFO: namespace configmap-3632 deletion completed in 22.312327127s

• [SLOW TEST:31.097 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:39:49.332: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Sep 19 13:39:49.521: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-889697622 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:39:49.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-438" for this suite.
Sep 19 13:39:55.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:39:55.903: INFO: namespace kubectl-438 deletion completed in 6.309155879s

• [SLOW TEST:6.571 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:39:55.905: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 19 13:39:56.109: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5905,SelfLink:/api/v1/namespaces/watch-5905/configmaps/e2e-watch-test-watch-closed,UID:1019e8ac-426f-4a25-b04f-5dbd36126a3a,ResourceVersion:25341,Generation:0,CreationTimestamp:2019-09-19 13:39:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 19 13:39:56.109: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5905,SelfLink:/api/v1/namespaces/watch-5905/configmaps/e2e-watch-test-watch-closed,UID:1019e8ac-426f-4a25-b04f-5dbd36126a3a,ResourceVersion:25342,Generation:0,CreationTimestamp:2019-09-19 13:39:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 19 13:39:56.142: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5905,SelfLink:/api/v1/namespaces/watch-5905/configmaps/e2e-watch-test-watch-closed,UID:1019e8ac-426f-4a25-b04f-5dbd36126a3a,ResourceVersion:25343,Generation:0,CreationTimestamp:2019-09-19 13:39:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 19 13:39:56.142: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5905,SelfLink:/api/v1/namespaces/watch-5905/configmaps/e2e-watch-test-watch-closed,UID:1019e8ac-426f-4a25-b04f-5dbd36126a3a,ResourceVersion:25344,Generation:0,CreationTimestamp:2019-09-19 13:39:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:39:56.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5905" for this suite.
Sep 19 13:40:02.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:40:02.459: INFO: namespace watch-5905 deletion completed in 6.307605612s

• [SLOW TEST:6.554 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:40:02.459: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4061
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 19 13:40:07.189: INFO: Successfully updated pod "pod-update-activedeadlineseconds-f3a07174-7209-4180-9462-da99dbbce995"
Sep 19 13:40:07.189: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-f3a07174-7209-4180-9462-da99dbbce995" in namespace "pods-4061" to be "terminated due to deadline exceeded"
Sep 19 13:40:07.198: INFO: Pod "pod-update-activedeadlineseconds-f3a07174-7209-4180-9462-da99dbbce995": Phase="Running", Reason="", readiness=true. Elapsed: 8.174037ms
Sep 19 13:40:09.205: INFO: Pod "pod-update-activedeadlineseconds-f3a07174-7209-4180-9462-da99dbbce995": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.015647727s
Sep 19 13:40:09.205: INFO: Pod "pod-update-activedeadlineseconds-f3a07174-7209-4180-9462-da99dbbce995" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:40:09.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4061" for this suite.
Sep 19 13:40:15.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:40:15.506: INFO: namespace pods-4061 deletion completed in 6.291800242s

• [SLOW TEST:13.047 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:40:15.507: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9345
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-54c50745-1237-4229-9264-0f9eee2aaf47
STEP: Creating a pod to test consume configMaps
Sep 19 13:40:15.704: INFO: Waiting up to 5m0s for pod "pod-configmaps-92451d34-5614-48d1-82bc-535030806a13" in namespace "configmap-9345" to be "success or failure"
Sep 19 13:40:15.713: INFO: Pod "pod-configmaps-92451d34-5614-48d1-82bc-535030806a13": Phase="Pending", Reason="", readiness=false. Elapsed: 8.640586ms
Sep 19 13:40:17.720: INFO: Pod "pod-configmaps-92451d34-5614-48d1-82bc-535030806a13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015789464s
Sep 19 13:40:19.730: INFO: Pod "pod-configmaps-92451d34-5614-48d1-82bc-535030806a13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025671434s
STEP: Saw pod success
Sep 19 13:40:19.730: INFO: Pod "pod-configmaps-92451d34-5614-48d1-82bc-535030806a13" satisfied condition "success or failure"
Sep 19 13:40:19.736: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod pod-configmaps-92451d34-5614-48d1-82bc-535030806a13 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 19 13:40:19.775: INFO: Waiting for pod pod-configmaps-92451d34-5614-48d1-82bc-535030806a13 to disappear
Sep 19 13:40:19.781: INFO: Pod pod-configmaps-92451d34-5614-48d1-82bc-535030806a13 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:40:19.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9345" for this suite.
Sep 19 13:40:25.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:40:26.093: INFO: namespace configmap-9345 deletion completed in 6.303631193s

• [SLOW TEST:10.586 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:40:26.094: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-66ddb641-81db-4452-b004-7956d7802b3f in namespace container-probe-327
Sep 19 13:40:30.295: INFO: Started pod liveness-66ddb641-81db-4452-b004-7956d7802b3f in namespace container-probe-327
STEP: checking the pod's current state and verifying that restartCount is present
Sep 19 13:40:30.302: INFO: Initial restart count of pod liveness-66ddb641-81db-4452-b004-7956d7802b3f is 0
Sep 19 13:40:48.377: INFO: Restart count of pod container-probe-327/liveness-66ddb641-81db-4452-b004-7956d7802b3f is now 1 (18.075245754s elapsed)
Sep 19 13:41:06.439: INFO: Restart count of pod container-probe-327/liveness-66ddb641-81db-4452-b004-7956d7802b3f is now 2 (36.137578049s elapsed)
Sep 19 13:41:26.522: INFO: Restart count of pod container-probe-327/liveness-66ddb641-81db-4452-b004-7956d7802b3f is now 3 (56.220535025s elapsed)
Sep 19 13:41:46.592: INFO: Restart count of pod container-probe-327/liveness-66ddb641-81db-4452-b004-7956d7802b3f is now 4 (1m16.290571609s elapsed)
Sep 19 13:43:00.869: INFO: Restart count of pod container-probe-327/liveness-66ddb641-81db-4452-b004-7956d7802b3f is now 5 (2m30.566781976s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:43:00.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-327" for this suite.
Sep 19 13:43:06.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:43:07.204: INFO: namespace container-probe-327 deletion completed in 6.308151735s

• [SLOW TEST:161.110 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:43:07.204: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 13:43:07.453: INFO: Create a RollingUpdate DaemonSet
Sep 19 13:43:07.467: INFO: Check that daemon pods launch on every node of the cluster
Sep 19 13:43:07.485: INFO: Number of nodes with available pods: 0
Sep 19 13:43:07.485: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:43:08.503: INFO: Number of nodes with available pods: 0
Sep 19 13:43:08.503: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:43:09.507: INFO: Number of nodes with available pods: 1
Sep 19 13:43:09.507: INFO: Node ip-172-31-36-147.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:43:10.505: INFO: Number of nodes with available pods: 5
Sep 19 13:43:10.505: INFO: Number of running nodes: 5, number of available pods: 5
Sep 19 13:43:10.505: INFO: Update the DaemonSet to trigger a rollout
Sep 19 13:43:10.522: INFO: Updating DaemonSet daemon-set
Sep 19 13:43:14.553: INFO: Roll back the DaemonSet before rollout is complete
Sep 19 13:43:14.568: INFO: Updating DaemonSet daemon-set
Sep 19 13:43:14.568: INFO: Make sure DaemonSet rollback is complete
Sep 19 13:43:14.577: INFO: Wrong image for pod: daemon-set-2t9v9. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep 19 13:43:14.577: INFO: Pod daemon-set-2t9v9 is not available
Sep 19 13:43:15.597: INFO: Wrong image for pod: daemon-set-2t9v9. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep 19 13:43:15.597: INFO: Pod daemon-set-2t9v9 is not available
Sep 19 13:43:16.604: INFO: Wrong image for pod: daemon-set-2t9v9. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep 19 13:43:16.604: INFO: Pod daemon-set-2t9v9 is not available
Sep 19 13:43:17.598: INFO: Wrong image for pod: daemon-set-2t9v9. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep 19 13:43:17.598: INFO: Pod daemon-set-2t9v9 is not available
Sep 19 13:43:18.597: INFO: Pod daemon-set-r9xgl is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2580, will wait for the garbage collector to delete the pods
Sep 19 13:43:18.703: INFO: Deleting DaemonSet.extensions daemon-set took: 16.347712ms
Sep 19 13:43:19.203: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.265687ms
Sep 19 13:44:34.811: INFO: Number of nodes with available pods: 0
Sep 19 13:44:34.811: INFO: Number of running nodes: 0, number of available pods: 0
Sep 19 13:44:34.818: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2580/daemonsets","resourceVersion":"26364"},"items":null}

Sep 19 13:44:34.825: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2580/pods","resourceVersion":"26364"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:44:34.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2580" for this suite.
Sep 19 13:44:40.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:44:41.152: INFO: namespace daemonsets-2580 deletion completed in 6.279693413s

• [SLOW TEST:93.948 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:44:41.153: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 19 13:44:41.330: INFO: Waiting up to 5m0s for pod "pod-65c3d505-068a-4893-90e4-df9d3e3a276d" in namespace "emptydir-3483" to be "success or failure"
Sep 19 13:44:41.337: INFO: Pod "pod-65c3d505-068a-4893-90e4-df9d3e3a276d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.232789ms
Sep 19 13:44:43.343: INFO: Pod "pod-65c3d505-068a-4893-90e4-df9d3e3a276d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01361309s
Sep 19 13:44:45.354: INFO: Pod "pod-65c3d505-068a-4893-90e4-df9d3e3a276d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023777482s
STEP: Saw pod success
Sep 19 13:44:45.354: INFO: Pod "pod-65c3d505-068a-4893-90e4-df9d3e3a276d" satisfied condition "success or failure"
Sep 19 13:44:45.360: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod pod-65c3d505-068a-4893-90e4-df9d3e3a276d container test-container: <nil>
STEP: delete the pod
Sep 19 13:44:45.466: INFO: Waiting for pod pod-65c3d505-068a-4893-90e4-df9d3e3a276d to disappear
Sep 19 13:44:45.473: INFO: Pod pod-65c3d505-068a-4893-90e4-df9d3e3a276d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:44:45.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3483" for this suite.
Sep 19 13:44:51.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:44:51.796: INFO: namespace emptydir-3483 deletion completed in 6.315122477s

• [SLOW TEST:10.643 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:44:51.797: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5523
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 19 13:44:51.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5523'
Sep 19 13:44:52.223: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 19 13:44:52.224: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Sep 19 13:44:52.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete jobs e2e-test-nginx-job --namespace=kubectl-5523'
Sep 19 13:44:52.349: INFO: stderr: ""
Sep 19 13:44:52.349: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:44:52.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5523" for this suite.
Sep 19 13:44:58.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:44:58.719: INFO: namespace kubectl-5523 deletion completed in 6.360894024s

• [SLOW TEST:6.923 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:44:58.719: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 19 13:44:58.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8957'
Sep 19 13:44:58.981: INFO: stderr: ""
Sep 19 13:44:58.981: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Sep 19 13:44:58.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete pods e2e-test-nginx-pod --namespace=kubectl-8957'
Sep 19 13:45:02.293: INFO: stderr: ""
Sep 19 13:45:02.293: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:45:02.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8957" for this suite.
Sep 19 13:45:08.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:45:08.619: INFO: namespace kubectl-8957 deletion completed in 6.316373179s

• [SLOW TEST:9.900 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:45:08.620: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3209
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 19 13:45:08.809: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 19 13:45:13.816: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:45:14.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3209" for this suite.
Sep 19 13:45:20.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:45:21.174: INFO: namespace replication-controller-3209 deletion completed in 6.319211258s

• [SLOW TEST:12.554 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:45:21.174: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3817
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:46:21.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3817" for this suite.
Sep 19 13:46:43.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:46:43.684: INFO: namespace container-probe-3817 deletion completed in 22.314267156s

• [SLOW TEST:82.510 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:46:43.684: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6500
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Sep 19 13:46:43.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 api-versions'
Sep 19 13:46:44.197: INFO: stderr: ""
Sep 19 13:46:44.197: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:46:44.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6500" for this suite.
Sep 19 13:46:50.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:46:50.551: INFO: namespace kubectl-6500 deletion completed in 6.34567474s

• [SLOW TEST:6.866 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:46:50.551: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2242
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 13:46:50.733: INFO: Creating deployment "test-recreate-deployment"
Sep 19 13:46:50.746: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 19 13:46:50.763: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep 19 13:46:52.777: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 19 13:46:52.785: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704497610, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704497610, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704497610, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704497610, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 19 13:46:54.797: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 19 13:46:54.813: INFO: Updating deployment test-recreate-deployment
Sep 19 13:46:54.813: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep 19 13:46:54.898: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-2242,SelfLink:/apis/apps/v1/namespaces/deployment-2242/deployments/test-recreate-deployment,UID:378acbdf-72f9-42b1-aa58-811d551cdb1c,ResourceVersion:27008,Generation:2,CreationTimestamp:2019-09-19 13:46:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-09-19 13:46:54 +0000 UTC 2019-09-19 13:46:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-09-19 13:46:54 +0000 UTC 2019-09-19 13:46:50 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Sep 19 13:46:54.905: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-2242,SelfLink:/apis/apps/v1/namespaces/deployment-2242/replicasets/test-recreate-deployment-5c8c9cc69d,UID:67e82c9f-90fd-470b-bc37-d391ca5c315e,ResourceVersion:27005,Generation:1,CreationTimestamp:2019-09-19 13:46:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 378acbdf-72f9-42b1-aa58-811d551cdb1c 0xc00284be47 0xc00284be48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 19 13:46:54.905: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 19 13:46:54.905: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-2242,SelfLink:/apis/apps/v1/namespaces/deployment-2242/replicasets/test-recreate-deployment-6df85df6b9,UID:d99c4f0a-f4e4-4926-9d31-cf1f51cef9b1,ResourceVersion:26996,Generation:2,CreationTimestamp:2019-09-19 13:46:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 378acbdf-72f9-42b1-aa58-811d551cdb1c 0xc00284bf17 0xc00284bf18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 19 13:46:54.913: INFO: Pod "test-recreate-deployment-5c8c9cc69d-tcd25" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-tcd25,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-2242,SelfLink:/api/v1/namespaces/deployment-2242/pods/test-recreate-deployment-5c8c9cc69d-tcd25,UID:ed48c38f-546a-4e37-85a5-b6f271139c67,ResourceVersion:27009,Generation:0,CreationTimestamp:2019-09-19 13:46:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 67e82c9f-90fd-470b-bc37-d391ca5c315e 0xc002d3efd7 0xc002d3efd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dt8qt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dt8qt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dt8qt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-83.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d3f1c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d3f1f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:46:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:46:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:46:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:46:54 +0000 UTC  }],Message:,Reason:,HostIP:172.31.33.83,PodIP:,StartTime:2019-09-19 13:46:54 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:46:54.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2242" for this suite.
Sep 19 13:47:00.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:47:01.252: INFO: namespace deployment-2242 deletion completed in 6.32774311s

• [SLOW TEST:10.701 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:47:01.252: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5330
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 13:47:01.424: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:47:05.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5330" for this suite.
Sep 19 13:47:45.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:47:46.058: INFO: namespace pods-5330 deletion completed in 40.299990493s

• [SLOW TEST:44.806 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:47:46.058: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5485
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0919 13:47:46.825191      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 19 13:47:46.825: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:47:46.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5485" for this suite.
Sep 19 13:47:52.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:47:53.155: INFO: namespace gc-5485 deletion completed in 6.321008808s

• [SLOW TEST:7.097 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:47:53.156: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep 19 13:47:53.326: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:47:57.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5148" for this suite.
Sep 19 13:48:03.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:48:03.775: INFO: namespace init-container-5148 deletion completed in 6.375566052s

• [SLOW TEST:10.620 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:48:03.777: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-43
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep 19 13:48:08.622: INFO: Successfully updated pod "labelsupdate61ed8f0c-2a38-4c32-8481-612193c01988"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:48:10.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-43" for this suite.
Sep 19 13:48:32.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:48:33.019: INFO: namespace downward-api-43 deletion completed in 22.336516072s

• [SLOW TEST:29.242 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:48:33.019: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7255
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Sep 19 13:48:33.238: INFO: Found 0 stateful pods, waiting for 3
Sep 19 13:48:43.247: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 19 13:48:43.247: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 19 13:48:43.247: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 19 13:48:43.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-7255 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 19 13:48:43.879: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 19 13:48:43.879: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 19 13:48:43.879: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep 19 13:48:53.936: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 19 13:49:03.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-7255 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 19 13:49:04.652: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 19 13:49:04.652: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 19 13:49:04.652: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

STEP: Rolling back to a previous revision
Sep 19 13:49:24.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-7255 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 19 13:49:25.308: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 19 13:49:25.308: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 19 13:49:25.308: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 19 13:49:35.373: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 19 13:49:35.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-7255 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 19 13:49:36.026: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 19 13:49:36.026: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 19 13:49:36.026: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep 19 13:50:06.066: INFO: Deleting all statefulset in ns statefulset-7255
Sep 19 13:50:06.073: INFO: Scaling statefulset ss2 to 0
Sep 19 13:50:26.107: INFO: Waiting for statefulset status.replicas updated to 0
Sep 19 13:50:26.114: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:50:26.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7255" for this suite.
Sep 19 13:50:32.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:50:32.469: INFO: namespace statefulset-7255 deletion completed in 6.322950849s

• [SLOW TEST:119.450 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:50:32.470: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6790
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 19 13:50:32.658: INFO: Waiting up to 5m0s for pod "pod-469de370-6b65-44b3-8778-3f32a18e656e" in namespace "emptydir-6790" to be "success or failure"
Sep 19 13:50:32.666: INFO: Pod "pod-469de370-6b65-44b3-8778-3f32a18e656e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.773376ms
Sep 19 13:50:34.673: INFO: Pod "pod-469de370-6b65-44b3-8778-3f32a18e656e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01457514s
Sep 19 13:50:36.680: INFO: Pod "pod-469de370-6b65-44b3-8778-3f32a18e656e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021882239s
STEP: Saw pod success
Sep 19 13:50:36.680: INFO: Pod "pod-469de370-6b65-44b3-8778-3f32a18e656e" satisfied condition "success or failure"
Sep 19 13:50:36.687: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod pod-469de370-6b65-44b3-8778-3f32a18e656e container test-container: <nil>
STEP: delete the pod
Sep 19 13:50:36.758: INFO: Waiting for pod pod-469de370-6b65-44b3-8778-3f32a18e656e to disappear
Sep 19 13:50:36.764: INFO: Pod pod-469de370-6b65-44b3-8778-3f32a18e656e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:50:36.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6790" for this suite.
Sep 19 13:50:42.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:50:43.052: INFO: namespace emptydir-6790 deletion completed in 6.279880146s

• [SLOW TEST:10.582 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:50:43.052: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4533
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 19 13:50:53.306: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 19 13:50:53.312: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 19 13:50:55.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 19 13:50:55.319: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 19 13:50:57.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 19 13:50:57.319: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 19 13:50:59.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 19 13:50:59.319: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 19 13:51:01.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 19 13:51:01.321: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 19 13:51:03.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 19 13:51:03.320: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 19 13:51:05.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 19 13:51:05.321: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 19 13:51:07.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 19 13:51:07.322: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 19 13:51:09.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 19 13:51:09.322: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 19 13:51:11.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 19 13:51:11.319: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 19 13:51:13.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 19 13:51:13.324: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 19 13:51:15.313: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 19 13:51:15.324: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 19 13:51:17.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 19 13:51:17.319: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:51:17.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4533" for this suite.
Sep 19 13:51:39.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:51:39.639: INFO: namespace container-lifecycle-hook-4533 deletion completed in 22.312322598s

• [SLOW TEST:56.587 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:51:39.640: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-8208
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep 19 13:51:43.848: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-9a140ac1-027c-42c1-ab65-36a88d2583c2,GenerateName:,Namespace:events-8208,SelfLink:/api/v1/namespaces/events-8208/pods/send-events-9a140ac1-027c-42c1-ab65-36a88d2583c2,UID:396f1a3f-c798-4aae-9955-8d9f035f807e,ResourceVersion:28410,Generation:0,CreationTimestamp:2019-09-19 13:51:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 808227587,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.11.41/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5hgdl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5hgdl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-5hgdl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-46-204.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003db98c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003db98e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:51:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:51:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:51:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 13:51:39 +0000 UTC  }],Message:,Reason:,HostIP:172.31.46.204,PodIP:172.25.11.41,StartTime:2019-09-19 13:51:39 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-09-19 13:51:43 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://229cfc6f1ebcbc265040976e0937fff1b6383940fe378e06711387e9cb2ecdd5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Sep 19 13:51:45.856: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep 19 13:51:47.863: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:51:47.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8208" for this suite.
Sep 19 13:52:29.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:52:30.173: INFO: namespace events-8208 deletion completed in 42.291854117s

• [SLOW TEST:50.533 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:52:30.173: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3328
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:52:34.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3328" for this suite.
Sep 19 13:53:26.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:53:26.692: INFO: namespace kubelet-test-3328 deletion completed in 52.293760455s

• [SLOW TEST:56.519 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:53:26.693: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5136
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0919 13:53:32.922605      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 19 13:53:32.922: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:53:32.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5136" for this suite.
Sep 19 13:53:38.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:53:39.220: INFO: namespace gc-5136 deletion completed in 6.286779134s

• [SLOW TEST:12.527 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:53:39.220: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5511
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5511.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5511.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5511.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5511.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5511.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5511.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 19 13:53:44.183: INFO: DNS probes using dns-5511/dns-test-50733166-e6f3-4a13-b80f-f2f9c0652fc2 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:53:44.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5511" for this suite.
Sep 19 13:53:50.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:53:50.531: INFO: namespace dns-5511 deletion completed in 6.321667943s

• [SLOW TEST:11.311 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:53:50.531: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1718
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1718
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1718
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1718
Sep 19 13:53:50.731: INFO: Found 0 stateful pods, waiting for 1
Sep 19 13:54:00.739: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 19 13:54:00.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-1718 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 19 13:54:01.827: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 19 13:54:01.827: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 19 13:54:01.827: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 19 13:54:01.834: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 19 13:54:11.842: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 19 13:54:11.842: INFO: Waiting for statefulset status.replicas updated to 0
Sep 19 13:54:11.877: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999666s
Sep 19 13:54:12.885: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992113106s
Sep 19 13:54:13.892: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.983830807s
Sep 19 13:54:14.901: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.976466094s
Sep 19 13:54:15.908: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.968483754s
Sep 19 13:54:16.915: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.961331319s
Sep 19 13:54:17.930: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.95415818s
Sep 19 13:54:18.937: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.938818988s
Sep 19 13:54:19.945: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.931776723s
Sep 19 13:54:20.952: INFO: Verifying statefulset ss doesn't scale past 1 for another 924.450035ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1718
Sep 19 13:54:21.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-1718 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 19 13:54:22.681: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 19 13:54:22.681: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 19 13:54:22.681: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 19 13:54:22.688: INFO: Found 1 stateful pods, waiting for 3
Sep 19 13:54:32.695: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 19 13:54:32.695: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 19 13:54:32.695: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 19 13:54:32.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-1718 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 19 13:54:33.326: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 19 13:54:33.326: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 19 13:54:33.326: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 19 13:54:33.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-1718 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 19 13:54:33.962: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 19 13:54:33.962: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 19 13:54:33.962: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 19 13:54:33.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-1718 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 19 13:54:34.569: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 19 13:54:34.569: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 19 13:54:34.569: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 19 13:54:34.569: INFO: Waiting for statefulset status.replicas updated to 0
Sep 19 13:54:34.576: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 19 13:54:44.591: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 19 13:54:44.591: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 19 13:54:44.591: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 19 13:54:44.616: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999505s
Sep 19 13:54:45.622: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992437298s
Sep 19 13:54:46.630: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986201488s
Sep 19 13:54:47.638: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978055263s
Sep 19 13:54:48.645: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970060842s
Sep 19 13:54:49.652: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963472268s
Sep 19 13:54:50.662: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.955591627s
Sep 19 13:54:51.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.945494855s
Sep 19 13:54:52.677: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.938445393s
Sep 19 13:54:53.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 931.329745ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1718
Sep 19 13:54:54.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-1718 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 19 13:54:55.491: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 19 13:54:55.491: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 19 13:54:55.491: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 19 13:54:55.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-1718 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 19 13:54:56.106: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 19 13:54:56.106: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 19 13:54:56.106: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 19 13:54:56.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 exec --namespace=statefulset-1718 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 19 13:54:56.702: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 19 13:54:56.702: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 19 13:54:56.702: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 19 13:54:56.702: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep 19 13:55:16.735: INFO: Deleting all statefulset in ns statefulset-1718
Sep 19 13:55:16.764: INFO: Scaling statefulset ss to 0
Sep 19 13:55:16.791: INFO: Waiting for statefulset status.replicas updated to 0
Sep 19 13:55:16.797: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:55:16.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1718" for this suite.
Sep 19 13:55:22.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:55:23.166: INFO: namespace statefulset-1718 deletion completed in 6.334165402s

• [SLOW TEST:92.636 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:55:23.167: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3636
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 13:55:23.347: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c0326d5-8e21-4e28-abb7-39c55f99d140" in namespace "projected-3636" to be "success or failure"
Sep 19 13:55:23.357: INFO: Pod "downwardapi-volume-9c0326d5-8e21-4e28-abb7-39c55f99d140": Phase="Pending", Reason="", readiness=false. Elapsed: 9.937411ms
Sep 19 13:55:25.366: INFO: Pod "downwardapi-volume-9c0326d5-8e21-4e28-abb7-39c55f99d140": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018719056s
Sep 19 13:55:27.373: INFO: Pod "downwardapi-volume-9c0326d5-8e21-4e28-abb7-39c55f99d140": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025702848s
STEP: Saw pod success
Sep 19 13:55:27.373: INFO: Pod "downwardapi-volume-9c0326d5-8e21-4e28-abb7-39c55f99d140" satisfied condition "success or failure"
Sep 19 13:55:27.382: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod downwardapi-volume-9c0326d5-8e21-4e28-abb7-39c55f99d140 container client-container: <nil>
STEP: delete the pod
Sep 19 13:55:27.428: INFO: Waiting for pod downwardapi-volume-9c0326d5-8e21-4e28-abb7-39c55f99d140 to disappear
Sep 19 13:55:27.434: INFO: Pod downwardapi-volume-9c0326d5-8e21-4e28-abb7-39c55f99d140 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:55:27.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3636" for this suite.
Sep 19 13:55:33.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:55:33.763: INFO: namespace projected-3636 deletion completed in 6.321290299s

• [SLOW TEST:10.596 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:55:33.763: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4458
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4458
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 19 13:55:33.941: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 19 13:55:54.184: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.12.56:8080/dial?request=hostName&protocol=http&host=172.25.9.19&port=8080&tries=1'] Namespace:pod-network-test-4458 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:55:54.184: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:55:54.691: INFO: Waiting for endpoints: map[]
Sep 19 13:55:54.698: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.12.56:8080/dial?request=hostName&protocol=http&host=172.25.12.55&port=8080&tries=1'] Namespace:pod-network-test-4458 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:55:54.698: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:55:55.312: INFO: Waiting for endpoints: map[]
Sep 19 13:55:55.319: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.12.56:8080/dial?request=hostName&protocol=http&host=172.25.11.47&port=8080&tries=1'] Namespace:pod-network-test-4458 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:55:55.319: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:55:55.850: INFO: Waiting for endpoints: map[]
Sep 19 13:55:55.859: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.12.56:8080/dial?request=hostName&protocol=http&host=172.25.10.50&port=8080&tries=1'] Namespace:pod-network-test-4458 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:55:55.859: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:55:56.348: INFO: Waiting for endpoints: map[]
Sep 19 13:55:56.357: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.12.56:8080/dial?request=hostName&protocol=http&host=172.25.8.60&port=8080&tries=1'] Namespace:pod-network-test-4458 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 13:55:56.357: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 13:55:56.921: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:55:56.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4458" for this suite.
Sep 19 13:56:10.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:56:11.300: INFO: namespace pod-network-test-4458 deletion completed in 14.370748798s

• [SLOW TEST:37.537 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:56:11.300: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7572
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 13:56:11.496: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d92afd5-bc38-4b43-8149-2d47903344df" in namespace "downward-api-7572" to be "success or failure"
Sep 19 13:56:11.502: INFO: Pod "downwardapi-volume-1d92afd5-bc38-4b43-8149-2d47903344df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.251352ms
Sep 19 13:56:13.513: INFO: Pod "downwardapi-volume-1d92afd5-bc38-4b43-8149-2d47903344df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016810598s
STEP: Saw pod success
Sep 19 13:56:13.513: INFO: Pod "downwardapi-volume-1d92afd5-bc38-4b43-8149-2d47903344df" satisfied condition "success or failure"
Sep 19 13:56:13.520: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod downwardapi-volume-1d92afd5-bc38-4b43-8149-2d47903344df container client-container: <nil>
STEP: delete the pod
Sep 19 13:56:13.577: INFO: Waiting for pod downwardapi-volume-1d92afd5-bc38-4b43-8149-2d47903344df to disappear
Sep 19 13:56:13.582: INFO: Pod downwardapi-volume-1d92afd5-bc38-4b43-8149-2d47903344df no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:56:13.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7572" for this suite.
Sep 19 13:56:19.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:56:19.907: INFO: namespace downward-api-7572 deletion completed in 6.316603461s

• [SLOW TEST:8.607 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:56:19.908: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1015
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep 19 13:56:20.104: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:56:29.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1015" for this suite.
Sep 19 13:56:35.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:56:35.571: INFO: namespace pods-1015 deletion completed in 6.296463779s

• [SLOW TEST:15.663 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:56:35.572: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7781
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-f6229708-8ad2-4565-bf94-35f0745d41ec
STEP: Creating secret with name s-test-opt-upd-6c93d7a7-71a9-440f-845b-05e3bdd07348
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f6229708-8ad2-4565-bf94-35f0745d41ec
STEP: Updating secret s-test-opt-upd-6c93d7a7-71a9-440f-845b-05e3bdd07348
STEP: Creating secret with name s-test-opt-create-3fe972ab-18b6-4524-9845-a29a57b5ae71
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:56:42.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7781" for this suite.
Sep 19 13:57:04.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:57:04.643: INFO: namespace projected-7781 deletion completed in 22.317658355s

• [SLOW TEST:29.071 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:57:04.643: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-324926ef-1fc8-40b7-afb0-9fd0f3c010f6
STEP: Creating a pod to test consume secrets
Sep 19 13:57:04.828: INFO: Waiting up to 5m0s for pod "pod-secrets-55079720-c622-452d-bb4c-59bec5af8db3" in namespace "secrets-7707" to be "success or failure"
Sep 19 13:57:04.835: INFO: Pod "pod-secrets-55079720-c622-452d-bb4c-59bec5af8db3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.425073ms
Sep 19 13:57:06.842: INFO: Pod "pod-secrets-55079720-c622-452d-bb4c-59bec5af8db3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014504023s
Sep 19 13:57:08.849: INFO: Pod "pod-secrets-55079720-c622-452d-bb4c-59bec5af8db3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020943326s
STEP: Saw pod success
Sep 19 13:57:08.849: INFO: Pod "pod-secrets-55079720-c622-452d-bb4c-59bec5af8db3" satisfied condition "success or failure"
Sep 19 13:57:08.855: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod pod-secrets-55079720-c622-452d-bb4c-59bec5af8db3 container secret-volume-test: <nil>
STEP: delete the pod
Sep 19 13:57:08.927: INFO: Waiting for pod pod-secrets-55079720-c622-452d-bb4c-59bec5af8db3 to disappear
Sep 19 13:57:08.933: INFO: Pod pod-secrets-55079720-c622-452d-bb4c-59bec5af8db3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:57:08.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7707" for this suite.
Sep 19 13:57:14.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:57:15.226: INFO: namespace secrets-7707 deletion completed in 6.284118048s

• [SLOW TEST:10.583 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:57:15.227: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6810
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Sep 19 13:57:45.466: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0919 13:57:45.466531      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:57:45.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6810" for this suite.
Sep 19 13:57:51.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:57:51.776: INFO: namespace gc-6810 deletion completed in 6.30279878s

• [SLOW TEST:36.549 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:57:51.776: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8182
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 13:57:51.959: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39d895ee-b973-4201-b8cf-6fc7e96b56e9" in namespace "downward-api-8182" to be "success or failure"
Sep 19 13:57:51.968: INFO: Pod "downwardapi-volume-39d895ee-b973-4201-b8cf-6fc7e96b56e9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.863683ms
Sep 19 13:57:53.976: INFO: Pod "downwardapi-volume-39d895ee-b973-4201-b8cf-6fc7e96b56e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016573929s
STEP: Saw pod success
Sep 19 13:57:53.976: INFO: Pod "downwardapi-volume-39d895ee-b973-4201-b8cf-6fc7e96b56e9" satisfied condition "success or failure"
Sep 19 13:57:53.982: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod downwardapi-volume-39d895ee-b973-4201-b8cf-6fc7e96b56e9 container client-container: <nil>
STEP: delete the pod
Sep 19 13:57:54.052: INFO: Waiting for pod downwardapi-volume-39d895ee-b973-4201-b8cf-6fc7e96b56e9 to disappear
Sep 19 13:57:54.058: INFO: Pod downwardapi-volume-39d895ee-b973-4201-b8cf-6fc7e96b56e9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:57:54.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8182" for this suite.
Sep 19 13:58:00.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:58:00.363: INFO: namespace downward-api-8182 deletion completed in 6.290668316s

• [SLOW TEST:8.587 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:58:00.363: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9075
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 13:58:00.587: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 19 13:58:00.611: INFO: Number of nodes with available pods: 0
Sep 19 13:58:00.611: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:58:01.628: INFO: Number of nodes with available pods: 0
Sep 19 13:58:01.628: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:58:02.625: INFO: Number of nodes with available pods: 0
Sep 19 13:58:02.625: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:58:03.626: INFO: Number of nodes with available pods: 4
Sep 19 13:58:03.626: INFO: Node ip-172-31-36-147.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:58:04.626: INFO: Number of nodes with available pods: 5
Sep 19 13:58:04.626: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 19 13:58:04.697: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:04.697: INFO: Wrong image for pod: daemon-set-rw2w7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:04.697: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:04.697: INFO: Wrong image for pod: daemon-set-zjzh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:04.697: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:05.715: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:05.715: INFO: Wrong image for pod: daemon-set-rw2w7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:05.715: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:05.715: INFO: Wrong image for pod: daemon-set-zjzh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:05.715: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:06.713: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:06.713: INFO: Wrong image for pod: daemon-set-rw2w7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:06.713: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:06.713: INFO: Wrong image for pod: daemon-set-zjzh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:06.713: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:07.714: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:07.714: INFO: Wrong image for pod: daemon-set-rw2w7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:07.714: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:07.714: INFO: Wrong image for pod: daemon-set-zjzh2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:07.714: INFO: Pod daemon-set-zjzh2 is not available
Sep 19 13:58:07.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:08.715: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:08.715: INFO: Wrong image for pod: daemon-set-rw2w7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:08.715: INFO: Pod daemon-set-w4p76 is not available
Sep 19 13:58:08.715: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:08.715: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:09.713: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:09.713: INFO: Wrong image for pod: daemon-set-rw2w7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:09.713: INFO: Pod daemon-set-w4p76 is not available
Sep 19 13:58:09.713: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:09.713: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:10.714: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:10.714: INFO: Wrong image for pod: daemon-set-rw2w7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:10.714: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:10.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:11.714: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:11.714: INFO: Wrong image for pod: daemon-set-rw2w7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:11.714: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:11.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:12.714: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:12.714: INFO: Wrong image for pod: daemon-set-rw2w7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:12.714: INFO: Pod daemon-set-rw2w7 is not available
Sep 19 13:58:12.714: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:12.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:13.713: INFO: Pod daemon-set-mdlfh is not available
Sep 19 13:58:13.713: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:13.713: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:13.713: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:14.714: INFO: Pod daemon-set-mdlfh is not available
Sep 19 13:58:14.714: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:14.714: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:14.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:15.717: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:15.717: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:15.717: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:16.714: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:16.714: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:16.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:17.714: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:17.714: INFO: Pod daemon-set-nxdkf is not available
Sep 19 13:58:17.714: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:17.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:18.716: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:18.716: INFO: Pod daemon-set-nxdkf is not available
Sep 19 13:58:18.716: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:18.716: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:19.722: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:19.722: INFO: Pod daemon-set-nxdkf is not available
Sep 19 13:58:19.722: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:19.722: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:20.714: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:20.714: INFO: Pod daemon-set-nxdkf is not available
Sep 19 13:58:20.714: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:20.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:21.715: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:21.715: INFO: Pod daemon-set-nxdkf is not available
Sep 19 13:58:21.715: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:21.715: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:22.713: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:22.713: INFO: Pod daemon-set-nxdkf is not available
Sep 19 13:58:22.713: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:22.713: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:23.714: INFO: Wrong image for pod: daemon-set-nxdkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:23.714: INFO: Pod daemon-set-nxdkf is not available
Sep 19 13:58:23.714: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:23.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:24.716: INFO: Pod daemon-set-f5zrh is not available
Sep 19 13:58:24.716: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:24.716: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:25.713: INFO: Pod daemon-set-f5zrh is not available
Sep 19 13:58:25.713: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:25.713: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:26.714: INFO: Pod daemon-set-f5zrh is not available
Sep 19 13:58:26.714: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:26.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:27.714: INFO: Pod daemon-set-f5zrh is not available
Sep 19 13:58:27.714: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:27.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:28.714: INFO: Pod daemon-set-f5zrh is not available
Sep 19 13:58:28.714: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:28.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:29.713: INFO: Pod daemon-set-f5zrh is not available
Sep 19 13:58:29.713: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:29.713: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:30.714: INFO: Pod daemon-set-f5zrh is not available
Sep 19 13:58:30.714: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:30.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:31.713: INFO: Pod daemon-set-f5zrh is not available
Sep 19 13:58:31.713: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:31.713: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:32.713: INFO: Pod daemon-set-f5zrh is not available
Sep 19 13:58:32.713: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:32.713: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:33.715: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:33.715: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:34.714: INFO: Wrong image for pod: daemon-set-xhnvx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:34.714: INFO: Pod daemon-set-xhnvx is not available
Sep 19 13:58:34.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:35.713: INFO: Pod daemon-set-plf6r is not available
Sep 19 13:58:35.713: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:36.714: INFO: Pod daemon-set-plf6r is not available
Sep 19 13:58:36.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:37.713: INFO: Pod daemon-set-plf6r is not available
Sep 19 13:58:37.713: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:38.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:39.713: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:39.713: INFO: Pod daemon-set-zxs4t is not available
Sep 19 13:58:40.713: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:40.713: INFO: Pod daemon-set-zxs4t is not available
Sep 19 13:58:41.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:41.714: INFO: Pod daemon-set-zxs4t is not available
Sep 19 13:58:42.713: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:42.713: INFO: Pod daemon-set-zxs4t is not available
Sep 19 13:58:43.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:43.715: INFO: Pod daemon-set-zxs4t is not available
Sep 19 13:58:44.714: INFO: Wrong image for pod: daemon-set-zxs4t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 19 13:58:44.714: INFO: Pod daemon-set-zxs4t is not available
Sep 19 13:58:45.714: INFO: Pod daemon-set-dtldr is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 19 13:58:45.738: INFO: Number of nodes with available pods: 4
Sep 19 13:58:45.738: INFO: Node ip-172-31-44-209.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:58:46.753: INFO: Number of nodes with available pods: 4
Sep 19 13:58:46.753: INFO: Node ip-172-31-44-209.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:58:47.753: INFO: Number of nodes with available pods: 4
Sep 19 13:58:47.753: INFO: Node ip-172-31-44-209.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 13:58:48.753: INFO: Number of nodes with available pods: 5
Sep 19 13:58:48.753: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9075, will wait for the garbage collector to delete the pods
Sep 19 13:58:48.856: INFO: Deleting DaemonSet.extensions daemon-set took: 14.70923ms
Sep 19 13:58:49.257: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.347384ms
Sep 19 13:58:55.463: INFO: Number of nodes with available pods: 0
Sep 19 13:58:55.463: INFO: Number of running nodes: 0, number of available pods: 0
Sep 19 13:58:55.470: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9075/daemonsets","resourceVersion":"30663"},"items":null}

Sep 19 13:58:55.476: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9075/pods","resourceVersion":"30663"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:58:55.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9075" for this suite.
Sep 19 13:59:01.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:59:01.869: INFO: namespace daemonsets-9075 deletion completed in 6.345170515s

• [SLOW TEST:61.506 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:59:01.869: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7571
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 19 13:59:02.096: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7571,SelfLink:/api/v1/namespaces/watch-7571/configmaps/e2e-watch-test-label-changed,UID:40bfa74e-051d-40f4-b2ec-414638db2311,ResourceVersion:30755,Generation:0,CreationTimestamp:2019-09-19 13:59:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 19 13:59:02.097: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7571,SelfLink:/api/v1/namespaces/watch-7571/configmaps/e2e-watch-test-label-changed,UID:40bfa74e-051d-40f4-b2ec-414638db2311,ResourceVersion:30756,Generation:0,CreationTimestamp:2019-09-19 13:59:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 19 13:59:02.097: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7571,SelfLink:/api/v1/namespaces/watch-7571/configmaps/e2e-watch-test-label-changed,UID:40bfa74e-051d-40f4-b2ec-414638db2311,ResourceVersion:30757,Generation:0,CreationTimestamp:2019-09-19 13:59:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 19 13:59:12.165: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7571,SelfLink:/api/v1/namespaces/watch-7571/configmaps/e2e-watch-test-label-changed,UID:40bfa74e-051d-40f4-b2ec-414638db2311,ResourceVersion:30786,Generation:0,CreationTimestamp:2019-09-19 13:59:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 19 13:59:12.165: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7571,SelfLink:/api/v1/namespaces/watch-7571/configmaps/e2e-watch-test-label-changed,UID:40bfa74e-051d-40f4-b2ec-414638db2311,ResourceVersion:30787,Generation:0,CreationTimestamp:2019-09-19 13:59:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Sep 19 13:59:12.165: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7571,SelfLink:/api/v1/namespaces/watch-7571/configmaps/e2e-watch-test-label-changed,UID:40bfa74e-051d-40f4-b2ec-414638db2311,ResourceVersion:30788,Generation:0,CreationTimestamp:2019-09-19 13:59:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:59:12.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7571" for this suite.
Sep 19 13:59:18.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:59:18.524: INFO: namespace watch-7571 deletion completed in 6.350631012s

• [SLOW TEST:16.655 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:59:18.525: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4081
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Sep 19 13:59:18.722: INFO: Waiting up to 5m0s for pod "client-containers-284a8ce1-a432-40c6-b356-10b787a8e36d" in namespace "containers-4081" to be "success or failure"
Sep 19 13:59:18.730: INFO: Pod "client-containers-284a8ce1-a432-40c6-b356-10b787a8e36d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.938533ms
Sep 19 13:59:20.737: INFO: Pod "client-containers-284a8ce1-a432-40c6-b356-10b787a8e36d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014103367s
Sep 19 13:59:22.743: INFO: Pod "client-containers-284a8ce1-a432-40c6-b356-10b787a8e36d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020850736s
STEP: Saw pod success
Sep 19 13:59:22.743: INFO: Pod "client-containers-284a8ce1-a432-40c6-b356-10b787a8e36d" satisfied condition "success or failure"
Sep 19 13:59:22.750: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod client-containers-284a8ce1-a432-40c6-b356-10b787a8e36d container test-container: <nil>
STEP: delete the pod
Sep 19 13:59:22.797: INFO: Waiting for pod client-containers-284a8ce1-a432-40c6-b356-10b787a8e36d to disappear
Sep 19 13:59:22.807: INFO: Pod client-containers-284a8ce1-a432-40c6-b356-10b787a8e36d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:59:22.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4081" for this suite.
Sep 19 13:59:28.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:59:29.147: INFO: namespace containers-4081 deletion completed in 6.329686402s

• [SLOW TEST:10.622 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:59:29.148: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8000
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-419ac6c0-7248-40f2-b425-48ea397e2d3b
STEP: Creating a pod to test consume configMaps
Sep 19 13:59:29.345: INFO: Waiting up to 5m0s for pod "pod-configmaps-1e7bfea5-9634-4e3a-b95f-08b17de8fef3" in namespace "configmap-8000" to be "success or failure"
Sep 19 13:59:29.351: INFO: Pod "pod-configmaps-1e7bfea5-9634-4e3a-b95f-08b17de8fef3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.79527ms
Sep 19 13:59:31.357: INFO: Pod "pod-configmaps-1e7bfea5-9634-4e3a-b95f-08b17de8fef3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011910384s
Sep 19 13:59:33.364: INFO: Pod "pod-configmaps-1e7bfea5-9634-4e3a-b95f-08b17de8fef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018464546s
STEP: Saw pod success
Sep 19 13:59:33.364: INFO: Pod "pod-configmaps-1e7bfea5-9634-4e3a-b95f-08b17de8fef3" satisfied condition "success or failure"
Sep 19 13:59:33.370: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod pod-configmaps-1e7bfea5-9634-4e3a-b95f-08b17de8fef3 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 19 13:59:33.411: INFO: Waiting for pod pod-configmaps-1e7bfea5-9634-4e3a-b95f-08b17de8fef3 to disappear
Sep 19 13:59:33.417: INFO: Pod pod-configmaps-1e7bfea5-9634-4e3a-b95f-08b17de8fef3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 13:59:33.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8000" for this suite.
Sep 19 13:59:39.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 13:59:39.707: INFO: namespace configmap-8000 deletion completed in 6.282405771s

• [SLOW TEST:10.559 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 13:59:39.708: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9207
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-5a3b63bd-84a0-441e-86a0-66baa9f5b64b in namespace container-probe-9207
Sep 19 13:59:45.913: INFO: Started pod busybox-5a3b63bd-84a0-441e-86a0-66baa9f5b64b in namespace container-probe-9207
STEP: checking the pod's current state and verifying that restartCount is present
Sep 19 13:59:45.919: INFO: Initial restart count of pod busybox-5a3b63bd-84a0-441e-86a0-66baa9f5b64b is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:03:46.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9207" for this suite.
Sep 19 14:03:52.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:03:53.212: INFO: namespace container-probe-9207 deletion completed in 6.309911278s

• [SLOW TEST:253.504 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:03:53.213: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4960
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-5042ee70-253f-47d5-964b-04dafc4ee012
STEP: Creating a pod to test consume secrets
Sep 19 14:03:53.419: INFO: Waiting up to 5m0s for pod "pod-secrets-d8d0b84c-aa1e-4380-af21-32ae0d2839d2" in namespace "secrets-4960" to be "success or failure"
Sep 19 14:03:53.425: INFO: Pod "pod-secrets-d8d0b84c-aa1e-4380-af21-32ae0d2839d2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.999896ms
Sep 19 14:03:55.432: INFO: Pod "pod-secrets-d8d0b84c-aa1e-4380-af21-32ae0d2839d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013186721s
Sep 19 14:03:57.439: INFO: Pod "pod-secrets-d8d0b84c-aa1e-4380-af21-32ae0d2839d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019894196s
STEP: Saw pod success
Sep 19 14:03:57.439: INFO: Pod "pod-secrets-d8d0b84c-aa1e-4380-af21-32ae0d2839d2" satisfied condition "success or failure"
Sep 19 14:03:57.445: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod pod-secrets-d8d0b84c-aa1e-4380-af21-32ae0d2839d2 container secret-volume-test: <nil>
STEP: delete the pod
Sep 19 14:03:57.485: INFO: Waiting for pod pod-secrets-d8d0b84c-aa1e-4380-af21-32ae0d2839d2 to disappear
Sep 19 14:03:57.490: INFO: Pod pod-secrets-d8d0b84c-aa1e-4380-af21-32ae0d2839d2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:03:57.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4960" for this suite.
Sep 19 14:04:03.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:04:03.820: INFO: namespace secrets-4960 deletion completed in 6.321202678s

• [SLOW TEST:10.606 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:04:03.820: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-466
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 19 14:04:04.001: INFO: Waiting up to 5m0s for pod "pod-6b39c397-7e1d-443d-ac45-4161e605511c" in namespace "emptydir-466" to be "success or failure"
Sep 19 14:04:04.010: INFO: Pod "pod-6b39c397-7e1d-443d-ac45-4161e605511c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.200927ms
Sep 19 14:04:06.018: INFO: Pod "pod-6b39c397-7e1d-443d-ac45-4161e605511c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016800212s
Sep 19 14:04:08.025: INFO: Pod "pod-6b39c397-7e1d-443d-ac45-4161e605511c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02385523s
STEP: Saw pod success
Sep 19 14:04:08.025: INFO: Pod "pod-6b39c397-7e1d-443d-ac45-4161e605511c" satisfied condition "success or failure"
Sep 19 14:04:08.031: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod pod-6b39c397-7e1d-443d-ac45-4161e605511c container test-container: <nil>
STEP: delete the pod
Sep 19 14:04:08.105: INFO: Waiting for pod pod-6b39c397-7e1d-443d-ac45-4161e605511c to disappear
Sep 19 14:04:08.112: INFO: Pod pod-6b39c397-7e1d-443d-ac45-4161e605511c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:04:08.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-466" for this suite.
Sep 19 14:04:14.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:04:14.407: INFO: namespace emptydir-466 deletion completed in 6.286622479s

• [SLOW TEST:10.587 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:04:14.408: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-303
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep 19 14:04:15.095: INFO: Pod name wrapped-volume-race-51907bb5-7112-40ae-a4bf-dc051854a1bc: Found 0 pods out of 5
Sep 19 14:04:20.104: INFO: Pod name wrapped-volume-race-51907bb5-7112-40ae-a4bf-dc051854a1bc: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-51907bb5-7112-40ae-a4bf-dc051854a1bc in namespace emptydir-wrapper-303, will wait for the garbage collector to delete the pods
Sep 19 14:04:32.225: INFO: Deleting ReplicationController wrapped-volume-race-51907bb5-7112-40ae-a4bf-dc051854a1bc took: 21.640118ms
Sep 19 14:04:32.725: INFO: Terminating ReplicationController wrapped-volume-race-51907bb5-7112-40ae-a4bf-dc051854a1bc pods took: 500.400682ms
STEP: Creating RC which spawns configmap-volume pods
Sep 19 14:05:14.762: INFO: Pod name wrapped-volume-race-ac2cf86c-fb41-4f42-bea9-d5225f2465e0: Found 0 pods out of 5
Sep 19 14:05:19.779: INFO: Pod name wrapped-volume-race-ac2cf86c-fb41-4f42-bea9-d5225f2465e0: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ac2cf86c-fb41-4f42-bea9-d5225f2465e0 in namespace emptydir-wrapper-303, will wait for the garbage collector to delete the pods
Sep 19 14:05:31.896: INFO: Deleting ReplicationController wrapped-volume-race-ac2cf86c-fb41-4f42-bea9-d5225f2465e0 took: 14.146545ms
Sep 19 14:05:32.496: INFO: Terminating ReplicationController wrapped-volume-race-ac2cf86c-fb41-4f42-bea9-d5225f2465e0 pods took: 600.181577ms
STEP: Creating RC which spawns configmap-volume pods
Sep 19 14:06:14.911: INFO: Pod name wrapped-volume-race-8fb2012b-18cd-48c4-bec8-04744d88fe0c: Found 3 pods out of 5
Sep 19 14:06:19.921: INFO: Pod name wrapped-volume-race-8fb2012b-18cd-48c4-bec8-04744d88fe0c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8fb2012b-18cd-48c4-bec8-04744d88fe0c in namespace emptydir-wrapper-303, will wait for the garbage collector to delete the pods
Sep 19 14:06:32.205: INFO: Deleting ReplicationController wrapped-volume-race-8fb2012b-18cd-48c4-bec8-04744d88fe0c took: 69.179441ms
Sep 19 14:06:32.706: INFO: Terminating ReplicationController wrapped-volume-race-8fb2012b-18cd-48c4-bec8-04744d88fe0c pods took: 500.260428ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:07:16.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-303" for this suite.
Sep 19 14:07:25.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:07:25.536: INFO: namespace emptydir-wrapper-303 deletion completed in 8.604649928s

• [SLOW TEST:191.129 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:07:25.536: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 19 14:07:25.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-797'
Sep 19 14:07:26.063: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 19 14:07:26.063: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Sep 19 14:07:26.080: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-nh8kb]
Sep 19 14:07:26.080: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-nh8kb" in namespace "kubectl-797" to be "running and ready"
Sep 19 14:07:26.092: INFO: Pod "e2e-test-nginx-rc-nh8kb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.913552ms
Sep 19 14:07:28.099: INFO: Pod "e2e-test-nginx-rc-nh8kb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019117308s
Sep 19 14:07:30.107: INFO: Pod "e2e-test-nginx-rc-nh8kb": Phase="Running", Reason="", readiness=true. Elapsed: 4.026602857s
Sep 19 14:07:30.107: INFO: Pod "e2e-test-nginx-rc-nh8kb" satisfied condition "running and ready"
Sep 19 14:07:30.107: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-nh8kb]
Sep 19 14:07:30.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 logs rc/e2e-test-nginx-rc --namespace=kubectl-797'
Sep 19 14:07:30.257: INFO: stderr: ""
Sep 19 14:07:30.257: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Sep 19 14:07:30.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete rc e2e-test-nginx-rc --namespace=kubectl-797'
Sep 19 14:07:30.354: INFO: stderr: ""
Sep 19 14:07:30.354: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:07:30.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-797" for this suite.
Sep 19 14:07:52.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:07:53.105: INFO: namespace kubectl-797 deletion completed in 22.742792695s

• [SLOW TEST:27.568 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:07:53.105: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-709
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 19 14:08:01.514: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-709 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 14:08:01.514: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 14:08:02.299: INFO: Exec stderr: ""
Sep 19 14:08:02.299: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-709 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 14:08:02.299: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 14:08:03.097: INFO: Exec stderr: ""
Sep 19 14:08:03.097: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-709 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 14:08:03.097: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 14:08:03.894: INFO: Exec stderr: ""
Sep 19 14:08:03.894: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-709 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 14:08:03.894: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 14:08:04.477: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 19 14:08:04.477: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-709 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 14:08:04.477: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 14:08:05.195: INFO: Exec stderr: ""
Sep 19 14:08:05.195: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-709 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 14:08:05.195: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 14:08:05.898: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 19 14:08:05.899: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-709 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 14:08:05.899: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 14:08:06.694: INFO: Exec stderr: ""
Sep 19 14:08:06.695: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-709 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 14:08:06.695: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 14:08:07.445: INFO: Exec stderr: ""
Sep 19 14:08:07.445: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-709 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 14:08:07.445: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 14:08:08.199: INFO: Exec stderr: ""
Sep 19 14:08:08.199: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-709 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 19 14:08:08.199: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
Sep 19 14:08:08.840: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:08:08.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-709" for this suite.
Sep 19 14:08:54.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:08:55.337: INFO: namespace e2e-kubelet-etc-hosts-709 deletion completed in 46.430067336s

• [SLOW TEST:62.232 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:08:55.337: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep 19 14:09:00.134: INFO: Successfully updated pod "annotationupdateb6bd0a33-dc96-48fb-a821-7012325b455f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:09:02.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7844" for this suite.
Sep 19 14:09:24.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:09:24.549: INFO: namespace projected-7844 deletion completed in 22.346629453s

• [SLOW TEST:29.211 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:09:24.549: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9465
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-3fab13d9-1b65-4df8-b07c-20b11539cd7a
STEP: Creating a pod to test consume secrets
Sep 19 14:09:24.835: INFO: Waiting up to 5m0s for pod "pod-secrets-791124a6-1122-49f8-a0f1-b3ab1c95a5c8" in namespace "secrets-9465" to be "success or failure"
Sep 19 14:09:24.867: INFO: Pod "pod-secrets-791124a6-1122-49f8-a0f1-b3ab1c95a5c8": Phase="Pending", Reason="", readiness=false. Elapsed: 31.763943ms
Sep 19 14:09:26.874: INFO: Pod "pod-secrets-791124a6-1122-49f8-a0f1-b3ab1c95a5c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03865478s
Sep 19 14:09:28.885: INFO: Pod "pod-secrets-791124a6-1122-49f8-a0f1-b3ab1c95a5c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049111062s
STEP: Saw pod success
Sep 19 14:09:28.885: INFO: Pod "pod-secrets-791124a6-1122-49f8-a0f1-b3ab1c95a5c8" satisfied condition "success or failure"
Sep 19 14:09:28.894: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod pod-secrets-791124a6-1122-49f8-a0f1-b3ab1c95a5c8 container secret-volume-test: <nil>
STEP: delete the pod
Sep 19 14:09:29.001: INFO: Waiting for pod pod-secrets-791124a6-1122-49f8-a0f1-b3ab1c95a5c8 to disappear
Sep 19 14:09:29.009: INFO: Pod pod-secrets-791124a6-1122-49f8-a0f1-b3ab1c95a5c8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:09:29.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9465" for this suite.
Sep 19 14:09:35.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:09:35.347: INFO: namespace secrets-9465 deletion completed in 6.33079061s

• [SLOW TEST:10.798 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:09:35.348: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2744
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 19 14:09:35.574: INFO: Waiting up to 5m0s for pod "pod-df07a925-77e3-4f09-96a3-579f01dbd3f9" in namespace "emptydir-2744" to be "success or failure"
Sep 19 14:09:35.595: INFO: Pod "pod-df07a925-77e3-4f09-96a3-579f01dbd3f9": Phase="Pending", Reason="", readiness=false. Elapsed: 21.346196ms
Sep 19 14:09:37.602: INFO: Pod "pod-df07a925-77e3-4f09-96a3-579f01dbd3f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028100356s
Sep 19 14:09:39.609: INFO: Pod "pod-df07a925-77e3-4f09-96a3-579f01dbd3f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035143313s
STEP: Saw pod success
Sep 19 14:09:39.609: INFO: Pod "pod-df07a925-77e3-4f09-96a3-579f01dbd3f9" satisfied condition "success or failure"
Sep 19 14:09:39.616: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod pod-df07a925-77e3-4f09-96a3-579f01dbd3f9 container test-container: <nil>
STEP: delete the pod
Sep 19 14:09:39.701: INFO: Waiting for pod pod-df07a925-77e3-4f09-96a3-579f01dbd3f9 to disappear
Sep 19 14:09:39.707: INFO: Pod pod-df07a925-77e3-4f09-96a3-579f01dbd3f9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:09:39.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2744" for this suite.
Sep 19 14:09:45.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:09:46.096: INFO: namespace emptydir-2744 deletion completed in 6.381452815s

• [SLOW TEST:10.748 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:09:46.097: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3062
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Sep 19 14:09:46.298: INFO: Waiting up to 5m0s for pod "var-expansion-0f64763c-7b08-455b-b774-1b348c17e2df" in namespace "var-expansion-3062" to be "success or failure"
Sep 19 14:09:46.306: INFO: Pod "var-expansion-0f64763c-7b08-455b-b774-1b348c17e2df": Phase="Pending", Reason="", readiness=false. Elapsed: 7.67234ms
Sep 19 14:09:48.317: INFO: Pod "var-expansion-0f64763c-7b08-455b-b774-1b348c17e2df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019469165s
Sep 19 14:09:50.325: INFO: Pod "var-expansion-0f64763c-7b08-455b-b774-1b348c17e2df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026591248s
STEP: Saw pod success
Sep 19 14:09:50.325: INFO: Pod "var-expansion-0f64763c-7b08-455b-b774-1b348c17e2df" satisfied condition "success or failure"
Sep 19 14:09:50.331: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod var-expansion-0f64763c-7b08-455b-b774-1b348c17e2df container dapi-container: <nil>
STEP: delete the pod
Sep 19 14:09:50.396: INFO: Waiting for pod var-expansion-0f64763c-7b08-455b-b774-1b348c17e2df to disappear
Sep 19 14:09:50.403: INFO: Pod var-expansion-0f64763c-7b08-455b-b774-1b348c17e2df no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:09:50.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3062" for this suite.
Sep 19 14:09:56.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:09:56.747: INFO: namespace var-expansion-3062 deletion completed in 6.335187923s

• [SLOW TEST:10.650 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:09:56.747: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:09:57.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7035" for this suite.
Sep 19 14:10:21.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:10:21.429: INFO: namespace pods-7035 deletion completed in 24.382239499s

• [SLOW TEST:24.682 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:10:21.430: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6588
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-e7cbc37f-6032-47df-b65d-8cd01e40a735
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:10:25.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6588" for this suite.
Sep 19 14:10:47.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:10:48.259: INFO: namespace configmap-6588 deletion completed in 22.402871105s

• [SLOW TEST:26.829 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:10:48.259: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1463
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-5f891ecd-2d79-4207-9262-20653ece6ab7
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:10:48.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1463" for this suite.
Sep 19 14:10:54.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:10:54.883: INFO: namespace configmap-1463 deletion completed in 6.361311652s

• [SLOW TEST:6.625 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:10:54.885: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5224
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 14:10:55.057: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:10:59.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5224" for this suite.
Sep 19 14:11:43.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:11:43.531: INFO: namespace pods-5224 deletion completed in 44.35562485s

• [SLOW TEST:48.646 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:11:43.532: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5039
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 14:11:43.792: INFO: (0) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 53.89193ms)
Sep 19 14:11:43.836: INFO: (1) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 44.170503ms)
Sep 19 14:11:43.847: INFO: (2) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 11.471321ms)
Sep 19 14:11:43.869: INFO: (3) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 21.250181ms)
Sep 19 14:11:43.882: INFO: (4) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 13.275716ms)
Sep 19 14:11:43.897: INFO: (5) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 14.755348ms)
Sep 19 14:11:43.910: INFO: (6) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 12.961939ms)
Sep 19 14:11:43.921: INFO: (7) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 11.100096ms)
Sep 19 14:11:43.932: INFO: (8) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 11.047652ms)
Sep 19 14:11:43.951: INFO: (9) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 18.877136ms)
Sep 19 14:11:43.964: INFO: (10) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 12.908846ms)
Sep 19 14:11:43.976: INFO: (11) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 12.090738ms)
Sep 19 14:11:43.992: INFO: (12) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 16.144972ms)
Sep 19 14:11:44.006: INFO: (13) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 13.327859ms)
Sep 19 14:11:44.017: INFO: (14) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 10.761665ms)
Sep 19 14:11:44.028: INFO: (15) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 11.7802ms)
Sep 19 14:11:44.040: INFO: (16) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 11.939651ms)
Sep 19 14:11:44.052: INFO: (17) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 11.503237ms)
Sep 19 14:11:44.065: INFO: (18) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 12.638624ms)
Sep 19 14:11:44.077: INFO: (19) /api/v1/nodes/ip-172-31-33-83.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 12.402121ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:11:44.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5039" for this suite.
Sep 19 14:11:50.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:11:50.494: INFO: namespace proxy-5039 deletion completed in 6.407438751s

• [SLOW TEST:6.963 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:11:50.495: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 14:11:50.701: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d1bfabd-0d22-48af-b7b0-12565a063193" in namespace "projected-6512" to be "success or failure"
Sep 19 14:11:50.714: INFO: Pod "downwardapi-volume-0d1bfabd-0d22-48af-b7b0-12565a063193": Phase="Pending", Reason="", readiness=false. Elapsed: 13.023562ms
Sep 19 14:11:52.722: INFO: Pod "downwardapi-volume-0d1bfabd-0d22-48af-b7b0-12565a063193": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020881592s
Sep 19 14:11:54.729: INFO: Pod "downwardapi-volume-0d1bfabd-0d22-48af-b7b0-12565a063193": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028562652s
STEP: Saw pod success
Sep 19 14:11:54.729: INFO: Pod "downwardapi-volume-0d1bfabd-0d22-48af-b7b0-12565a063193" satisfied condition "success or failure"
Sep 19 14:11:54.738: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod downwardapi-volume-0d1bfabd-0d22-48af-b7b0-12565a063193 container client-container: <nil>
STEP: delete the pod
Sep 19 14:11:54.802: INFO: Waiting for pod downwardapi-volume-0d1bfabd-0d22-48af-b7b0-12565a063193 to disappear
Sep 19 14:11:54.809: INFO: Pod downwardapi-volume-0d1bfabd-0d22-48af-b7b0-12565a063193 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:11:54.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6512" for this suite.
Sep 19 14:12:00.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:12:01.146: INFO: namespace projected-6512 deletion completed in 6.329050801s

• [SLOW TEST:10.652 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:12:01.147: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 14:12:01.368: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17550955-d474-46ce-be6a-5140a5ed73a7" in namespace "downward-api-5018" to be "success or failure"
Sep 19 14:12:01.377: INFO: Pod "downwardapi-volume-17550955-d474-46ce-be6a-5140a5ed73a7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.254545ms
Sep 19 14:12:03.387: INFO: Pod "downwardapi-volume-17550955-d474-46ce-be6a-5140a5ed73a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018885451s
Sep 19 14:12:05.394: INFO: Pod "downwardapi-volume-17550955-d474-46ce-be6a-5140a5ed73a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02599309s
STEP: Saw pod success
Sep 19 14:12:05.394: INFO: Pod "downwardapi-volume-17550955-d474-46ce-be6a-5140a5ed73a7" satisfied condition "success or failure"
Sep 19 14:12:05.401: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod downwardapi-volume-17550955-d474-46ce-be6a-5140a5ed73a7 container client-container: <nil>
STEP: delete the pod
Sep 19 14:12:05.467: INFO: Waiting for pod downwardapi-volume-17550955-d474-46ce-be6a-5140a5ed73a7 to disappear
Sep 19 14:12:05.480: INFO: Pod downwardapi-volume-17550955-d474-46ce-be6a-5140a5ed73a7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:12:05.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5018" for this suite.
Sep 19 14:12:13.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:12:14.787: INFO: namespace downward-api-5018 deletion completed in 9.299913604s

• [SLOW TEST:13.640 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:12:14.788: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4256
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 14:12:15.025: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 19 14:12:15.120: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 19 14:12:20.135: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 19 14:12:20.135: INFO: Creating deployment "test-rolling-update-deployment"
Sep 19 14:12:20.149: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 19 14:12:20.177: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 19 14:12:22.195: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 19 14:12:22.221: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499140, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499140, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499140, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499140, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 19 14:12:24.228: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep 19 14:12:24.282: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-4256,SelfLink:/apis/apps/v1/namespaces/deployment-4256/deployments/test-rolling-update-deployment,UID:b969f680-1ce0-40be-bb6e-dc6b64888374,ResourceVersion:34475,Generation:1,CreationTimestamp:2019-09-19 14:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-19 14:12:20 +0000 UTC 2019-09-19 14:12:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-19 14:12:22 +0000 UTC 2019-09-19 14:12:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep 19 14:12:24.312: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-4256,SelfLink:/apis/apps/v1/namespaces/deployment-4256/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:94623a67-7295-4a20-928b-32aa62d892ba,ResourceVersion:34464,Generation:1,CreationTimestamp:2019-09-19 14:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment b969f680-1ce0-40be-bb6e-dc6b64888374 0xc0009c6747 0xc0009c6748}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep 19 14:12:24.312: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 19 14:12:24.312: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-4256,SelfLink:/apis/apps/v1/namespaces/deployment-4256/replicasets/test-rolling-update-controller,UID:84d86c2a-2840-4536-9ac3-fbc45d003667,ResourceVersion:34474,Generation:2,CreationTimestamp:2019-09-19 14:12:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment b969f680-1ce0-40be-bb6e-dc6b64888374 0xc0009c6667 0xc0009c6668}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 19 14:12:24.327: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-r2q9p" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-r2q9p,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-4256,SelfLink:/api/v1/namespaces/deployment-4256/pods/test-rolling-update-deployment-79f6b9d75c-r2q9p,UID:61aa1f6f-6ae3-4630-a605-e02c5385c264,ResourceVersion:34463,Generation:0,CreationTimestamp:2019-09-19 14:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.12.63/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 94623a67-7295-4a20-928b-32aa62d892ba 0xc0009c7697 0xc0009c7698}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-6wn44 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6wn44,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-6wn44 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-36-147.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0009c7700} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0009c7720}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 14:12:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 14:12:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 14:12:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 14:12:20 +0000 UTC  }],Message:,Reason:,HostIP:172.31.36.147,PodIP:172.25.12.63,StartTime:2019-09-19 14:12:20 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-19 14:12:21 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://3bfdc98d47204b89800159036fb5407c5e242ebca91f6a9ee86b6fad42b40b5f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:12:24.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4256" for this suite.
Sep 19 14:12:32.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:12:33.116: INFO: namespace deployment-4256 deletion completed in 8.774882448s

• [SLOW TEST:18.328 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:12:33.116: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1454
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-f0908544-5609-440a-9df0-580a9533ae05
STEP: Creating configMap with name cm-test-opt-upd-7dbf2cac-09c9-423f-bf25-2b09ec964569
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f0908544-5609-440a-9df0-580a9533ae05
STEP: Updating configmap cm-test-opt-upd-7dbf2cac-09c9-423f-bf25-2b09ec964569
STEP: Creating configMap with name cm-test-opt-create-1fd02c71-d6b7-492d-9966-1940a5de4980
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:12:40.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1454" for this suite.
Sep 19 14:13:04.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:13:05.051: INFO: namespace projected-1454 deletion completed in 24.574711225s

• [SLOW TEST:31.935 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:13:05.051: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4133
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 19 14:13:07.428: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:13:07.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4133" for this suite.
Sep 19 14:13:13.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:13:14.418: INFO: namespace container-runtime-4133 deletion completed in 6.904049306s

• [SLOW TEST:9.367 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:13:14.419: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8820
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-93572e7a-d837-4e1c-9c7e-0acc80f71094
STEP: Creating a pod to test consume secrets
Sep 19 14:13:14.701: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a14a6403-bd66-417d-bcb4-6a360cdbf97a" in namespace "projected-8820" to be "success or failure"
Sep 19 14:13:14.712: INFO: Pod "pod-projected-secrets-a14a6403-bd66-417d-bcb4-6a360cdbf97a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.340883ms
Sep 19 14:13:16.721: INFO: Pod "pod-projected-secrets-a14a6403-bd66-417d-bcb4-6a360cdbf97a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020568695s
Sep 19 14:13:18.728: INFO: Pod "pod-projected-secrets-a14a6403-bd66-417d-bcb4-6a360cdbf97a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026881707s
STEP: Saw pod success
Sep 19 14:13:18.728: INFO: Pod "pod-projected-secrets-a14a6403-bd66-417d-bcb4-6a360cdbf97a" satisfied condition "success or failure"
Sep 19 14:13:18.734: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod pod-projected-secrets-a14a6403-bd66-417d-bcb4-6a360cdbf97a container secret-volume-test: <nil>
STEP: delete the pod
Sep 19 14:13:18.824: INFO: Waiting for pod pod-projected-secrets-a14a6403-bd66-417d-bcb4-6a360cdbf97a to disappear
Sep 19 14:13:18.831: INFO: Pod pod-projected-secrets-a14a6403-bd66-417d-bcb4-6a360cdbf97a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:13:18.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8820" for this suite.
Sep 19 14:13:24.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:13:25.722: INFO: namespace projected-8820 deletion completed in 6.882658116s

• [SLOW TEST:11.303 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:13:25.722: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5171
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5171.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5171.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5171.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5171.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 19 14:13:30.316: INFO: DNS probes using dns-test-2942c986-407a-432d-b146-8c69dfa8721a succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5171.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5171.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5171.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5171.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 19 14:13:42.746: INFO: DNS probes using dns-test-d7162e3b-8700-4480-8263-3433505e2e57 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5171.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5171.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5171.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5171.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 19 14:13:55.147: INFO: DNS probes using dns-test-23710083-a8b4-41ab-aaa8-c621ea482b0e succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:13:55.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5171" for this suite.
Sep 19 14:14:03.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:14:04.405: INFO: namespace dns-5171 deletion completed in 9.145739784s

• [SLOW TEST:38.683 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:14:04.405: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Sep 19 14:14:04.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 create -f - --namespace=kubectl-2867'
Sep 19 14:14:05.117: INFO: stderr: ""
Sep 19 14:14:05.117: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Sep 19 14:14:06.126: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:14:06.126: INFO: Found 0 / 1
Sep 19 14:14:07.138: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:14:07.138: INFO: Found 0 / 1
Sep 19 14:14:08.124: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:14:08.124: INFO: Found 1 / 1
Sep 19 14:14:08.125: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 19 14:14:08.131: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:14:08.131: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Sep 19 14:14:08.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 logs redis-master-l84zn redis-master --namespace=kubectl-2867'
Sep 19 14:14:08.304: INFO: stderr: ""
Sep 19 14:14:08.304: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 19 Sep 14:14:06.681 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 19 Sep 14:14:06.681 # Server started, Redis version 3.2.12\n1:M 19 Sep 14:14:06.681 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 19 Sep 14:14:06.681 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Sep 19 14:14:08.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 log redis-master-l84zn redis-master --namespace=kubectl-2867 --tail=1'
Sep 19 14:14:08.534: INFO: stderr: ""
Sep 19 14:14:08.534: INFO: stdout: "1:M 19 Sep 14:14:06.681 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Sep 19 14:14:08.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 log redis-master-l84zn redis-master --namespace=kubectl-2867 --limit-bytes=1'
Sep 19 14:14:08.711: INFO: stderr: ""
Sep 19 14:14:08.711: INFO: stdout: " "
STEP: exposing timestamps
Sep 19 14:14:08.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 log redis-master-l84zn redis-master --namespace=kubectl-2867 --tail=1 --timestamps'
Sep 19 14:14:08.914: INFO: stderr: ""
Sep 19 14:14:08.914: INFO: stdout: "2019-09-19T14:14:06.682035169Z 1:M 19 Sep 14:14:06.681 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Sep 19 14:14:11.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 log redis-master-l84zn redis-master --namespace=kubectl-2867 --since=1s'
Sep 19 14:14:11.570: INFO: stderr: ""
Sep 19 14:14:11.570: INFO: stdout: ""
Sep 19 14:14:11.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 log redis-master-l84zn redis-master --namespace=kubectl-2867 --since=24h'
Sep 19 14:14:11.725: INFO: stderr: ""
Sep 19 14:14:11.725: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 19 Sep 14:14:06.681 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 19 Sep 14:14:06.681 # Server started, Redis version 3.2.12\n1:M 19 Sep 14:14:06.681 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 19 Sep 14:14:06.681 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Sep 19 14:14:11.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete --grace-period=0 --force -f - --namespace=kubectl-2867'
Sep 19 14:14:11.838: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 19 14:14:11.838: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Sep 19 14:14:11.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get rc,svc -l name=nginx --no-headers --namespace=kubectl-2867'
Sep 19 14:14:11.975: INFO: stderr: "No resources found.\n"
Sep 19 14:14:11.975: INFO: stdout: ""
Sep 19 14:14:11.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -l name=nginx --namespace=kubectl-2867 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 19 14:14:12.057: INFO: stderr: ""
Sep 19 14:14:12.057: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:14:12.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2867" for this suite.
Sep 19 14:14:18.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:14:18.576: INFO: namespace kubectl-2867 deletion completed in 6.50728596s

• [SLOW TEST:14.171 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:14:18.578: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7556
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep 19 14:14:18.792: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:14:22.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7556" for this suite.
Sep 19 14:14:28.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:14:28.447: INFO: namespace init-container-7556 deletion completed in 6.355374526s

• [SLOW TEST:9.869 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:14:28.448: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4033
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-5cd393a4-ad33-4768-a54d-ba01bba5ee71
STEP: Creating a pod to test consume configMaps
Sep 19 14:14:28.751: INFO: Waiting up to 5m0s for pod "pod-configmaps-70b3fb07-d4b2-4561-ae0c-ec537f2fe279" in namespace "configmap-4033" to be "success or failure"
Sep 19 14:14:28.769: INFO: Pod "pod-configmaps-70b3fb07-d4b2-4561-ae0c-ec537f2fe279": Phase="Pending", Reason="", readiness=false. Elapsed: 17.71266ms
Sep 19 14:14:30.780: INFO: Pod "pod-configmaps-70b3fb07-d4b2-4561-ae0c-ec537f2fe279": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029350706s
Sep 19 14:14:32.786: INFO: Pod "pod-configmaps-70b3fb07-d4b2-4561-ae0c-ec537f2fe279": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035443984s
STEP: Saw pod success
Sep 19 14:14:32.786: INFO: Pod "pod-configmaps-70b3fb07-d4b2-4561-ae0c-ec537f2fe279" satisfied condition "success or failure"
Sep 19 14:14:32.792: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod pod-configmaps-70b3fb07-d4b2-4561-ae0c-ec537f2fe279 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 19 14:14:32.836: INFO: Waiting for pod pod-configmaps-70b3fb07-d4b2-4561-ae0c-ec537f2fe279 to disappear
Sep 19 14:14:32.842: INFO: Pod pod-configmaps-70b3fb07-d4b2-4561-ae0c-ec537f2fe279 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:14:32.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4033" for this suite.
Sep 19 14:14:38.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:14:39.210: INFO: namespace configmap-4033 deletion completed in 6.359693281s

• [SLOW TEST:10.763 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:14:39.211: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5032
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 14:14:39.405: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76f0a948-ce23-40bf-9b3f-418495fcdfb2" in namespace "projected-5032" to be "success or failure"
Sep 19 14:14:39.419: INFO: Pod "downwardapi-volume-76f0a948-ce23-40bf-9b3f-418495fcdfb2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.140559ms
Sep 19 14:14:41.426: INFO: Pod "downwardapi-volume-76f0a948-ce23-40bf-9b3f-418495fcdfb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020813144s
Sep 19 14:14:43.433: INFO: Pod "downwardapi-volume-76f0a948-ce23-40bf-9b3f-418495fcdfb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027766119s
STEP: Saw pod success
Sep 19 14:14:43.433: INFO: Pod "downwardapi-volume-76f0a948-ce23-40bf-9b3f-418495fcdfb2" satisfied condition "success or failure"
Sep 19 14:14:43.439: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod downwardapi-volume-76f0a948-ce23-40bf-9b3f-418495fcdfb2 container client-container: <nil>
STEP: delete the pod
Sep 19 14:14:43.594: INFO: Waiting for pod downwardapi-volume-76f0a948-ce23-40bf-9b3f-418495fcdfb2 to disappear
Sep 19 14:14:43.599: INFO: Pod downwardapi-volume-76f0a948-ce23-40bf-9b3f-418495fcdfb2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:14:43.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5032" for this suite.
Sep 19 14:14:49.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:14:49.919: INFO: namespace projected-5032 deletion completed in 6.309803808s

• [SLOW TEST:10.709 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:14:49.921: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2030
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 19 14:14:50.114: INFO: Waiting up to 5m0s for pod "pod-48730e61-8a79-4b3d-b6e7-da226e3ec349" in namespace "emptydir-2030" to be "success or failure"
Sep 19 14:14:50.122: INFO: Pod "pod-48730e61-8a79-4b3d-b6e7-da226e3ec349": Phase="Pending", Reason="", readiness=false. Elapsed: 8.353043ms
Sep 19 14:14:52.129: INFO: Pod "pod-48730e61-8a79-4b3d-b6e7-da226e3ec349": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015272884s
Sep 19 14:14:54.136: INFO: Pod "pod-48730e61-8a79-4b3d-b6e7-da226e3ec349": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022208749s
Sep 19 14:14:56.143: INFO: Pod "pod-48730e61-8a79-4b3d-b6e7-da226e3ec349": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029132907s
Sep 19 14:14:58.150: INFO: Pod "pod-48730e61-8a79-4b3d-b6e7-da226e3ec349": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036232503s
Sep 19 14:15:00.159: INFO: Pod "pod-48730e61-8a79-4b3d-b6e7-da226e3ec349": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.045770807s
STEP: Saw pod success
Sep 19 14:15:00.159: INFO: Pod "pod-48730e61-8a79-4b3d-b6e7-da226e3ec349" satisfied condition "success or failure"
Sep 19 14:15:00.170: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod pod-48730e61-8a79-4b3d-b6e7-da226e3ec349 container test-container: <nil>
STEP: delete the pod
Sep 19 14:15:00.213: INFO: Waiting for pod pod-48730e61-8a79-4b3d-b6e7-da226e3ec349 to disappear
Sep 19 14:15:00.219: INFO: Pod pod-48730e61-8a79-4b3d-b6e7-da226e3ec349 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:15:00.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2030" for this suite.
Sep 19 14:15:06.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:15:06.552: INFO: namespace emptydir-2030 deletion completed in 6.324820378s

• [SLOW TEST:16.631 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:15:06.552: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 19 14:15:06.834: INFO: Number of nodes with available pods: 0
Sep 19 14:15:06.834: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:15:07.853: INFO: Number of nodes with available pods: 0
Sep 19 14:15:07.854: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:15:08.852: INFO: Number of nodes with available pods: 0
Sep 19 14:15:08.852: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:15:09.853: INFO: Number of nodes with available pods: 5
Sep 19 14:15:09.853: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 19 14:15:09.913: INFO: Number of nodes with available pods: 4
Sep 19 14:15:09.913: INFO: Node ip-172-31-42-91.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:15:10.946: INFO: Number of nodes with available pods: 4
Sep 19 14:15:10.946: INFO: Node ip-172-31-42-91.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:15:11.932: INFO: Number of nodes with available pods: 4
Sep 19 14:15:11.932: INFO: Node ip-172-31-42-91.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:15:12.940: INFO: Number of nodes with available pods: 4
Sep 19 14:15:12.940: INFO: Node ip-172-31-42-91.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:15:13.929: INFO: Number of nodes with available pods: 4
Sep 19 14:15:13.929: INFO: Node ip-172-31-42-91.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:15:14.929: INFO: Number of nodes with available pods: 4
Sep 19 14:15:14.929: INFO: Node ip-172-31-42-91.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:15:15.929: INFO: Number of nodes with available pods: 5
Sep 19 14:15:15.929: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5957, will wait for the garbage collector to delete the pods
Sep 19 14:15:16.015: INFO: Deleting DaemonSet.extensions daemon-set took: 22.706274ms
Sep 19 14:15:16.415: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.165101ms
Sep 19 14:15:29.322: INFO: Number of nodes with available pods: 0
Sep 19 14:15:29.322: INFO: Number of running nodes: 0, number of available pods: 0
Sep 19 14:15:29.329: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5957/daemonsets","resourceVersion":"35497"},"items":null}

Sep 19 14:15:29.335: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5957/pods","resourceVersion":"35497"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:15:29.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5957" for this suite.
Sep 19 14:15:35.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:15:35.778: INFO: namespace daemonsets-5957 deletion completed in 6.381071097s

• [SLOW TEST:29.226 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:15:35.779: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7683
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7683
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7683
STEP: Creating statefulset with conflicting port in namespace statefulset-7683
STEP: Waiting until pod test-pod will start running in namespace statefulset-7683
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7683
Sep 19 14:15:40.041: INFO: Observed stateful pod in namespace: statefulset-7683, name: ss-0, uid: a64779b8-da36-467d-8076-af552502c665, status phase: Failed. Waiting for statefulset controller to delete.
Sep 19 14:15:40.044: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7683
STEP: Removing pod with conflicting port in namespace statefulset-7683
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7683 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep 19 14:15:44.127: INFO: Deleting all statefulset in ns statefulset-7683
Sep 19 14:15:44.134: INFO: Scaling statefulset ss to 0
Sep 19 14:16:04.184: INFO: Waiting for statefulset status.replicas updated to 0
Sep 19 14:16:04.193: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:16:04.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7683" for this suite.
Sep 19 14:16:10.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:16:10.616: INFO: namespace statefulset-7683 deletion completed in 6.381787885s

• [SLOW TEST:34.837 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:16:10.617: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6337
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3043
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:16:38.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4974" for this suite.
Sep 19 14:16:44.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:16:44.548: INFO: namespace namespaces-4974 deletion completed in 6.337410628s
STEP: Destroying namespace "nsdeletetest-6337" for this suite.
Sep 19 14:16:44.557: INFO: Namespace nsdeletetest-6337 was already deleted
STEP: Destroying namespace "nsdeletetest-3043" for this suite.
Sep 19 14:16:50.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:16:50.952: INFO: namespace nsdeletetest-3043 deletion completed in 6.394693501s

• [SLOW TEST:40.334 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:16:50.952: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2448
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep 19 14:16:51.139: INFO: PodSpec: initContainers in spec.initContainers
Sep 19 14:17:33.846: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-0a9399e0-2925-4e9e-ad37-53cb4e164187", GenerateName:"", Namespace:"init-container-2448", SelfLink:"/api/v1/namespaces/init-container-2448/pods/pod-init-0a9399e0-2925-4e9e-ad37-53cb4e164187", UID:"20500b88-37fc-4603-a665-e05e6c1f0dcf", ResourceVersion:"36094", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63704499411, loc:(*time.Location)(0x7ec7a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"139183082"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.25.12.68/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-skbtl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001dd00c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-skbtl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-skbtl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-skbtl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000f30108), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-36-147.eu-central-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0026f4000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000f30180)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000f301a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000f301a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000f301ac), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499411, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499411, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499411, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499411, loc:(*time.Location)(0x7ec7a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.36.147", PodIP:"172.25.12.68", StartTime:(*v1.Time)(0xc002cba300), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001e680e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001e68150)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://621efe28fc64e897719c73b2eaeb3ff55bf6df342ff20c4bfc374462525c05a7"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002cba580), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002cba440), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:17:33.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2448" for this suite.
Sep 19 14:17:55.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:17:56.197: INFO: namespace init-container-2448 deletion completed in 22.330910954s

• [SLOW TEST:65.245 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:17:56.197: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-5faf377f-d028-4c34-a461-bb9d5948315f in namespace container-probe-9541
Sep 19 14:18:00.411: INFO: Started pod test-webserver-5faf377f-d028-4c34-a461-bb9d5948315f in namespace container-probe-9541
STEP: checking the pod's current state and verifying that restartCount is present
Sep 19 14:18:00.428: INFO: Initial restart count of pod test-webserver-5faf377f-d028-4c34-a461-bb9d5948315f is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:22:01.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9541" for this suite.
Sep 19 14:22:07.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:22:07.762: INFO: namespace container-probe-9541 deletion completed in 6.270282193s

• [SLOW TEST:251.565 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:22:07.763: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1623
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 14:22:07.929: INFO: Creating ReplicaSet my-hostname-basic-7e1b32e9-7607-4185-aa23-eb5281ab3104
Sep 19 14:22:07.943: INFO: Pod name my-hostname-basic-7e1b32e9-7607-4185-aa23-eb5281ab3104: Found 0 pods out of 1
Sep 19 14:22:12.949: INFO: Pod name my-hostname-basic-7e1b32e9-7607-4185-aa23-eb5281ab3104: Found 1 pods out of 1
Sep 19 14:22:12.949: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-7e1b32e9-7607-4185-aa23-eb5281ab3104" is running
Sep 19 14:22:12.957: INFO: Pod "my-hostname-basic-7e1b32e9-7607-4185-aa23-eb5281ab3104-bwvk8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-19 14:22:07 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-19 14:22:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-19 14:22:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-19 14:22:07 +0000 UTC Reason: Message:}])
Sep 19 14:22:12.957: INFO: Trying to dial the pod
Sep 19 14:22:18.090: INFO: Controller my-hostname-basic-7e1b32e9-7607-4185-aa23-eb5281ab3104: Got expected result from replica 1 [my-hostname-basic-7e1b32e9-7607-4185-aa23-eb5281ab3104-bwvk8]: "my-hostname-basic-7e1b32e9-7607-4185-aa23-eb5281ab3104-bwvk8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:22:18.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1623" for this suite.
Sep 19 14:22:24.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:22:24.401: INFO: namespace replicaset-1623 deletion completed in 6.298829856s

• [SLOW TEST:16.639 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:22:24.401: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-7280
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Sep 19 14:22:24.575: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Sep 19 14:22:25.121: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep 19 14:22:27.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 19 14:22:29.199: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 19 14:22:31.201: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 19 14:22:33.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704499745, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 19 14:22:37.382: INFO: Waited 2.173354269s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:22:38.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7280" for this suite.
Sep 19 14:22:44.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:22:44.499: INFO: namespace aggregator-7280 deletion completed in 6.254610839s

• [SLOW TEST:20.098 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:22:44.500: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5303
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 14:22:44.691: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba07c39c-1ae6-4cb2-b329-badcf72f78c6" in namespace "downward-api-5303" to be "success or failure"
Sep 19 14:22:44.699: INFO: Pod "downwardapi-volume-ba07c39c-1ae6-4cb2-b329-badcf72f78c6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.615508ms
Sep 19 14:22:46.706: INFO: Pod "downwardapi-volume-ba07c39c-1ae6-4cb2-b329-badcf72f78c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015548578s
Sep 19 14:22:48.713: INFO: Pod "downwardapi-volume-ba07c39c-1ae6-4cb2-b329-badcf72f78c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021835631s
STEP: Saw pod success
Sep 19 14:22:48.713: INFO: Pod "downwardapi-volume-ba07c39c-1ae6-4cb2-b329-badcf72f78c6" satisfied condition "success or failure"
Sep 19 14:22:48.718: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod downwardapi-volume-ba07c39c-1ae6-4cb2-b329-badcf72f78c6 container client-container: <nil>
STEP: delete the pod
Sep 19 14:22:48.759: INFO: Waiting for pod downwardapi-volume-ba07c39c-1ae6-4cb2-b329-badcf72f78c6 to disappear
Sep 19 14:22:48.764: INFO: Pod downwardapi-volume-ba07c39c-1ae6-4cb2-b329-badcf72f78c6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:22:48.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5303" for this suite.
Sep 19 14:22:54.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:22:55.082: INFO: namespace downward-api-5303 deletion completed in 6.310204042s

• [SLOW TEST:10.582 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:22:55.082: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3008
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Sep 19 14:22:55.275: INFO: Waiting up to 5m0s for pod "var-expansion-a47f0cf9-8b29-4790-89b0-211d9e77a0ec" in namespace "var-expansion-3008" to be "success or failure"
Sep 19 14:22:55.283: INFO: Pod "var-expansion-a47f0cf9-8b29-4790-89b0-211d9e77a0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.909405ms
Sep 19 14:22:57.354: INFO: Pod "var-expansion-a47f0cf9-8b29-4790-89b0-211d9e77a0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078403848s
Sep 19 14:22:59.365: INFO: Pod "var-expansion-a47f0cf9-8b29-4790-89b0-211d9e77a0ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.088974795s
STEP: Saw pod success
Sep 19 14:22:59.365: INFO: Pod "var-expansion-a47f0cf9-8b29-4790-89b0-211d9e77a0ec" satisfied condition "success or failure"
Sep 19 14:22:59.383: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod var-expansion-a47f0cf9-8b29-4790-89b0-211d9e77a0ec container dapi-container: <nil>
STEP: delete the pod
Sep 19 14:22:59.462: INFO: Waiting for pod var-expansion-a47f0cf9-8b29-4790-89b0-211d9e77a0ec to disappear
Sep 19 14:22:59.481: INFO: Pod var-expansion-a47f0cf9-8b29-4790-89b0-211d9e77a0ec no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:22:59.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3008" for this suite.
Sep 19 14:23:05.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:23:06.188: INFO: namespace var-expansion-3008 deletion completed in 6.637234914s

• [SLOW TEST:11.106 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:23:06.188: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5961
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-5961
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5961 to expose endpoints map[]
Sep 19 14:23:06.392: INFO: Get endpoints failed (6.16636ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Sep 19 14:23:07.400: INFO: successfully validated that service endpoint-test2 in namespace services-5961 exposes endpoints map[] (1.014741843s elapsed)
STEP: Creating pod pod1 in namespace services-5961
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5961 to expose endpoints map[pod1:[80]]
Sep 19 14:23:10.478: INFO: successfully validated that service endpoint-test2 in namespace services-5961 exposes endpoints map[pod1:[80]] (3.064022698s elapsed)
STEP: Creating pod pod2 in namespace services-5961
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5961 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 19 14:23:13.730: INFO: successfully validated that service endpoint-test2 in namespace services-5961 exposes endpoints map[pod1:[80] pod2:[80]] (3.227940493s elapsed)
STEP: Deleting pod pod1 in namespace services-5961
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5961 to expose endpoints map[pod2:[80]]
Sep 19 14:23:14.778: INFO: successfully validated that service endpoint-test2 in namespace services-5961 exposes endpoints map[pod2:[80]] (1.038434227s elapsed)
STEP: Deleting pod pod2 in namespace services-5961
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5961 to expose endpoints map[]
Sep 19 14:23:14.796: INFO: successfully validated that service endpoint-test2 in namespace services-5961 exposes endpoints map[] (4.490929ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:23:14.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5961" for this suite.
Sep 19 14:23:36.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:23:37.320: INFO: namespace services-5961 deletion completed in 22.479763349s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:31.132 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:23:37.320: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6678
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 14:23:37.575: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ea614296-df2b-4d0a-aae2-0e575387004e" in namespace "downward-api-6678" to be "success or failure"
Sep 19 14:23:37.583: INFO: Pod "downwardapi-volume-ea614296-df2b-4d0a-aae2-0e575387004e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.85583ms
Sep 19 14:23:39.589: INFO: Pod "downwardapi-volume-ea614296-df2b-4d0a-aae2-0e575387004e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013779943s
Sep 19 14:23:41.596: INFO: Pod "downwardapi-volume-ea614296-df2b-4d0a-aae2-0e575387004e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021004542s
STEP: Saw pod success
Sep 19 14:23:41.596: INFO: Pod "downwardapi-volume-ea614296-df2b-4d0a-aae2-0e575387004e" satisfied condition "success or failure"
Sep 19 14:23:41.602: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod downwardapi-volume-ea614296-df2b-4d0a-aae2-0e575387004e container client-container: <nil>
STEP: delete the pod
Sep 19 14:23:41.638: INFO: Waiting for pod downwardapi-volume-ea614296-df2b-4d0a-aae2-0e575387004e to disappear
Sep 19 14:23:41.643: INFO: Pod downwardapi-volume-ea614296-df2b-4d0a-aae2-0e575387004e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:23:41.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6678" for this suite.
Sep 19 14:23:47.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:23:48.006: INFO: namespace downward-api-6678 deletion completed in 6.352829467s

• [SLOW TEST:10.686 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:23:48.007: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1214
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-c67f5b08-b8b1-46cd-899c-301713b14e54
STEP: Creating a pod to test consume secrets
Sep 19 14:23:48.283: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d60cd4d9-0ad1-4c59-a37f-ddb68977b848" in namespace "projected-1214" to be "success or failure"
Sep 19 14:23:48.352: INFO: Pod "pod-projected-secrets-d60cd4d9-0ad1-4c59-a37f-ddb68977b848": Phase="Pending", Reason="", readiness=false. Elapsed: 69.2956ms
Sep 19 14:23:50.365: INFO: Pod "pod-projected-secrets-d60cd4d9-0ad1-4c59-a37f-ddb68977b848": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082207819s
Sep 19 14:23:52.375: INFO: Pod "pod-projected-secrets-d60cd4d9-0ad1-4c59-a37f-ddb68977b848": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.092496799s
STEP: Saw pod success
Sep 19 14:23:52.375: INFO: Pod "pod-projected-secrets-d60cd4d9-0ad1-4c59-a37f-ddb68977b848" satisfied condition "success or failure"
Sep 19 14:23:52.382: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod pod-projected-secrets-d60cd4d9-0ad1-4c59-a37f-ddb68977b848 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 19 14:23:52.566: INFO: Waiting for pod pod-projected-secrets-d60cd4d9-0ad1-4c59-a37f-ddb68977b848 to disappear
Sep 19 14:23:52.579: INFO: Pod pod-projected-secrets-d60cd4d9-0ad1-4c59-a37f-ddb68977b848 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:23:52.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1214" for this suite.
Sep 19 14:23:58.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:23:59.108: INFO: namespace projected-1214 deletion completed in 6.51603786s

• [SLOW TEST:11.102 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:23:59.109: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9614
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Sep 19 14:24:39.388: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0919 14:24:39.388662      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:24:39.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9614" for this suite.
Sep 19 14:24:47.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:24:47.960: INFO: namespace gc-9614 deletion completed in 8.557257426s

• [SLOW TEST:48.852 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:24:47.961: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 14:24:48.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a03fe7d4-dd08-468b-a1da-a86d3c181537" in namespace "downward-api-1271" to be "success or failure"
Sep 19 14:24:48.193: INFO: Pod "downwardapi-volume-a03fe7d4-dd08-468b-a1da-a86d3c181537": Phase="Pending", Reason="", readiness=false. Elapsed: 13.307349ms
Sep 19 14:24:50.199: INFO: Pod "downwardapi-volume-a03fe7d4-dd08-468b-a1da-a86d3c181537": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019640009s
Sep 19 14:24:52.257: INFO: Pod "downwardapi-volume-a03fe7d4-dd08-468b-a1da-a86d3c181537": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07737654s
STEP: Saw pod success
Sep 19 14:24:52.257: INFO: Pod "downwardapi-volume-a03fe7d4-dd08-468b-a1da-a86d3c181537" satisfied condition "success or failure"
Sep 19 14:24:52.266: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod downwardapi-volume-a03fe7d4-dd08-468b-a1da-a86d3c181537 container client-container: <nil>
STEP: delete the pod
Sep 19 14:24:52.304: INFO: Waiting for pod downwardapi-volume-a03fe7d4-dd08-468b-a1da-a86d3c181537 to disappear
Sep 19 14:24:52.309: INFO: Pod downwardapi-volume-a03fe7d4-dd08-468b-a1da-a86d3c181537 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:24:52.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1271" for this suite.
Sep 19 14:24:58.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:24:58.837: INFO: namespace downward-api-1271 deletion completed in 6.478090295s

• [SLOW TEST:10.876 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:24:58.837: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8565
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-d3a12442-2f00-49bb-b608-642aff2dd9a1
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-d3a12442-2f00-49bb-b608-642aff2dd9a1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:26:21.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8565" for this suite.
Sep 19 14:26:45.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:26:45.779: INFO: namespace projected-8565 deletion completed in 24.537799879s

• [SLOW TEST:106.942 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:26:45.780: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5075
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 14:26:46.018: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2bc6cd62-cfe5-4e75-a58b-87b8d6183001" in namespace "downward-api-5075" to be "success or failure"
Sep 19 14:26:46.033: INFO: Pod "downwardapi-volume-2bc6cd62-cfe5-4e75-a58b-87b8d6183001": Phase="Pending", Reason="", readiness=false. Elapsed: 14.801557ms
Sep 19 14:26:48.040: INFO: Pod "downwardapi-volume-2bc6cd62-cfe5-4e75-a58b-87b8d6183001": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021904382s
Sep 19 14:26:50.068: INFO: Pod "downwardapi-volume-2bc6cd62-cfe5-4e75-a58b-87b8d6183001": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049439963s
STEP: Saw pod success
Sep 19 14:26:50.068: INFO: Pod "downwardapi-volume-2bc6cd62-cfe5-4e75-a58b-87b8d6183001" satisfied condition "success or failure"
Sep 19 14:26:50.079: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod downwardapi-volume-2bc6cd62-cfe5-4e75-a58b-87b8d6183001 container client-container: <nil>
STEP: delete the pod
Sep 19 14:26:50.320: INFO: Waiting for pod downwardapi-volume-2bc6cd62-cfe5-4e75-a58b-87b8d6183001 to disappear
Sep 19 14:26:50.333: INFO: Pod downwardapi-volume-2bc6cd62-cfe5-4e75-a58b-87b8d6183001 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:26:50.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5075" for this suite.
Sep 19 14:26:56.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:26:56.741: INFO: namespace downward-api-5075 deletion completed in 6.400065654s

• [SLOW TEST:10.961 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:26:56.741: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3615
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 19 14:27:01.558: INFO: Successfully updated pod "pod-update-e78d3548-7f37-40d8-89f8-e00ade8e4c92"
STEP: verifying the updated pod is in kubernetes
Sep 19 14:27:01.576: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:27:01.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3615" for this suite.
Sep 19 14:27:25.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:27:25.883: INFO: namespace pods-3615 deletion completed in 24.295624622s

• [SLOW TEST:29.142 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:27:25.883: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-3092
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3092
I0919 14:27:26.055907      15 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3092, replica count: 1
I0919 14:27:27.106297      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0919 14:27:28.106449      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0919 14:27:29.106648      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 19 14:27:29.222: INFO: Created: latency-svc-5jtw8
Sep 19 14:27:29.229: INFO: Got endpoints: latency-svc-5jtw8 [22.935524ms]
Sep 19 14:27:29.243: INFO: Created: latency-svc-t2b8s
Sep 19 14:27:29.251: INFO: Created: latency-svc-mbv46
Sep 19 14:27:29.251: INFO: Got endpoints: latency-svc-t2b8s [21.924905ms]
Sep 19 14:27:29.260: INFO: Got endpoints: latency-svc-mbv46 [30.5646ms]
Sep 19 14:27:29.264: INFO: Created: latency-svc-dbf9g
Sep 19 14:27:29.271: INFO: Got endpoints: latency-svc-dbf9g [41.304437ms]
Sep 19 14:27:29.277: INFO: Created: latency-svc-nqgjb
Sep 19 14:27:29.282: INFO: Got endpoints: latency-svc-nqgjb [52.150312ms]
Sep 19 14:27:29.283: INFO: Created: latency-svc-nxtgz
Sep 19 14:27:29.295: INFO: Got endpoints: latency-svc-nxtgz [65.156448ms]
Sep 19 14:27:29.297: INFO: Created: latency-svc-zz8br
Sep 19 14:27:29.305: INFO: Got endpoints: latency-svc-zz8br [74.95431ms]
Sep 19 14:27:29.305: INFO: Created: latency-svc-fjrdd
Sep 19 14:27:29.308: INFO: Got endpoints: latency-svc-fjrdd [78.154385ms]
Sep 19 14:27:29.319: INFO: Created: latency-svc-fskct
Sep 19 14:27:29.322: INFO: Got endpoints: latency-svc-fskct [92.15329ms]
Sep 19 14:27:29.328: INFO: Created: latency-svc-kx56s
Sep 19 14:27:29.335: INFO: Created: latency-svc-vjnkj
Sep 19 14:27:29.337: INFO: Got endpoints: latency-svc-kx56s [107.27187ms]
Sep 19 14:27:29.343: INFO: Got endpoints: latency-svc-vjnkj [112.933925ms]
Sep 19 14:27:29.346: INFO: Created: latency-svc-b5tx7
Sep 19 14:27:29.353: INFO: Got endpoints: latency-svc-b5tx7 [122.88572ms]
Sep 19 14:27:29.355: INFO: Created: latency-svc-wd5cz
Sep 19 14:27:29.361: INFO: Got endpoints: latency-svc-wd5cz [131.142488ms]
Sep 19 14:27:29.364: INFO: Created: latency-svc-5qlzc
Sep 19 14:27:29.375: INFO: Got endpoints: latency-svc-5qlzc [145.223884ms]
Sep 19 14:27:29.375: INFO: Created: latency-svc-8wbzk
Sep 19 14:27:29.391: INFO: Got endpoints: latency-svc-8wbzk [161.11891ms]
Sep 19 14:27:29.392: INFO: Created: latency-svc-kvrgc
Sep 19 14:27:29.404: INFO: Got endpoints: latency-svc-kvrgc [173.801808ms]
Sep 19 14:27:29.405: INFO: Created: latency-svc-mp6b8
Sep 19 14:27:29.423: INFO: Created: latency-svc-prwqf
Sep 19 14:27:29.423: INFO: Got endpoints: latency-svc-mp6b8 [171.677534ms]
Sep 19 14:27:29.424: INFO: Got endpoints: latency-svc-prwqf [163.653239ms]
Sep 19 14:27:29.428: INFO: Created: latency-svc-m4tvw
Sep 19 14:27:29.434: INFO: Got endpoints: latency-svc-m4tvw [163.317666ms]
Sep 19 14:27:29.438: INFO: Created: latency-svc-qt4bd
Sep 19 14:27:29.445: INFO: Got endpoints: latency-svc-qt4bd [163.113794ms]
Sep 19 14:27:29.447: INFO: Created: latency-svc-568s5
Sep 19 14:27:29.462: INFO: Got endpoints: latency-svc-568s5 [167.025501ms]
Sep 19 14:27:29.467: INFO: Created: latency-svc-sq7lz
Sep 19 14:27:29.476: INFO: Created: latency-svc-dgp95
Sep 19 14:27:29.476: INFO: Got endpoints: latency-svc-sq7lz [171.406687ms]
Sep 19 14:27:29.480: INFO: Created: latency-svc-rns82
Sep 19 14:27:29.484: INFO: Got endpoints: latency-svc-dgp95 [175.669731ms]
Sep 19 14:27:29.487: INFO: Got endpoints: latency-svc-rns82 [164.959717ms]
Sep 19 14:27:29.490: INFO: Created: latency-svc-t8qgw
Sep 19 14:27:29.495: INFO: Got endpoints: latency-svc-t8qgw [158.015856ms]
Sep 19 14:27:29.503: INFO: Created: latency-svc-hf4t6
Sep 19 14:27:29.505: INFO: Got endpoints: latency-svc-hf4t6 [162.311125ms]
Sep 19 14:27:29.507: INFO: Created: latency-svc-j4lv9
Sep 19 14:27:29.512: INFO: Got endpoints: latency-svc-j4lv9 [159.402981ms]
Sep 19 14:27:29.514: INFO: Created: latency-svc-rvbzt
Sep 19 14:27:29.521: INFO: Got endpoints: latency-svc-rvbzt [159.449412ms]
Sep 19 14:27:29.521: INFO: Created: latency-svc-g2qdq
Sep 19 14:27:29.529: INFO: Got endpoints: latency-svc-g2qdq [153.707169ms]
Sep 19 14:27:29.531: INFO: Created: latency-svc-2tr8z
Sep 19 14:27:29.538: INFO: Got endpoints: latency-svc-2tr8z [146.473501ms]
Sep 19 14:27:29.542: INFO: Created: latency-svc-9fsc4
Sep 19 14:27:29.549: INFO: Got endpoints: latency-svc-9fsc4 [145.098774ms]
Sep 19 14:27:29.551: INFO: Created: latency-svc-mng2c
Sep 19 14:27:29.556: INFO: Got endpoints: latency-svc-mng2c [132.91656ms]
Sep 19 14:27:29.560: INFO: Created: latency-svc-2k5kt
Sep 19 14:27:29.569: INFO: Got endpoints: latency-svc-2k5kt [144.934854ms]
Sep 19 14:27:29.571: INFO: Created: latency-svc-fqjpl
Sep 19 14:27:29.578: INFO: Got endpoints: latency-svc-fqjpl [143.193275ms]
Sep 19 14:27:29.584: INFO: Created: latency-svc-qx47x
Sep 19 14:27:29.592: INFO: Got endpoints: latency-svc-qx47x [146.401236ms]
Sep 19 14:27:29.592: INFO: Created: latency-svc-nh5zx
Sep 19 14:27:29.601: INFO: Got endpoints: latency-svc-nh5zx [138.719593ms]
Sep 19 14:27:29.602: INFO: Created: latency-svc-gr7pl
Sep 19 14:27:29.607: INFO: Got endpoints: latency-svc-gr7pl [130.842641ms]
Sep 19 14:27:29.610: INFO: Created: latency-svc-lbbxl
Sep 19 14:27:29.618: INFO: Created: latency-svc-2dvd7
Sep 19 14:27:29.623: INFO: Created: latency-svc-9qvkl
Sep 19 14:27:29.633: INFO: Got endpoints: latency-svc-lbbxl [149.654958ms]
Sep 19 14:27:29.638: INFO: Created: latency-svc-wx9j2
Sep 19 14:27:29.648: INFO: Created: latency-svc-m8blx
Sep 19 14:27:29.662: INFO: Created: latency-svc-v7q94
Sep 19 14:27:29.674: INFO: Created: latency-svc-jcd2c
Sep 19 14:27:29.680: INFO: Got endpoints: latency-svc-2dvd7 [193.238553ms]
Sep 19 14:27:29.686: INFO: Created: latency-svc-5lv7l
Sep 19 14:27:29.689: INFO: Created: latency-svc-jthhn
Sep 19 14:27:29.698: INFO: Created: latency-svc-xln2c
Sep 19 14:27:29.710: INFO: Created: latency-svc-rlzsq
Sep 19 14:27:29.714: INFO: Created: latency-svc-ms59v
Sep 19 14:27:29.722: INFO: Created: latency-svc-hqfpz
Sep 19 14:27:29.728: INFO: Got endpoints: latency-svc-9qvkl [232.588471ms]
Sep 19 14:27:29.729: INFO: Created: latency-svc-lqlhr
Sep 19 14:27:29.755: INFO: Created: latency-svc-jtnbp
Sep 19 14:27:29.764: INFO: Created: latency-svc-hcdph
Sep 19 14:27:29.771: INFO: Created: latency-svc-9b5x6
Sep 19 14:27:29.781: INFO: Got endpoints: latency-svc-wx9j2 [275.183121ms]
Sep 19 14:27:29.787: INFO: Created: latency-svc-9wh9r
Sep 19 14:27:29.795: INFO: Created: latency-svc-tzl4r
Sep 19 14:27:29.830: INFO: Got endpoints: latency-svc-m8blx [318.048633ms]
Sep 19 14:27:29.849: INFO: Created: latency-svc-hfqjk
Sep 19 14:27:29.880: INFO: Got endpoints: latency-svc-v7q94 [359.870947ms]
Sep 19 14:27:29.895: INFO: Created: latency-svc-ggp4x
Sep 19 14:27:29.931: INFO: Got endpoints: latency-svc-jcd2c [402.03327ms]
Sep 19 14:27:29.948: INFO: Created: latency-svc-cqbqv
Sep 19 14:27:29.981: INFO: Got endpoints: latency-svc-5lv7l [443.44384ms]
Sep 19 14:27:29.998: INFO: Created: latency-svc-bzhf6
Sep 19 14:27:30.031: INFO: Got endpoints: latency-svc-jthhn [482.002732ms]
Sep 19 14:27:30.044: INFO: Created: latency-svc-gfmrz
Sep 19 14:27:30.080: INFO: Got endpoints: latency-svc-xln2c [523.562523ms]
Sep 19 14:27:30.097: INFO: Created: latency-svc-gcvlq
Sep 19 14:27:30.129: INFO: Got endpoints: latency-svc-rlzsq [560.467804ms]
Sep 19 14:27:30.145: INFO: Created: latency-svc-cxbpj
Sep 19 14:27:30.180: INFO: Got endpoints: latency-svc-ms59v [602.403033ms]
Sep 19 14:27:30.195: INFO: Created: latency-svc-9fc6t
Sep 19 14:27:30.233: INFO: Got endpoints: latency-svc-hqfpz [641.442073ms]
Sep 19 14:27:30.247: INFO: Created: latency-svc-mh64z
Sep 19 14:27:30.279: INFO: Got endpoints: latency-svc-lqlhr [677.723378ms]
Sep 19 14:27:30.301: INFO: Created: latency-svc-jfw7v
Sep 19 14:27:30.331: INFO: Got endpoints: latency-svc-jtnbp [723.595368ms]
Sep 19 14:27:30.347: INFO: Created: latency-svc-ctcrw
Sep 19 14:27:30.379: INFO: Got endpoints: latency-svc-hcdph [745.876111ms]
Sep 19 14:27:30.408: INFO: Created: latency-svc-qtbsz
Sep 19 14:27:30.430: INFO: Got endpoints: latency-svc-9b5x6 [749.901658ms]
Sep 19 14:27:30.446: INFO: Created: latency-svc-9s7sn
Sep 19 14:27:30.481: INFO: Got endpoints: latency-svc-9wh9r [753.337486ms]
Sep 19 14:27:30.499: INFO: Created: latency-svc-dwd4m
Sep 19 14:27:30.533: INFO: Got endpoints: latency-svc-tzl4r [752.861932ms]
Sep 19 14:27:30.555: INFO: Created: latency-svc-28jzr
Sep 19 14:27:30.594: INFO: Got endpoints: latency-svc-hfqjk [763.474545ms]
Sep 19 14:27:30.624: INFO: Created: latency-svc-xwjz9
Sep 19 14:27:30.631: INFO: Got endpoints: latency-svc-ggp4x [750.295538ms]
Sep 19 14:27:30.645: INFO: Created: latency-svc-pxxsb
Sep 19 14:27:30.679: INFO: Got endpoints: latency-svc-cqbqv [747.950106ms]
Sep 19 14:27:30.695: INFO: Created: latency-svc-rpm6h
Sep 19 14:27:30.732: INFO: Got endpoints: latency-svc-bzhf6 [750.889259ms]
Sep 19 14:27:30.746: INFO: Created: latency-svc-8v66w
Sep 19 14:27:30.781: INFO: Got endpoints: latency-svc-gfmrz [749.696286ms]
Sep 19 14:27:30.793: INFO: Created: latency-svc-fsd9s
Sep 19 14:27:30.832: INFO: Got endpoints: latency-svc-gcvlq [752.792115ms]
Sep 19 14:27:30.855: INFO: Created: latency-svc-rpfnj
Sep 19 14:27:30.880: INFO: Got endpoints: latency-svc-cxbpj [750.800097ms]
Sep 19 14:27:30.897: INFO: Created: latency-svc-6flf2
Sep 19 14:27:30.930: INFO: Got endpoints: latency-svc-9fc6t [749.465648ms]
Sep 19 14:27:30.946: INFO: Created: latency-svc-hq7fz
Sep 19 14:27:30.980: INFO: Got endpoints: latency-svc-mh64z [747.226279ms]
Sep 19 14:27:30.995: INFO: Created: latency-svc-xblq6
Sep 19 14:27:31.031: INFO: Got endpoints: latency-svc-jfw7v [752.657128ms]
Sep 19 14:27:31.047: INFO: Created: latency-svc-82csg
Sep 19 14:27:31.081: INFO: Got endpoints: latency-svc-ctcrw [749.42324ms]
Sep 19 14:27:31.110: INFO: Created: latency-svc-vcmt6
Sep 19 14:27:31.131: INFO: Got endpoints: latency-svc-qtbsz [752.11052ms]
Sep 19 14:27:31.149: INFO: Created: latency-svc-44n6w
Sep 19 14:27:31.178: INFO: Got endpoints: latency-svc-9s7sn [747.943059ms]
Sep 19 14:27:31.191: INFO: Created: latency-svc-n2thq
Sep 19 14:27:31.230: INFO: Got endpoints: latency-svc-dwd4m [748.36262ms]
Sep 19 14:27:31.250: INFO: Created: latency-svc-8h8xl
Sep 19 14:27:31.281: INFO: Got endpoints: latency-svc-28jzr [747.678144ms]
Sep 19 14:27:31.293: INFO: Created: latency-svc-2d7lw
Sep 19 14:27:31.330: INFO: Got endpoints: latency-svc-xwjz9 [735.762699ms]
Sep 19 14:27:31.346: INFO: Created: latency-svc-d4zb5
Sep 19 14:27:31.378: INFO: Got endpoints: latency-svc-pxxsb [747.394445ms]
Sep 19 14:27:31.402: INFO: Created: latency-svc-xj5ln
Sep 19 14:27:31.430: INFO: Got endpoints: latency-svc-rpm6h [750.711425ms]
Sep 19 14:27:31.449: INFO: Created: latency-svc-w5dfp
Sep 19 14:27:31.481: INFO: Got endpoints: latency-svc-8v66w [749.113781ms]
Sep 19 14:27:31.495: INFO: Created: latency-svc-j8njx
Sep 19 14:27:31.529: INFO: Got endpoints: latency-svc-fsd9s [748.597192ms]
Sep 19 14:27:31.545: INFO: Created: latency-svc-ktjqw
Sep 19 14:27:31.579: INFO: Got endpoints: latency-svc-rpfnj [746.82368ms]
Sep 19 14:27:31.593: INFO: Created: latency-svc-x4gz2
Sep 19 14:27:31.630: INFO: Got endpoints: latency-svc-6flf2 [749.537773ms]
Sep 19 14:27:31.658: INFO: Created: latency-svc-7xjmm
Sep 19 14:27:31.686: INFO: Got endpoints: latency-svc-hq7fz [756.132249ms]
Sep 19 14:27:31.700: INFO: Created: latency-svc-j6h6j
Sep 19 14:27:31.730: INFO: Got endpoints: latency-svc-xblq6 [749.550971ms]
Sep 19 14:27:31.750: INFO: Created: latency-svc-dwkbm
Sep 19 14:27:31.782: INFO: Got endpoints: latency-svc-82csg [750.536642ms]
Sep 19 14:27:31.795: INFO: Created: latency-svc-k7zdm
Sep 19 14:27:31.830: INFO: Got endpoints: latency-svc-vcmt6 [749.00251ms]
Sep 19 14:27:31.844: INFO: Created: latency-svc-2tkqf
Sep 19 14:27:31.882: INFO: Got endpoints: latency-svc-44n6w [751.081867ms]
Sep 19 14:27:31.898: INFO: Created: latency-svc-fnx8d
Sep 19 14:27:31.931: INFO: Got endpoints: latency-svc-n2thq [752.483893ms]
Sep 19 14:27:31.947: INFO: Created: latency-svc-ljr5p
Sep 19 14:27:31.979: INFO: Got endpoints: latency-svc-8h8xl [749.700123ms]
Sep 19 14:27:31.994: INFO: Created: latency-svc-xszrf
Sep 19 14:27:32.031: INFO: Got endpoints: latency-svc-2d7lw [749.413181ms]
Sep 19 14:27:32.043: INFO: Created: latency-svc-bj995
Sep 19 14:27:32.079: INFO: Got endpoints: latency-svc-d4zb5 [749.562275ms]
Sep 19 14:27:32.094: INFO: Created: latency-svc-pfnwd
Sep 19 14:27:32.129: INFO: Got endpoints: latency-svc-xj5ln [750.872479ms]
Sep 19 14:27:32.145: INFO: Created: latency-svc-rtqw8
Sep 19 14:27:32.183: INFO: Got endpoints: latency-svc-w5dfp [753.405994ms]
Sep 19 14:27:32.205: INFO: Created: latency-svc-mhd52
Sep 19 14:27:32.232: INFO: Got endpoints: latency-svc-j8njx [750.598637ms]
Sep 19 14:27:32.244: INFO: Created: latency-svc-jq9x9
Sep 19 14:27:32.283: INFO: Got endpoints: latency-svc-ktjqw [753.510677ms]
Sep 19 14:27:32.299: INFO: Created: latency-svc-b4chd
Sep 19 14:27:32.331: INFO: Got endpoints: latency-svc-x4gz2 [751.337778ms]
Sep 19 14:27:32.347: INFO: Created: latency-svc-q8922
Sep 19 14:27:32.380: INFO: Got endpoints: latency-svc-7xjmm [750.298487ms]
Sep 19 14:27:32.397: INFO: Created: latency-svc-ttrrs
Sep 19 14:27:32.432: INFO: Got endpoints: latency-svc-j6h6j [746.007269ms]
Sep 19 14:27:32.448: INFO: Created: latency-svc-2mqz7
Sep 19 14:27:32.480: INFO: Got endpoints: latency-svc-dwkbm [750.07426ms]
Sep 19 14:27:32.495: INFO: Created: latency-svc-sh6pk
Sep 19 14:27:32.528: INFO: Got endpoints: latency-svc-k7zdm [746.32785ms]
Sep 19 14:27:32.543: INFO: Created: latency-svc-b9r7l
Sep 19 14:27:32.582: INFO: Got endpoints: latency-svc-2tkqf [751.973108ms]
Sep 19 14:27:32.601: INFO: Created: latency-svc-sp7xc
Sep 19 14:27:32.632: INFO: Got endpoints: latency-svc-fnx8d [749.754695ms]
Sep 19 14:27:32.648: INFO: Created: latency-svc-wcczt
Sep 19 14:27:32.680: INFO: Got endpoints: latency-svc-ljr5p [749.164384ms]
Sep 19 14:27:32.699: INFO: Created: latency-svc-l528z
Sep 19 14:27:32.730: INFO: Got endpoints: latency-svc-xszrf [750.305338ms]
Sep 19 14:27:32.743: INFO: Created: latency-svc-hxsgz
Sep 19 14:27:32.779: INFO: Got endpoints: latency-svc-bj995 [748.79405ms]
Sep 19 14:27:32.798: INFO: Created: latency-svc-l6qpc
Sep 19 14:27:32.832: INFO: Got endpoints: latency-svc-pfnwd [752.787865ms]
Sep 19 14:27:32.848: INFO: Created: latency-svc-swwl4
Sep 19 14:27:32.880: INFO: Got endpoints: latency-svc-rtqw8 [749.477342ms]
Sep 19 14:27:32.896: INFO: Created: latency-svc-mdvjg
Sep 19 14:27:32.930: INFO: Got endpoints: latency-svc-mhd52 [746.355353ms]
Sep 19 14:27:32.942: INFO: Created: latency-svc-vt5x4
Sep 19 14:27:32.979: INFO: Got endpoints: latency-svc-jq9x9 [747.41256ms]
Sep 19 14:27:32.994: INFO: Created: latency-svc-dcv4c
Sep 19 14:27:33.030: INFO: Got endpoints: latency-svc-b4chd [747.286579ms]
Sep 19 14:27:33.045: INFO: Created: latency-svc-kg9fx
Sep 19 14:27:33.081: INFO: Got endpoints: latency-svc-q8922 [750.421433ms]
Sep 19 14:27:33.097: INFO: Created: latency-svc-xmz79
Sep 19 14:27:33.131: INFO: Got endpoints: latency-svc-ttrrs [750.812577ms]
Sep 19 14:27:33.144: INFO: Created: latency-svc-zj7ws
Sep 19 14:27:33.181: INFO: Got endpoints: latency-svc-2mqz7 [748.797189ms]
Sep 19 14:27:33.198: INFO: Created: latency-svc-g676z
Sep 19 14:27:33.236: INFO: Got endpoints: latency-svc-sh6pk [756.253619ms]
Sep 19 14:27:33.268: INFO: Created: latency-svc-r26wn
Sep 19 14:27:33.281: INFO: Got endpoints: latency-svc-b9r7l [752.642798ms]
Sep 19 14:27:33.295: INFO: Created: latency-svc-cst9v
Sep 19 14:27:33.331: INFO: Got endpoints: latency-svc-sp7xc [748.999097ms]
Sep 19 14:27:33.345: INFO: Created: latency-svc-tvm65
Sep 19 14:27:33.380: INFO: Got endpoints: latency-svc-wcczt [747.314733ms]
Sep 19 14:27:33.399: INFO: Created: latency-svc-wb28b
Sep 19 14:27:33.430: INFO: Got endpoints: latency-svc-l528z [750.161612ms]
Sep 19 14:27:33.446: INFO: Created: latency-svc-p79wv
Sep 19 14:27:33.480: INFO: Got endpoints: latency-svc-hxsgz [750.015279ms]
Sep 19 14:27:33.494: INFO: Created: latency-svc-g6bhb
Sep 19 14:27:33.531: INFO: Got endpoints: latency-svc-l6qpc [751.182575ms]
Sep 19 14:27:33.547: INFO: Created: latency-svc-9btgk
Sep 19 14:27:33.580: INFO: Got endpoints: latency-svc-swwl4 [747.791526ms]
Sep 19 14:27:33.594: INFO: Created: latency-svc-5slc8
Sep 19 14:27:33.630: INFO: Got endpoints: latency-svc-mdvjg [750.422036ms]
Sep 19 14:27:33.645: INFO: Created: latency-svc-bv5zg
Sep 19 14:27:33.680: INFO: Got endpoints: latency-svc-vt5x4 [749.924731ms]
Sep 19 14:27:33.693: INFO: Created: latency-svc-8b558
Sep 19 14:27:33.730: INFO: Got endpoints: latency-svc-dcv4c [750.272522ms]
Sep 19 14:27:33.746: INFO: Created: latency-svc-245rk
Sep 19 14:27:33.781: INFO: Got endpoints: latency-svc-kg9fx [750.582897ms]
Sep 19 14:27:33.795: INFO: Created: latency-svc-wrhrp
Sep 19 14:27:33.830: INFO: Got endpoints: latency-svc-xmz79 [748.451416ms]
Sep 19 14:27:33.843: INFO: Created: latency-svc-n26dn
Sep 19 14:27:33.881: INFO: Got endpoints: latency-svc-zj7ws [750.446003ms]
Sep 19 14:27:33.896: INFO: Created: latency-svc-r46b6
Sep 19 14:27:33.932: INFO: Got endpoints: latency-svc-g676z [750.960214ms]
Sep 19 14:27:33.948: INFO: Created: latency-svc-rsvsz
Sep 19 14:27:33.983: INFO: Got endpoints: latency-svc-r26wn [746.259076ms]
Sep 19 14:27:33.996: INFO: Created: latency-svc-rkfrl
Sep 19 14:27:34.029: INFO: Got endpoints: latency-svc-cst9v [747.738136ms]
Sep 19 14:27:34.043: INFO: Created: latency-svc-cz4cr
Sep 19 14:27:34.080: INFO: Got endpoints: latency-svc-tvm65 [749.565438ms]
Sep 19 14:27:34.094: INFO: Created: latency-svc-5zgdx
Sep 19 14:27:34.131: INFO: Got endpoints: latency-svc-wb28b [751.73912ms]
Sep 19 14:27:34.145: INFO: Created: latency-svc-fgsdz
Sep 19 14:27:34.179: INFO: Got endpoints: latency-svc-p79wv [748.77695ms]
Sep 19 14:27:34.204: INFO: Created: latency-svc-cz9bb
Sep 19 14:27:34.233: INFO: Got endpoints: latency-svc-g6bhb [752.938533ms]
Sep 19 14:27:34.250: INFO: Created: latency-svc-gdpz2
Sep 19 14:27:34.301: INFO: Got endpoints: latency-svc-9btgk [770.522767ms]
Sep 19 14:27:34.329: INFO: Got endpoints: latency-svc-5slc8 [748.57633ms]
Sep 19 14:27:34.329: INFO: Created: latency-svc-97xzd
Sep 19 14:27:34.344: INFO: Created: latency-svc-hdwv5
Sep 19 14:27:34.383: INFO: Got endpoints: latency-svc-bv5zg [753.433903ms]
Sep 19 14:27:34.398: INFO: Created: latency-svc-9rbnc
Sep 19 14:27:34.431: INFO: Got endpoints: latency-svc-8b558 [751.751797ms]
Sep 19 14:27:34.446: INFO: Created: latency-svc-56cb6
Sep 19 14:27:34.480: INFO: Got endpoints: latency-svc-245rk [750.268702ms]
Sep 19 14:27:34.497: INFO: Created: latency-svc-72595
Sep 19 14:27:34.529: INFO: Got endpoints: latency-svc-wrhrp [748.383249ms]
Sep 19 14:27:34.544: INFO: Created: latency-svc-t7p95
Sep 19 14:27:34.580: INFO: Got endpoints: latency-svc-n26dn [750.434803ms]
Sep 19 14:27:34.595: INFO: Created: latency-svc-wntb2
Sep 19 14:27:34.631: INFO: Got endpoints: latency-svc-r46b6 [749.133952ms]
Sep 19 14:27:34.648: INFO: Created: latency-svc-bpx5t
Sep 19 14:27:34.680: INFO: Got endpoints: latency-svc-rsvsz [748.088433ms]
Sep 19 14:27:34.695: INFO: Created: latency-svc-x2zvx
Sep 19 14:27:34.729: INFO: Got endpoints: latency-svc-rkfrl [746.173037ms]
Sep 19 14:27:34.744: INFO: Created: latency-svc-x8rnb
Sep 19 14:27:34.780: INFO: Got endpoints: latency-svc-cz4cr [751.170649ms]
Sep 19 14:27:34.806: INFO: Created: latency-svc-ltnnn
Sep 19 14:27:34.830: INFO: Got endpoints: latency-svc-5zgdx [749.766129ms]
Sep 19 14:27:34.854: INFO: Created: latency-svc-6jfbt
Sep 19 14:27:34.881: INFO: Got endpoints: latency-svc-fgsdz [749.30574ms]
Sep 19 14:27:34.894: INFO: Created: latency-svc-swnds
Sep 19 14:27:34.929: INFO: Got endpoints: latency-svc-cz9bb [750.34029ms]
Sep 19 14:27:34.944: INFO: Created: latency-svc-glmdv
Sep 19 14:27:34.980: INFO: Got endpoints: latency-svc-gdpz2 [746.797591ms]
Sep 19 14:27:34.994: INFO: Created: latency-svc-7cm5j
Sep 19 14:27:35.030: INFO: Got endpoints: latency-svc-97xzd [728.356668ms]
Sep 19 14:27:35.045: INFO: Created: latency-svc-hmbdl
Sep 19 14:27:35.080: INFO: Got endpoints: latency-svc-hdwv5 [751.514713ms]
Sep 19 14:27:35.097: INFO: Created: latency-svc-rcxs2
Sep 19 14:27:35.130: INFO: Got endpoints: latency-svc-9rbnc [746.386904ms]
Sep 19 14:27:35.145: INFO: Created: latency-svc-xz2sk
Sep 19 14:27:35.181: INFO: Got endpoints: latency-svc-56cb6 [749.681998ms]
Sep 19 14:27:35.198: INFO: Created: latency-svc-vxrtj
Sep 19 14:27:35.230: INFO: Got endpoints: latency-svc-72595 [750.483108ms]
Sep 19 14:27:35.246: INFO: Created: latency-svc-9zzzl
Sep 19 14:27:35.282: INFO: Got endpoints: latency-svc-t7p95 [753.238309ms]
Sep 19 14:27:35.300: INFO: Created: latency-svc-krbs6
Sep 19 14:27:35.329: INFO: Got endpoints: latency-svc-wntb2 [749.423007ms]
Sep 19 14:27:35.344: INFO: Created: latency-svc-n8xhj
Sep 19 14:27:35.383: INFO: Got endpoints: latency-svc-bpx5t [752.581039ms]
Sep 19 14:27:35.424: INFO: Created: latency-svc-x5957
Sep 19 14:27:35.431: INFO: Got endpoints: latency-svc-x2zvx [751.758631ms]
Sep 19 14:27:35.446: INFO: Created: latency-svc-nczkq
Sep 19 14:27:35.482: INFO: Got endpoints: latency-svc-x8rnb [753.281237ms]
Sep 19 14:27:35.504: INFO: Created: latency-svc-b5pcx
Sep 19 14:27:35.530: INFO: Got endpoints: latency-svc-ltnnn [749.593002ms]
Sep 19 14:27:35.546: INFO: Created: latency-svc-j5d2d
Sep 19 14:27:35.586: INFO: Got endpoints: latency-svc-6jfbt [755.623353ms]
Sep 19 14:27:35.599: INFO: Created: latency-svc-hkpz2
Sep 19 14:27:35.629: INFO: Got endpoints: latency-svc-swnds [748.545141ms]
Sep 19 14:27:35.645: INFO: Created: latency-svc-8bqcq
Sep 19 14:27:35.679: INFO: Got endpoints: latency-svc-glmdv [749.899357ms]
Sep 19 14:27:35.692: INFO: Created: latency-svc-fz8dp
Sep 19 14:27:35.729: INFO: Got endpoints: latency-svc-7cm5j [749.505146ms]
Sep 19 14:27:35.744: INFO: Created: latency-svc-6x5mn
Sep 19 14:27:35.780: INFO: Got endpoints: latency-svc-hmbdl [749.925336ms]
Sep 19 14:27:35.794: INFO: Created: latency-svc-tlbwn
Sep 19 14:27:35.830: INFO: Got endpoints: latency-svc-rcxs2 [749.202032ms]
Sep 19 14:27:35.845: INFO: Created: latency-svc-zk7g6
Sep 19 14:27:35.881: INFO: Got endpoints: latency-svc-xz2sk [750.730964ms]
Sep 19 14:27:35.898: INFO: Created: latency-svc-5pqd5
Sep 19 14:27:35.930: INFO: Got endpoints: latency-svc-vxrtj [749.22108ms]
Sep 19 14:27:35.946: INFO: Created: latency-svc-nmx45
Sep 19 14:27:35.980: INFO: Got endpoints: latency-svc-9zzzl [749.206218ms]
Sep 19 14:27:35.997: INFO: Created: latency-svc-hvbzv
Sep 19 14:27:36.029: INFO: Got endpoints: latency-svc-krbs6 [746.041055ms]
Sep 19 14:27:36.044: INFO: Created: latency-svc-5ctd6
Sep 19 14:27:36.082: INFO: Got endpoints: latency-svc-n8xhj [752.527107ms]
Sep 19 14:27:36.099: INFO: Created: latency-svc-m6vj9
Sep 19 14:27:36.129: INFO: Got endpoints: latency-svc-x5957 [746.172651ms]
Sep 19 14:27:36.141: INFO: Created: latency-svc-5cph2
Sep 19 14:27:36.182: INFO: Got endpoints: latency-svc-nczkq [750.67297ms]
Sep 19 14:27:36.199: INFO: Created: latency-svc-p8rhv
Sep 19 14:27:36.230: INFO: Got endpoints: latency-svc-b5pcx [747.356032ms]
Sep 19 14:27:36.244: INFO: Created: latency-svc-fswpd
Sep 19 14:27:36.281: INFO: Got endpoints: latency-svc-j5d2d [750.833086ms]
Sep 19 14:27:36.301: INFO: Created: latency-svc-5f965
Sep 19 14:27:36.329: INFO: Got endpoints: latency-svc-hkpz2 [743.094968ms]
Sep 19 14:27:36.346: INFO: Created: latency-svc-vwqgl
Sep 19 14:27:36.380: INFO: Got endpoints: latency-svc-8bqcq [750.311534ms]
Sep 19 14:27:36.398: INFO: Created: latency-svc-rm4l5
Sep 19 14:27:36.435: INFO: Got endpoints: latency-svc-fz8dp [755.376919ms]
Sep 19 14:27:36.456: INFO: Created: latency-svc-2fwcr
Sep 19 14:27:36.483: INFO: Got endpoints: latency-svc-6x5mn [753.883267ms]
Sep 19 14:27:36.500: INFO: Created: latency-svc-w9psx
Sep 19 14:27:36.529: INFO: Got endpoints: latency-svc-tlbwn [749.668738ms]
Sep 19 14:27:36.544: INFO: Created: latency-svc-5gqh7
Sep 19 14:27:36.580: INFO: Got endpoints: latency-svc-zk7g6 [750.19216ms]
Sep 19 14:27:36.597: INFO: Created: latency-svc-hsfss
Sep 19 14:27:36.630: INFO: Got endpoints: latency-svc-5pqd5 [749.786554ms]
Sep 19 14:27:36.657: INFO: Created: latency-svc-qplkq
Sep 19 14:27:36.679: INFO: Got endpoints: latency-svc-nmx45 [748.93473ms]
Sep 19 14:27:36.701: INFO: Created: latency-svc-b2m6b
Sep 19 14:27:36.729: INFO: Got endpoints: latency-svc-hvbzv [749.671691ms]
Sep 19 14:27:36.748: INFO: Created: latency-svc-qnwr2
Sep 19 14:27:36.780: INFO: Got endpoints: latency-svc-5ctd6 [750.96488ms]
Sep 19 14:27:36.797: INFO: Created: latency-svc-mf5vp
Sep 19 14:27:36.829: INFO: Got endpoints: latency-svc-m6vj9 [747.234669ms]
Sep 19 14:27:36.844: INFO: Created: latency-svc-cz9n8
Sep 19 14:27:36.883: INFO: Got endpoints: latency-svc-5cph2 [753.95635ms]
Sep 19 14:27:36.908: INFO: Created: latency-svc-pm624
Sep 19 14:27:36.929: INFO: Got endpoints: latency-svc-p8rhv [746.325996ms]
Sep 19 14:27:36.943: INFO: Created: latency-svc-dj57d
Sep 19 14:27:36.980: INFO: Got endpoints: latency-svc-fswpd [750.935667ms]
Sep 19 14:27:37.004: INFO: Created: latency-svc-tbz8l
Sep 19 14:27:37.029: INFO: Got endpoints: latency-svc-5f965 [748.855363ms]
Sep 19 14:27:37.051: INFO: Created: latency-svc-lqw27
Sep 19 14:27:37.082: INFO: Got endpoints: latency-svc-vwqgl [752.513471ms]
Sep 19 14:27:37.131: INFO: Got endpoints: latency-svc-rm4l5 [751.337361ms]
Sep 19 14:27:37.180: INFO: Got endpoints: latency-svc-2fwcr [745.184745ms]
Sep 19 14:27:37.230: INFO: Got endpoints: latency-svc-w9psx [746.920126ms]
Sep 19 14:27:37.281: INFO: Got endpoints: latency-svc-5gqh7 [752.153769ms]
Sep 19 14:27:37.332: INFO: Got endpoints: latency-svc-hsfss [752.355918ms]
Sep 19 14:27:37.379: INFO: Got endpoints: latency-svc-qplkq [748.597933ms]
Sep 19 14:27:37.431: INFO: Got endpoints: latency-svc-b2m6b [751.399465ms]
Sep 19 14:27:37.480: INFO: Got endpoints: latency-svc-qnwr2 [750.390353ms]
Sep 19 14:27:37.530: INFO: Got endpoints: latency-svc-mf5vp [750.023575ms]
Sep 19 14:27:37.579: INFO: Got endpoints: latency-svc-cz9n8 [749.352037ms]
Sep 19 14:27:37.629: INFO: Got endpoints: latency-svc-pm624 [745.886595ms]
Sep 19 14:27:37.680: INFO: Got endpoints: latency-svc-dj57d [751.004414ms]
Sep 19 14:27:37.731: INFO: Got endpoints: latency-svc-tbz8l [750.772599ms]
Sep 19 14:27:37.784: INFO: Got endpoints: latency-svc-lqw27 [754.758985ms]
Sep 19 14:27:37.784: INFO: Latencies: [21.924905ms 30.5646ms 41.304437ms 52.150312ms 65.156448ms 74.95431ms 78.154385ms 92.15329ms 107.27187ms 112.933925ms 122.88572ms 130.842641ms 131.142488ms 132.91656ms 138.719593ms 143.193275ms 144.934854ms 145.098774ms 145.223884ms 146.401236ms 146.473501ms 149.654958ms 153.707169ms 158.015856ms 159.402981ms 159.449412ms 161.11891ms 162.311125ms 163.113794ms 163.317666ms 163.653239ms 164.959717ms 167.025501ms 171.406687ms 171.677534ms 173.801808ms 175.669731ms 193.238553ms 232.588471ms 275.183121ms 318.048633ms 359.870947ms 402.03327ms 443.44384ms 482.002732ms 523.562523ms 560.467804ms 602.403033ms 641.442073ms 677.723378ms 723.595368ms 728.356668ms 735.762699ms 743.094968ms 745.184745ms 745.876111ms 745.886595ms 746.007269ms 746.041055ms 746.172651ms 746.173037ms 746.259076ms 746.325996ms 746.32785ms 746.355353ms 746.386904ms 746.797591ms 746.82368ms 746.920126ms 747.226279ms 747.234669ms 747.286579ms 747.314733ms 747.356032ms 747.394445ms 747.41256ms 747.678144ms 747.738136ms 747.791526ms 747.943059ms 747.950106ms 748.088433ms 748.36262ms 748.383249ms 748.451416ms 748.545141ms 748.57633ms 748.597192ms 748.597933ms 748.77695ms 748.79405ms 748.797189ms 748.855363ms 748.93473ms 748.999097ms 749.00251ms 749.113781ms 749.133952ms 749.164384ms 749.202032ms 749.206218ms 749.22108ms 749.30574ms 749.352037ms 749.413181ms 749.423007ms 749.42324ms 749.465648ms 749.477342ms 749.505146ms 749.537773ms 749.550971ms 749.562275ms 749.565438ms 749.593002ms 749.668738ms 749.671691ms 749.681998ms 749.696286ms 749.700123ms 749.754695ms 749.766129ms 749.786554ms 749.899357ms 749.901658ms 749.924731ms 749.925336ms 750.015279ms 750.023575ms 750.07426ms 750.161612ms 750.19216ms 750.268702ms 750.272522ms 750.295538ms 750.298487ms 750.305338ms 750.311534ms 750.34029ms 750.390353ms 750.421433ms 750.422036ms 750.434803ms 750.446003ms 750.483108ms 750.536642ms 750.582897ms 750.598637ms 750.67297ms 750.711425ms 750.730964ms 750.772599ms 750.800097ms 750.812577ms 750.833086ms 750.872479ms 750.889259ms 750.935667ms 750.960214ms 750.96488ms 751.004414ms 751.081867ms 751.170649ms 751.182575ms 751.337361ms 751.337778ms 751.399465ms 751.514713ms 751.73912ms 751.751797ms 751.758631ms 751.973108ms 752.11052ms 752.153769ms 752.355918ms 752.483893ms 752.513471ms 752.527107ms 752.581039ms 752.642798ms 752.657128ms 752.787865ms 752.792115ms 752.861932ms 752.938533ms 753.238309ms 753.281237ms 753.337486ms 753.405994ms 753.433903ms 753.510677ms 753.883267ms 753.95635ms 754.758985ms 755.376919ms 755.623353ms 756.132249ms 756.253619ms 763.474545ms 770.522767ms]
Sep 19 14:27:37.785: INFO: 50 %ile: 749.206218ms
Sep 19 14:27:37.785: INFO: 90 %ile: 752.657128ms
Sep 19 14:27:37.785: INFO: 99 %ile: 763.474545ms
Sep 19 14:27:37.785: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:27:37.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3092" for this suite.
Sep 19 14:27:55.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:27:56.072: INFO: namespace svc-latency-3092 deletion completed in 18.280442353s

• [SLOW TEST:30.189 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:27:56.073: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1415
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep 19 14:27:56.263: INFO: Waiting up to 5m0s for pod "downward-api-5ccf6d38-9dbb-4257-8b4d-63effa08e3e4" in namespace "downward-api-1415" to be "success or failure"
Sep 19 14:27:56.272: INFO: Pod "downward-api-5ccf6d38-9dbb-4257-8b4d-63effa08e3e4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.471998ms
Sep 19 14:27:58.278: INFO: Pod "downward-api-5ccf6d38-9dbb-4257-8b4d-63effa08e3e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015508603s
Sep 19 14:28:00.289: INFO: Pod "downward-api-5ccf6d38-9dbb-4257-8b4d-63effa08e3e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02607643s
STEP: Saw pod success
Sep 19 14:28:00.289: INFO: Pod "downward-api-5ccf6d38-9dbb-4257-8b4d-63effa08e3e4" satisfied condition "success or failure"
Sep 19 14:28:00.302: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod downward-api-5ccf6d38-9dbb-4257-8b4d-63effa08e3e4 container dapi-container: <nil>
STEP: delete the pod
Sep 19 14:28:00.360: INFO: Waiting for pod downward-api-5ccf6d38-9dbb-4257-8b4d-63effa08e3e4 to disappear
Sep 19 14:28:00.369: INFO: Pod downward-api-5ccf6d38-9dbb-4257-8b4d-63effa08e3e4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:28:00.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1415" for this suite.
Sep 19 14:28:06.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:28:06.934: INFO: namespace downward-api-1415 deletion completed in 6.55765595s

• [SLOW TEST:10.861 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:28:06.934: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9328
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 14:28:07.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 create -f - --namespace=kubectl-9328'
Sep 19 14:28:07.667: INFO: stderr: ""
Sep 19 14:28:07.667: INFO: stdout: "replicationcontroller/redis-master created\n"
Sep 19 14:28:07.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 create -f - --namespace=kubectl-9328'
Sep 19 14:28:07.900: INFO: stderr: ""
Sep 19 14:28:07.900: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 19 14:28:08.916: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:28:08.916: INFO: Found 0 / 1
Sep 19 14:28:09.959: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:28:09.959: INFO: Found 1 / 1
Sep 19 14:28:09.959: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 19 14:28:09.979: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:28:09.979: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 19 14:28:09.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 describe pod redis-master-ql9gm --namespace=kubectl-9328'
Sep 19 14:28:10.117: INFO: stderr: ""
Sep 19 14:28:10.117: INFO: stdout: "Name:           redis-master-ql9gm\nNamespace:      kubectl-9328\nPriority:       0\nNode:           ip-172-31-44-209.eu-central-1.compute.internal/172.31.44.209\nStart Time:     Thu, 19 Sep 2019 14:28:07 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 172.25.10.66/32\nStatus:         Running\nIP:             172.25.10.66\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://a695e1eb4a1df9c121f4037d47be35b10e430915c9727ed9c2e70d52dd72ab9a\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 19 Sep 2019 14:28:09 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-k5qkq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-k5qkq:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-k5qkq\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                     Message\n  ----    ------     ----  ----                                                     -------\n  Normal  Scheduled  3s    default-scheduler                                        Successfully assigned kubectl-9328/redis-master-ql9gm to ip-172-31-44-209.eu-central-1.compute.internal\n  Normal  Pulled     2s    kubelet, ip-172-31-44-209.eu-central-1.compute.internal  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, ip-172-31-44-209.eu-central-1.compute.internal  Created container redis-master\n  Normal  Started    1s    kubelet, ip-172-31-44-209.eu-central-1.compute.internal  Started container redis-master\n"
Sep 19 14:28:10.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 describe rc redis-master --namespace=kubectl-9328'
Sep 19 14:28:10.233: INFO: stderr: ""
Sep 19 14:28:10.233: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-9328\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-ql9gm\n"
Sep 19 14:28:10.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 describe service redis-master --namespace=kubectl-9328'
Sep 19 14:28:10.408: INFO: stderr: ""
Sep 19 14:28:10.408: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-9328\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.240.21.38\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.25.10.66:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 19 14:28:10.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 describe node ip-172-31-33-83.eu-central-1.compute.internal'
Sep 19 14:28:10.614: INFO: stderr: ""
Sep 19 14:28:10.614: INFO: stdout: "Name:               ip-172-31-33-83.eu-central-1.compute.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-central-1\n                    failure-domain.beta.kubernetes.io/zone=eu-central-1c\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-33-83\n                    kubernetes.io/os=linux\n                    machine-controller/owned-by=383b5d02-ab8a-4aa0-b376-c46712cf031c\n                    system/cluster=glbbkt4n56\n                    system/project=zdcr98vvt7\nAnnotations:        cluster.k8s.io/machine: kube-system/worker-fgxc5-59dcf8c64-psf9z\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"f6:4c:0b:2a:6f:f5\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.33.83\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.8.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 19 Sep 2019 12:49:54 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 19 Sep 2019 14:28:00 +0000   Thu, 19 Sep 2019 12:49:54 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 19 Sep 2019 14:28:00 +0000   Thu, 19 Sep 2019 12:49:54 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 19 Sep 2019 14:28:00 +0000   Thu, 19 Sep 2019 12:49:54 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 19 Sep 2019 14:28:00 +0000   Thu, 19 Sep 2019 12:50:15 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.31.33.83\n  ExternalIP:   35.158.107.7\n  Hostname:     ip-172-31-33-83.eu-central-1.compute.internal\n  InternalDNS:  ip-172-31-33-83.eu-central-1.compute.internal\n  ExternalDNS:  ec2-35-158-107-7.eu-central-1.compute.amazonaws.com\nCapacity:\n attachable-volumes-aws-ebs:  25\n cpu:                         2\n ephemeral-storage:           25346000Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      3978716Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  25\n cpu:                         1800m\n ephemeral-storage:           21211389914\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      3671516Ki\n pods:                        110\nSystem Info:\n Machine ID:                 ec2e648b2790c93fcdc3cc88f4df69b7\n System UUID:                EC2E648B-2790-C93F-CDC3-CC88F4DF69B7\n Boot ID:                    65c734a7-7765-4d92-ac1f-0f302be68d73\n Kernel Version:             4.15.0-1050-aws\n OS Image:                   Ubuntu 18.04.3 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.2\n Kubelet Version:            v1.15.3\n Kube-Proxy Version:         v1.15.3\nPodCIDR:                     172.25.8.0/24\nProviderID:                  aws:///eu-central-1c/i-003d3000747ba73fd\nNon-terminated Pods:         (6 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                canal-4wkpv                                                250m (13%)    0 (0%)      0 (0%)           0 (0%)         98m\n  kube-system                kube-proxy-wcj22                                           75m (4%)      250m (13%)  50Mi (1%)        250Mi (6%)     98m\n  kube-system                kubernetes-dashboard-584d5ffc75-jvnx5                      75m (4%)      75m (4%)    50Mi (1%)        50Mi (1%)      97m\n  kube-system                node-exporter-7txrv                                        20m (1%)      45m (2%)    48Mi (1%)        96Mi (2%)      98m\n  kube-system                node-local-dns-twjtr                                       25m (1%)      0 (0%)      5Mi (0%)         30Mi (0%)      97m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-zgrfd    0 (0%)        0 (0%)      0 (0%)           0 (0%)         83m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         445m (24%)  370m (20%)\n  memory                      153Mi (4%)  426Mi (11%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:                       <none>\n"
Sep 19 14:28:10.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 describe namespace kubectl-9328'
Sep 19 14:28:10.764: INFO: stderr: ""
Sep 19 14:28:10.764: INFO: stdout: "Name:         kubectl-9328\nLabels:       e2e-framework=kubectl\n              e2e-run=96e9650b-c6d4-4375-b956-0d40d4726d0b\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:28:10.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9328" for this suite.
Sep 19 14:28:34.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:28:35.402: INFO: namespace kubectl-9328 deletion completed in 24.625401183s

• [SLOW TEST:28.468 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:28:35.402: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9454
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-26z9
STEP: Creating a pod to test atomic-volume-subpath
Sep 19 14:28:35.691: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-26z9" in namespace "subpath-9454" to be "success or failure"
Sep 19 14:28:35.713: INFO: Pod "pod-subpath-test-downwardapi-26z9": Phase="Pending", Reason="", readiness=false. Elapsed: 21.561892ms
Sep 19 14:28:37.719: INFO: Pod "pod-subpath-test-downwardapi-26z9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027908521s
Sep 19 14:28:39.726: INFO: Pod "pod-subpath-test-downwardapi-26z9": Phase="Running", Reason="", readiness=true. Elapsed: 4.03449831s
Sep 19 14:28:41.776: INFO: Pod "pod-subpath-test-downwardapi-26z9": Phase="Running", Reason="", readiness=true. Elapsed: 6.084705278s
Sep 19 14:28:43.785: INFO: Pod "pod-subpath-test-downwardapi-26z9": Phase="Running", Reason="", readiness=true. Elapsed: 8.094045971s
Sep 19 14:28:45.793: INFO: Pod "pod-subpath-test-downwardapi-26z9": Phase="Running", Reason="", readiness=true. Elapsed: 10.101696849s
Sep 19 14:28:47.801: INFO: Pod "pod-subpath-test-downwardapi-26z9": Phase="Running", Reason="", readiness=true. Elapsed: 12.110033229s
Sep 19 14:28:49.814: INFO: Pod "pod-subpath-test-downwardapi-26z9": Phase="Running", Reason="", readiness=true. Elapsed: 14.122245895s
Sep 19 14:28:51.822: INFO: Pod "pod-subpath-test-downwardapi-26z9": Phase="Running", Reason="", readiness=true. Elapsed: 16.130903081s
Sep 19 14:28:53.829: INFO: Pod "pod-subpath-test-downwardapi-26z9": Phase="Running", Reason="", readiness=true. Elapsed: 18.137157771s
Sep 19 14:28:55.835: INFO: Pod "pod-subpath-test-downwardapi-26z9": Phase="Running", Reason="", readiness=true. Elapsed: 20.143922508s
Sep 19 14:28:57.866: INFO: Pod "pod-subpath-test-downwardapi-26z9": Phase="Running", Reason="", readiness=true. Elapsed: 22.174998065s
Sep 19 14:28:59.876: INFO: Pod "pod-subpath-test-downwardapi-26z9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.184505308s
STEP: Saw pod success
Sep 19 14:28:59.876: INFO: Pod "pod-subpath-test-downwardapi-26z9" satisfied condition "success or failure"
Sep 19 14:28:59.882: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod pod-subpath-test-downwardapi-26z9 container test-container-subpath-downwardapi-26z9: <nil>
STEP: delete the pod
Sep 19 14:29:00.104: INFO: Waiting for pod pod-subpath-test-downwardapi-26z9 to disappear
Sep 19 14:29:00.110: INFO: Pod pod-subpath-test-downwardapi-26z9 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-26z9
Sep 19 14:29:00.110: INFO: Deleting pod "pod-subpath-test-downwardapi-26z9" in namespace "subpath-9454"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:29:00.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9454" for this suite.
Sep 19 14:29:06.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:29:06.806: INFO: namespace subpath-9454 deletion completed in 6.642500821s

• [SLOW TEST:31.404 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:29:06.807: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7658
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 14:29:07.000: INFO: Waiting up to 5m0s for pod "downwardapi-volume-342c698e-3136-483f-9423-3d1954d717b6" in namespace "downward-api-7658" to be "success or failure"
Sep 19 14:29:07.008: INFO: Pod "downwardapi-volume-342c698e-3136-483f-9423-3d1954d717b6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.612153ms
Sep 19 14:29:09.061: INFO: Pod "downwardapi-volume-342c698e-3136-483f-9423-3d1954d717b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061503822s
Sep 19 14:29:11.071: INFO: Pod "downwardapi-volume-342c698e-3136-483f-9423-3d1954d717b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0711957s
STEP: Saw pod success
Sep 19 14:29:11.071: INFO: Pod "downwardapi-volume-342c698e-3136-483f-9423-3d1954d717b6" satisfied condition "success or failure"
Sep 19 14:29:11.079: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod downwardapi-volume-342c698e-3136-483f-9423-3d1954d717b6 container client-container: <nil>
STEP: delete the pod
Sep 19 14:29:11.364: INFO: Waiting for pod downwardapi-volume-342c698e-3136-483f-9423-3d1954d717b6 to disappear
Sep 19 14:29:11.370: INFO: Pod downwardapi-volume-342c698e-3136-483f-9423-3d1954d717b6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:29:11.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7658" for this suite.
Sep 19 14:29:17.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:29:17.948: INFO: namespace downward-api-7658 deletion completed in 6.567983836s

• [SLOW TEST:11.141 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:29:17.948: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1705
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 19 14:29:18.177: INFO: Waiting up to 5m0s for pod "pod-03b9dcbe-c669-4ace-97b5-8004ec8dcf1c" in namespace "emptydir-1705" to be "success or failure"
Sep 19 14:29:18.189: INFO: Pod "pod-03b9dcbe-c669-4ace-97b5-8004ec8dcf1c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.608907ms
Sep 19 14:29:20.201: INFO: Pod "pod-03b9dcbe-c669-4ace-97b5-8004ec8dcf1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023348043s
Sep 19 14:29:22.207: INFO: Pod "pod-03b9dcbe-c669-4ace-97b5-8004ec8dcf1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029929984s
STEP: Saw pod success
Sep 19 14:29:22.207: INFO: Pod "pod-03b9dcbe-c669-4ace-97b5-8004ec8dcf1c" satisfied condition "success or failure"
Sep 19 14:29:22.213: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod pod-03b9dcbe-c669-4ace-97b5-8004ec8dcf1c container test-container: <nil>
STEP: delete the pod
Sep 19 14:29:22.244: INFO: Waiting for pod pod-03b9dcbe-c669-4ace-97b5-8004ec8dcf1c to disappear
Sep 19 14:29:22.260: INFO: Pod pod-03b9dcbe-c669-4ace-97b5-8004ec8dcf1c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:29:22.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1705" for this suite.
Sep 19 14:29:28.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:29:28.778: INFO: namespace emptydir-1705 deletion completed in 6.503437549s

• [SLOW TEST:10.830 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:29:28.778: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9338
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep 19 14:29:28.998: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:29:34.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9338" for this suite.
Sep 19 14:29:56.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:29:56.625: INFO: namespace init-container-9338 deletion completed in 22.254234285s

• [SLOW TEST:27.847 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:29:56.625: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3570
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-9b627e1c-6d05-4354-9763-e32823a0887c
STEP: Creating a pod to test consume configMaps
Sep 19 14:29:56.813: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-31055492-1113-4744-aada-7a3a44c52e0e" in namespace "projected-3570" to be "success or failure"
Sep 19 14:29:56.820: INFO: Pod "pod-projected-configmaps-31055492-1113-4744-aada-7a3a44c52e0e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.091365ms
Sep 19 14:29:58.825: INFO: Pod "pod-projected-configmaps-31055492-1113-4744-aada-7a3a44c52e0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012558269s
STEP: Saw pod success
Sep 19 14:29:58.825: INFO: Pod "pod-projected-configmaps-31055492-1113-4744-aada-7a3a44c52e0e" satisfied condition "success or failure"
Sep 19 14:29:58.831: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod pod-projected-configmaps-31055492-1113-4744-aada-7a3a44c52e0e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 19 14:29:58.871: INFO: Waiting for pod pod-projected-configmaps-31055492-1113-4744-aada-7a3a44c52e0e to disappear
Sep 19 14:29:58.877: INFO: Pod pod-projected-configmaps-31055492-1113-4744-aada-7a3a44c52e0e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:29:58.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3570" for this suite.
Sep 19 14:30:04.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:30:05.167: INFO: namespace projected-3570 deletion completed in 6.283164233s

• [SLOW TEST:8.542 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:30:05.168: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1709
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 19 14:30:05.342: INFO: Waiting up to 5m0s for pod "pod-5bf3b7f1-7694-4eb8-891c-6f67ce3396ec" in namespace "emptydir-1709" to be "success or failure"
Sep 19 14:30:05.351: INFO: Pod "pod-5bf3b7f1-7694-4eb8-891c-6f67ce3396ec": Phase="Pending", Reason="", readiness=false. Elapsed: 8.651856ms
Sep 19 14:30:07.358: INFO: Pod "pod-5bf3b7f1-7694-4eb8-891c-6f67ce3396ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015991215s
Sep 19 14:30:09.368: INFO: Pod "pod-5bf3b7f1-7694-4eb8-891c-6f67ce3396ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025971826s
STEP: Saw pod success
Sep 19 14:30:09.368: INFO: Pod "pod-5bf3b7f1-7694-4eb8-891c-6f67ce3396ec" satisfied condition "success or failure"
Sep 19 14:30:09.375: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod pod-5bf3b7f1-7694-4eb8-891c-6f67ce3396ec container test-container: <nil>
STEP: delete the pod
Sep 19 14:30:09.414: INFO: Waiting for pod pod-5bf3b7f1-7694-4eb8-891c-6f67ce3396ec to disappear
Sep 19 14:30:09.421: INFO: Pod pod-5bf3b7f1-7694-4eb8-891c-6f67ce3396ec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:30:09.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1709" for this suite.
Sep 19 14:30:15.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:30:15.710: INFO: namespace emptydir-1709 deletion completed in 6.279735558s

• [SLOW TEST:10.542 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:30:15.711: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2217
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 19 14:30:20.945: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:30:20.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2217" for this suite.
Sep 19 14:30:43.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:30:43.225: INFO: namespace replicaset-2217 deletion completed in 22.226805808s

• [SLOW TEST:27.514 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:30:43.226: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3220
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep 19 14:30:47.955: INFO: Successfully updated pod "labelsupdatea2d5b4df-9acf-4ccb-bcfd-0fe31882b027"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:30:50.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3220" for this suite.
Sep 19 14:31:06.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:31:06.251: INFO: namespace projected-3220 deletion completed in 16.243018962s

• [SLOW TEST:23.025 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:31:06.252: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6661
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep 19 14:31:06.431: INFO: Waiting up to 5m0s for pod "downward-api-9f1e29ad-498a-43e3-b4bb-2a4397957278" in namespace "downward-api-6661" to be "success or failure"
Sep 19 14:31:06.441: INFO: Pod "downward-api-9f1e29ad-498a-43e3-b4bb-2a4397957278": Phase="Pending", Reason="", readiness=false. Elapsed: 9.724996ms
Sep 19 14:31:08.447: INFO: Pod "downward-api-9f1e29ad-498a-43e3-b4bb-2a4397957278": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015810087s
Sep 19 14:31:10.454: INFO: Pod "downward-api-9f1e29ad-498a-43e3-b4bb-2a4397957278": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022433782s
STEP: Saw pod success
Sep 19 14:31:10.454: INFO: Pod "downward-api-9f1e29ad-498a-43e3-b4bb-2a4397957278" satisfied condition "success or failure"
Sep 19 14:31:10.459: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod downward-api-9f1e29ad-498a-43e3-b4bb-2a4397957278 container dapi-container: <nil>
STEP: delete the pod
Sep 19 14:31:10.526: INFO: Waiting for pod downward-api-9f1e29ad-498a-43e3-b4bb-2a4397957278 to disappear
Sep 19 14:31:10.532: INFO: Pod downward-api-9f1e29ad-498a-43e3-b4bb-2a4397957278 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:31:10.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6661" for this suite.
Sep 19 14:31:16.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:31:16.788: INFO: namespace downward-api-6661 deletion completed in 6.246185853s

• [SLOW TEST:10.536 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:31:16.788: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3920
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep 19 14:31:16.961: INFO: Waiting up to 5m0s for pod "downward-api-223409b6-2d96-4be0-b53a-2deb9c682b60" in namespace "downward-api-3920" to be "success or failure"
Sep 19 14:31:16.974: INFO: Pod "downward-api-223409b6-2d96-4be0-b53a-2deb9c682b60": Phase="Pending", Reason="", readiness=false. Elapsed: 12.772666ms
Sep 19 14:31:18.980: INFO: Pod "downward-api-223409b6-2d96-4be0-b53a-2deb9c682b60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019024544s
Sep 19 14:31:20.988: INFO: Pod "downward-api-223409b6-2d96-4be0-b53a-2deb9c682b60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027049536s
STEP: Saw pod success
Sep 19 14:31:20.988: INFO: Pod "downward-api-223409b6-2d96-4be0-b53a-2deb9c682b60" satisfied condition "success or failure"
Sep 19 14:31:20.994: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod downward-api-223409b6-2d96-4be0-b53a-2deb9c682b60 container dapi-container: <nil>
STEP: delete the pod
Sep 19 14:31:21.038: INFO: Waiting for pod downward-api-223409b6-2d96-4be0-b53a-2deb9c682b60 to disappear
Sep 19 14:31:21.043: INFO: Pod downward-api-223409b6-2d96-4be0-b53a-2deb9c682b60 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:31:21.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3920" for this suite.
Sep 19 14:31:27.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:31:27.322: INFO: namespace downward-api-3920 deletion completed in 6.269968151s

• [SLOW TEST:10.534 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:31:27.322: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:31:31.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2735" for this suite.
Sep 19 14:32:19.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:32:19.795: INFO: namespace kubelet-test-2735 deletion completed in 48.237267317s

• [SLOW TEST:52.473 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:32:19.795: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7459
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:32:25.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7459" for this suite.
Sep 19 14:32:31.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:32:31.775: INFO: namespace watch-7459 deletion completed in 6.305460512s

• [SLOW TEST:11.979 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:32:31.775: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5723
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-5723
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5723 to expose endpoints map[]
Sep 19 14:32:31.959: INFO: Get endpoints failed (9.002154ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Sep 19 14:32:32.965: INFO: successfully validated that service multi-endpoint-test in namespace services-5723 exposes endpoints map[] (1.014450655s elapsed)
STEP: Creating pod pod1 in namespace services-5723
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5723 to expose endpoints map[pod1:[100]]
Sep 19 14:32:36.023: INFO: successfully validated that service multi-endpoint-test in namespace services-5723 exposes endpoints map[pod1:[100]] (3.047651066s elapsed)
STEP: Creating pod pod2 in namespace services-5723
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5723 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 19 14:32:39.111: INFO: successfully validated that service multi-endpoint-test in namespace services-5723 exposes endpoints map[pod1:[100] pod2:[101]] (3.077668667s elapsed)
STEP: Deleting pod pod1 in namespace services-5723
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5723 to expose endpoints map[pod2:[101]]
Sep 19 14:32:39.139: INFO: successfully validated that service multi-endpoint-test in namespace services-5723 exposes endpoints map[pod2:[101]] (14.350146ms elapsed)
STEP: Deleting pod pod2 in namespace services-5723
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5723 to expose endpoints map[]
Sep 19 14:32:39.161: INFO: successfully validated that service multi-endpoint-test in namespace services-5723 exposes endpoints map[] (8.291678ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:32:39.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5723" for this suite.
Sep 19 14:33:01.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:33:01.496: INFO: namespace services-5723 deletion completed in 22.276188002s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:29.722 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:33:01.497: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9001
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Sep 19 14:33:01.674: INFO: namespace kubectl-9001
Sep 19 14:33:01.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 create -f - --namespace=kubectl-9001'
Sep 19 14:33:02.084: INFO: stderr: ""
Sep 19 14:33:02.084: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 19 14:33:03.091: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:33:03.091: INFO: Found 0 / 1
Sep 19 14:33:04.091: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:33:04.091: INFO: Found 0 / 1
Sep 19 14:33:05.090: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:33:05.090: INFO: Found 1 / 1
Sep 19 14:33:05.090: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 19 14:33:05.096: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:33:05.096: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 19 14:33:05.096: INFO: wait on redis-master startup in kubectl-9001 
Sep 19 14:33:05.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 logs redis-master-l6dt2 redis-master --namespace=kubectl-9001'
Sep 19 14:33:05.210: INFO: stderr: ""
Sep 19 14:33:05.210: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 19 Sep 14:33:03.688 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 19 Sep 14:33:03.688 # Server started, Redis version 3.2.12\n1:M 19 Sep 14:33:03.688 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 19 Sep 14:33:03.689 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Sep 19 14:33:05.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9001'
Sep 19 14:33:05.325: INFO: stderr: ""
Sep 19 14:33:05.325: INFO: stdout: "service/rm2 exposed\n"
Sep 19 14:33:05.333: INFO: Service rm2 in namespace kubectl-9001 found.
STEP: exposing service
Sep 19 14:33:07.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9001'
Sep 19 14:33:07.448: INFO: stderr: ""
Sep 19 14:33:07.448: INFO: stdout: "service/rm3 exposed\n"
Sep 19 14:33:07.456: INFO: Service rm3 in namespace kubectl-9001 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:33:09.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9001" for this suite.
Sep 19 14:33:31.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:33:31.707: INFO: namespace kubectl-9001 deletion completed in 22.234591962s

• [SLOW TEST:30.210 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:33:31.707: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7072
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 14:33:31.883: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 19 14:33:36.889: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 19 14:33:36.889: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 19 14:33:38.895: INFO: Creating deployment "test-rollover-deployment"
Sep 19 14:33:38.909: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 19 14:33:40.920: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 19 14:33:40.930: INFO: Ensure that both replica sets have 1 created replica
Sep 19 14:33:40.945: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 19 14:33:40.958: INFO: Updating deployment test-rollover-deployment
Sep 19 14:33:40.958: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 19 14:33:42.972: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 19 14:33:42.987: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 19 14:33:43.000: INFO: all replica sets need to contain the pod-template-hash label
Sep 19 14:33:43.000: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500421, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 19 14:33:45.015: INFO: all replica sets need to contain the pod-template-hash label
Sep 19 14:33:45.015: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500423, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 19 14:33:47.012: INFO: all replica sets need to contain the pod-template-hash label
Sep 19 14:33:47.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500423, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 19 14:33:49.012: INFO: all replica sets need to contain the pod-template-hash label
Sep 19 14:33:49.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500423, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 19 14:33:51.015: INFO: all replica sets need to contain the pod-template-hash label
Sep 19 14:33:51.015: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500423, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 19 14:33:53.012: INFO: all replica sets need to contain the pod-template-hash label
Sep 19 14:33:53.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500423, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704500418, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 19 14:33:55.015: INFO: 
Sep 19 14:33:55.015: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep 19 14:33:55.030: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-7072,SelfLink:/apis/apps/v1/namespaces/deployment-7072/deployments/test-rollover-deployment,UID:ac90ee63-e9e5-4d4f-a51a-4e5fde65c395,ResourceVersion:41373,Generation:2,CreationTimestamp:2019-09-19 14:33:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-19 14:33:38 +0000 UTC 2019-09-19 14:33:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-19 14:33:53 +0000 UTC 2019-09-19 14:33:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep 19 14:33:55.036: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-7072,SelfLink:/apis/apps/v1/namespaces/deployment-7072/replicasets/test-rollover-deployment-854595fc44,UID:e5f1d13d-ece2-4c6b-8958-6b65d0d500cc,ResourceVersion:41361,Generation:2,CreationTimestamp:2019-09-19 14:33:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment ac90ee63-e9e5-4d4f-a51a-4e5fde65c395 0xc003fe2807 0xc003fe2808}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep 19 14:33:55.036: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 19 14:33:55.036: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-7072,SelfLink:/apis/apps/v1/namespaces/deployment-7072/replicasets/test-rollover-controller,UID:a7e18830-fd93-4401-9302-5b8bfc24088f,ResourceVersion:41372,Generation:2,CreationTimestamp:2019-09-19 14:33:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment ac90ee63-e9e5-4d4f-a51a-4e5fde65c395 0xc003fe2737 0xc003fe2738}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 19 14:33:55.037: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-7072,SelfLink:/apis/apps/v1/namespaces/deployment-7072/replicasets/test-rollover-deployment-9b8b997cf,UID:0c3c54ec-237f-488c-b87e-80e32bc08441,ResourceVersion:41317,Generation:2,CreationTimestamp:2019-09-19 14:33:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment ac90ee63-e9e5-4d4f-a51a-4e5fde65c395 0xc003fe28d0 0xc003fe28d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 19 14:33:55.042: INFO: Pod "test-rollover-deployment-854595fc44-6p5p9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-6p5p9,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-7072,SelfLink:/api/v1/namespaces/deployment-7072/pods/test-rollover-deployment-854595fc44-6p5p9,UID:3fc21203-f7d6-480e-a19b-9b85e65e0b45,ResourceVersion:41331,Generation:0,CreationTimestamp:2019-09-19 14:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.12.78/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 e5f1d13d-ece2-4c6b-8958-6b65d0d500cc 0xc003fe3507 0xc003fe3508}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7twsb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7twsb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-7twsb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-36-147.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003fe3570} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003fe3590}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 14:33:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 14:33:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 14:33:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 14:33:41 +0000 UTC  }],Message:,Reason:,HostIP:172.31.36.147,PodIP:172.25.12.78,StartTime:2019-09-19 14:33:41 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-19 14:33:42 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://3b9a6b3546b70a9080c226eb809345afdf05e4508218efb34fdbdd5f461b147b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:33:55.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7072" for this suite.
Sep 19 14:34:01.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:34:01.318: INFO: namespace deployment-7072 deletion completed in 6.269375317s

• [SLOW TEST:29.611 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:34:01.318: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2521
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep 19 14:34:11.533: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0919 14:34:11.533891      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:34:11.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2521" for this suite.
Sep 19 14:34:17.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:34:17.826: INFO: namespace gc-2521 deletion completed in 6.287232395s

• [SLOW TEST:16.508 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:34:17.826: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 19 14:34:18.000: INFO: Waiting up to 5m0s for pod "pod-b49d1a24-e8b3-459e-8c8e-8b4b887c0393" in namespace "emptydir-8834" to be "success or failure"
Sep 19 14:34:18.009: INFO: Pod "pod-b49d1a24-e8b3-459e-8c8e-8b4b887c0393": Phase="Pending", Reason="", readiness=false. Elapsed: 8.628402ms
Sep 19 14:34:20.019: INFO: Pod "pod-b49d1a24-e8b3-459e-8c8e-8b4b887c0393": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01803792s
Sep 19 14:34:22.028: INFO: Pod "pod-b49d1a24-e8b3-459e-8c8e-8b4b887c0393": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027257107s
STEP: Saw pod success
Sep 19 14:34:22.028: INFO: Pod "pod-b49d1a24-e8b3-459e-8c8e-8b4b887c0393" satisfied condition "success or failure"
Sep 19 14:34:22.033: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod pod-b49d1a24-e8b3-459e-8c8e-8b4b887c0393 container test-container: <nil>
STEP: delete the pod
Sep 19 14:34:22.073: INFO: Waiting for pod pod-b49d1a24-e8b3-459e-8c8e-8b4b887c0393 to disappear
Sep 19 14:34:22.079: INFO: Pod pod-b49d1a24-e8b3-459e-8c8e-8b4b887c0393 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:34:22.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8834" for this suite.
Sep 19 14:34:28.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:34:28.315: INFO: namespace emptydir-8834 deletion completed in 6.230185563s

• [SLOW TEST:10.489 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:34:28.316: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4366
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-5487186e-812f-4ce4-81de-3dea2f5edfd9
STEP: Creating a pod to test consume configMaps
Sep 19 14:34:28.512: INFO: Waiting up to 5m0s for pod "pod-configmaps-54244f38-16c1-4439-9a83-e78e0fcc6fe9" in namespace "configmap-4366" to be "success or failure"
Sep 19 14:34:28.519: INFO: Pod "pod-configmaps-54244f38-16c1-4439-9a83-e78e0fcc6fe9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.497527ms
Sep 19 14:34:30.527: INFO: Pod "pod-configmaps-54244f38-16c1-4439-9a83-e78e0fcc6fe9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015683142s
Sep 19 14:34:32.533: INFO: Pod "pod-configmaps-54244f38-16c1-4439-9a83-e78e0fcc6fe9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021620187s
STEP: Saw pod success
Sep 19 14:34:32.533: INFO: Pod "pod-configmaps-54244f38-16c1-4439-9a83-e78e0fcc6fe9" satisfied condition "success or failure"
Sep 19 14:34:32.539: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod pod-configmaps-54244f38-16c1-4439-9a83-e78e0fcc6fe9 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 19 14:34:32.581: INFO: Waiting for pod pod-configmaps-54244f38-16c1-4439-9a83-e78e0fcc6fe9 to disappear
Sep 19 14:34:32.586: INFO: Pod pod-configmaps-54244f38-16c1-4439-9a83-e78e0fcc6fe9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:34:32.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4366" for this suite.
Sep 19 14:34:38.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:34:38.836: INFO: namespace configmap-4366 deletion completed in 6.242433107s

• [SLOW TEST:10.520 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:34:38.836: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-9239600a-7ce2-42ed-a987-536b34bca694
STEP: Creating a pod to test consume configMaps
Sep 19 14:34:39.020: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3a4178f5-271f-4657-b1e7-61c15192dfa2" in namespace "projected-7903" to be "success or failure"
Sep 19 14:34:39.028: INFO: Pod "pod-projected-configmaps-3a4178f5-271f-4657-b1e7-61c15192dfa2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.118656ms
Sep 19 14:34:41.035: INFO: Pod "pod-projected-configmaps-3a4178f5-271f-4657-b1e7-61c15192dfa2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015129588s
Sep 19 14:34:43.041: INFO: Pod "pod-projected-configmaps-3a4178f5-271f-4657-b1e7-61c15192dfa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021149131s
STEP: Saw pod success
Sep 19 14:34:43.041: INFO: Pod "pod-projected-configmaps-3a4178f5-271f-4657-b1e7-61c15192dfa2" satisfied condition "success or failure"
Sep 19 14:34:43.046: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod pod-projected-configmaps-3a4178f5-271f-4657-b1e7-61c15192dfa2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 19 14:34:43.086: INFO: Waiting for pod pod-projected-configmaps-3a4178f5-271f-4657-b1e7-61c15192dfa2 to disappear
Sep 19 14:34:43.092: INFO: Pod pod-projected-configmaps-3a4178f5-271f-4657-b1e7-61c15192dfa2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:34:43.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7903" for this suite.
Sep 19 14:34:49.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:34:49.337: INFO: namespace projected-7903 deletion completed in 6.238664739s

• [SLOW TEST:10.501 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:34:49.337: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8987
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 14:34:49.529: INFO: Waiting up to 5m0s for pod "downwardapi-volume-97611d05-fc75-46a5-9f9e-f4751c771486" in namespace "projected-8987" to be "success or failure"
Sep 19 14:34:49.537: INFO: Pod "downwardapi-volume-97611d05-fc75-46a5-9f9e-f4751c771486": Phase="Pending", Reason="", readiness=false. Elapsed: 8.540038ms
Sep 19 14:34:51.543: INFO: Pod "downwardapi-volume-97611d05-fc75-46a5-9f9e-f4751c771486": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014023767s
STEP: Saw pod success
Sep 19 14:34:51.543: INFO: Pod "downwardapi-volume-97611d05-fc75-46a5-9f9e-f4751c771486" satisfied condition "success or failure"
Sep 19 14:34:51.548: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod downwardapi-volume-97611d05-fc75-46a5-9f9e-f4751c771486 container client-container: <nil>
STEP: delete the pod
Sep 19 14:34:51.587: INFO: Waiting for pod downwardapi-volume-97611d05-fc75-46a5-9f9e-f4751c771486 to disappear
Sep 19 14:34:51.592: INFO: Pod downwardapi-volume-97611d05-fc75-46a5-9f9e-f4751c771486 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:34:51.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8987" for this suite.
Sep 19 14:34:57.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:34:57.871: INFO: namespace projected-8987 deletion completed in 6.271697978s

• [SLOW TEST:8.534 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:34:57.872: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9858
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 19 14:35:01.078: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:35:01.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9858" for this suite.
Sep 19 14:35:07.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:35:07.398: INFO: namespace container-runtime-9858 deletion completed in 6.27156394s

• [SLOW TEST:9.526 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:35:07.398: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9626
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep 19 14:35:07.580: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 19 14:35:12.588: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 19 14:35:12.588: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep 19 14:35:16.639: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-9626,SelfLink:/apis/apps/v1/namespaces/deployment-9626/deployments/test-cleanup-deployment,UID:fd19f9c3-ff00-4ab9-961f-adf5051a0912,ResourceVersion:41871,Generation:1,CreationTimestamp:2019-09-19 14:35:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-19 14:35:12 +0000 UTC 2019-09-19 14:35:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-19 14:35:14 +0000 UTC 2019-09-19 14:35:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep 19 14:35:16.650: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-9626,SelfLink:/apis/apps/v1/namespaces/deployment-9626/replicasets/test-cleanup-deployment-55bbcbc84c,UID:4353222a-55a5-4bc6-8e1a-6729aa556336,ResourceVersion:41860,Generation:1,CreationTimestamp:2019-09-19 14:35:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment fd19f9c3-ff00-4ab9-961f-adf5051a0912 0xc002d760d7 0xc002d760d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep 19 14:35:16.659: INFO: Pod "test-cleanup-deployment-55bbcbc84c-m6rtl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-m6rtl,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-9626,SelfLink:/api/v1/namespaces/deployment-9626/pods/test-cleanup-deployment-55bbcbc84c-m6rtl,UID:89d710e4-a92c-4566-bac7-26e7fcee5810,ResourceVersion:41859,Generation:0,CreationTimestamp:2019-09-19 14:35:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.25.8.108/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 4353222a-55a5-4bc6-8e1a-6729aa556336 0xc003e883f7 0xc003e883f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zkc67 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zkc67,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-zkc67 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-83.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003e88460} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003e88480}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 14:35:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 14:35:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 14:35:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-19 14:35:12 +0000 UTC  }],Message:,Reason:,HostIP:172.31.33.83,PodIP:172.25.8.108,StartTime:2019-09-19 14:35:12 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-19 14:35:14 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://b3ac562651f935c18612b0403ae4a9b335fafafb1183a3f31355b2d4762e0d27}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:35:16.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9626" for this suite.
Sep 19 14:35:22.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:35:22.937: INFO: namespace deployment-9626 deletion completed in 6.270677948s

• [SLOW TEST:15.539 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:35:22.937: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4637
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Sep 19 14:35:23.126: INFO: Waiting up to 5m0s for pod "client-containers-380960c1-630b-4bbb-9958-f17d8b85c52b" in namespace "containers-4637" to be "success or failure"
Sep 19 14:35:23.131: INFO: Pod "client-containers-380960c1-630b-4bbb-9958-f17d8b85c52b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.505631ms
Sep 19 14:35:25.137: INFO: Pod "client-containers-380960c1-630b-4bbb-9958-f17d8b85c52b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011525296s
Sep 19 14:35:27.145: INFO: Pod "client-containers-380960c1-630b-4bbb-9958-f17d8b85c52b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018952219s
STEP: Saw pod success
Sep 19 14:35:27.145: INFO: Pod "client-containers-380960c1-630b-4bbb-9958-f17d8b85c52b" satisfied condition "success or failure"
Sep 19 14:35:27.150: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod client-containers-380960c1-630b-4bbb-9958-f17d8b85c52b container test-container: <nil>
STEP: delete the pod
Sep 19 14:35:27.184: INFO: Waiting for pod client-containers-380960c1-630b-4bbb-9958-f17d8b85c52b to disappear
Sep 19 14:35:27.189: INFO: Pod client-containers-380960c1-630b-4bbb-9958-f17d8b85c52b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:35:27.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4637" for this suite.
Sep 19 14:35:33.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:35:33.435: INFO: namespace containers-4637 deletion completed in 6.23909171s

• [SLOW TEST:10.498 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:35:33.435: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8238
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:35:38.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8238" for this suite.
Sep 19 14:36:00.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:36:00.895: INFO: namespace replication-controller-8238 deletion completed in 22.2247451s

• [SLOW TEST:27.461 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:36:00.896: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5257
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5257.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5257.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5257.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5257.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5257.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5257.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5257.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5257.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5257.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5257.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5257.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5257.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5257.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 152.26.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.26.152_udp@PTR;check="$$(dig +tcp +noall +answer +search 152.26.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.26.152_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5257.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5257.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5257.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5257.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5257.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5257.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5257.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5257.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5257.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5257.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5257.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5257.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5257.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 152.26.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.26.152_udp@PTR;check="$$(dig +tcp +noall +answer +search 152.26.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.26.152_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 19 14:36:05.263: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5257.svc.cluster.local from pod dns-5257/dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8: the server could not find the requested resource (get pods dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8)
Sep 19 14:36:05.275: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5257.svc.cluster.local from pod dns-5257/dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8: the server could not find the requested resource (get pods dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8)
Sep 19 14:36:05.286: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5257.svc.cluster.local from pod dns-5257/dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8: the server could not find the requested resource (get pods dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8)
Sep 19 14:36:05.836: INFO: Unable to read jessie_udp@dns-test-service.dns-5257.svc.cluster.local from pod dns-5257/dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8: the server could not find the requested resource (get pods dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8)
Sep 19 14:36:05.846: INFO: Unable to read jessie_tcp@dns-test-service.dns-5257.svc.cluster.local from pod dns-5257/dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8: the server could not find the requested resource (get pods dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8)
Sep 19 14:36:05.856: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5257.svc.cluster.local from pod dns-5257/dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8: the server could not find the requested resource (get pods dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8)
Sep 19 14:36:05.871: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5257.svc.cluster.local from pod dns-5257/dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8: the server could not find the requested resource (get pods dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8)
Sep 19 14:36:06.381: INFO: Lookups using dns-5257/dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8 failed for: [wheezy_tcp@dns-test-service.dns-5257.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5257.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5257.svc.cluster.local jessie_udp@dns-test-service.dns-5257.svc.cluster.local jessie_tcp@dns-test-service.dns-5257.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5257.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5257.svc.cluster.local]

Sep 19 14:36:13.066: INFO: DNS probes using dns-5257/dns-test-9590d1f3-14d4-459b-9525-27edb9f355b8 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:36:13.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5257" for this suite.
Sep 19 14:36:19.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:36:19.414: INFO: namespace dns-5257 deletion completed in 6.257172737s

• [SLOW TEST:18.518 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:36:19.415: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1290
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep 19 14:36:29.702: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0919 14:36:29.702591      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:36:29.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1290" for this suite.
Sep 19 14:36:35.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:36:35.959: INFO: namespace gc-1290 deletion completed in 6.24980594s

• [SLOW TEST:16.544 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:36:35.959: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9627
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 19 14:36:36.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9627'
Sep 19 14:36:36.218: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 19 14:36:36.218: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Sep 19 14:36:36.231: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Sep 19 14:36:36.237: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Sep 19 14:36:36.245: INFO: scanned /root for discovery docs: <nil>
Sep 19 14:36:36.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-9627'
Sep 19 14:36:52.069: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 19 14:36:52.070: INFO: stdout: "Created e2e-test-nginx-rc-53d9a12ec10425e43b2fb84e6e8a9a79\nScaling up e2e-test-nginx-rc-53d9a12ec10425e43b2fb84e6e8a9a79 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-53d9a12ec10425e43b2fb84e6e8a9a79 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-53d9a12ec10425e43b2fb84e6e8a9a79 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Sep 19 14:36:52.070: INFO: stdout: "Created e2e-test-nginx-rc-53d9a12ec10425e43b2fb84e6e8a9a79\nScaling up e2e-test-nginx-rc-53d9a12ec10425e43b2fb84e6e8a9a79 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-53d9a12ec10425e43b2fb84e6e8a9a79 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-53d9a12ec10425e43b2fb84e6e8a9a79 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Sep 19 14:36:52.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9627'
Sep 19 14:36:52.151: INFO: stderr: ""
Sep 19 14:36:52.151: INFO: stdout: "e2e-test-nginx-rc-53d9a12ec10425e43b2fb84e6e8a9a79-k4w2k "
Sep 19 14:36:52.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods e2e-test-nginx-rc-53d9a12ec10425e43b2fb84e6e8a9a79-k4w2k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9627'
Sep 19 14:36:52.271: INFO: stderr: ""
Sep 19 14:36:52.271: INFO: stdout: "true"
Sep 19 14:36:52.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 get pods e2e-test-nginx-rc-53d9a12ec10425e43b2fb84e6e8a9a79-k4w2k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9627'
Sep 19 14:36:52.370: INFO: stderr: ""
Sep 19 14:36:52.370: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Sep 19 14:36:52.370: INFO: e2e-test-nginx-rc-53d9a12ec10425e43b2fb84e6e8a9a79-k4w2k is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Sep 19 14:36:52.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 delete rc e2e-test-nginx-rc --namespace=kubectl-9627'
Sep 19 14:36:52.465: INFO: stderr: ""
Sep 19 14:36:52.465: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:36:52.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9627" for this suite.
Sep 19 14:36:58.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:36:58.858: INFO: namespace kubectl-9627 deletion completed in 6.383524857s

• [SLOW TEST:22.900 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:36:58.859: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-7484b252-b16d-4e73-a500-3aa32ad883b1
STEP: Creating secret with name secret-projected-all-test-volume-a637fa49-8dd4-47b0-9629-be14a2d1adbd
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 19 14:36:59.104: INFO: Waiting up to 5m0s for pod "projected-volume-0c844384-78fb-44f1-aa1e-8c76fe98af59" in namespace "projected-3" to be "success or failure"
Sep 19 14:36:59.117: INFO: Pod "projected-volume-0c844384-78fb-44f1-aa1e-8c76fe98af59": Phase="Pending", Reason="", readiness=false. Elapsed: 12.661305ms
Sep 19 14:37:01.126: INFO: Pod "projected-volume-0c844384-78fb-44f1-aa1e-8c76fe98af59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021718156s
Sep 19 14:37:03.132: INFO: Pod "projected-volume-0c844384-78fb-44f1-aa1e-8c76fe98af59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027998298s
STEP: Saw pod success
Sep 19 14:37:03.132: INFO: Pod "projected-volume-0c844384-78fb-44f1-aa1e-8c76fe98af59" satisfied condition "success or failure"
Sep 19 14:37:03.138: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod projected-volume-0c844384-78fb-44f1-aa1e-8c76fe98af59 container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 19 14:37:03.227: INFO: Waiting for pod projected-volume-0c844384-78fb-44f1-aa1e-8c76fe98af59 to disappear
Sep 19 14:37:03.232: INFO: Pod projected-volume-0c844384-78fb-44f1-aa1e-8c76fe98af59 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:37:03.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3" for this suite.
Sep 19 14:37:09.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:37:09.476: INFO: namespace projected-3 deletion completed in 6.237150761s

• [SLOW TEST:10.617 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:37:09.476: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1067
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Sep 19 14:37:09.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 cluster-info'
Sep 19 14:37:09.742: INFO: stderr: ""
Sep 19 14:37:09.742: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:37:09.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1067" for this suite.
Sep 19 14:37:15.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:37:15.987: INFO: namespace kubectl-1067 deletion completed in 6.235098829s

• [SLOW TEST:6.511 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:37:15.989: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4686
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-pmk4
STEP: Creating a pod to test atomic-volume-subpath
Sep 19 14:37:16.183: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pmk4" in namespace "subpath-4686" to be "success or failure"
Sep 19 14:37:16.194: INFO: Pod "pod-subpath-test-projected-pmk4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.171587ms
Sep 19 14:37:18.200: INFO: Pod "pod-subpath-test-projected-pmk4": Phase="Running", Reason="", readiness=true. Elapsed: 2.017477197s
Sep 19 14:37:20.207: INFO: Pod "pod-subpath-test-projected-pmk4": Phase="Running", Reason="", readiness=true. Elapsed: 4.024245105s
Sep 19 14:37:22.213: INFO: Pod "pod-subpath-test-projected-pmk4": Phase="Running", Reason="", readiness=true. Elapsed: 6.03040967s
Sep 19 14:37:24.219: INFO: Pod "pod-subpath-test-projected-pmk4": Phase="Running", Reason="", readiness=true. Elapsed: 8.036272649s
Sep 19 14:37:26.225: INFO: Pod "pod-subpath-test-projected-pmk4": Phase="Running", Reason="", readiness=true. Elapsed: 10.042461329s
Sep 19 14:37:28.232: INFO: Pod "pod-subpath-test-projected-pmk4": Phase="Running", Reason="", readiness=true. Elapsed: 12.048933602s
Sep 19 14:37:30.238: INFO: Pod "pod-subpath-test-projected-pmk4": Phase="Running", Reason="", readiness=true. Elapsed: 14.054906074s
Sep 19 14:37:32.244: INFO: Pod "pod-subpath-test-projected-pmk4": Phase="Running", Reason="", readiness=true. Elapsed: 16.06104052s
Sep 19 14:37:34.250: INFO: Pod "pod-subpath-test-projected-pmk4": Phase="Running", Reason="", readiness=true. Elapsed: 18.066909413s
Sep 19 14:37:36.261: INFO: Pod "pod-subpath-test-projected-pmk4": Phase="Running", Reason="", readiness=true. Elapsed: 20.077585056s
Sep 19 14:37:38.272: INFO: Pod "pod-subpath-test-projected-pmk4": Phase="Running", Reason="", readiness=true. Elapsed: 22.088610448s
Sep 19 14:37:40.281: INFO: Pod "pod-subpath-test-projected-pmk4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.098021169s
STEP: Saw pod success
Sep 19 14:37:40.281: INFO: Pod "pod-subpath-test-projected-pmk4" satisfied condition "success or failure"
Sep 19 14:37:40.286: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod pod-subpath-test-projected-pmk4 container test-container-subpath-projected-pmk4: <nil>
STEP: delete the pod
Sep 19 14:37:40.360: INFO: Waiting for pod pod-subpath-test-projected-pmk4 to disappear
Sep 19 14:37:40.371: INFO: Pod pod-subpath-test-projected-pmk4 no longer exists
STEP: Deleting pod pod-subpath-test-projected-pmk4
Sep 19 14:37:40.371: INFO: Deleting pod "pod-subpath-test-projected-pmk4" in namespace "subpath-4686"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:37:40.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4686" for this suite.
Sep 19 14:37:46.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:37:46.652: INFO: namespace subpath-4686 deletion completed in 6.267225316s

• [SLOW TEST:30.664 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:37:46.654: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6885
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-183f8336-706a-480c-93ad-bc5d2e3fe69b
STEP: Creating a pod to test consume configMaps
Sep 19 14:37:46.870: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9798fb26-c710-46ff-a808-22f90d78306f" in namespace "projected-6885" to be "success or failure"
Sep 19 14:37:46.878: INFO: Pod "pod-projected-configmaps-9798fb26-c710-46ff-a808-22f90d78306f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.504544ms
Sep 19 14:37:48.883: INFO: Pod "pod-projected-configmaps-9798fb26-c710-46ff-a808-22f90d78306f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012549751s
Sep 19 14:37:50.889: INFO: Pod "pod-projected-configmaps-9798fb26-c710-46ff-a808-22f90d78306f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018996125s
STEP: Saw pod success
Sep 19 14:37:50.889: INFO: Pod "pod-projected-configmaps-9798fb26-c710-46ff-a808-22f90d78306f" satisfied condition "success or failure"
Sep 19 14:37:50.894: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod pod-projected-configmaps-9798fb26-c710-46ff-a808-22f90d78306f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 19 14:37:50.932: INFO: Waiting for pod pod-projected-configmaps-9798fb26-c710-46ff-a808-22f90d78306f to disappear
Sep 19 14:37:50.936: INFO: Pod pod-projected-configmaps-9798fb26-c710-46ff-a808-22f90d78306f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:37:50.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6885" for this suite.
Sep 19 14:37:56.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:37:57.179: INFO: namespace projected-6885 deletion completed in 6.236034122s

• [SLOW TEST:10.525 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:37:57.179: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2832
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Sep 19 14:37:57.349: INFO: Waiting up to 5m0s for pod "var-expansion-e2d6544d-0515-43ef-b122-bdaf8e2e5de2" in namespace "var-expansion-2832" to be "success or failure"
Sep 19 14:37:57.357: INFO: Pod "var-expansion-e2d6544d-0515-43ef-b122-bdaf8e2e5de2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.864029ms
Sep 19 14:37:59.362: INFO: Pod "var-expansion-e2d6544d-0515-43ef-b122-bdaf8e2e5de2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013561341s
Sep 19 14:38:01.372: INFO: Pod "var-expansion-e2d6544d-0515-43ef-b122-bdaf8e2e5de2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023074032s
STEP: Saw pod success
Sep 19 14:38:01.372: INFO: Pod "var-expansion-e2d6544d-0515-43ef-b122-bdaf8e2e5de2" satisfied condition "success or failure"
Sep 19 14:38:01.378: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod var-expansion-e2d6544d-0515-43ef-b122-bdaf8e2e5de2 container dapi-container: <nil>
STEP: delete the pod
Sep 19 14:38:01.427: INFO: Waiting for pod var-expansion-e2d6544d-0515-43ef-b122-bdaf8e2e5de2 to disappear
Sep 19 14:38:01.433: INFO: Pod var-expansion-e2d6544d-0515-43ef-b122-bdaf8e2e5de2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:38:01.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2832" for this suite.
Sep 19 14:38:07.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:38:07.682: INFO: namespace var-expansion-2832 deletion completed in 6.242646527s

• [SLOW TEST:10.503 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:38:07.682: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9580
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-54c6c49e-101b-4617-b7c2-76700f136c34
STEP: Creating a pod to test consume configMaps
Sep 19 14:38:07.922: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f28d524d-5fcc-420d-8b58-a6ebbb3bdd7d" in namespace "projected-9580" to be "success or failure"
Sep 19 14:38:07.929: INFO: Pod "pod-projected-configmaps-f28d524d-5fcc-420d-8b58-a6ebbb3bdd7d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.737944ms
Sep 19 14:38:09.935: INFO: Pod "pod-projected-configmaps-f28d524d-5fcc-420d-8b58-a6ebbb3bdd7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013003154s
Sep 19 14:38:11.942: INFO: Pod "pod-projected-configmaps-f28d524d-5fcc-420d-8b58-a6ebbb3bdd7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019875881s
STEP: Saw pod success
Sep 19 14:38:11.942: INFO: Pod "pod-projected-configmaps-f28d524d-5fcc-420d-8b58-a6ebbb3bdd7d" satisfied condition "success or failure"
Sep 19 14:38:11.949: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod pod-projected-configmaps-f28d524d-5fcc-420d-8b58-a6ebbb3bdd7d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 19 14:38:11.992: INFO: Waiting for pod pod-projected-configmaps-f28d524d-5fcc-420d-8b58-a6ebbb3bdd7d to disappear
Sep 19 14:38:11.997: INFO: Pod pod-projected-configmaps-f28d524d-5fcc-420d-8b58-a6ebbb3bdd7d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:38:11.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9580" for this suite.
Sep 19 14:38:18.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:38:18.242: INFO: namespace projected-9580 deletion completed in 6.237085735s

• [SLOW TEST:10.560 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:38:18.243: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 19 14:38:18.478: INFO: Number of nodes with available pods: 0
Sep 19 14:38:18.478: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:38:19.492: INFO: Number of nodes with available pods: 0
Sep 19 14:38:19.492: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:38:20.492: INFO: Number of nodes with available pods: 0
Sep 19 14:38:20.492: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:38:21.491: INFO: Number of nodes with available pods: 4
Sep 19 14:38:21.491: INFO: Node ip-172-31-42-91.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:38:22.492: INFO: Number of nodes with available pods: 5
Sep 19 14:38:22.492: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 19 14:38:22.530: INFO: Number of nodes with available pods: 4
Sep 19 14:38:22.530: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:38:23.543: INFO: Number of nodes with available pods: 4
Sep 19 14:38:23.543: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:38:24.542: INFO: Number of nodes with available pods: 4
Sep 19 14:38:24.542: INFO: Node ip-172-31-33-83.eu-central-1.compute.internal is running more than one daemon pod
Sep 19 14:38:25.551: INFO: Number of nodes with available pods: 5
Sep 19 14:38:25.551: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6736, will wait for the garbage collector to delete the pods
Sep 19 14:38:25.631: INFO: Deleting DaemonSet.extensions daemon-set took: 12.516973ms
Sep 19 14:38:26.033: INFO: Terminating DaemonSet.extensions daemon-set pods took: 401.450746ms
Sep 19 14:38:39.378: INFO: Number of nodes with available pods: 0
Sep 19 14:38:39.378: INFO: Number of running nodes: 0, number of available pods: 0
Sep 19 14:38:39.396: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6736/daemonsets","resourceVersion":"43155"},"items":null}

Sep 19 14:38:39.410: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6736/pods","resourceVersion":"43155"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:38:39.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6736" for this suite.
Sep 19 14:38:45.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:38:45.807: INFO: namespace daemonsets-6736 deletion completed in 6.327507398s

• [SLOW TEST:27.563 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:38:45.808: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Sep 19 14:38:45.982: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 19 14:38:45.996: INFO: Waiting for terminating namespaces to be deleted...
Sep 19 14:38:46.013: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-33-83.eu-central-1.compute.internal before test
Sep 19 14:38:46.035: INFO: kubernetes-dashboard-584d5ffc75-jvnx5 from kube-system started at 2019-09-19 12:50:46 +0000 UTC (1 container statuses recorded)
Sep 19 14:38:46.035: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep 19 14:38:46.035: INFO: kube-proxy-wcj22 from kube-system started at 2019-09-19 12:49:55 +0000 UTC (1 container statuses recorded)
Sep 19 14:38:46.035: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 19 14:38:46.035: INFO: canal-4wkpv from kube-system started at 2019-09-19 12:49:55 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.035: INFO: 	Container calico-node ready: true, restart count 0
Sep 19 14:38:46.035: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 19 14:38:46.035: INFO: node-exporter-7txrv from kube-system started at 2019-09-19 12:49:55 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.035: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 19 14:38:46.035: INFO: 	Container node-exporter ready: true, restart count 0
Sep 19 14:38:46.035: INFO: node-local-dns-twjtr from kube-system started at 2019-09-19 12:50:15 +0000 UTC (1 container statuses recorded)
Sep 19 14:38:46.035: INFO: 	Container node-cache ready: true, restart count 0
Sep 19 14:38:46.035: INFO: sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-zgrfd from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.035: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 19 14:38:46.035: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 19 14:38:46.035: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-36-147.eu-central-1.compute.internal before test
Sep 19 14:38:46.135: INFO: node-exporter-hnxl5 from kube-system started at 2019-09-19 12:50:25 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.135: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 19 14:38:46.135: INFO: 	Container node-exporter ready: true, restart count 0
Sep 19 14:38:46.135: INFO: canal-txnzq from kube-system started at 2019-09-19 12:50:25 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.135: INFO: 	Container calico-node ready: true, restart count 0
Sep 19 14:38:46.135: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 19 14:38:46.135: INFO: kube-proxy-hk5tn from kube-system started at 2019-09-19 12:50:25 +0000 UTC (1 container statuses recorded)
Sep 19 14:38:46.135: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 19 14:38:46.135: INFO: node-local-dns-zdxc4 from kube-system started at 2019-09-19 12:50:45 +0000 UTC (1 container statuses recorded)
Sep 19 14:38:46.135: INFO: 	Container node-cache ready: true, restart count 0
Sep 19 14:38:46.135: INFO: coredns-9b6865ff9-mhjqc from kube-system started at 2019-09-19 12:50:46 +0000 UTC (1 container statuses recorded)
Sep 19 14:38:46.135: INFO: 	Container coredns ready: true, restart count 0
Sep 19 14:38:46.135: INFO: sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-ccgl7 from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.135: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 19 14:38:46.135: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 19 14:38:46.135: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-42-91.eu-central-1.compute.internal before test
Sep 19 14:38:46.318: INFO: node-exporter-5vls6 from kube-system started at 2019-09-19 12:50:04 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.318: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 19 14:38:46.318: INFO: 	Container node-exporter ready: true, restart count 0
Sep 19 14:38:46.318: INFO: canal-dnm9w from kube-system started at 2019-09-19 12:50:04 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.318: INFO: 	Container calico-node ready: true, restart count 0
Sep 19 14:38:46.318: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 19 14:38:46.318: INFO: kube-proxy-h4md4 from kube-system started at 2019-09-19 12:50:04 +0000 UTC (1 container statuses recorded)
Sep 19 14:38:46.318: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 19 14:38:46.318: INFO: node-local-dns-7rcvh from kube-system started at 2019-09-19 12:50:14 +0000 UTC (1 container statuses recorded)
Sep 19 14:38:46.318: INFO: 	Container node-cache ready: true, restart count 0
Sep 19 14:38:46.318: INFO: sonobuoy-e2e-job-57f4e2d668874fc2 from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.318: INFO: 	Container e2e ready: true, restart count 0
Sep 19 14:38:46.318: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 19 14:38:46.318: INFO: sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-q2sj9 from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.318: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 19 14:38:46.318: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 19 14:38:46.318: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-44-209.eu-central-1.compute.internal before test
Sep 19 14:38:46.400: INFO: node-local-dns-rd9dn from kube-system started at 2019-09-19 12:50:28 +0000 UTC (1 container statuses recorded)
Sep 19 14:38:46.400: INFO: 	Container node-cache ready: true, restart count 0
Sep 19 14:38:46.400: INFO: kube-proxy-29rs8 from kube-system started at 2019-09-19 12:50:08 +0000 UTC (1 container statuses recorded)
Sep 19 14:38:46.400: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 19 14:38:46.400: INFO: canal-rg8t2 from kube-system started at 2019-09-19 12:50:08 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.400: INFO: 	Container calico-node ready: true, restart count 0
Sep 19 14:38:46.400: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 19 14:38:46.400: INFO: node-exporter-6lvwr from kube-system started at 2019-09-19 12:50:08 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.400: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 19 14:38:46.400: INFO: 	Container node-exporter ready: true, restart count 0
Sep 19 14:38:46.400: INFO: sonobuoy from sonobuoy started at 2019-09-19 13:04:24 +0000 UTC (1 container statuses recorded)
Sep 19 14:38:46.400: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 19 14:38:46.400: INFO: sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-7bv2k from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.400: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 19 14:38:46.400: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 19 14:38:46.400: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-46-204.eu-central-1.compute.internal before test
Sep 19 14:38:46.498: INFO: node-exporter-tsf9h from kube-system started at 2019-09-19 12:50:22 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.498: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 19 14:38:46.498: INFO: 	Container node-exporter ready: true, restart count 0
Sep 19 14:38:46.498: INFO: kube-proxy-p4xds from kube-system started at 2019-09-19 12:50:22 +0000 UTC (1 container statuses recorded)
Sep 19 14:38:46.498: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 19 14:38:46.498: INFO: node-local-dns-t4859 from kube-system started at 2019-09-19 12:50:42 +0000 UTC (1 container statuses recorded)
Sep 19 14:38:46.498: INFO: 	Container node-cache ready: true, restart count 0
Sep 19 14:38:46.498: INFO: openvpn-client-5fbd4fdb44-g28ss from kube-system started at 2019-09-19 12:50:46 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.498: INFO: 	Container dnat-controller ready: true, restart count 0
Sep 19 14:38:46.498: INFO: 	Container openvpn-client ready: true, restart count 0
Sep 19 14:38:46.498: INFO: coredns-9b6865ff9-mks62 from kube-system started at 2019-09-19 12:53:47 +0000 UTC (1 container statuses recorded)
Sep 19 14:38:46.498: INFO: 	Container coredns ready: true, restart count 0
Sep 19 14:38:46.498: INFO: sonobuoy-systemd-logs-daemon-set-57d691c3e3714447-k8z66 from sonobuoy started at 2019-09-19 13:04:29 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.498: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 19 14:38:46.498: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 19 14:38:46.498: INFO: canal-9jtwv from kube-system started at 2019-09-19 12:50:22 +0000 UTC (2 container statuses recorded)
Sep 19 14:38:46.498: INFO: 	Container calico-node ready: true, restart count 0
Sep 19 14:38:46.498: INFO: 	Container kube-flannel ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e1603bee-2e07-417e-909e-267f4ac15e81 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-e1603bee-2e07-417e-909e-267f4ac15e81 off the node ip-172-31-44-209.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e1603bee-2e07-417e-909e-267f4ac15e81
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:38:52.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1154" for this suite.
Sep 19 14:39:06.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:39:06.898: INFO: namespace sched-pred-1154 deletion completed in 14.230754846s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:21.089 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:39:06.899: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7996
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep 19 14:39:07.142: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3e15460-97ca-4b4c-a6dd-6fdd74dbd95e" in namespace "projected-7996" to be "success or failure"
Sep 19 14:39:07.149: INFO: Pod "downwardapi-volume-b3e15460-97ca-4b4c-a6dd-6fdd74dbd95e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.989313ms
Sep 19 14:39:09.164: INFO: Pod "downwardapi-volume-b3e15460-97ca-4b4c-a6dd-6fdd74dbd95e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022172841s
Sep 19 14:39:11.169: INFO: Pod "downwardapi-volume-b3e15460-97ca-4b4c-a6dd-6fdd74dbd95e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027549708s
STEP: Saw pod success
Sep 19 14:39:11.169: INFO: Pod "downwardapi-volume-b3e15460-97ca-4b4c-a6dd-6fdd74dbd95e" satisfied condition "success or failure"
Sep 19 14:39:11.175: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod downwardapi-volume-b3e15460-97ca-4b4c-a6dd-6fdd74dbd95e container client-container: <nil>
STEP: delete the pod
Sep 19 14:39:11.206: INFO: Waiting for pod downwardapi-volume-b3e15460-97ca-4b4c-a6dd-6fdd74dbd95e to disappear
Sep 19 14:39:11.211: INFO: Pod downwardapi-volume-b3e15460-97ca-4b4c-a6dd-6fdd74dbd95e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:39:11.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7996" for this suite.
Sep 19 14:39:17.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:39:17.450: INFO: namespace projected-7996 deletion completed in 6.228942823s

• [SLOW TEST:10.551 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:39:17.450: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2372
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-39acde54-7bb4-445c-b7e6-0a9b3c01ae8d
STEP: Creating secret with name s-test-opt-upd-75a9ecc4-d12d-4018-a47e-34d45f191bc0
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-39acde54-7bb4-445c-b7e6-0a9b3c01ae8d
STEP: Updating secret s-test-opt-upd-75a9ecc4-d12d-4018-a47e-34d45f191bc0
STEP: Creating secret with name s-test-opt-create-a3d09be3-4bc6-4187-be1c-90e6ea5b578d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:40:33.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2372" for this suite.
Sep 19 14:40:55.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:40:55.408: INFO: namespace secrets-2372 deletion completed in 22.271361384s

• [SLOW TEST:97.958 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:40:55.408: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2543
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 19 14:40:55.590: INFO: Waiting up to 5m0s for pod "pod-4f817dfd-8651-45dc-a47b-6e469a87b1dc" in namespace "emptydir-2543" to be "success or failure"
Sep 19 14:40:55.597: INFO: Pod "pod-4f817dfd-8651-45dc-a47b-6e469a87b1dc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.996999ms
Sep 19 14:40:57.608: INFO: Pod "pod-4f817dfd-8651-45dc-a47b-6e469a87b1dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018532122s
Sep 19 14:40:59.622: INFO: Pod "pod-4f817dfd-8651-45dc-a47b-6e469a87b1dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032678575s
STEP: Saw pod success
Sep 19 14:40:59.623: INFO: Pod "pod-4f817dfd-8651-45dc-a47b-6e469a87b1dc" satisfied condition "success or failure"
Sep 19 14:40:59.628: INFO: Trying to get logs from node ip-172-31-33-83.eu-central-1.compute.internal pod pod-4f817dfd-8651-45dc-a47b-6e469a87b1dc container test-container: <nil>
STEP: delete the pod
Sep 19 14:40:59.661: INFO: Waiting for pod pod-4f817dfd-8651-45dc-a47b-6e469a87b1dc to disappear
Sep 19 14:40:59.666: INFO: Pod pod-4f817dfd-8651-45dc-a47b-6e469a87b1dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:40:59.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2543" for this suite.
Sep 19 14:41:05.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:41:05.916: INFO: namespace emptydir-2543 deletion completed in 6.244630595s

• [SLOW TEST:10.508 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:41:05.918: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-513
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-513/secret-test-92233a95-85ae-4431-8603-c18308ddb634
STEP: Creating a pod to test consume secrets
Sep 19 14:41:06.108: INFO: Waiting up to 5m0s for pod "pod-configmaps-3f207437-0729-4264-86a2-80faf20fd7ef" in namespace "secrets-513" to be "success or failure"
Sep 19 14:41:06.115: INFO: Pod "pod-configmaps-3f207437-0729-4264-86a2-80faf20fd7ef": Phase="Pending", Reason="", readiness=false. Elapsed: 7.760408ms
Sep 19 14:41:08.122: INFO: Pod "pod-configmaps-3f207437-0729-4264-86a2-80faf20fd7ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014575458s
Sep 19 14:41:10.129: INFO: Pod "pod-configmaps-3f207437-0729-4264-86a2-80faf20fd7ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021365512s
STEP: Saw pod success
Sep 19 14:41:10.129: INFO: Pod "pod-configmaps-3f207437-0729-4264-86a2-80faf20fd7ef" satisfied condition "success or failure"
Sep 19 14:41:10.134: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod pod-configmaps-3f207437-0729-4264-86a2-80faf20fd7ef container env-test: <nil>
STEP: delete the pod
Sep 19 14:41:10.175: INFO: Waiting for pod pod-configmaps-3f207437-0729-4264-86a2-80faf20fd7ef to disappear
Sep 19 14:41:10.181: INFO: Pod pod-configmaps-3f207437-0729-4264-86a2-80faf20fd7ef no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:41:10.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-513" for this suite.
Sep 19 14:41:16.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:41:16.438: INFO: namespace secrets-513 deletion completed in 6.249848416s

• [SLOW TEST:10.519 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:41:16.438: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4582
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-dda474d9-dcf3-4ccb-a54a-5ceccaf860e6
STEP: Creating a pod to test consume configMaps
Sep 19 14:41:16.627: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dd62e322-caee-4fa2-940e-119001f9567b" in namespace "projected-4582" to be "success or failure"
Sep 19 14:41:16.635: INFO: Pod "pod-projected-configmaps-dd62e322-caee-4fa2-940e-119001f9567b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.785864ms
Sep 19 14:41:18.641: INFO: Pod "pod-projected-configmaps-dd62e322-caee-4fa2-940e-119001f9567b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013786239s
Sep 19 14:41:20.647: INFO: Pod "pod-projected-configmaps-dd62e322-caee-4fa2-940e-119001f9567b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019642295s
STEP: Saw pod success
Sep 19 14:41:20.647: INFO: Pod "pod-projected-configmaps-dd62e322-caee-4fa2-940e-119001f9567b" satisfied condition "success or failure"
Sep 19 14:41:20.653: INFO: Trying to get logs from node ip-172-31-46-204.eu-central-1.compute.internal pod pod-projected-configmaps-dd62e322-caee-4fa2-940e-119001f9567b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 19 14:41:20.689: INFO: Waiting for pod pod-projected-configmaps-dd62e322-caee-4fa2-940e-119001f9567b to disappear
Sep 19 14:41:20.694: INFO: Pod pod-projected-configmaps-dd62e322-caee-4fa2-940e-119001f9567b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:41:20.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4582" for this suite.
Sep 19 14:41:26.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:41:26.924: INFO: namespace projected-4582 deletion completed in 6.223585495s

• [SLOW TEST:10.487 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:41:26.925: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8629
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 19 14:41:27.093: INFO: Waiting up to 5m0s for pod "pod-cbb8205f-9632-4edb-a11a-a5b18da31dbc" in namespace "emptydir-8629" to be "success or failure"
Sep 19 14:41:27.100: INFO: Pod "pod-cbb8205f-9632-4edb-a11a-a5b18da31dbc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.118794ms
Sep 19 14:41:29.107: INFO: Pod "pod-cbb8205f-9632-4edb-a11a-a5b18da31dbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013340631s
Sep 19 14:41:31.113: INFO: Pod "pod-cbb8205f-9632-4edb-a11a-a5b18da31dbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019563001s
STEP: Saw pod success
Sep 19 14:41:31.113: INFO: Pod "pod-cbb8205f-9632-4edb-a11a-a5b18da31dbc" satisfied condition "success or failure"
Sep 19 14:41:31.118: INFO: Trying to get logs from node ip-172-31-36-147.eu-central-1.compute.internal pod pod-cbb8205f-9632-4edb-a11a-a5b18da31dbc container test-container: <nil>
STEP: delete the pod
Sep 19 14:41:31.161: INFO: Waiting for pod pod-cbb8205f-9632-4edb-a11a-a5b18da31dbc to disappear
Sep 19 14:41:31.165: INFO: Pod pod-cbb8205f-9632-4edb-a11a-a5b18da31dbc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:41:31.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8629" for this suite.
Sep 19 14:41:37.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:41:37.430: INFO: namespace emptydir-8629 deletion completed in 6.259319923s

• [SLOW TEST:10.506 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:41:37.431: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4708
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Sep 19 14:41:37.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 create -f - --namespace=kubectl-4708'
Sep 19 14:41:38.695: INFO: stderr: ""
Sep 19 14:41:38.695: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 19 14:41:39.702: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:41:39.702: INFO: Found 0 / 1
Sep 19 14:41:40.702: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:41:40.703: INFO: Found 0 / 1
Sep 19 14:41:41.702: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:41:41.702: INFO: Found 1 / 1
Sep 19 14:41:41.702: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 19 14:41:41.710: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:41:41.710: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 19 14:41:41.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-889697622 patch pod redis-master-pzfjs --namespace=kubectl-4708 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 19 14:41:41.797: INFO: stderr: ""
Sep 19 14:41:41.798: INFO: stdout: "pod/redis-master-pzfjs patched\n"
STEP: checking annotations
Sep 19 14:41:41.804: INFO: Selector matched 1 pods for map[app:redis]
Sep 19 14:41:41.804: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:41:41.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4708" for this suite.
Sep 19 14:42:03.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:42:04.042: INFO: namespace kubectl-4708 deletion completed in 22.231091393s

• [SLOW TEST:26.611 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep 19 14:42:04.042: INFO: >>> kubeConfig: /tmp/kubeconfig-889697622
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-6lg7
STEP: Creating a pod to test atomic-volume-subpath
Sep 19 14:42:04.230: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6lg7" in namespace "subpath-2356" to be "success or failure"
Sep 19 14:42:04.235: INFO: Pod "pod-subpath-test-configmap-6lg7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.434825ms
Sep 19 14:42:06.241: INFO: Pod "pod-subpath-test-configmap-6lg7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010744793s
Sep 19 14:42:08.247: INFO: Pod "pod-subpath-test-configmap-6lg7": Phase="Running", Reason="", readiness=true. Elapsed: 4.016619644s
Sep 19 14:42:10.254: INFO: Pod "pod-subpath-test-configmap-6lg7": Phase="Running", Reason="", readiness=true. Elapsed: 6.023775055s
Sep 19 14:42:12.262: INFO: Pod "pod-subpath-test-configmap-6lg7": Phase="Running", Reason="", readiness=true. Elapsed: 8.031465335s
Sep 19 14:42:14.269: INFO: Pod "pod-subpath-test-configmap-6lg7": Phase="Running", Reason="", readiness=true. Elapsed: 10.038338375s
Sep 19 14:42:16.274: INFO: Pod "pod-subpath-test-configmap-6lg7": Phase="Running", Reason="", readiness=true. Elapsed: 12.044134385s
Sep 19 14:42:18.281: INFO: Pod "pod-subpath-test-configmap-6lg7": Phase="Running", Reason="", readiness=true. Elapsed: 14.050239457s
Sep 19 14:42:20.287: INFO: Pod "pod-subpath-test-configmap-6lg7": Phase="Running", Reason="", readiness=true. Elapsed: 16.056375334s
Sep 19 14:42:22.296: INFO: Pod "pod-subpath-test-configmap-6lg7": Phase="Running", Reason="", readiness=true. Elapsed: 18.065234335s
Sep 19 14:42:24.303: INFO: Pod "pod-subpath-test-configmap-6lg7": Phase="Running", Reason="", readiness=true. Elapsed: 20.072331961s
Sep 19 14:42:26.309: INFO: Pod "pod-subpath-test-configmap-6lg7": Phase="Running", Reason="", readiness=true. Elapsed: 22.078276748s
Sep 19 14:42:28.315: INFO: Pod "pod-subpath-test-configmap-6lg7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.084240029s
STEP: Saw pod success
Sep 19 14:42:28.315: INFO: Pod "pod-subpath-test-configmap-6lg7" satisfied condition "success or failure"
Sep 19 14:42:28.320: INFO: Trying to get logs from node ip-172-31-44-209.eu-central-1.compute.internal pod pod-subpath-test-configmap-6lg7 container test-container-subpath-configmap-6lg7: <nil>
STEP: delete the pod
Sep 19 14:42:28.370: INFO: Waiting for pod pod-subpath-test-configmap-6lg7 to disappear
Sep 19 14:42:28.375: INFO: Pod pod-subpath-test-configmap-6lg7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6lg7
Sep 19 14:42:28.375: INFO: Deleting pod "pod-subpath-test-configmap-6lg7" in namespace "subpath-2356"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep 19 14:42:28.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2356" for this suite.
Sep 19 14:42:34.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 19 14:42:34.623: INFO: namespace subpath-2356 deletion completed in 6.235123634s

• [SLOW TEST:30.581 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSep 19 14:42:34.623: INFO: Running AfterSuite actions on all nodes
Sep 19 14:42:34.623: INFO: Running AfterSuite actions on node 1
Sep 19 14:42:34.623: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 5870.457 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h37m52.569853389s
Test Suite Passed
